{"pr_number": 894, "pr_title": "Spark: Implement an action to remove orphan files", "pr_createdAt": "2020-04-06T15:20:00Z", "pr_url": "https://github.com/apache/iceberg/pull/894", "timeline": [{"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "url": "https://github.com/apache/iceberg/commit/32bd1bd72a9586f559b0e30b37975bac64e688e5", "message": "Spark: Implement an action to remove orphan files", "committedDate": "2020-04-06T15:18:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r404419552", "bodyText": "Can you also add a test for Write-Audit-Publish (WAP) workflow case where a snapshot can be staged (using the cherrypicking operation), where it's not part of the list of active snapshots. So expected behavior should be that this action should not delete those staged files as orphan files.\nThere are tests in TestWapWorkflow that illustrate this case.", "author": "prodeezy", "createdAt": "2020-04-06T22:11:44Z", "path": "spark/src/test/java/org/apache/iceberg/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRemoveOrphanFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+      .truncate(\"c2\", 2)\n+      .identity(\"c3\")\n+      .build();\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRemoveOrphanFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRemoveOrphanFilesAction.spark;\n+    TestRemoveOrphanFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  private String tableLocation = null;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    File tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+  }\n+\n+  @Test\n+  public void testDryRun() throws IOException, InterruptedException {\n+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    List<String> validFiles = spark.read().format(\"iceberg\")\n+        .load(tableLocation + \"#files\")\n+        .select(\"file_path\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+    Assert.assertEquals(\"Should be 2 valid files\", 2, validFiles.size());\n+\n+    df.write().mode(\"append\").parquet(tableLocation + \"/data\");\n+\n+    Path dataPath = new Path(tableLocation + \"/data\");\n+    FileSystem fs = dataPath.getFileSystem(spark.sessionState().newHadoopConf());\n+    List<String> allFiles = Arrays.stream(fs.listStatus(dataPath, HiddenPathFilter.get()))\n+        .filter(FileStatus::isFile)\n+        .map(file -> file.getPath().toString())\n+        .collect(Collectors.toList());\n+    Assert.assertEquals(\"Should be 3 files\", 3, allFiles.size());\n+\n+    List<String> invalidFiles = Lists.newArrayList(allFiles);\n+    invalidFiles.removeAll(validFiles);\n+    Assert.assertEquals(\"Should be 1 invalid file\", 1, invalidFiles.size());\n+\n+    // sleep for 1 second to unsure files will be old enough\n+    Thread.sleep(1000);\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RemoveOrphanFilesActionResult result1 = actions.removeOrphanFiles()\n+        .allDataFilesTable(tableLocation + \"#all_data_files\")\n+        .olderThan(System.currentTimeMillis())\n+        .dryRun(true)\n+        .execute();\n+    Assert.assertEquals(\"Action should find 1 data file\", invalidFiles, result1.dataFiles());\n+    Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n+\n+    RemoveOrphanFilesActionResult result2 = actions.removeOrphanFiles()\n+        .allDataFilesTable(tableLocation + \"#all_data_files\")\n+        .olderThan(System.currentTimeMillis())\n+        .execute();\n+    Assert.assertEquals(\"Action should delete 1 data file\", invalidFiles, result2.dataFiles());\n+    Assert.assertFalse(\"Invalid file should not be present\", fs.exists(new Path(invalidFiles.get(0))));\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records);\n+    expectedRecords.addAll(records);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testAllValidFilesAreKept() throws IOException, InterruptedException {", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQyNDk0MA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r404424940", "bodyText": "Good idea, will add a case for that.", "author": "aokolnychyi", "createdAt": "2020-04-06T22:24:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4NjA2MQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405186061", "bodyText": "I agree it would be nice to have a test for it. This should work because the metadata tables used return all files reachable by metadata, not just the ones in a single snapshot. We use the same table for a similar check in our environment.", "author": "rdblue", "createdAt": "2020-04-08T00:13:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg3NzcxNA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405877714", "bodyText": "There is one case when all_data_files won't report WAP files: when there is no snapshot.\n  @Override\n  public CloseableIterable<FileScanTask> planFiles() {\n    Snapshot snapshot = snapshot();\n    if (snapshot != null) {\n      LOG.info(\"Scanning table {} snapshot {} created at {} with filter {}\", table,\n          snapshot.snapshotId(), formatTimestampMillis(snapshot.timestampMillis()),\n          rowFilter);\n\n      Listeners.notifyAll(\n          new ScanEvent(table.toString(), snapshot.snapshotId(), rowFilter, schema()));\n\n      return planFiles(ops, snapshot, rowFilter, caseSensitive, colStats);\n\n    } else {\n      LOG.info(\"Scanning empty table {}\", table);\n      return CloseableIterable.empty();\n    }\n  }", "author": "aokolnychyi", "createdAt": "2020-04-08T23:47:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg3ODk2OA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405878968", "bodyText": "Also, table.currentSnapshot().manifestListLocation() in location()can lead to a NPE.", "author": "aokolnychyi", "createdAt": "2020-04-08T23:51:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4MzY5NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405883694", "bodyText": "I've created #904 and #905 to address later.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:05:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4ODE3Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405888172", "bodyText": "As long as we have a current snapshot, it does work correctly. I've added a test.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:21:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMyOTkxNw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406329917", "bodyText": "I thought we addressed the metadata table problem when there is no current snapshot in #801. I'll check to see why that doesn't work. My initial guess is that PR refers to static tables and this is a parallel table.", "author": "rdblue", "createdAt": "2020-04-09T16:33:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzMTE0NQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406331145", "bodyText": "It's just individual tables that were fixed. The solution is to override planFiles:\n    @Override\n    public CloseableIterable<FileScanTask> planFiles() {\n      // override planFiles to avoid the check for a current snapshot because this metadata table is for all snapshots\n      return CloseableIterable.withNoopClose(HistoryTable.this.task(this));\n    }", "author": "rdblue", "createdAt": "2020-04-09T16:35:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzgxNQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405147815", "bodyText": "What about providers that don't have a reliable location? Do they return null?", "author": "rdblue", "createdAt": "2020-04-07T22:21:54Z", "path": "api/src/main/java/org/apache/iceberg/io/LocationProvider.java", "diffHunk": "@@ -29,6 +29,12 @@\n  * Implementations must be {@link Serializable} because instances will be serialized to tasks.\n  */\n public interface LocationProvider extends Serializable {\n+\n+  /**\n+   * Return a fully-qualified data location.\n+   */\n+  String dataLocation();", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1Njc3NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405156774", "bodyText": "Instead of requiring a data location, why not use table.location()? We could filter out metadata paths if configured to do it.", "author": "rdblue", "createdAt": "2020-04-07T22:44:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NDY4MA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405174680", "bodyText": "My assumption was that there would always be at least some base reliable data location. Is that assumption false?\nTechnically, data location can be arbitrary today and might be outside table.location(). That would be odd but possible.", "author": "aokolnychyi", "createdAt": "2020-04-07T23:36:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3OTc3Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405179772", "bodyText": "I think we should clean table.location() by default and allow the user to configure a different path. That covers all reasonable cases.\nIt should be safe to clean table.location() because we need to protect all metadata file paths individually anyway, instead of using a prefix. So we may as well clean the whole table location.", "author": "rdblue", "createdAt": "2020-04-07T23:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MDg3NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405180874", "bodyText": "Yeah, that's what I was thinking as well. If we spend a bit more effort to also collect all metadata files, we can clean the table location completely.", "author": "aokolnychyi", "createdAt": "2020-04-07T23:55:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4ODI0Mw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405888243", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzgxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NDkxOQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405154919", "bodyText": "You probably don't need distinct if you're using this in a left anti-join. That just introduces an additional shuffle.", "author": "rdblue", "createdAt": "2020-04-07T22:39:57Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {\n+    this.dryRun = newDryRun;\n+    return this;\n+  }\n+\n+  @Override\n+  public RemoveOrphanFilesActionResult execute() {\n+    Preconditions.checkArgument(allDataFilesTable != null, \"allDataFilesTable must be set\");\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp should be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> actualDataFileDF = buildActualDataFileDF();\n+\n+    Column joinCond = validDataFileDF.col(\"file_path\").equalTo(actualDataFileDF.col(\"file_path\"));\n+    List<String> orphanDataFiles = actualDataFileDF.join(validDataFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    if (!dryRun) {\n+      Tasks.foreach(orphanDataFiles)\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.warn(\"Failed to delete data file: {}\", file, exc))\n+          .run(fileIO::deleteFile);\n+    }\n+\n+    return new RemoveOrphanFilesActionResult(orphanDataFiles);\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesTable)\n+        .select(\"file_path\")\n+        .distinct();", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MTQ4OA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405891488", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:34:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NDkxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTIzNw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405155237", "bodyText": "Are you planning to make removeOrphanMetadataFiles as well, or would that be added to this?", "author": "rdblue", "createdAt": "2020-04-07T22:40:51Z", "path": "spark/src/main/java/org/apache/iceberg/Actions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import org.apache.spark.sql.SparkSession;\n+\n+public class Actions {\n+\n+  private SparkSession spark;\n+  private Table table;\n+\n+  private Actions(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+  }\n+\n+  public static Actions forTable(SparkSession spark, Table table) {\n+    return new Actions(spark, table);\n+  }\n+\n+  public static Actions forTable(Table table) {\n+    return new Actions(SparkSession.active(), table);\n+  }\n+\n+  public RemoveOrphanFilesAction removeOrphanFiles() {", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE2ODc2NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405168764", "bodyText": "Both options would work. In order to keep the actions API smaller, I think we can extend the existing API with more properties.", "author": "aokolnychyi", "createdAt": "2020-04-07T23:17:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTIzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MjQ1NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405172454", "bodyText": "The name is generic enough to cover both data and metadata files.", "author": "aokolnychyi", "createdAt": "2020-04-07T23:29:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTIzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg4ODMzOQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405888339", "bodyText": "It cleans both now.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:21:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTIzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTU1OA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405155558", "bodyText": "Why return a result class when you could just return List<String>? Do we need these wrappers?", "author": "rdblue", "createdAt": "2020-04-07T22:41:42Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesActionResult.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import java.util.List;\n+\n+public class RemoveOrphanFilesActionResult {", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3MjIyNQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405172225", "bodyText": "That's something we need to discuss. The idea behind this was to report deleted metadata and data files separately when we extend this API in the future. If we want to report all deleted files together or if we want to provide a separate action for metadata, we can return List<String>.\nSince it is a new API, we need to decide what actions should report back. For example, do actions need to return a summary? In RewriteManifestsAction, this can be a list of manifests that were replaced and a list of manifests that were added. People can infer that themselves but it will require more steps. They would need to assign a label to the produced snapshot, load that snapshot and then access the snapshot summary which will contain only numbers.", "author": "aokolnychyi", "createdAt": "2020-04-07T23:28:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTU1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NTY1NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405175654", "bodyText": "If this will return two lists in the future, then I think it makes sense to keep the class. Thanks!", "author": "rdblue", "createdAt": "2020-04-07T23:39:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTU1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MDk0NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405890944", "bodyText": "I removed the wrapper as it would require additional complexity to report data and metadata files separately. Users can call the action on data and metadata separately if they need such functionality.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:32:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTU1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjE0Mw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405156143", "bodyText": "In other places, we use deleteWith instead of a dry run flag. Should we do that for consistency here?", "author": "rdblue", "createdAt": "2020-04-07T22:43:09Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NTYxNg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405175616", "bodyText": "Makes sense.", "author": "aokolnychyi", "createdAt": "2020-04-07T23:39:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjE0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjU1Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405156552", "bodyText": "This seems awkward, but I'm not sure a better way to do it.", "author": "rdblue", "createdAt": "2020-04-07T22:44:16Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NTcwNw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405175707", "bodyText": "Any other ideas are more than welcome :)", "author": "aokolnychyi", "createdAt": "2020-04-07T23:39:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MTA0Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405181042", "bodyText": "In other places in Spark, we detect path tables by checking contains(\"/\"). We could do that here to construct the metadata table names:\npublic String metadataTableName(Table table, String metaTable) {\n  String tableName = table.toString()\n  if (tableName.contains(\"/\")) {\n    return tableName + \"#\" + metaTable;\n  } else {\n    return tableName + \".\" + metaTable;\n  }\n}\n\nI think that convention for naming isn't unreasonable considering how we do it by default for Spark.\nWe could also allow passing in BiFunction<String, String, String> metadataTableName that we just default to the implementation above.", "author": "rdblue", "createdAt": "2020-04-07T23:56:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY2MDMzNw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405660337", "bodyText": "This almost works. Unfortunately, table.toString() might return anything. For example, it will prepend hive. for friendly names in the Hive catalog and we won't be able to resolve hive.db.table.all_data_files. Exposing a correct TableIdentifier in Table would help but that would mean modifying public BaseTable as well.", "author": "aokolnychyi", "createdAt": "2020-04-08T16:37:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY3MDg2MQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405670861", "bodyText": "Okay, I thought this would work because it is what we do in our Spark 2.4 build. Since we are using DSv2 catalogs, when the catalog adds its name to the identifier we actually get a working multi-catalog identifier.\nMaybe we should add something to remove hive. for now, and take it out for Spark 3.0.", "author": "rdblue", "createdAt": "2020-04-08T16:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NzcyMQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405157721", "bodyText": "We should add Javadoc here that explains what happens. That should note that the table location needs to be accessible for listing via Hadoop FileSystem, but the table's FileIO will be used to delete.", "author": "rdblue", "createdAt": "2020-04-07T22:47:22Z", "path": "spark/src/main/java/org/apache/iceberg/Actions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import org.apache.spark.sql.SparkSession;\n+\n+public class Actions {\n+\n+  private SparkSession spark;\n+  private Table table;\n+\n+  private Actions(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+  }\n+\n+  public static Actions forTable(SparkSession spark, Table table) {\n+    return new Actions(spark, table);\n+  }\n+\n+  public static Actions forTable(Table table) {\n+    return new Actions(SparkSession.active(), table);\n+  }\n+\n+  public RemoveOrphanFilesAction removeOrphanFiles() {", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MjY2Ng==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405892666", "bodyText": "Will add docs in the next pass. If we decide to extend FileIO with list operations too, we should migrate to that here.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:38:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NzcyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzMTcwMA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406331700", "bodyText": "Hm. I've been avoiding a list operation in FileIO. Maybe we could add it as an optional trait? I just don't want that interface to get huge and complicated.", "author": "rdblue", "createdAt": "2020-04-09T16:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NzcyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0MTc0NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406441744", "bodyText": "I would not worry about that for now. We will need to expose some sort of FileStatus too if we decide to support list operations. Let's first have that use case when the location cannot be listed by Hadoop FileSystem and requires a custom FileIO implementation.", "author": "aokolnychyi", "createdAt": "2020-04-09T19:55:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NzcyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3ODk0Ng==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406478946", "bodyText": "I agree", "author": "rdblue", "createdAt": "2020-04-09T21:09:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NzcyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1ODE5NQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405158195", "bodyText": "Do we?", "author": "rdblue", "createdAt": "2020-04-07T22:48:29Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {\n+    this.dryRun = newDryRun;\n+    return this;\n+  }\n+\n+  @Override\n+  public RemoveOrphanFilesActionResult execute() {\n+    Preconditions.checkArgument(allDataFilesTable != null, \"allDataFilesTable must be set\");\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp should be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> actualDataFileDF = buildActualDataFileDF();\n+\n+    Column joinCond = validDataFileDF.col(\"file_path\").equalTo(actualDataFileDF.col(\"file_path\"));\n+    List<String> orphanDataFiles = actualDataFileDF.join(validDataFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    if (!dryRun) {\n+      Tasks.foreach(orphanDataFiles)\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.warn(\"Failed to delete data file: {}\", file, exc))\n+          .run(fileIO::deleteFile);\n+    }\n+\n+    return new RemoveOrphanFilesActionResult(orphanDataFiles);\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesTable)\n+        .select(\"file_path\")\n+        .distinct();\n+  }\n+\n+  private Dataset<Row> buildActualDataFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path dataPath = new Path(dataLocation);\n+      FileSystem fs = dataPath.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(dataPath, HiddenPathFilter.get())) {\n+        // TODO: handle custom metadata folders\n+        // we need to ignore the metadata folder when data is written to the root table location", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NjQwMg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405176402", "bodyText": "It is a leftanti join between all files and valid data files. If we don't ignore the metadata folder, we will remove all metadata files, right?", "author": "aokolnychyi", "createdAt": "2020-04-07T23:41:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1ODE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3OTQ0MQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405179441", "bodyText": "I think we want better protection for the metadata files. We could have a case where the user put metadata files into the data folder and then relocated the config to a different place. We still want to protect the metadata.\nI think that means that we should do the anti-join with all data and metadata file paths. We can get recent metadata files from:\n\nThe all_manifests table has manifest locations\nThe snapshots table has manifest list locations\nThe table metadata has the current metadata location and a list of old metadata locations\n\nI think it should be sufficient to protect all of those paths. Then we don't need to worry about a prefix to protect metadata.", "author": "rdblue", "createdAt": "2020-04-07T23:51:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1ODE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4NDkyNA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405184924", "bodyText": "Since we limit the number of previous metadata locations we keep in each new metadata file, the list of old metadata locations might not be complete in the current metadata file. It seems safe to remove others (assuming they are old enough). What do you think, @rdblue?", "author": "aokolnychyi", "createdAt": "2020-04-08T00:09:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1ODE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4NTQ3Mw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405185473", "bodyText": "I think it's a good thing to remove them.", "author": "rdblue", "createdAt": "2020-04-08T00:11:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1ODE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MTI1MQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405891251", "bodyText": "Added a test for old metadata files.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:33:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1ODE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE2MDE5Ng==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405160196", "bodyText": "I would probably opt not to do this check. Instead, there is little cost to doing the anti-join with all known data and metadata file paths. I'd probably do that instead of worrying about metadata location.", "author": "rdblue", "createdAt": "2020-04-07T22:53:42Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {\n+    this.dryRun = newDryRun;\n+    return this;\n+  }\n+\n+  @Override\n+  public RemoveOrphanFilesActionResult execute() {\n+    Preconditions.checkArgument(allDataFilesTable != null, \"allDataFilesTable must be set\");\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp should be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> actualDataFileDF = buildActualDataFileDF();\n+\n+    Column joinCond = validDataFileDF.col(\"file_path\").equalTo(actualDataFileDF.col(\"file_path\"));\n+    List<String> orphanDataFiles = actualDataFileDF.join(validDataFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    if (!dryRun) {\n+      Tasks.foreach(orphanDataFiles)\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.warn(\"Failed to delete data file: {}\", file, exc))\n+          .run(fileIO::deleteFile);\n+    }\n+\n+    return new RemoveOrphanFilesActionResult(orphanDataFiles);\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesTable)\n+        .select(\"file_path\")\n+        .distinct();\n+  }\n+\n+  private Dataset<Row> buildActualDataFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path dataPath = new Path(dataLocation);\n+      FileSystem fs = dataPath.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(dataPath, HiddenPathFilter.get())) {\n+        // TODO: handle custom metadata folders\n+        // we need to ignore the metadata folder when data is written to the root table location\n+        if (file.isDirectory() && !\"metadata\".equals(file.getPath().getName())) {\n+          topLevelDirs.add(file.getPath().toString());", "originalCommit": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MTM3NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405891374", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:33:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE2MDE5Ng=="}], "type": "inlineReview"}, {"oid": "2c25af79c5f646b342080aa8a0a414eed0b9f6d2", "url": "https://github.com/apache/iceberg/commit/2c25af79c5f646b342080aa8a0a414eed0b9f6d2", "message": "Rework the action to clean the whole table location", "committedDate": "2020-04-09T00:18:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MzE2NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405893164", "bodyText": "We have to clean the location properly.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:40:39Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java", "diffHunk": "@@ -78,12 +81,12 @@ public static void stopMetastoreAndSpark() {\n   }\n \n   @After\n-  public void dropTable() throws Exception {\n-    clients.run(client -> {\n-      client.dropTable(TestIcebergSourceHiveTables.currentIdentifier.namespace().level(0),\n-          TestIcebergSourceHiveTables.currentIdentifier.name());\n-      return null;\n-    });\n+  public void dropTable() throws IOException {", "originalCommit": "2c25af79c5f646b342080aa8a0a414eed0b9f6d2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "url": "https://github.com/apache/iceberg/commit/e0c96f353f6abc6804bdf1944e8677c1a62c3191", "message": "Remove extra line", "committedDate": "2020-04-09T00:41:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5NDY5OQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405894699", "bodyText": "I could probably add a condition to this branch:\nelse if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\"))\n\nAlso, we need a comment here.", "author": "aokolnychyi", "createdAt": "2020-04-09T00:46:22Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);\n+      FileSystem fs = path.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          topLevelDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && file.getModificationTime() < olderThanTimestamp) {\n+          matchingTopLevelFiles.add(file.getPath().toString());\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to determine top-level files and dirs in {}\", location);\n+    }\n+\n+    JavaRDD<String> matchingTopLevelFileRDD = sparkContext.parallelize(matchingTopLevelFiles, 1);\n+\n+    if (topLevelDirs.isEmpty()) {\n+      return spark.createDataset(matchingTopLevelFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(topLevelDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> topLevelDirRDD = sparkContext.parallelize(topLevelDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = topLevelDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> matchingFileRDD = matchingTopLevelFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> files = Lists.newArrayList();\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+      dirs.forEachRemaining(dir -> {\n+        List<String> dirFiles = listDirRecursively(dir, predicate, conf.value().value());\n+        files.addAll(dirFiles);\n+      });\n+      return files.iterator();\n+    };\n+  }\n+\n+  private static List<String> listDirRecursively(String dir, Predicate<FileStatus> predicate, Configuration conf) {\n+    try {\n+      Path path = new Path(dir);\n+      FileSystem fs = path.getFileSystem(conf);\n+\n+      List<String> childDirs = Lists.newArrayList();\n+      List<String> matchingFiles = Lists.newArrayList();\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          childDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && predicate.test(file)) {\n+          matchingFiles.add(file.getPath().toString());\n+        }\n+      }\n+\n+      for (String childDir : childDirs) {\n+        List<String> childDirFiles = listDirRecursively(childDir, predicate, conf);\n+        matchingFiles.addAll(childDirFiles);\n+      }\n+\n+      return matchingFiles;\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e);\n+    }\n+  }\n+\n+  private String metadataTableName(MetadataTableType type) {\n+    String tableName = table.toString();\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else {", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzMzU4OQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406333589", "bodyText": "+1 for a comment", "author": "rdblue", "createdAt": "2020-04-09T16:39:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5NDY5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3NDIyNw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406574227", "bodyText": "Added.", "author": "aokolnychyi", "createdAt": "2020-04-10T02:30:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5NDY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzMjMxNA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406332314", "bodyText": "Should we create an actions package?", "author": "rdblue", "createdAt": "2020-04-09T16:37:03Z", "path": "spark/src/main/java/org/apache/iceberg/Action.java", "diffHunk": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+public interface Action<R> {", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzNDE5OA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406334198", "bodyText": "Good catch!", "author": "rdblue", "createdAt": "2020-04-09T16:40:07Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzOTU2Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406339562", "bodyText": "Now that this is the table's location, we expect this to contain just two directories: data and metadata. I think the intent of this methods was to parallelize on the first level of partition directories, but that's not what will happen here.\nIt's a bit more tricky because we don't know the convention actually matches the default structure, but I think it would be reasonable to traverse the first 2 layers of directories to build the top-level set. To do that, adding a depth parameter to the recursive traversal makes sense so you can use it here and return after 2 levels (or a configurable number). That would also be a good thing for the parallel traversal to ensure this won't get caught in a symlink loop.", "author": "rdblue", "createdAt": "2020-04-09T16:49:26Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3MzY4Mw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406573683", "bodyText": "There is one more problem: the initial number of locations might be pretty small. It seems beneficial to list, for example, 3 levels on the driver and then parallelize. The number of top-level partitions might be small. At the same time, we should avoid listing too much on the driver if the data is written to the root table location. That's why I modified the listing logic so that we list 3 levels by default by don't list locations that have more than 10 sub-locations. The latter ones will be listed in a distributed manner. This should cover cases with a lot of top-level and leaf partitions.", "author": "aokolnychyi", "createdAt": "2020-04-10T02:27:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzOTU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgwMjg3Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406802872", "bodyText": "Actually, ignore my previous comment. I'll think more about this.", "author": "aokolnychyi", "createdAt": "2020-04-10T15:14:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzOTU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzMTc3Mw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406831773", "bodyText": "I think it's fine to list a fixed-number of levels. If all the data is written to the root location, there's nothing we can do anyway because it can't be parallelized.", "author": "rdblue", "createdAt": "2020-04-10T16:21:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzOTU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzNDY1MQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406834651", "bodyText": "After thinking about this more, I think it still makes sense to list 2 levels and stop whenever we hit let's say 10 sub-locations. It won't solve the problem when the number of top-level partitions is small. However, it should help if the table is partitioned but the data is written to the table location. Consider tables that were migrated from Hive. If they have 1000 top-level partitions and 10 sub-partitions, we will be listing 10000 locations on the driver only for the first two levels.", "author": "aokolnychyi", "createdAt": "2020-04-10T16:28:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzOTU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzNjE1Mw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406836153", "bodyText": "@rdblue, what do you think?", "author": "aokolnychyi", "createdAt": "2020-04-10T16:32:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzOTU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0MTYyOQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406341629", "bodyText": "Nit: predicate isn't a very descriptive name. Maybe pastOperationTimeLimit instead?", "author": "rdblue", "createdAt": "2020-04-09T16:53:07Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);\n+      FileSystem fs = path.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          topLevelDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && file.getModificationTime() < olderThanTimestamp) {\n+          matchingTopLevelFiles.add(file.getPath().toString());\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to determine top-level files and dirs in {}\", location);\n+    }\n+\n+    JavaRDD<String> matchingTopLevelFileRDD = sparkContext.parallelize(matchingTopLevelFiles, 1);\n+\n+    if (topLevelDirs.isEmpty()) {\n+      return spark.createDataset(matchingTopLevelFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(topLevelDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> topLevelDirRDD = sparkContext.parallelize(topLevelDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = topLevelDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> matchingFileRDD = matchingTopLevelFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> files = Lists.newArrayList();\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3NzE2Mw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406577163", "bodyText": "I kept it to stay on one line below.", "author": "aokolnychyi", "createdAt": "2020-04-10T02:43:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0MTYyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0NDg2Ng==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406344866", "bodyText": "What about passing a result list into this method to avoid creating lots of small lists and merging them together in every call?", "author": "rdblue", "createdAt": "2020-04-09T16:58:25Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);\n+      FileSystem fs = path.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          topLevelDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && file.getModificationTime() < olderThanTimestamp) {\n+          matchingTopLevelFiles.add(file.getPath().toString());\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to determine top-level files and dirs in {}\", location);\n+    }\n+\n+    JavaRDD<String> matchingTopLevelFileRDD = sparkContext.parallelize(matchingTopLevelFiles, 1);\n+\n+    if (topLevelDirs.isEmpty()) {\n+      return spark.createDataset(matchingTopLevelFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(topLevelDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> topLevelDirRDD = sparkContext.parallelize(topLevelDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = topLevelDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> matchingFileRDD = matchingTopLevelFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> files = Lists.newArrayList();\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+      dirs.forEachRemaining(dir -> {\n+        List<String> dirFiles = listDirRecursively(dir, predicate, conf.value().value());\n+        files.addAll(dirFiles);\n+      });\n+      return files.iterator();\n+    };\n+  }\n+\n+  private static List<String> listDirRecursively(String dir, Predicate<FileStatus> predicate, Configuration conf) {\n+    try {\n+      Path path = new Path(dir);\n+      FileSystem fs = path.getFileSystem(conf);\n+\n+      List<String> childDirs = Lists.newArrayList();\n+      List<String> matchingFiles = Lists.newArrayList();\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          childDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && predicate.test(file)) {\n+          matchingFiles.add(file.getPath().toString());\n+        }\n+      }\n+\n+      for (String childDir : childDirs) {\n+        List<String> childDirFiles = listDirRecursively(childDir, predicate, conf);", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3Njg2NQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406576865", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-04-10T02:42:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0NDg2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0ODMzNw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406348337", "bodyText": "Can we default it to System.currentTimeMillis() - THREE_DAYS_MS?", "author": "rdblue", "createdAt": "2020-04-09T17:04:25Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3Njg5MQ==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406576891", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-04-10T02:42:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0ODMzNw=="}], "type": "inlineReview"}, {"oid": "f3e694856dbf5aaa0c4577dcc38ae1fce413b3fe", "url": "https://github.com/apache/iceberg/commit/f3e694856dbf5aaa0c4577dcc38ae1fce413b3fe", "message": "Rework listing and add javadocs", "committedDate": "2020-04-10T02:07:37Z", "type": "commit"}, {"oid": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6", "url": "https://github.com/apache/iceberg/commit/6c01b75d0d738bc4b227b3deb5ffb29e73071dc6", "message": "Explain metadataTableName", "committedDate": "2020-04-10T02:17:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3NDM2NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406574364", "bodyText": "a comment here on criteria for collecting all actual files would be helpful.", "author": "prodeezy", "createdAt": "2020-04-10T02:30:54Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {", "originalCommit": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzNTU0Nw==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406835547", "bodyText": "@prodeezy, do you mean the actual algorithm or that we select files older than a given timestamp?", "author": "aokolnychyi", "createdAt": "2020-04-10T16:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3NDM2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzMjc4NA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406832784", "bodyText": "Nit: we should be able to write this a table.io()::deleteFile", "author": "rdblue", "createdAt": "2020-04-10T16:24:04Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that removes orphan metadata and data files by listing a given location and comparing\n+ * the actual files in that location with data and metadata files referenced by all valid snapshots.\n+ * The location must be accessible for listing via the Hadoop {@link FileSystem}.\n+ * <p>\n+ * By default, this action cleans up the table location returned by {@link Table#location()} and\n+ * removes unreachable files that are older than 3 days using {@link Table#io()}. The behavior can be modified\n+ * by passing a custom location to {@link #location} and a custom timestamp to {@link #olderThan(long)}.\n+ * For example, someone might point this action to the data folder to clean up only orphan data files.\n+ * In addition, there is a way to configure an alternative delete method via {@link #deleteWith(Consumer)}.\n+ * <p>\n+ * <em>Note:</em> It is dangerous to call this action with a short retention interval as it might corrupt\n+ * the state of the table if another operation is writing at the same time.\n+ */\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private long olderThanTimestamp = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(3);\n+  private Consumer<String> deleteFunc = new Consumer<String>() {", "originalCommit": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzNzI1OA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406837258", "bodyText": "That's what I tried in the first place. Unfortunately, it complains with:\nVariable 'table' might not have been initialized\n\nI think we had the same problem in RemoveSnapshots.", "author": "aokolnychyi", "createdAt": "2020-04-10T16:34:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzMjc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg3MDM1Mg==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406870352", "bodyText": "That makes sense. Let's go with this then.", "author": "rdblue", "createdAt": "2020-04-10T17:53:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzMjc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzNjU1OA==", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406836558", "bodyText": "This seems excessive, but not really that dangerous. When listing in executors, the purpose is to exit even if there is a reference cycle in the file system. This would technically do that, but would recurse 2 billion levels so the more likely failure is a stack overflow.\nThat's alright since it's the behavior that was here before, but I think it would be better to set this to 2,000 or something large but reasonable and then throw an exception if there are remaining directories when it returns.", "author": "rdblue", "createdAt": "2020-04-10T16:33:19Z", "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that removes orphan metadata and data files by listing a given location and comparing\n+ * the actual files in that location with data and metadata files referenced by all valid snapshots.\n+ * The location must be accessible for listing via the Hadoop {@link FileSystem}.\n+ * <p>\n+ * By default, this action cleans up the table location returned by {@link Table#location()} and\n+ * removes unreachable files that are older than 3 days using {@link Table#io()}. The behavior can be modified\n+ * by passing a custom location to {@link #location} and a custom timestamp to {@link #olderThan(long)}.\n+ * For example, someone might point this action to the data folder to clean up only orphan data files.\n+ * In addition, there is a way to configure an alternative delete method via {@link #deleteWith(Consumer)}.\n+ * <p>\n+ * <em>Note:</em> It is dangerous to call this action with a short retention interval as it might corrupt\n+ * the state of the table if another operation is writing at the same time.\n+ */\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private long olderThanTimestamp = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(3);\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  /**\n+   * Removes orphan files in the given location.\n+   *\n+   * @param newLocation a location\n+   * @return this for method chaining\n+   */\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  /**\n+   * Removes orphan files that are older than the given timestamp.\n+   *\n+   * @param newOlderThanTimestamp a timestamp in milliseconds\n+   * @return this for method chaining\n+   */\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  /**\n+   * Passes an alternative delete implementation that will be used to delete orphan files.\n+   *\n+   * @param newDeleteFunc a delete func\n+   * @return this for method chaining\n+   */\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> subDirs = Lists.newArrayList();\n+    List<String> matchingFiles = Lists.newArrayList();\n+\n+    Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+\n+    // list at most 3 levels and only dirs that have less than 10 direct sub dirs on the driver\n+    listDirRecursively(location, predicate, hadoopConf.value(), 3, 10, subDirs, matchingFiles);\n+\n+    JavaRDD<String> matchingFileRDD = sparkContext.parallelize(matchingFiles, 1);\n+\n+    if (subDirs.isEmpty()) {\n+      return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(subDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> subDirRDD = sparkContext.parallelize(subDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = subDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> completeMatchingFileRDD = matchingFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(completeMatchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static void listDirRecursively(\n+      String dir, Predicate<FileStatus> predicate, Configuration conf, int maxDepth,\n+      int maxDirectSubDirs, List<String> remainingSubDirs, List<String> matchingFiles) {\n+\n+    // stop listing whenever we reach the max depth\n+    if (maxDepth <= 0) {\n+      remainingSubDirs.add(dir);\n+      return;\n+    }\n+\n+    try {\n+      Path path = new Path(dir);\n+      FileSystem fs = path.getFileSystem(conf);\n+\n+      List<String> subDirs = Lists.newArrayList();\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          subDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && predicate.test(file)) {\n+          matchingFiles.add(file.getPath().toString());\n+        }\n+      }\n+\n+      // stop listing if the number of direct sub dirs is bigger than maxDirectSubDirs\n+      if (subDirs.size() > maxDirectSubDirs) {\n+        remainingSubDirs.addAll(subDirs);\n+        return;\n+      }\n+\n+      for (String subDir : subDirs) {\n+        listDirRecursively(subDir, predicate, conf, maxDepth - 1, maxDirectSubDirs, remainingSubDirs, matchingFiles);\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e);\n+    }\n+  }\n+\n+  private String metadataTableName(MetadataTableType type) {\n+    String tableName = table.toString();\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+      return tableName.replaceFirst(\"(hadoop\\\\.)|(hive\\\\.)\", \"\") + \".\" + type;\n+    } else {\n+      return tableName + \".\" + type;\n+    }\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> subDirs = Lists.newArrayList();\n+      List<String> files = Lists.newArrayList();\n+\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+\n+      int maxDepth = Integer.MAX_VALUE;", "originalCommit": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}