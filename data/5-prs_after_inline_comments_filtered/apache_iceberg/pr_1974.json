{"pr_number": 1974, "pr_title": "Flink: Add ChangeLog DataStream end-to-end unit tests.", "pr_createdAt": "2020-12-22T04:33:40Z", "pr_url": "https://github.com/apache/iceberg/pull/1974", "timeline": [{"oid": "59d77aa0e1864ed71142eee2c7736353a4503f83", "url": "https://github.com/apache/iceberg/commit/59d77aa0e1864ed71142eee2c7736353a4503f83", "message": "Flink: Add ChangeLog DataStream end-to-end unit tests.", "committedDate": "2020-12-22T04:30:25Z", "type": "commit"}, {"oid": "8a64f26e7088759802b176f3a630e5dd7765349a", "url": "https://github.com/apache/iceberg/commit/8a64f26e7088759802b176f3a630e5dd7765349a", "message": "Minor changes.", "committedDate": "2020-12-22T04:57:35Z", "type": "commit"}, {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "url": "https://github.com/apache/iceberg/commit/c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "message": "Add equalityFieldColumns in FlinkSink API.", "committedDate": "2020-12-22T12:20:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMTczNg==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547411736", "bodyText": "Do you think that we should consider adding primary key columns to the spec?", "author": "rdblue", "createdAt": "2020-12-22T17:42:30Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -169,6 +172,17 @@ public Builder writeParallelism(int newWriteParallelism) {\n       return this;\n     }\n \n+    /**\n+     * Configuring the equality field columns for iceberg table that accept CDC or UPSERT events.\n+     *\n+     * @param columns defines the iceberg table's key.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder equalityFieldColumns(List<String> columns) {", "originalCommit": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxNjQ5NA==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547616494", "bodyText": "In the next PR openinx@a863c66,  The flink table SQL's primary key  will act as the equality field columns.   The semantic of iceberg equality columns is almost the same as primary key,  one difference I can think of is:   the uniqueness of key are not enforced. In this discussion,  we don't guarantee the uniqueness when writing a key which has been also wrote in the previous committed txn, that means if :\nTxn-1:  INSERT key1,  txn commit; \nTxn-2:  INSERT key1,  txn commit;\nThen the table will have two records with the same key.\nIf people really need iceberg to maintain the key's uniqueness, then they will need to transform all the INSERT to UPSERT, which means DELETE firstly and then INSERT the new values.\nIt will introduce another issues:  Each INSERT will be regarded as an UPSERT,  so it write a DELETE and a INSERT.  Finally the size of delete files will be almost same as the size of data files.    The process of merging on read will be quite inefficient   because there are too many useless DELETE to JOIN.\nThe direct way is using bloom filter to reduce the useless DELETE, say we will generate bloom filter binary for each committed data file.  When bootstrap the flink/spark job we will need to prefetch all the bloom filter binary from parquet/avro data files's metadata. Before writing a equality delete, we will check the bloom filter, and if the bloom filter indicate that all the committed data files are not containing the given key, then we could skip to append that equality-delete. That would reduce lots of useless DELETE in delete files. Of course, the bloom filter will have 'false positive' issue, but that probability is less than 1%, that means we may append\nsmall amout of deletes whose keys don't exist in the current table.  In my view, that should be OK.\nIn summary, I think it's reasonable to regard those equality fields as primary key in iceberg table, people could choose to use UNIQUENESS ENFORCED or UNIQUENESS NOT-ENFORCED, in this way they could trade off between strong semantic and performance.", "author": "openinx", "createdAt": "2020-12-23T03:14:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMTczNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg4OTcwMQ==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r549889701", "bodyText": "For the bloom filter idea, @wangmiao1981 has been working on a proposal for secondary indexes. I think that could be used for the check you're suggesting here.\n\npeople could choose to use UNIQUENESS ENFORCED or UNIQUENESS NOT-ENFORCED, in this way they could trade off between strong semantic and performance.\n\nAre you saying that if uniqueness is enforced, each insert becomes an upsert. But if uniqueness is not enforced, then the sink would assume that whatever is emitting records will correctly delete before inserting? That sounds reasonable to me.\n\nFinally the size of delete files will be almost same as the size of data files. The process of merging on read will be quite inefficient because there are too many useless DELETE to JOIN.\n\nI think that even if uniqueness is not enforced, tables will quickly require compaction to rewrite the equality deletes. I think we should spend some time making sure that we have good ways to maintain tables and compact equality deletes into position deletes, and position deletes into data files.", "author": "rdblue", "createdAt": "2020-12-29T23:48:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMTczNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTkyMTA5NQ==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r549921095", "bodyText": "Are you saying that if uniqueness is enforced, each insert becomes an upsert. But if uniqueness is not enforced, then the sink would assume that whatever is emitting records will correctly delete before inserting?\n\nYes.  If someone are exporting relational database's change log events to apache iceberg table and they could guarantee the exactly-once semantics (For example,  the flink-cdc-connector could guarantee that), then the uniqueness is always correct when we just write the INSERT/DELETE/UPDATE_BEFORE/UPDATE_AFTER to iceberg.  While in some other cases,  for example flink aggregate job to refresh the metrics count value,  we will write the same key several times without deleting first, then we should regard all the INSERT as UPSERT.\n\neven if uniqueness is not enforced, tables will quickly require compaction to rewrite the equality deletes.\n\nThat was planned in the second phase, include:\n\nUse bloom filter to reduce lots of useless deletes;\nMinor compaction to convert parts of equality deletes to pos-deletes\nMajor compaction to eliminate all the deletes.\nMake the whole read path & write path more stable. For example,  cache policy reduce duplicated delete files loading when merging on read in the same tasks;  Spill to disk if the insertedRowMap is exceeding the task's memory threshold, etc.  I will evaluate the read & write & compaction paths in a large dataset, making this to be a stable solution for production.\n\nIt's good to have a document to collect all those things for reviewing.", "author": "openinx", "createdAt": "2020-12-30T03:15:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMTczNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3OTMxNg==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r550279316", "bodyText": "I\u2019d vote for not ensuring uniqueness as it is really hard at scale. If we are to ensure this at write, we have to join the incoming data with the target table making it really expensive. Doing this at read would require sorting the data not only by the sort key but also by the sequence number.", "author": "aokolnychyi", "createdAt": "2020-12-30T18:01:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMTczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMjM5NA==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547412394", "bodyText": "Why not do this conversion in equalityFieldColumns and keep the column ids in the builder instead of the source column names?", "author": "rdblue", "createdAt": "2020-12-22T17:43:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -184,7 +198,18 @@ public Builder writeParallelism(int newWriteParallelism) {\n         }\n       }\n \n-      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      // Find out the equality field id list based on the user-provided equality field column names.\n+      List<Integer> equalityFieldIds = Lists.newArrayList();\n+      if (equalityFieldColumns != null && equalityFieldColumns.size() > 0) {\n+        for (String column : equalityFieldColumns) {\n+          org.apache.iceberg.types.Types.NestedField field = table.schema().findField(column);\n+          Preconditions.checkNotNull(field, \"Missing required equality field column '%s' in table schema %s\",\n+              column, table.schema());\n+          equalityFieldIds.add(field.fieldId());\n+        }\n+      }", "originalCommit": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxNzExNQ==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547617115", "bodyText": "Because the FlinkSink is an API which will be exposed to flink's DataStream users,   the concept of equality field id is harder to understand for those flink users.  Equality field column names will be more friendly.", "author": "openinx", "createdAt": "2020-12-23T03:17:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMjM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMzEwMw==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547413103", "bodyText": "Nit: I think you mean \"executing tasks in parallel\" rather than \"parallelism\".", "author": "rdblue", "createdAt": "2020-12-22T17:45:25Z", "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.", "originalCommit": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxNzMyNg==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547617326", "bodyText": "Thanks for pointing it out, will address it in next update.", "author": "openinx", "createdAt": "2020-12-23T03:18:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMzEwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMzU4MA==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547413580", "bodyText": "Could this be a private static map instead of defining it each time a row is created?", "author": "rdblue", "createdAt": "2020-12-22T17:46:07Z", "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.\n+    dataStream = dataStream.keyBy(keySelector);\n+\n+    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+        .tableLoader(tableLoader)\n+        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n+        .writeParallelism(parallelism)\n+        .equalityFieldColumns(equalityFieldColumns)\n+        .build();\n+\n+    // Execute the program.\n+    env.execute(\"Test Iceberg Change-Log DataStream.\");\n+\n+    table.refresh();\n+    List<Snapshot> snapshots = findValidSnapshots(table);\n+    int expectedSnapshotNum = expectedRecordsPerCheckpoint.size();\n+    Assert.assertEquals(\"Should have the expected snapshot number\", expectedSnapshotNum, snapshots.size());\n+\n+    for (int i = 0; i < expectedSnapshotNum; i++) {\n+      long snapshotId = snapshots.get(i).snapshotId();\n+      List<Record> expectedRecords = expectedRecordsPerCheckpoint.get(i);\n+      Assert.assertEquals(\"Should have the expected records for the checkpoint#\" + i,\n+          expectedRowSet(expectedRecords.toArray(new Record[0])), actualRowSet(snapshotId, \"*\"));\n+    }\n+  }\n+\n+  private Row row(String rowKind, int id, String data) {\n+    Map<String, RowKind> mapping = ImmutableMap.of(\n+        \"+I\", RowKind.INSERT,\n+        \"-D\", RowKind.DELETE,\n+        \"-U\", RowKind.UPDATE_BEFORE,\n+        \"+U\", RowKind.UPDATE_AFTER);", "originalCommit": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxNTQ3Mg==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547415472", "bodyText": "Should this be equalityFieldNames?", "author": "rdblue", "createdAt": "2020-12-22T17:49:54Z", "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.\n+    dataStream = dataStream.keyBy(keySelector);\n+\n+    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+        .tableLoader(tableLoader)\n+        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n+        .writeParallelism(parallelism)\n+        .equalityFieldColumns(equalityFieldColumns)\n+        .build();\n+\n+    // Execute the program.\n+    env.execute(\"Test Iceberg Change-Log DataStream.\");\n+\n+    table.refresh();\n+    List<Snapshot> snapshots = findValidSnapshots(table);\n+    int expectedSnapshotNum = expectedRecordsPerCheckpoint.size();\n+    Assert.assertEquals(\"Should have the expected snapshot number\", expectedSnapshotNum, snapshots.size());\n+\n+    for (int i = 0; i < expectedSnapshotNum; i++) {\n+      long snapshotId = snapshots.get(i).snapshotId();\n+      List<Record> expectedRecords = expectedRecordsPerCheckpoint.get(i);\n+      Assert.assertEquals(\"Should have the expected records for the checkpoint#\" + i,\n+          expectedRowSet(expectedRecords.toArray(new Record[0])), actualRowSet(snapshotId, \"*\"));\n+    }\n+  }\n+\n+  private Row row(String rowKind, int id, String data) {\n+    Map<String, RowKind> mapping = ImmutableMap.of(\n+        \"+I\", RowKind.INSERT,\n+        \"-D\", RowKind.DELETE,\n+        \"-U\", RowKind.UPDATE_BEFORE,\n+        \"+U\", RowKind.UPDATE_AFTER);\n+\n+    RowKind kind = mapping.get(rowKind);\n+    if (kind == null) {\n+      throw new IllegalArgumentException(\"Unknown row kind: \" + rowKind);\n+    }\n+\n+    return Row.ofKind(kind, id, data);\n+  }\n+\n+  private Record record(int id, String data) {\n+    return SimpleDataUtil.createRecord(id, data);\n+  }\n+\n+  @Test\n+  public void testChangeLogOnIdKey() throws Exception {\n+    List<String> equalityFieldIds = ImmutableList.of(\"id\");", "originalCommit": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYxNzUyMw==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547617523", "bodyText": "Yes.", "author": "openinx", "createdAt": "2020-12-23T03:19:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxNTQ3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxNzU5MQ==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547417591", "bodyText": "Minor: This makes it look like the row has an operation as its first column, but that doesn't align with the key selector below that uses row.getField(0) to get the ID. I think it would make tests easier to read if row passed the row kind at the end. That way the fields align.\nI'm not sure if it is worth changing all of the rows. Up to you.", "author": "rdblue", "createdAt": "2020-12-22T17:54:38Z", "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.\n+    dataStream = dataStream.keyBy(keySelector);\n+\n+    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+        .tableLoader(tableLoader)\n+        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n+        .writeParallelism(parallelism)\n+        .equalityFieldColumns(equalityFieldColumns)\n+        .build();\n+\n+    // Execute the program.\n+    env.execute(\"Test Iceberg Change-Log DataStream.\");\n+\n+    table.refresh();\n+    List<Snapshot> snapshots = findValidSnapshots(table);\n+    int expectedSnapshotNum = expectedRecordsPerCheckpoint.size();\n+    Assert.assertEquals(\"Should have the expected snapshot number\", expectedSnapshotNum, snapshots.size());\n+\n+    for (int i = 0; i < expectedSnapshotNum; i++) {\n+      long snapshotId = snapshots.get(i).snapshotId();\n+      List<Record> expectedRecords = expectedRecordsPerCheckpoint.get(i);\n+      Assert.assertEquals(\"Should have the expected records for the checkpoint#\" + i,\n+          expectedRowSet(expectedRecords.toArray(new Record[0])), actualRowSet(snapshotId, \"*\"));\n+    }\n+  }\n+\n+  private Row row(String rowKind, int id, String data) {\n+    Map<String, RowKind> mapping = ImmutableMap.of(\n+        \"+I\", RowKind.INSERT,\n+        \"-D\", RowKind.DELETE,\n+        \"-U\", RowKind.UPDATE_BEFORE,\n+        \"+U\", RowKind.UPDATE_AFTER);\n+\n+    RowKind kind = mapping.get(rowKind);\n+    if (kind == null) {\n+      throw new IllegalArgumentException(\"Unknown row kind: \" + rowKind);\n+    }\n+\n+    return Row.ofKind(kind, id, data);\n+  }\n+\n+  private Record record(int id, String data) {\n+    return SimpleDataUtil.createRecord(id, data);\n+  }\n+\n+  @Test\n+  public void testChangeLogOnIdKey() throws Exception {\n+    List<String> equalityFieldIds = ImmutableList.of(\"id\");\n+    List<List<Row>> elementsPerCheckpoint = ImmutableList.of(\n+        ImmutableList.of(\n+            row(\"+I\", 1, \"aaa\"),", "originalCommit": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYyMTQwNQ==", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547621405", "bodyText": "The current way is correct because it will maintain rowKind in a separate field ( rather than in the shared fields array) , see here.", "author": "openinx", "createdAt": "2020-12-23T03:36:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxNzU5MQ=="}], "type": "inlineReview"}, {"oid": "113be2520c330fecc1b0029d509d8a3866069a32", "url": "https://github.com/apache/iceberg/commit/113be2520c330fecc1b0029d509d8a3866069a32", "message": "Addressing comments from Ryan.", "committedDate": "2020-12-23T03:42:45Z", "type": "commit"}]}