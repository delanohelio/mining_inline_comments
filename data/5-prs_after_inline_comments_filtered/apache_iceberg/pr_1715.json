{"pr_number": 1715, "pr_title": "Fix Actions to be Compatible with V2 Catalogs", "pr_createdAt": "2020-11-04T02:11:04Z", "pr_url": "https://github.com/apache/iceberg/pull/1715", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA3MTUzNA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517071534", "bodyText": "Maybe this method can not handle some corner cases.\nFor example, in the spark2 action, the file_path is very special (file://level1.level2.level3.level4/database/table).", "author": "liukun4515", "createdAt": "2020-11-04T03:00:11Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -144,6 +123,32 @@ protected String metadataTableName(String tableName, MetadataTableType type) {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    String metadataTableName = metadataTableName(tableName, type);", "originalCommit": "9ab39c789cf014a8b27749e8736e914c4d882b15", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA3Nzk2NQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517077965", "bodyText": "can we add some examples for explaining the case?", "author": "liukun4515", "createdAt": "2020-11-04T03:26:49Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -144,6 +123,32 @@ protected String metadataTableName(String tableName, MetadataTableType type) {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    String metadataTableName = metadataTableName(tableName, type);\n+    if (metadataTableName.split(\"\\\\.\").length > 3) {", "originalCommit": "9ab39c789cf014a8b27749e8736e914c4d882b15", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "url": "https://github.com/apache/iceberg/commit/2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "message": "Fix Actions to be Compatible with V2 Catalogs\n\nPreviously the Action code would use the table name when attempting to look\nup MetadataTable names. This is an issue in Spark3 with V2 catalogs because\nwe strip off the catalog information if the catalog is named \"hadoop\" or \"hive\"\nand include it in all other cases. When we include the catalog name this breaks\nthe lookup code which uses the DataSourceRegistry pathway and not the V2 Table\npathway. To fix this we introduce a method for using the V2 pathway when a catalog\nname is present.", "committedDate": "2020-11-04T03:29:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NzgyMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517087823", "bodyText": "I only added this, (and the test inside orphan files) because we'll have additional coverage for the use cases once the SQL api is introduced and vectorizing the entire suite of actions was getting a bit onerous.", "author": "RussellSpitzer", "createdAt": "2020-11-04T04:12:54Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -19,5 +19,46 @@\n \n package org.apache.iceberg.actions;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestRemoveOrphanFilesAction3 extends TestRemoveOrphanFilesAction {\n+  @Test", "originalCommit": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4ODEzMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517088133", "bodyText": "I didn't want to make these static, but because two of the actions no longer extend BaseSparkActions they cannot access these methods anymore. I think in the future we should try our best to keep from having Actions implementing  \"BaseAction\" if we can help it so that the implementations of frame work specific code can use their shared methods.", "author": "RussellSpitzer", "createdAt": "2020-11-04T04:14:22Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +125,33 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,", "originalCommit": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "url": "https://github.com/apache/iceberg/commit/7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "message": "Fix Actions to be Compatible with V2 Catalogs\n\nPreviously the Action code would use the table name when attempting to look\nup MetadataTable names. This is an issue in Spark3 with V2 catalogs because\nwe strip off the catalog information if the catalog is named \"hadoop\" or \"hive\"\nand include it in all other cases. When we include the catalog name this breaks\nthe lookup code which uses the DataSourceRegistry pathway and not the V2 Table\npathway. To fix this we introduce a method for using the V2 pathway when a catalog\nname is present.", "committedDate": "2020-11-04T04:50:58Z", "type": "commit"}, {"oid": "7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "url": "https://github.com/apache/iceberg/commit/7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "message": "Fix Actions to be Compatible with V2 Catalogs\n\nPreviously the Action code would use the table name when attempting to look\nup MetadataTable names. This is an issue in Spark3 with V2 catalogs because\nwe strip off the catalog information if the catalog is named \"hadoop\" or \"hive\"\nand include it in all other cases. When we include the catalog name this breaks\nthe lookup code which uses the DataSourceRegistry pathway and not the V2 Table\npathway. To fix this we introduce a method for using the V2 pathway when a catalog\nname is present.", "committedDate": "2020-11-04T04:50:58Z", "type": "forcePushed"}, {"oid": "7594cc7d9bde91ccb2795b300960a927c648f83f", "url": "https://github.com/apache/iceberg/commit/7594cc7d9bde91ccb2795b300960a927c648f83f", "message": "Fix Hadoop/Hive Catalog Naming Issue\n\nPreviously there was a conflict between manually non-Spark-catalogs\nand Spark Catalogs in the metadata table name resoultion code. Because\nof this ambiguity we will attempt to load all tables as if their catalog\nis a named SparkCatalog, if this fails we will fall back to the old\npath of removing the \"hadoop\" or \"hive\" catalog names and resolving.", "committedDate": "2020-11-17T22:03:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NTQ4OA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525595488", "bodyText": "nit: It may not necessarily be a metadata location. It can be a valid Hadoop table location too.", "author": "aokolnychyi", "createdAt": "2020-11-17T23:33:13Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NjU2OQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525596569", "bodyText": "nit: do we think this else helps clarity? We have a return statement anyway. Without this else, we could reduce the indentation of the block below.", "author": "aokolnychyi", "createdAt": "2020-11-17T23:36:11Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog\n+      return noCatalogReader.load(tableName + \"#\" + type);\n+    } else {", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYwMzc0Ng==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525603746", "bodyText": "It helps for my brain,  but I can drop the if else if you like. I tend to prefer nesting over implied control flow breaks but I think I'm not in the majority here. I'll change it since its what we did on the other pr as well", "author": "RussellSpitzer", "createdAt": "2020-11-17T23:56:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NjU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5ODgwMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525598803", "bodyText": "If we do a static import for ALL_MANIFESTS, will it fit on one line?", "author": "aokolnychyi", "createdAt": "2020-11-17T23:42:23Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -47,9 +87,9 @@\n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n     JavaSparkContext context = new JavaSparkContext(spark.sparkContext());\n     Broadcast<FileIO> ioBroadcast = context.broadcast(SparkUtil.serializableFileIO(table()));\n-    String allManifestsMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_MANIFESTS);\n \n-    Dataset<ManifestFileBean> allManifests = spark.read().format(\"iceberg\").load(allManifestsMetadataTable)\n+    Dataset<ManifestFileBean> allManifests = loadMetadataTable(spark, tableName, table().location(),\n+        MetadataTableType.ALL_MANIFESTS)", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5OTA4OQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525599089", "bodyText": "If not, we can make a var String tableLocation = table().location() if we need to reduce the length further.", "author": "aokolnychyi", "createdAt": "2020-11-17T23:43:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5ODgwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5OTM5Nw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525599397", "bodyText": "Same here. Any way to fit on one line?", "author": "aokolnychyi", "createdAt": "2020-11-17T23:44:04Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -202,9 +202,8 @@ public RewriteManifestsActionResult execute() {\n         .createDataset(Lists.transform(manifests, ManifestFile::path), Encoders.STRING())\n         .toDF(\"manifest\");\n \n-    String entriesMetadataTable = metadataTableName(MetadataTableType.ENTRIES);\n-    Dataset<Row> manifestEntryDF = spark.read().format(\"iceberg\")\n-        .load(entriesMetadataTable)\n+    Dataset<Row> manifestEntryDF = BaseSparkAction.loadMetadataTable(spark, table.name(), table().location(),", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTIxMQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525621211", "bodyText": "Can this be a more specific exception? Seems dangerous to catch 'em all.", "author": "rdblue", "createdAt": "2020-11-18T00:46:05Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog\n+      return noCatalogReader.load(tableName + \"#\" + type);\n+    } else {\n+      // Try catalog based name based resolution\n+      try {\n+        return spark.table(tableName + \".\" + type);\n+      } catch (Exception e) {", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzNTI1Mg==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525635252", "bodyText": ":pokemon: - Let me check what gets thrown and I\"ll try to narrow it down to reasonable classes of exception", "author": "RussellSpitzer", "createdAt": "2020-11-18T01:28:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjIwOTU5OA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r526209598", "bodyText": "Unfortunately these are unchecked from Scala so we do need to catch all runtime exceptions, but I can do a check after the exception is caught if it is a ParseException (2.4) or AnalysisException(3.0) and rethrow if it isn't.", "author": "RussellSpitzer", "createdAt": "2020-11-18T16:06:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTIxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkyNg==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525622926", "bodyText": "This logic is just for Spark 2.x right? If so then maybe we can add a comment explaining it and noting when we can remove it.", "author": "rdblue", "createdAt": "2020-11-18T00:51:03Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog\n+      return noCatalogReader.load(tableName + \"#\" + type);\n+    } else {\n+      // Try catalog based name based resolution\n+      try {\n+        return spark.table(tableName + \".\" + type);\n+      } catch (Exception e) {\n+        // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n+        if (tableName.startsWith(\"hadoop.\")) {", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMjEyMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525632123", "bodyText": "Anton I were talking about this, the issue is that it isn't just for SparkV2 as it's quite possible for a user to be using manual catalogs in a Spark3 application (or ported Spark2 application.) If we remove this we break anyone who is accessing tables by this method.\nI think that's actually something that's fine to break eventually but I think it would be pretty big breaking change.\nExample\nActions.forTable(tableCreatedFromHadoopOrHiveCatalog)", "author": "RussellSpitzer", "createdAt": "2020-11-18T01:18:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzNDIyMQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525634221", "bodyText": "Sounds reasonable to me.", "author": "rdblue", "createdAt": "2020-11-18T01:25:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkyNg=="}], "type": "inlineReview"}, {"oid": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "url": "https://github.com/apache/iceberg/commit/1e989b78a3f284415de9cacf8b95bb7e8da248e7", "message": "Minor changes based on reviewer comments", "committedDate": "2020-11-18T16:41:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5NDA2MA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r526294060", "bodyText": "I think it should be fine to add it to our exceptions in checkstyle.xml. I thought it was already there.", "author": "aokolnychyi", "createdAt": "2020-11-18T17:42:08Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -24,21 +24,67 @@\n import org.apache.iceberg.BaseTable;\n import org.apache.iceberg.ManifestFiles;\n import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.StaticTableOperations;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.io.ClosingIterator;\n import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkUtil;\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.api.java.function.FlatMapFunction;\n import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.DataFrameReader;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+\n+// CHECKSTYLE:OFF\n+import static org.apache.iceberg.MetadataTableType.ALL_MANIFESTS;", "originalCommit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5NDc0NQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r526294745", "bodyText": "same here", "author": "aokolnychyi", "createdAt": "2020-11-18T17:43:11Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -67,6 +66,10 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+// CHECKSTYLE:OFF\n+import static org.apache.iceberg.MetadataTableType.ENTRIES;", "originalCommit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ff4f8fb8e8d3cbfd7406e650f1b0a1ea4253e269", "url": "https://github.com/apache/iceberg/commit/ff4f8fb8e8d3cbfd7406e650f1b0a1ea4253e269", "message": "Sort StaticImport Exclusions and Add MetadataTableType.*", "committedDate": "2020-11-18T18:05:59Z", "type": "commit"}]}