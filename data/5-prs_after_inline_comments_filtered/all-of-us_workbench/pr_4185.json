{"pr_number": 4185, "pr_title": "[RW-5045][risk=no] Hail Microarray Demo codegen", "pr_createdAt": "2020-10-19T14:58:21Z", "pr_url": "https://github.com/all-of-us/workbench/pull/4185", "timeline": [{"oid": "c6185bff23ee1272c3312375d8cd0932ed9a7420", "url": "https://github.com/all-of-us/workbench/commit/c6185bff23ee1272c3312375d8cd0932ed9a7420", "message": "add Hail codegen", "committedDate": "2020-10-19T14:38:27Z", "type": "commit"}, {"oid": "e6de1325b9540e32dcf4bde52b2d584fd07ab073", "url": "https://github.com/all-of-us/workbench/commit/e6de1325b9540e32dcf4bde52b2d584fd07ab073", "message": "revert test changes", "committedDate": "2020-10-19T14:46:53Z", "type": "commit"}, {"oid": "54623c35cee9a77b8e6ccd70c48f3fc9f1f8e135", "url": "https://github.com/all-of-us/workbench/commit/54623c35cee9a77b8e6ccd70c48f3fc9f1f8e135", "message": "remove unused line", "committedDate": "2020-10-19T14:48:03Z", "type": "commit"}, {"oid": "7a5b3d91ffe48c99fc05fcc5012f29b81b7f1deb", "url": "https://github.com/all-of-us/workbench/commit/7a5b3d91ffe48c99fc05fcc5012f29b81b7f1deb", "message": "flip bool", "committedDate": "2020-10-19T14:49:13Z", "type": "commit"}, {"oid": "fbe99ae106e6d713bd9624c9a80106d27e752648", "url": "https://github.com/all-of-us/workbench/commit/fbe99ae106e6d713bd9624c9a80106d27e752648", "message": "Merge branch 'master' of github.com:all-of-us/workbench into songe/RW-5045-2", "committedDate": "2020-10-19T17:56:42Z", "type": "commit"}, {"oid": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb", "url": "https://github.com/all-of-us/workbench/commit/5d33e62b4a74cef08c417a76bf173b4bd06a63cb", "message": "change project id arg", "committedDate": "2020-10-19T18:43:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MTYwNQ==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508051605", "bodyText": ".tsv?", "author": "calbach", "createdAt": "2020-10-19T20:46:03Z", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";", "originalCommit": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY2MTg3Mg==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508661872", "bodyText": "The Hail tutorial had it as \".txt\" but tsv makes sense.", "author": "ericsong", "createdAt": "2020-10-20T16:14:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MTYwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MjA3NQ==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508052075", "bodyText": "Please spell it out in the variable name", "author": "calbach", "createdAt": "2020-10-19T20:46:59Z", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";\n+    final String cohortQualifier = \"cohort_\" + qualifier;\n+    final String cohortVcfFilename = cohortQualifier + \".vcf\";\n+    final String cohortMtFilename = cohortQualifier + \".mt\";", "originalCommit": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MjYwNA==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508052604", "bodyText": "Why?", "author": "calbach", "createdAt": "2020-10-19T20:48:01Z", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";\n+    final String cohortQualifier = \"cohort_\" + qualifier;\n+    final String cohortVcfFilename = cohortQualifier + \".vcf\";\n+    final String cohortMtFilename = cohortQualifier + \".mt\";\n+\n+    return ImmutableList.of(\n+        \"import subprocess, os\\n\"\n+            + \"import random\\n\"\n+            + \"\\n\"\n+            + \"# Creating phenotype annotations file\\n\"\n+            + \"phenotypes_table = []\\n\"\n+            + \"for person_id in person_ids:\\n\"\n+            + \"    person_id = person_id\\n\"", "originalCommit": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY2MjAzNQ==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508662035", "bodyText": "oops. leftover from Plink", "author": "ericsong", "createdAt": "2020-10-20T16:14:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MjYwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA1MzAzNA==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508053034", "bodyText": "If covariates is something the user should expect to be modifying, then please comment or pull out into a variable", "author": "calbach", "createdAt": "2020-10-19T20:48:51Z", "path": "api/src/main/java/org/pmiops/workbench/dataset/DataSetServiceImpl.java", "diffHunk": "@@ -815,6 +815,58 @@ private String getQualifiedColumnName(Domain currentDomain, String columnName) {\n             + \"head results.P2.assoc\");\n   }\n \n+  @Override\n+  public List<String> generateHailDemoCode(String qualifier) {\n+    final String phenotypeFilename = \"phenotypes_annotations_\" + qualifier + \".txt\";\n+    final String cohortQualifier = \"cohort_\" + qualifier;\n+    final String cohortVcfFilename = cohortQualifier + \".vcf\";\n+    final String cohortMtFilename = cohortQualifier + \".mt\";\n+\n+    return ImmutableList.of(\n+        \"import subprocess, os\\n\"\n+            + \"import random\\n\"\n+            + \"\\n\"\n+            + \"# Creating phenotype annotations file\\n\"\n+            + \"phenotypes_table = []\\n\"\n+            + \"for person_id in person_ids:\\n\"\n+            + \"    person_id = person_id\\n\"\n+            + \"    phenotype_1 = random.randint(0, 2) # Change this value to what makes sense for your research by looking through the dataset(s)\\n\"\n+            + \"    phenotype_2 = random.randint(0, 2) # Change this value as well or remove if you are only processing one phenotype \\n\"\n+            + \"    phenotypes_table.append([person_id, phenotype_1, phenotype_2])\\n\"\n+            + \"\\n\"\n+            + \"cohort_phenotypes = pandas.DataFrame(phenotypes_table,columns=[\\\"sample_name\\\", \\\"phenotype1\\\", \\\"phenotype2\\\"]) \\n\"\n+            + \"cohort_phenotypes.to_csv('\"\n+            + phenotypeFilename\n+            + \"', index=False, sep='\\\\t')\\n\"\n+            + \"\\n\"\n+            + \"subprocess.run([\\\"gsutil\\\", \\\"cp\\\", \\\"\"\n+            + phenotypeFilename\n+            + \"\\\", os.environ['WORKSPACE_BUCKET']])\",\n+        \"import hail as hl\\n\"\n+            + \"import os\\n\"\n+            + \"from hail.plot import show\\n\"\n+            + \"\\n\"\n+            + \"hl.plot.output_notebook()\\n\"\n+            + \"bucket = os.environ['WORKSPACE_BUCKET']\\n\"\n+            + \"hl.import_vcf(f'{bucket}/\"\n+            + cohortVcfFilename\n+            + \"').write(f'{bucket}/\"\n+            + cohortMtFilename\n+            + \"')\\n\"\n+            + \"table = hl.import_table(f'{bucket}/\"\n+            + phenotypeFilename\n+            + \"', types={'sample_name': hl.tstr}, impute=True, key='sample_name')\\n\"\n+            + \"\\n\"\n+            + \"mt = hl.read_matrix_table(f'{bucket}/\"\n+            + cohortMtFilename\n+            + \"');\\n\"\n+            + \"mt = mt.annotate_cols(pheno = table[mt.s])\\n\"\n+            + \"\\n\"\n+            + \"gwas = hl.linear_regression_rows(y=mt.pheno.phenotype1, x=mt.GT.n_alt_alleles(), covariates=[1.0])\\n\"", "originalCommit": "5d33e62b4a74cef08c417a76bf173b4bd06a63cb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1d32e360eefa1c371ebe2be044158164d952bf6c", "url": "https://github.com/all-of-us/workbench/commit/1d32e360eefa1c371ebe2be044158164d952bf6c", "message": "code review", "committedDate": "2020-10-20T15:00:18Z", "type": "commit"}, {"oid": "b4580de882fe131020465a6a4013c8e73b719a3f", "url": "https://github.com/all-of-us/workbench/commit/b4580de882fe131020465a6a4013c8e73b719a3f", "message": "add microarray codegen tests", "committedDate": "2020-10-20T16:13:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5MDkwNQ==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508690905", "bodyText": "opt: if you wanted to use streams here, you could use flatMap to expand the inner arrays. Fine as currently written though.", "author": "calbach", "createdAt": "2020-10-20T16:57:35Z", "path": "api/src/test/java/org/pmiops/workbench/api/DataSetControllerTest.java", "diffHunk": "@@ -739,4 +732,169 @@ public void testGetValuesFromDomain() {\n         .containsExactly(\n             new DomainValue().value(\"field_one\"), new DomainValue().value(\"field_two\"));\n   }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_cdrCheck() {\n+    DbCdrVersion cdrVersion =\n+        cdrVersionDao.findByCdrVersionId(Long.parseLong(workspace.getCdrVersionId()));\n+    cdrVersion.setMicroarrayBigqueryDataset(null);\n+    cdrVersionDao.save(cdrVersion);\n+\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest().genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    FailedPreconditionException e =\n+        assertThrows(\n+            FailedPreconditionException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e)\n+        .hasMessageThat()\n+        .contains(\"The workspace CDR version does not have microarray data\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_kernelCheck() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .kernelType(KernelTypeEnum.R)\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY);\n+\n+    BadRequestException e =\n+        assertThrows(\n+            BadRequestException.class,\n+            () ->\n+                dataSetController.exportToNotebook(\n+                    workspace.getNamespace(), WORKSPACE_NAME, request));\n+    assertThat(e).hasMessageThat().contains(\"Genomics code generation is only supported in Python\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_noGenomicsTool() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.NONE);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(3);\n+    assertThat(codeCells.get(2)).contains(\"raw_array_cohort_extract.py\");\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(2)).contains(\"gsutil cp\");\n+  }\n+\n+  @Test\n+  public void exportToNotebook_microarrayCodegen_plink() {\n+    DataSetExportRequest request =\n+        setUpValidDataSetExportRequest()\n+            .genomicsDataType(GenomicsDataTypeEnum.MICROARRAY)\n+            .genomicsAnalysisTool(GenomicsAnalysisToolEnum.PLINK);\n+\n+    dataSetController.exportToNotebook(workspace.getNamespace(), WORKSPACE_NAME, request);\n+\n+    verify(mockNotebooksService, times(1))\n+        .saveNotebook(\n+            eq(WORKSPACE_BUCKET_NAME),\n+            eq(request.getNotebookName()),\n+            notebookContentsCaptor.capture());\n+\n+    List<String> codeCells = notebookContentsToStrings(notebookContentsCaptor.getValue());\n+\n+    assertThat(codeCells.size()).isEqualTo(5);\n+    assertThat(codeCells.get(2)).contains(\"gatk ArrayExtractCohort\");\n+    assertThat(codeCells.get(3)).contains(\"cohort_phenotypes.to_csv\");\n+    assertThat(codeCells.get(3)).contains(\".phe\");\n+    assertThat(codeCells.get(4)).contains(\"plink\");\n+  }\n+\n+  List<String> notebookContentsToStrings(JSONObject notebookContents) {\n+    List<String> codeCellStrings = new ArrayList<>();\n+\n+    JSONArray cells = notebookContents.getJSONArray(\"cells\");\n+    for (int i = 0; i < cells.length(); i++) {\n+      String cellString = \"\";\n+      JSONArray innerCells = cells.getJSONObject(i).getJSONArray(\"source\");", "originalCommit": "b4580de882fe131020465a6a4013c8e73b719a3f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc4NjcxMA==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508786710", "bodyText": "I tried using streams but the issue I ran into is that the JSONArray type isn't iterable and it doesn't easily translate to streams. Would flatmap fix that issue?", "author": "ericsong", "createdAt": "2020-10-20T19:32:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5MDkwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODc4OTczMQ==", "url": "https://github.com/all-of-us/workbench/pull/4185#discussion_r508789731", "bodyText": "Ah I see. Probably not. You'd probably need to do something like:\n.flatMap(c -> StreamSupport.stream(\n          Spliterators.spliteratorUnknownSize(c.iterator()));\n\nwhich is already getting pretty hairy. I'd leave it as you have it, in this case", "author": "calbach", "createdAt": "2020-10-20T19:38:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5MDkwNQ=="}], "type": "inlineReview"}]}