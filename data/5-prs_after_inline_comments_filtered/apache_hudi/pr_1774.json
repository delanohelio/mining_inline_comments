{"pr_number": 1774, "pr_title": "[HUDI-703]Add unit test for HoodieSyncCommand", "pr_createdAt": "2020-06-30T04:51:24Z", "pr_url": "https://github.com/apache/hudi/pull/1774", "timeline": [{"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "url": "https://github.com/apache/hudi/commit/92f36ae11a8bb52e5e068cf565da95fbd6f29906", "message": "fix", "committedDate": "2020-07-16T14:14:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI3ODcyOQ==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457278729", "bodyText": "Good catch!", "author": "yanghua", "createdAt": "2020-07-20T10:57:20Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieSyncCommand.java", "diffHunk": "@@ -74,9 +74,9 @@ public String validateSync(\n     }\n \n     String targetLatestCommit =\n-        targetTimeline.getInstants().iterator().hasNext() ? \"0\" : targetTimeline.lastInstant().get().getTimestamp();\n+        targetTimeline.getInstants().iterator().hasNext() ? targetTimeline.lastInstant().get().getTimestamp() : \"0\";", "originalCommit": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4MjE3MA==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457282170", "bodyText": "Can we define a default constant for this?", "author": "yanghua", "createdAt": "2020-07-20T11:02:58Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {\n+      // Drop Table if it exists\n+      try {\n+        dropHiveTables(hiveTableName, tableType);\n+      } catch (AssertionError ex) {\n+        // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+        // Workaround to sleep for 5 secs and retry\n+        // Set sleep time by hoodie.hiveserver.time.wait\n+        Thread.sleep(getTimeWait());\n+        dropHiveTables(hiveTableName, tableType);\n+      }\n+\n+      // Ensure table does not exist\n+      stdOutErr = executeHiveCommand(\"show tables like '\" + hiveTableName + \"'\");\n+      assertTrue(stdOutErr.getLeft().isEmpty(), \"Dropped table \" + hiveTableName + \" exists!\");\n+    }\n+\n+    // Run Hoodie Java App\n+    String cmd = String.format(\"%s %s --hive-sync --table-path %s  --hive-url %s  --table-type %s  --hive-table %s\" +\n+        \" --commit-type %s  --table-name %s\", HOODIE_JAVA_APP, \"HoodieJavaGenerateApp\", hdfsUrl, HIVE_SERVER_JDBC_URL,\n+        tableType, hiveTableName, commitType, hoodieTableName);\n+    if (partitionType == PartitionType.MULTI_KEYS_PARTITIONED) {\n+      cmd = cmd + \" --use-multi-partition-keys\";\n+    } else if (partitionType == PartitionType.NON_PARTITIONED){\n+      cmd = cmd + \" --non-partitioned\";\n+    }\n+    executeCommandStringInDocker(ADHOC_1_CONTAINER, cmd, true);\n+\n+    String snapshotTableName = getSnapshotTableName(tableType, hiveTableName);\n+\n+    // Ensure table does exist\n+    stdOutErr = executeHiveCommand(\"show tables like '\" + snapshotTableName + \"'\");\n+    assertEquals(snapshotTableName, stdOutErr.getLeft(), \"Table exists\");\n+  }\n+\n+  protected void dropHiveTables(String hiveTableName, String tableType) throws Exception {\n+    if (tableType.equals(HoodieTableType.MERGE_ON_READ.name())) {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_rt\");\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_ro\");\n+    } else {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName);\n+    }\n+  }\n+\n+  protected String getHdfsPath(String hiveTableName) {\n+    return \"/\" + hiveTableName;\n+  }\n+\n+  protected String getSnapshotTableName(String tableType, String hiveTableName) {\n+    return tableType.equals(HoodieTableType.MERGE_ON_READ.name())\n+        ? hiveTableName + \"_rt\" : hiveTableName;\n+  }\n+\n+  private int getTimeWait() {\n+    int timeWait = 5000;", "originalCommit": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4MzEzMg==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457283132", "bodyText": "I would not suggest using assertation in normal methods. We can throw an exception directly. WDYT?", "author": "yanghua", "createdAt": "2020-07-20T11:04:35Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {\n+      // Drop Table if it exists\n+      try {\n+        dropHiveTables(hiveTableName, tableType);\n+      } catch (AssertionError ex) {\n+        // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+        // Workaround to sleep for 5 secs and retry\n+        // Set sleep time by hoodie.hiveserver.time.wait\n+        Thread.sleep(getTimeWait());\n+        dropHiveTables(hiveTableName, tableType);\n+      }\n+\n+      // Ensure table does not exist\n+      stdOutErr = executeHiveCommand(\"show tables like '\" + hiveTableName + \"'\");\n+      assertTrue(stdOutErr.getLeft().isEmpty(), \"Dropped table \" + hiveTableName + \" exists!\");", "originalCommit": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4NDA0Nw==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457284047", "bodyText": "Can we define a constant for multiple usages? IMO, hard code is not a good practice.", "author": "yanghua", "createdAt": "2020-07-20T11:06:10Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {", "originalCommit": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4NTg5Mw==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457285893", "bodyText": "getHDFSPath looks better?", "author": "yanghua", "createdAt": "2020-07-20T11:09:19Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {\n+      // Drop Table if it exists\n+      try {\n+        dropHiveTables(hiveTableName, tableType);\n+      } catch (AssertionError ex) {\n+        // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+        // Workaround to sleep for 5 secs and retry\n+        // Set sleep time by hoodie.hiveserver.time.wait\n+        Thread.sleep(getTimeWait());\n+        dropHiveTables(hiveTableName, tableType);\n+      }\n+\n+      // Ensure table does not exist\n+      stdOutErr = executeHiveCommand(\"show tables like '\" + hiveTableName + \"'\");\n+      assertTrue(stdOutErr.getLeft().isEmpty(), \"Dropped table \" + hiveTableName + \" exists!\");\n+    }\n+\n+    // Run Hoodie Java App\n+    String cmd = String.format(\"%s %s --hive-sync --table-path %s  --hive-url %s  --table-type %s  --hive-table %s\" +\n+        \" --commit-type %s  --table-name %s\", HOODIE_JAVA_APP, \"HoodieJavaGenerateApp\", hdfsUrl, HIVE_SERVER_JDBC_URL,\n+        tableType, hiveTableName, commitType, hoodieTableName);\n+    if (partitionType == PartitionType.MULTI_KEYS_PARTITIONED) {\n+      cmd = cmd + \" --use-multi-partition-keys\";\n+    } else if (partitionType == PartitionType.NON_PARTITIONED){\n+      cmd = cmd + \" --non-partitioned\";\n+    }\n+    executeCommandStringInDocker(ADHOC_1_CONTAINER, cmd, true);\n+\n+    String snapshotTableName = getSnapshotTableName(tableType, hiveTableName);\n+\n+    // Ensure table does exist\n+    stdOutErr = executeHiveCommand(\"show tables like '\" + snapshotTableName + \"'\");\n+    assertEquals(snapshotTableName, stdOutErr.getLeft(), \"Table exists\");\n+  }\n+\n+  protected void dropHiveTables(String hiveTableName, String tableType) throws Exception {\n+    if (tableType.equals(HoodieTableType.MERGE_ON_READ.name())) {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_rt\");\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_ro\");\n+    } else {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName);\n+    }\n+  }\n+\n+  protected String getHdfsPath(String hiveTableName) {", "originalCommit": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4NzczMw==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457287733", "bodyText": "Using StringBuilder  to build cmd is a better choice?", "author": "yanghua", "createdAt": "2020-07-20T11:12:17Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/command/ITTestHoodieSyncCommand.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.command;\n+\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieTableType;\n+\n+import org.apache.hudi.integ.HoodieTestHiveBase;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for HoodieSyncCommand in hudi-cli module.\n+ */\n+public class ITTestHoodieSyncCommand extends HoodieTestHiveBase {\n+\n+  private static final String HUDI_CLI_TOOL = HOODIE_WS_ROOT + \"/hudi-cli/hudi-cli.sh\";\n+  private static final String SYNC_VALIDATE_COMMANDS = HOODIE_WS_ROOT + \"/docker/demo/sync-validate.commands\";\n+\n+  @Test\n+  public void testValidateSync() throws Exception {\n+    String hiveTableName = \"docker_hoodie_sync_valid_test\";\n+    String hiveTableName2 = \"docker_hoodie_sync_valid_test_2\";\n+\n+    generateDataByHoodieJavaApp(\n+        hiveTableName, HoodieTableType.COPY_ON_WRITE.name(), PartitionType.SINGLE_KEY_PARTITIONED, \"overwrite\", hiveTableName);\n+\n+    syncHoodieTable(hiveTableName2, \"INSERT\");\n+\n+    generateDataByHoodieJavaApp(\n+        hiveTableName, HoodieTableType.COPY_ON_WRITE.name(), PartitionType.SINGLE_KEY_PARTITIONED, \"append\", hiveTableName);\n+\n+    TestExecStartResultCallback result =\n+        executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + \" --cmdfile \" + SYNC_VALIDATE_COMMANDS, true);\n+\n+    String expected = String.format(\"Count difference now is (count(%s) - count(%s) == %d. Catch up count is %d\",\n+        hiveTableName, hiveTableName2, 100, 200);\n+    assertTrue(result.getStderr().toString().contains(expected));\n+\n+    dropHiveTables(hiveTableName, HoodieTableType.COPY_ON_WRITE.name());\n+    dropHiveTables(hiveTableName2, HoodieTableType.COPY_ON_WRITE.name());\n+  }\n+\n+  private void syncHoodieTable(String hiveTableName, String op) throws Exception {\n+    String cmd = \"spark-submit --packages org.apache.spark:spark-avro_2.11:2.4.4 \"", "originalCommit": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4OTAyNw==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457289027", "bodyText": "HoodieJavaGenerateApp?", "author": "yanghua", "createdAt": "2020-07-20T11:14:31Z", "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hive.MultiPartKeysValueExtractor;\n+import org.apache.hudi.hive.NonPartitionedExtractor;\n+import org.apache.hudi.keygen.NonpartitionedKeyGenerator;\n+import org.apache.hudi.keygen.SimpleKeyGenerator;\n+import org.apache.hudi.testutils.DataSourceTestUtils;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieJavaGenerateApp {\n+  @Parameter(names = {\"--table-path\", \"-p\"}, description = \"path for Hoodie sample table\")\n+  private String tablePath = \"file:///tmp/hoodie/sample-table\";\n+\n+  @Parameter(names = {\"--table-name\", \"-n\"}, description = \"table name for Hoodie sample table\")\n+  private String tableName = \"hoodie_test\";\n+\n+  @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();\n+\n+  @Parameter(names = {\"--hive-sync\", \"-hv\"}, description = \"Enable syncing to hive\")\n+  private Boolean enableHiveSync = false;\n+\n+  @Parameter(names = {\"--hive-db\", \"-hd\"}, description = \"hive database\")\n+  private String hiveDB = \"default\";\n+\n+  @Parameter(names = {\"--hive-table\", \"-ht\"}, description = \"hive table\")\n+  private String hiveTable = \"hoodie_sample_test\";\n+\n+  @Parameter(names = {\"--hive-user\", \"-hu\"}, description = \"hive username\")\n+  private String hiveUser = \"hive\";\n+\n+  @Parameter(names = {\"--hive-password\", \"-hp\"}, description = \"hive password\")\n+  private String hivePass = \"hive\";\n+\n+  @Parameter(names = {\"--hive-url\", \"-hl\"}, description = \"hive JDBC URL\")\n+  private String hiveJdbcUrl = \"jdbc:hive2://localhost:10000\";\n+\n+  @Parameter(names = {\"--non-partitioned\", \"-np\"}, description = \"Use non-partitioned Table\")\n+  private Boolean nonPartitionedTable = false;\n+\n+  @Parameter(names = {\"--use-multi-partition-keys\", \"-mp\"}, description = \"Use Multiple Partition Keys\")\n+  private Boolean useMultiPartitionKeys = false;\n+\n+  @Parameter(names = {\"--commit-type\", \"-ct\"}, description = \"How may commits will run\")\n+  private String commitType = \"overwrite\";\n+\n+  @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+  public Boolean help = false;\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieJavaApp.class);", "originalCommit": "92f36ae11a8bb52e5e068cf565da95fbd6f29906", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTM3NjA1Nw==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r459376057", "bodyText": "Why it is not -hs? Does it cause conflicts?", "author": "yanghua", "createdAt": "2020-07-23T11:15:02Z", "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hive.MultiPartKeysValueExtractor;\n+import org.apache.hudi.hive.NonPartitionedExtractor;\n+import org.apache.hudi.keygen.NonpartitionedKeyGenerator;\n+import org.apache.hudi.keygen.SimpleKeyGenerator;\n+import org.apache.hudi.testutils.DataSourceTestUtils;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieJavaGenerateApp {\n+  @Parameter(names = {\"--table-path\", \"-p\"}, description = \"path for Hoodie sample table\")\n+  private String tablePath = \"file:///tmp/hoodie/sample-table\";\n+\n+  @Parameter(names = {\"--table-name\", \"-n\"}, description = \"table name for Hoodie sample table\")\n+  private String tableName = \"hoodie_test\";\n+\n+  @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();\n+\n+  @Parameter(names = {\"--hive-sync\", \"-hv\"}, description = \"Enable syncing to hive\")", "originalCommit": "1b9c5ba145ab010706677d962472f4e8e5287974", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTM3NjcwNw==", "url": "https://github.com/apache/hudi/pull/1774#discussion_r459376707", "bodyText": "Let's apply a unified rule about the first latter of the description?", "author": "yanghua", "createdAt": "2020-07-23T11:16:28Z", "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hive.MultiPartKeysValueExtractor;\n+import org.apache.hudi.hive.NonPartitionedExtractor;\n+import org.apache.hudi.keygen.NonpartitionedKeyGenerator;\n+import org.apache.hudi.keygen.SimpleKeyGenerator;\n+import org.apache.hudi.testutils.DataSourceTestUtils;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieJavaGenerateApp {\n+  @Parameter(names = {\"--table-path\", \"-p\"}, description = \"path for Hoodie sample table\")\n+  private String tablePath = \"file:///tmp/hoodie/sample-table\";\n+\n+  @Parameter(names = {\"--table-name\", \"-n\"}, description = \"table name for Hoodie sample table\")\n+  private String tableName = \"hoodie_test\";\n+\n+  @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();\n+\n+  @Parameter(names = {\"--hive-sync\", \"-hv\"}, description = \"Enable syncing to hive\")\n+  private Boolean enableHiveSync = false;\n+\n+  @Parameter(names = {\"--hive-db\", \"-hd\"}, description = \"hive database\")\n+  private String hiveDB = \"default\";\n+\n+  @Parameter(names = {\"--hive-table\", \"-ht\"}, description = \"hive table\")\n+  private String hiveTable = \"hoodie_sample_test\";\n+\n+  @Parameter(names = {\"--hive-user\", \"-hu\"}, description = \"hive username\")", "originalCommit": "1b9c5ba145ab010706677d962472f4e8e5287974", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e2e7ec86a15a82397d45ead08e76405fe83ff22d", "url": "https://github.com/apache/hudi/commit/e2e7ec86a15a82397d45ead08e76405fe83ff22d", "message": "[HUDI-703]Add test for HoodieSyncCommand", "committedDate": "2020-07-27T09:03:02Z", "type": "commit"}, {"oid": "e2e7ec86a15a82397d45ead08e76405fe83ff22d", "url": "https://github.com/apache/hudi/commit/e2e7ec86a15a82397d45ead08e76405fe83ff22d", "message": "[HUDI-703]Add test for HoodieSyncCommand", "committedDate": "2020-07-27T09:03:02Z", "type": "forcePushed"}]}