{"pr_number": 1554, "pr_title": "[HUDI-704]Add test for RepairsCommand", "pr_createdAt": "2020-04-23T02:23:26Z", "pr_url": "https://github.com/apache/hudi/pull/1554", "timeline": [{"oid": "22955d0e28152138d32be764265fbbd227a4122f", "url": "https://github.com/apache/hudi/commit/22955d0e28152138d32be764265fbbd227a4122f", "message": "Add test for RepairsCommand rebase to master", "committedDate": "2020-05-04T02:31:50Z", "type": "commit"}, {"oid": "22955d0e28152138d32be764265fbbd227a4122f", "url": "https://github.com/apache/hudi/commit/22955d0e28152138d32be764265fbbd227a4122f", "message": "Add test for RepairsCommand rebase to master", "committedDate": "2020-05-04T02:31:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NDkyMA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420784920", "bodyText": "\"Spark Master \" -> \"Spark Master\"?", "author": "yanghua", "createdAt": "2020-05-06T13:22:01Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,19 +69,35 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          unspecifiedDefaultValue = \"\") String sparkPropertiesPath,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NTc1OQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420785759", "bodyText": "The same suggestion, we should try to define a data structure? We can refactor it later.", "author": "yanghua", "createdAt": "2020-05-06T13:23:10Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,19 +69,35 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          unspecifiedDefaultValue = \"\") String sparkPropertiesPath,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,\n+      @CliOption(key = \"sparkMemory\", unspecifiedDefaultValue = \"4G\",\n+          help = \"Spark executor memory\") final String sparkMemory,\n+      @CliOption(key = {\"dryrun\"},\n+          help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n       throws Exception {\n+    if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n+      sparkPropertiesPath =\n+          Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n+    }\n+\n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n-    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), duplicatedPartitionPath, repairedOutputPath,\n-        HoodieCLI.getTableMetaClient().getBasePath());\n+    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM4NzU4OA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r421387588", "bodyText": "The same suggestion, we should try to define a data structure? We can refactor it later.\n\nWe can focus on PR(#1174), but it was left behind for too long.", "author": "hddong", "createdAt": "2020-05-07T10:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NTc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNDQxNA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r421734414", "bodyText": "@hddong Yeah its been open for some time now. The work was mostly done, I was stuck at fixing test cases. Will take a look at it soon. :)", "author": "pratyakshsharma", "createdAt": "2020-05-07T19:15:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NTc1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NjgzMQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420786831", "bodyText": "IMHO, we also need to refactor the arg parse. But not in this PR.", "author": "yanghua", "createdAt": "2020-05-06T13:24:39Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -73,8 +73,8 @@ public static void main(String[] args) throws Exception {\n         returnCode = rollback(jsc, args[1], args[2]);\n         break;\n       case DEDUPLICATE:\n-        assert (args.length == 4);\n-        returnCode = deduplicatePartitionPath(jsc, args[1], args[2], args[3]);\n+        assert (args.length == 7);\n+        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], args[6]);", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc5MDQ0Ng==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420790446", "bodyText": "Can we use String.format(xxx) here?", "author": "yanghua", "createdAt": "2020-05-06T13:29:33Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ * <p/>\n+ * A command use SparkLauncher need load jars under lib which generate during mvn package.\n+ * Use integration test instead of unit test.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String duplicatedPartitionPath;\n+  private String repairedOutputPath;\n+\n+  @BeforeEach\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    Files.createFile(Paths.get(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken)));\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc5MDU5MQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420790591", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-05-06T13:29:44Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ * <p/>\n+ * A command use SparkLauncher need load jars under lib which generate during mvn package.\n+ * Use integration test instead of unit test.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String duplicatedPartitionPath;\n+  private String repairedOutputPath;\n+\n+  @BeforeEach\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    Files.createFile(Paths.get(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken)));\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "95b9e09a708f778262ba0040d651f408e9d6a3d1", "url": "https://github.com/apache/hudi/commit/95b9e09a708f778262ba0040d651f408e9d6a3d1", "message": "fix", "committedDate": "2020-05-07T09:58:47Z", "type": "commit"}]}