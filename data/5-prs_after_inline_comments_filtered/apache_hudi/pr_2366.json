{"pr_number": 2366, "pr_title": "[HUDI-1312] [RFC-15] Support for metadata listing for snapshot queries through Hive/SparkSQL", "pr_createdAt": "2020-12-22T18:10:57Z", "pr_url": "https://github.com/apache/hudi/pull/2366", "timeline": [{"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585", "url": "https://github.com/apache/hudi/commit/0d0ad7405b7d170684ddb76725a9a75aa4061585", "message": "[RFC-15] Support for metadata listing for snapshot queries through Hive/SparkSQL", "committedDate": "2020-12-22T09:46:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NjgzMw==", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547566833", "bodyText": "hmmm. slightly orthogonal, but the HoodieBaseFile itself should hand us a FileStatus object right? we should probably rethink the need for refreshing file status.", "author": "vinothchandar", "createdAt": "2020-12-23T00:10:56Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -391,27 +397,48 @@ public static FileStatus getFileStatus(HoodieBaseFile baseFile) throws IOExcepti\n     return grouped;\n   }\n \n+  public static Map<HoodieTableMetaClient, List<Path>> groupSnapshotPathsByMetaClient(\n+          Collection<HoodieTableMetaClient> metaClientList,\n+          List<Path> snapshotPaths\n+  ) {\n+    Map<HoodieTableMetaClient, List<Path>> grouped = new HashMap<>();\n+    metaClientList.forEach(metaClient -> grouped.put(metaClient, new ArrayList<>()));\n+    for (Path path : snapshotPaths) {\n+      // Find meta client associated with the input path\n+      metaClientList.stream().filter(metaClient -> path.toString().contains(metaClient.getBasePath()))\n+              .forEach(metaClient -> grouped.get(metaClient).add(path));\n+    }\n+    return grouped;\n+  }\n+\n   /**\n-   * Filters data files for a snapshot queried table.\n+   * Filters data files under @param paths for a snapshot queried table.\n    * @param job\n-   * @param metadata\n-   * @param fileStatuses\n+   * @param metaClient\n+   * @param paths\n    * @return\n    */\n   public static List<FileStatus> filterFileStatusForSnapshotMode(\n-      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) throws IOException {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+          JobConf job, HoodieTableMetaClient metaClient, List<Path> paths) throws IOException {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metaClient);\n     }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+\n+    boolean useFileListingFromMetadata = job.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = job.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    HoodieTableFileSystemView fsView = FileSystemViewManager.createInMemoryFileSystemView(metaClient,\n+            useFileListingFromMetadata, verifyFileListing);\n+\n+    List<HoodieBaseFile> filteredBaseFiles = new ArrayList<>();\n+    for (Path p : paths) {\n+      String relativePartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), p);\n+      List<HoodieBaseFile> matched = fsView.getLatestBaseFiles(relativePartitionPath).collect(Collectors.toList());\n+      filteredBaseFiles.addAll(matched);\n+    }\n+\n+    LOG.info(\"Total paths to process after hoodie filter \" + filteredBaseFiles.size());\n     List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n+    for (HoodieBaseFile filteredFile : filteredBaseFiles) {", "originalCommit": "0d0ad7405b7d170684ddb76725a9a75aa4061585", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NzA2MQ==", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547567061", "bodyText": "note to self: doing this by path is ok, since the FileSystemView internally caches per partition.", "author": "vinothchandar", "createdAt": "2020-12-23T00:11:51Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -391,27 +397,48 @@ public static FileStatus getFileStatus(HoodieBaseFile baseFile) throws IOExcepti\n     return grouped;\n   }\n \n+  public static Map<HoodieTableMetaClient, List<Path>> groupSnapshotPathsByMetaClient(\n+          Collection<HoodieTableMetaClient> metaClientList,\n+          List<Path> snapshotPaths\n+  ) {\n+    Map<HoodieTableMetaClient, List<Path>> grouped = new HashMap<>();\n+    metaClientList.forEach(metaClient -> grouped.put(metaClient, new ArrayList<>()));\n+    for (Path path : snapshotPaths) {\n+      // Find meta client associated with the input path\n+      metaClientList.stream().filter(metaClient -> path.toString().contains(metaClient.getBasePath()))\n+              .forEach(metaClient -> grouped.get(metaClient).add(path));\n+    }\n+    return grouped;\n+  }\n+\n   /**\n-   * Filters data files for a snapshot queried table.\n+   * Filters data files under @param paths for a snapshot queried table.\n    * @param job\n-   * @param metadata\n-   * @param fileStatuses\n+   * @param metaClient\n+   * @param paths\n    * @return\n    */\n   public static List<FileStatus> filterFileStatusForSnapshotMode(\n-      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) throws IOException {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+          JobConf job, HoodieTableMetaClient metaClient, List<Path> paths) throws IOException {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metaClient);\n     }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+\n+    boolean useFileListingFromMetadata = job.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = job.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    HoodieTableFileSystemView fsView = FileSystemViewManager.createInMemoryFileSystemView(metaClient,\n+            useFileListingFromMetadata, verifyFileListing);\n+\n+    List<HoodieBaseFile> filteredBaseFiles = new ArrayList<>();\n+    for (Path p : paths) {\n+      String relativePartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), p);\n+      List<HoodieBaseFile> matched = fsView.getLatestBaseFiles(relativePartitionPath).collect(Collectors.toList());", "originalCommit": "0d0ad7405b7d170684ddb76725a9a75aa4061585", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NzcyNA==", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547567724", "bodyText": "makes sense", "author": "vinothchandar", "createdAt": "2020-12-23T00:14:37Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -63,13 +69,25 @@\n     // TODO(vc): Should we handle also non-hoodie splits here?\n     Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());\n \n+    boolean useFileListingFromMetadata = conf.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = conf.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    // Create file system cache so metadata table is only instantiated once. Also can benefit normal file listing if\n+    // partition path is listed twice so file groups will already be loaded in file system\n+    Map<HoodieTableMetaClient, HoodieTableFileSystemView> fsCache = new HashMap<>();", "originalCommit": "0d0ad7405b7d170684ddb76725a9a75aa4061585", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODUxODQyNA==", "url": "https://github.com/apache/hudi/pull/2366#discussion_r548518424", "bodyText": "These 2 config options is every where, can we just pass the JobConf to the tool method FileSystemViewManager.createInMemoryFileSystemView and fetch them inside the method ? That would make the invocation more clean, IMO.", "author": "danny0405", "createdAt": "2020-12-24T12:37:15Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -63,13 +69,25 @@\n     // TODO(vc): Should we handle also non-hoodie splits here?\n     Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());\n \n+    boolean useFileListingFromMetadata = conf.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = conf.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);", "originalCommit": "0d0ad7405b7d170684ddb76725a9a75aa4061585", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}