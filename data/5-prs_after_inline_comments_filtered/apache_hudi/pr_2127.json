{"pr_number": 2127, "pr_title": "[HUDI-284] add more test for UpdateSchemaEvolution", "pr_createdAt": "2020-09-28T16:38:19Z", "pr_url": "https://github.com/apache/hudi/pull/2127", "timeline": [{"oid": "4f742ef781b17b9c9f80b43fde99434b9e9f8607", "url": "https://github.com/apache/hudi/commit/4f742ef781b17b9c9f80b43fde99434b9e9f8607", "message": "[HUDI-284] add more test for UpdateSchemaEvolution", "committedDate": "2020-09-29T01:53:18Z", "type": "commit"}, {"oid": "4f742ef781b17b9c9f80b43fde99434b9e9f8607", "url": "https://github.com/apache/hudi/commit/4f742ef781b17b9c9f80b43fde99434b9e9f8607", "message": "[HUDI-284] add more test for UpdateSchemaEvolution", "committedDate": "2020-09-29T01:53:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDMxOA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154318", "bodyText": "these 3 local vars could be removed", "author": "xushiyan", "createdAt": "2020-10-03T14:51:25Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);", "originalCommit": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjM5MA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499162390", "bodyText": "thanks ,3 local vars will  use for prepareFirstRecordCommit method .i will move it to a method", "author": "lw309637554", "createdAt": "2020-10-03T16:36:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDMxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDQxOA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154418", "bodyText": "to avoid ambiguity, could we call this prepareFirstRecordCommit()?", "author": "xushiyan", "createdAt": "2020-10-03T14:52:46Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {", "originalCommit": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTkwOA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499161908", "bodyText": "thanks, make sense", "author": "lw309637554", "createdAt": "2020-10-03T16:29:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDQxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDU3NA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154574", "bodyText": "could we call them table and config instead of table2 and config2? since the method is split and they are the only vars left.", "author": "xushiyan", "createdAt": "2020-10-03T14:54:09Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);", "originalCommit": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjQwMg==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499162402", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-10-03T16:36:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDU3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDczOA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154738", "bodyText": "seeing this is original code. i don't see why it needs to assert 1 equals to 1 returned eventually. could we simplify this by removing jsc.parallelize() and just retain the lambda block?", "author": "xushiyan", "createdAt": "2020-10-03T14:56:12Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);\n     assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {", "originalCommit": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTk0Mw==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499165943", "bodyText": "can not removing  jsc.parallelize() and just retain the lambda block.\nbecause HoodieMergeHandle need the param supplier.  supplier implement need\n\" return () -> TaskContext.get().stageId();\"\nso jsc.parallelize can init the TaskContext.   if we mock supplier, will bring  \"Task not serializable\".\nso i will remove assertEquals, but remain jsc.parallelize just like TestCopyOnWriteActionExecutor", "author": "lw309637554", "createdAt": "2020-10-03T17:23:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDczOA=="}], "type": "inlineReview"}, {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "url": "https://github.com/apache/hudi/commit/8852471a7cdef76ed1db2a4dc95b8239119356ac", "message": "[HUDI-284] merge master", "committedDate": "2020-10-03T17:24:30Z", "type": "commit"}, {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "url": "https://github.com/apache/hudi/commit/8852471a7cdef76ed1db2a4dc95b8239119356ac", "message": "[HUDI-284] merge master", "committedDate": "2020-10-03T17:24:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMzE4Mg==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499303182", "bodyText": "I think we can merge these two catch blocks? also it's probably better to move the LOG.error to LOG.debug and only log record data when such debug/tracing is enabled? This might just flood the logs.", "author": "vinothchandar", "createdAt": "2020-10-05T00:06:25Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -254,6 +253,10 @@ public void write(GenericRecord oldRecord) {\n         LOG.error(\"Failed to merge old record into new file for key \" + key + \" from old file \" + getOldFilePath()\n             + \" to new file \" + newFilePath, e);\n         throw new HoodieUpsertException(errMsg, e);\n+      } catch (RuntimeException e) {\n+        LOG.error(\"Summary is \" + e.getMessage() + \", detail is schema mismatch when rewriting old record \" + oldRecord + \" from file \" + getOldFilePath()", "originalCommit": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0ODYxNw==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499648617", "bodyText": "make sense .done", "author": "lw309637554", "createdAt": "2020-10-05T14:38:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMzE4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDA1NA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304054", "bodyText": "we can reuse some code in this file by pulling the common structure into a helper function ?", "author": "vinothchandar", "createdAt": "2020-10-05T00:15:54Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {", "originalCommit": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDAzNA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304034", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                jsc.parallelize(Arrays.asList(1)).map(x -> {\n          \n          \n            \n                jsc.parallelize(Arrays.asList(1)).foreach(x -> {", "author": "xushiyan", "createdAt": "2020-10-05T00:15:44Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {", "originalCommit": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDEyMA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304120", "bodyText": "Default sealed is false. we could skip unsealing.", "author": "xushiyan", "createdAt": "2020-10-05T00:16:35Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();", "originalCommit": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDE3OQ==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304179", "bodyText": "instead of creating a new Configuration, would it be better with table.getHadoopConf()?", "author": "xushiyan", "createdAt": "2020-10-05T00:17:26Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());", "originalCommit": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9f8f53785681a0d6e0005dc4cb784daf96e724bc", "url": "https://github.com/apache/hudi/commit/9f8f53785681a0d6e0005dc4cb784daf96e724bc", "message": "Update hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java\n\nCo-authored-by: Raymond Xu <2701446+xushiyan@users.noreply.github.com>", "committedDate": "2020-10-05T14:44:32Z", "type": "commit"}, {"oid": "724429e7284e723b09d5aef898155f66982af15e", "url": "https://github.com/apache/hudi/commit/724429e7284e723b09d5aef898155f66982af15e", "message": "Revert \"Update hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java\"\n\nThis reverts commit 9f8f53785681a0d6e0005dc4cb784daf96e724bc.", "committedDate": "2020-10-05T15:49:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyNzUxNw==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504227517", "bodyText": "a old -> an old", "author": "pratyakshsharma", "createdAt": "2020-10-13T20:13:29Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema", "originalCommit": "3d637855f337aad14e59ff944068a31aa2529a5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyODc4NQ==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504228785", "bodyText": "generateMultiRecordsForExampleSchema -> generateMultipleRecordsForExampleSchema?\n@lw309637554 I would leave it upto you to decide though.", "author": "pratyakshsharma", "createdAt": "2020-10-13T20:16:03Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {", "originalCommit": "3d637855f337aad14e59ff944068a31aa2529a5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzMzc2MA==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504233760", "bodyText": "oldrecords -> old records", "author": "pratyakshsharma", "createdAt": "2020-10-13T20:21:08Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, InvalidRecordException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\";", "originalCommit": "3d637855f337aad14e59ff944068a31aa2529a5e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNDI2OQ==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504234269", "bodyText": "exampleEvolvedSchemaColumnRequire.txt ,because -> exampleEvolvedSchemaColumnRequire.txt, because", "author": "pratyakshsharma", "createdAt": "2020-10-13T20:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzMzc2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNDk4OQ==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504234989", "bodyText": "delete column ,Parquet/Avro -> delete column, Parquet/Avro", "author": "pratyakshsharma", "createdAt": "2020-10-13T20:22:23Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";", "originalCommit": "3d637855f337aad14e59ff944068a31aa2529a5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNTg5Mg==", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504235892", "bodyText": "type ,org. -> type, org.", "author": "pratyakshsharma", "createdAt": "2020-10-13T20:23:14Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, InvalidRecordException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, HoodieUpsertException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when change column type ,org.apache.parquet.avro.AvroConverters$FieldUTF8Converter\";", "originalCommit": "3d637855f337aad14e59ff944068a31aa2529a5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "81daca90c5fce73cc0634daca5d2cd70ed87df15", "url": "https://github.com/apache/hudi/commit/81daca90c5fce73cc0634daca5d2cd70ed87df15", "message": "[HUDI-284] pulling the common structure", "committedDate": "2020-10-14T03:33:08Z", "type": "forcePushed"}, {"oid": "d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "url": "https://github.com/apache/hudi/commit/d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "message": "[HUDI-284] pulling the common structure", "committedDate": "2020-10-14T05:55:02Z", "type": "commit"}, {"oid": "d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "url": "https://github.com/apache/hudi/commit/d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "message": "[HUDI-284] pulling the common structure", "committedDate": "2020-10-14T05:55:02Z", "type": "forcePushed"}]}