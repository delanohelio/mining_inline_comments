{"pr_number": 2266, "pr_title": "[RFC-15] Adding interfaces for HoodieMetadata, HoodieMetadataWriter", "pr_createdAt": "2020-11-20T09:52:50Z", "pr_url": "https://github.com/apache/hudi/pull/2266", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNDQ1MQ==", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532814451", "bodyText": "nit: The jsc parameter here seems unnecessary since the constructor already takes it.", "author": "prashantwason", "createdAt": "2020-11-30T18:36:40Z", "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Interface that supports updating metadata for a given table, as actions complete.\n+ */\n+public interface HoodieTableMetadataWriter extends Serializable {\n+\n+  static HoodieTableMetadataWriter create(Configuration conf, HoodieWriteConfig writeConfig, JavaSparkContext jsc) {\n+    return new FSBackedTableMetadataWriter(conf, writeConfig, jsc);\n+  }\n+\n+  void update(JavaSparkContext jsc, HoodieCommitMetadata commitMetadata, String instantTime);", "originalCommit": "de733e90f26118700956a08584e1debbc5499d60", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyMTAyMg==", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532821022", "bodyText": "yes. I also mulled that. I will change", "author": "vinothchandar", "createdAt": "2020-11-30T18:47:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNDQ1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNTQwNQ==", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532815405", "bodyText": "Is FSBased the appropriate name here? FSBased seems to imply that the metadata is being saved directly on filesystem. But technically we are saving the metadata in a hoodie table.", "author": "prashantwason", "createdAt": "2020-11-30T18:38:17Z", "path": "hudi-client/src/main/java/org/apache/hudi/metadata/FSBackedTableMetadataWriter.java", "diffHunk": "@@ -74,84 +60,109 @@\n import org.apache.hudi.exception.HoodieMetadataException;\n import org.apache.hudi.metrics.DistributedRegistry;\n import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n \n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n import scala.Tuple2;\n \n+import static org.apache.hudi.metadata.HoodieTableMetadata.METADATA_TABLE_NAME_SUFFIX;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.NON_PARTITIONED_NAME;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.SOLO_COMMIT_TIMESTAMP;\n+\n /**\n  * Writer for Metadata Table.\n  *\n  * Partition and file listing are saved within an internal MOR table called Metadata Table. This table is created\n  * by listing files and partitions (first time) and kept in sync using the instants on the main dataset.\n  */\n-public class HoodieMetadataWriter extends HoodieMetadataReader implements Serializable {\n-  private static final Logger LOG = LogManager.getLogger(HoodieMetadataWriter.class);\n+public class FSBackedTableMetadataWriter implements HoodieTableMetadataWriter {", "originalCommit": "de733e90f26118700956a08584e1debbc5499d60", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyMDYwOQ==", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532820609", "bodyText": "Good point. Call it - HoodieBackedTableMetadataWriter?  wdyt @prashantwason", "author": "vinothchandar", "createdAt": "2020-11-30T18:47:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNTQwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyNzc4MA==", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532827780", "bodyText": "Yep, that sounds fine.", "author": "prashantwason", "createdAt": "2020-11-30T18:59:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNTQwNQ=="}], "type": "inlineReview"}, {"oid": "2ef28b67fe2c8e618d0fbcdd4f163018df1d4c01", "url": "https://github.com/apache/hudi/commit/2ef28b67fe2c8e618d0fbcdd4f163018df1d4c01", "message": "[RFC-15] Adding interfaces for HoodieMetadata, HoodieMetadataWriter\n\n - moved MetadataReader to HoodieBackedTableMetadata, under the HoodieTableMetadata interface\n - moved MetadataWriter to HoodieBackedTableMetadataWriter, under the HoodieTableMetadataWriter\n - Pulled all the metrics into HoodieMetadataMetrics\n - Writer now wraps the metadata, instead of extending it\n - New enum for MetadataPartitionType\n - Streamlined code flow inside HoodieBackedTableMetadataWriter w.r.t initializing metadata state", "committedDate": "2020-12-01T03:12:11Z", "type": "forcePushed"}, {"oid": "7f84b129d2b5eb17336ba645644f4054387e72fd", "url": "https://github.com/apache/hudi/commit/7f84b129d2b5eb17336ba645644f4054387e72fd", "message": "[RFC-15] Adding interfaces for HoodieMetadata, HoodieMetadataWriter\n\n - moved MetadataReader to HoodieBackedTableMetadata, under the HoodieTableMetadata interface\n - moved MetadataWriter to HoodieBackedTableMetadataWriter, under the HoodieTableMetadataWriter\n - Pulled all the metrics into HoodieMetadataMetrics\n - Writer now wraps the metadata, instead of extending it\n - New enum for MetadataPartitionType\n - Streamlined code flow inside HoodieBackedTableMetadataWriter w.r.t initializing metadata state", "committedDate": "2020-12-01T04:10:08Z", "type": "commit"}, {"oid": "7f84b129d2b5eb17336ba645644f4054387e72fd", "url": "https://github.com/apache/hudi/commit/7f84b129d2b5eb17336ba645644f4054387e72fd", "message": "[RFC-15] Adding interfaces for HoodieMetadata, HoodieMetadataWriter\n\n - moved MetadataReader to HoodieBackedTableMetadata, under the HoodieTableMetadata interface\n - moved MetadataWriter to HoodieBackedTableMetadataWriter, under the HoodieTableMetadataWriter\n - Pulled all the metrics into HoodieMetadataMetrics\n - Writer now wraps the metadata, instead of extending it\n - New enum for MetadataPartitionType\n - Streamlined code flow inside HoodieBackedTableMetadataWriter w.r.t initializing metadata state", "committedDate": "2020-12-01T04:10:08Z", "type": "forcePushed"}]}