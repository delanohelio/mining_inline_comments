{"pr_number": 1702, "pr_title": "[HUDI-426] Bootstrap datasource integration", "pr_createdAt": "2020-06-03T23:57:53Z", "pr_url": "https://github.com/apache/hudi/pull/1702", "timeline": [{"oid": "c8295f28784d0b5cd7882a8b767b15fc271421f7", "url": "https://github.com/apache/hudi/commit/c8295f28784d0b5cd7882a8b767b15fc271421f7", "message": "Bootstrap datasource integration", "committedDate": "2020-08-04T21:41:16Z", "type": "forcePushed"}, {"oid": "9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "url": "https://github.com/apache/hudi/commit/9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "message": "Bootstrap datasource integration", "committedDate": "2020-08-05T00:02:24Z", "type": "forcePushed"}, {"oid": "313385d95fa406a5adb7b198899abc14b572ee4a", "url": "https://github.com/apache/hudi/commit/313385d95fa406a5adb7b198899abc14b572ee4a", "message": "Bootstrap datasource integration", "committedDate": "2020-08-05T23:41:36Z", "type": "forcePushed"}, {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623", "url": "https://github.com/apache/hudi/commit/08e84812173d8126bc01c00b1c79ffaa5b80d623", "message": "Bootstrap datasource integration", "committedDate": "2020-08-06T02:30:25Z", "type": "forcePushed"}, {"oid": "4fcd7fee5a57f7e38089dea31f713e36892c43e1", "url": "https://github.com/apache/hudi/commit/4fcd7fee5a57f7e38089dea31f713e36892c43e1", "message": "Bootstrap datasource integration", "committedDate": "2020-08-06T23:44:12Z", "type": "forcePushed"}, {"oid": "923a67826ee904ccadc6225917bfc8974b118ef6", "url": "https://github.com/apache/hudi/commit/923a67826ee904ccadc6225917bfc8974b118ef6", "message": "Bootstrap datasource integration", "committedDate": "2020-08-07T09:00:10Z", "type": "forcePushed"}, {"oid": "7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "url": "https://github.com/apache/hudi/commit/7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "message": "Bootstrap datasource integration", "committedDate": "2020-08-07T22:16:41Z", "type": "forcePushed"}, {"oid": "3f7ecde4438fe79d282187616557c5ca451055bf", "url": "https://github.com/apache/hudi/commit/3f7ecde4438fe79d282187616557c5ca451055bf", "message": "Bootstrap datasource integration", "committedDate": "2020-08-07T22:56:08Z", "type": "forcePushed"}, {"oid": "caa597a30293eeb7c09d50c03a7feb3f79678495", "url": "https://github.com/apache/hudi/commit/caa597a30293eeb7c09d50c03a7feb3f79678495", "message": "Bootstrap datasource integration", "committedDate": "2020-08-08T01:59:15Z", "type": "forcePushed"}, {"oid": "952a499de55625b66eaab84829c369d8d5e77d22", "url": "https://github.com/apache/hudi/commit/952a499de55625b66eaab84829c369d8d5e77d22", "message": "Bootstrap datasource integration", "committedDate": "2020-08-08T02:03:07Z", "type": "commit"}, {"oid": "952a499de55625b66eaab84829c369d8d5e77d22", "url": "https://github.com/apache/hudi/commit/952a499de55625b66eaab84829c369d8d5e77d22", "message": "Bootstrap datasource integration", "committedDate": "2020-08-08T02:03:07Z", "type": "forcePushed"}, {"oid": "ff41ded339c5fc9d079c08bf40a51b97891bfeb9", "url": "https://github.com/apache/hudi/commit/ff41ded339c5fc9d079c08bf40a51b97891bfeb9", "message": "Merge branch 'master' into umehrot2_hudi_rfc12_code_review", "committedDate": "2020-08-09T09:31:44Z", "type": "commit"}, {"oid": "e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "url": "https://github.com/apache/hudi/commit/e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "message": "Fix unit-tests", "committedDate": "2020-08-09T16:18:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxOTg0OQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r467619849", "bodyText": "@umehrot2 : ITTestBootstrapCommand is failing with the below exception. Adding a sanitization API to remove illegal characters from avro field names\nException in thread \"main\" org.apache.avro.SchemaParseException: Illegal character in: test-table_record\n    at org.apache.avro.Schema.validateName(Schema.java:1151)\n    at org.apache.avro.Schema.access$200(Schema.java:81)\n    at org.apache.avro.Schema$Name.<init>(Schema.java:489)\n    at org.apache.avro.Schema.createRecord(Schema.java:161)\n    at org.apache.avro.SchemaBuilder$RecordBuilder.fields(SchemaBuilder.java:1732)\n    at org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:173)\n    at org.apache.spark.sql.avro.SchemaConverters.toAvroType(SchemaConverters.scala)\n    at org.apache.hudi.client.bootstrap.BootstrapSchemaProvider.getBootstrapSourceSchema(BootstrapSchemaProvider.java:97)\n    at org.apache.hudi.client.bootstrap.BootstrapSchemaProvider.getBootstrapSchema(BootstrapSchemaProvider.java:66)\n    at org.apache.hudi.table.action.bootstrap.BootstrapCommitActionExecutor.listAndProcessSourcePartitions(BootstrapCommitActionExecutor.java:288)", "author": "bvaradar", "createdAt": "2020-08-09T19:29:00Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/bootstrap/BootstrapSchemaProvider.java", "diffHunk": "@@ -64,14 +75,25 @@ public final Schema getBootstrapSchema(JavaSparkContext jsc, List<Pair<String, L\n    */\n   protected Schema getBootstrapSourceSchema(JavaSparkContext jsc,\n       List<Pair<String, List<HoodieFileStatus>>> partitions) {\n-    return partitions.stream().flatMap(p -> p.getValue().stream())\n-        .map(fs -> {\n-          try {\n-            Path filePath = FileStatusUtils.toPath(fs.getPath());\n-            return ParquetUtils.readAvroSchema(jsc.hadoopConfiguration(), filePath);\n-          } catch (Exception ex) {\n-            return null;\n-          }\n-        }).filter(x -> x != null).findAny().get();\n+    MessageType parquetSchema = partitions.stream().flatMap(p -> p.getValue().stream()).map(fs -> {\n+      try {\n+        Path filePath = FileStatusUtils.toPath(fs.getPath());\n+        return ParquetUtils.readSchema(jsc.hadoopConfiguration(), filePath);\n+      } catch (Exception ex) {\n+        return null;\n+      }\n+    }).filter(Objects::nonNull).findAny()\n+        .orElseThrow(() -> new HoodieException(\"Could not determine schema from the data files.\"));\n+\n+\n+    ParquetToSparkSchemaConverter converter = new ParquetToSparkSchemaConverter(\n+            Boolean.parseBoolean(SQLConf.PARQUET_BINARY_AS_STRING().defaultValueString()),\n+            Boolean.parseBoolean(SQLConf.PARQUET_INT96_AS_TIMESTAMP().defaultValueString()));\n+    StructType sparkSchema = converter.convert(parquetSchema);\n+    String tableName = writeConfig.getTableName();\n+    String structName = tableName + \"_record\";", "originalCommit": "e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1ce422e53f6de4fbec05918ac489b29a65de5bd5", "url": "https://github.com/apache/hudi/commit/1ce422e53f6de4fbec05918ac489b29a65de5bd5", "message": "Add spark-avro dependency in hudi-cli and sanitize Avro field names", "committedDate": "2020-08-09T19:37:11Z", "type": "commit"}]}