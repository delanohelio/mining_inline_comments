{"pr_number": 1752, "pr_title": "[HUDI-575] Support Async Compaction for spark streaming writes to hudi table", "pr_createdAt": "2020-06-20T21:52:55Z", "pr_url": "https://github.com/apache/hudi/pull/1752", "timeline": [{"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8", "url": "https://github.com/apache/hudi/commit/88f34ab0253cf269225f7cb8de381985812e8ad8", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-06-28T09:14:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjAwOA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446626008", "bodyText": "move comments that refer to a sub-class impl to that class itself?", "author": "vinothchandar", "createdAt": "2020-06-28T09:26:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.client.Compactor;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Async Compactor Service that runs in separate thread. Currently, only one compactor is allowed to run at any time.\n+ */\n+public class AsyncCompactService extends AbstractAsyncService {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(AsyncCompactService.class);\n+\n+  /**\n+   * This is the job pool used by async compaction.\n+   * In case of deltastreamer, Spark job scheduling configs are automatically set.\n+   * As the configs needs to be set before spark context is initiated, it is not\n+   * automated for Structured Streaming.\n+   * https://spark.apache.org/docs/latest/job-scheduling.html\n+   */\n+  public static final String COMPACT_POOL_NAME = \"hoodiecompact\";\n+\n+  private final int maxConcurrentCompaction;\n+  private transient Compactor compactor;\n+  private transient JavaSparkContext jssc;\n+  private transient BlockingQueue<HoodieInstant> pendingCompactions = new LinkedBlockingQueue<>();\n+  private transient ReentrantLock queueLock = new ReentrantLock();\n+  private transient Condition consumed = queueLock.newCondition();\n+\n+  public AsyncCompactService(JavaSparkContext jssc, HoodieWriteClient client) {\n+    this.jssc = jssc;\n+    this.compactor = new Compactor(client, jssc);\n+    this.maxConcurrentCompaction = 1;\n+  }\n+\n+  /**\n+   * Enqueues new Pending compaction.\n+   */\n+  public void enqueuePendingCompaction(HoodieInstant instant) {\n+    pendingCompactions.add(instant);\n+  }\n+\n+  /**\n+   * Wait till outstanding pending compactions reduces to the passed in value.\n+   *\n+   * @param numPendingCompactions Maximum pending compactions allowed\n+   * @throws InterruptedException\n+   */\n+  public void waitTillPendingCompactionsReducesTo(int numPendingCompactions) throws InterruptedException {\n+    try {\n+      queueLock.lock();\n+      while (!isShutdown() && (pendingCompactions.size() > numPendingCompactions)) {\n+        consumed.await();\n+      }\n+    } finally {\n+      queueLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Fetch Next pending compaction if available.\n+   *\n+   * @return\n+   * @throws InterruptedException\n+   */\n+  private HoodieInstant fetchNextCompactionInstant() throws InterruptedException {\n+    LOG.info(\"Compactor waiting for next instant for compaction upto 60 seconds\");\n+    HoodieInstant instant = pendingCompactions.poll(10, TimeUnit.SECONDS);\n+    if (instant != null) {\n+      try {\n+        queueLock.lock();\n+        // Signal waiting thread\n+        consumed.signal();\n+      } finally {\n+        queueLock.unlock();\n+      }\n+    }\n+    return instant;\n+  }\n+\n+  /**\n+   * Start Compaction Service.\n+   */\n+  @Override\n+  protected Pair<CompletableFuture, ExecutorService> startService() {\n+    ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction,\n+        r -> new Thread(r, \"async_compact_thread\"));\n+    return Pair.of(CompletableFuture.allOf(IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {\n+      try {\n+        // Set Compactor Pool Name for allowing users to prioritize compaction\n+        LOG.info(\"Setting Spark Pool name for compaction to \" + COMPACT_POOL_NAME);\n+        jssc.setLocalProperty(\"spark.scheduler.pool\", COMPACT_POOL_NAME);\n+\n+        while (!isShutdownRequested()) {\n+          final HoodieInstant instant = fetchNextCompactionInstant();\n+\n+          if (null != instant) {\n+            LOG.info(\"Starting Compaction for instant \" + instant);\n+            compactor.compact(instant);\n+            LOG.info(\"Finished Compaction for instant \" + instant);\n+          }\n+\n+          if (shouldStopCompactor()) {\n+            return true;\n+          }\n+        }\n+        LOG.info(\"Compactor shutting down properly!!\");\n+      } catch (InterruptedException ie) {\n+        LOG.warn(\"Compactor executor thread got interrupted exception. Stopping\", ie);\n+      } catch (IOException e) {\n+        LOG.error(\"Compactor executor failed\", e);\n+        throw new HoodieIOException(e.getMessage(), e);\n+      }\n+      return true;\n+    }, executor)).toArray(CompletableFuture[]::new)), executor);\n+  }\n+\n+  /**\n+   * Spark Structured Streaming Sink implementation do not have mechanism to know when the stream is shutdown.", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzU2Mg==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333562", "bodyText": "Done", "author": "bvaradar", "createdAt": "2020-08-04T21:12:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjAwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI2OA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446626268", "bodyText": "this does not mean there is no work for compaction right?", "author": "vinothchandar", "createdAt": "2020-06-28T09:29:31Z", "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import java.util.function.Supplier;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * This class is used to detect activity of spark streaming writer. THis is used to decide if HoodieWriteClient\n+ * and async compactor needs to be closed. Spark Structured Streaming do not have explicit API on the Sink side to\n+ * determine if the stream is done. In this absence, async compactor proactively checks with the sink if it is\n+ * active. If there is no activity for sufficient period, async compactor shuts down. If the sink was indeed active,\n+ * a subsequent batch will re-trigger async compaction.\n+ */\n+public class SparkStreamingWriterActivityDetector {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(SparkStreamingWriterActivityDetector.class);\n+\n+  private final Supplier<Long> lastStartBatchNanoTimeSupplier;\n+  private final Supplier<Long> lastEndBatchNanoTimeSupplier;\n+  private final long sinkInactivityTimeoutSecs;\n+\n+  private static final long SECS_TO_NANOS = 1000000000L;\n+\n+  public SparkStreamingWriterActivityDetector(\n+      Supplier<Long> lastStartBatchNanoTimeSupplier, Supplier<Long> lastEndBatchNanoTimeSupplier,\n+      long sinkInactivityTimeoutSecs) {\n+    this.lastStartBatchNanoTimeSupplier = lastStartBatchNanoTimeSupplier;\n+    this.lastEndBatchNanoTimeSupplier = lastEndBatchNanoTimeSupplier;\n+    this.sinkInactivityTimeoutSecs = sinkInactivityTimeoutSecs;\n+  }\n+\n+  /**\n+   * Detects if spark streaming write is still active based on time.\n+   * @return\n+   */\n+  public boolean hasRecentlyWritten() {\n+    long lastStartBatchTime = lastStartBatchNanoTimeSupplier.get();\n+    long lastEndBatchTime = lastEndBatchNanoTimeSupplier.get();\n+\n+    LOG.info(\"Checking if compactor needs to be stopped. \"\n+        + \"lastStartBatchTime=\" + lastStartBatchTime + \", lastEndBatchTime=\" + lastEndBatchTime\n+        + \", CurrTime=\" + System.nanoTime());\n+\n+    if (lastEndBatchTime - lastStartBatchTime < 0) {\n+      LOG.info(\"End Batch Time (\" + lastEndBatchTime + \") is less than Start Batch Time (\" + lastStartBatchTime + \")\"\n+          + \"Sink is running. So, no need to stop\");\n+      return true;\n+    }\n+\n+    long currTime = System.nanoTime();\n+    long elapsedTimeSecs = Double.valueOf(Math.ceil(1.0 * (currTime - lastEndBatchTime) / SECS_TO_NANOS)).longValue();\n+    if (elapsedTimeSecs > sinkInactivityTimeoutSecs) {\n+      LOG.warn(\"Streaming Sink has been idle for \" + elapsedTimeSecs + \" seconds\");", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzUxNA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333514", "bodyText": "This code is deleted.", "author": "bvaradar", "createdAt": "2020-08-04T21:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzE2NA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446627164", "bodyText": "more importantly, we should also renable the test in TestDataSource", "author": "vinothchandar", "createdAt": "2020-06-28T09:37:55Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java", "diffHunk": "@@ -58,6 +58,7 @@\n   protected static final String PRESTO_COORDINATOR = \"/presto-coordinator-1\";\n   protected static final String HOODIE_WS_ROOT = \"/var/hoodie/ws\";\n   protected static final String HOODIE_JAVA_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_app.sh\";\n+  protected static final String HOODIE_JAVA_STREAMING_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_streaming_app.sh\";", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMTcwOA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465331708", "bodyText": "Enabled it after adding timed retry logic to wait for commits", "author": "bvaradar", "createdAt": "2020-08-04T21:09:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzE2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODE2OQ==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446628169", "bodyText": "why move to COW?", "author": "vinothchandar", "createdAt": "2020-06-28T09:48:22Z", "path": "hudi-spark/src/test/java/HoodieJavaStreamingApp.java", "diffHunk": "@@ -68,7 +74,7 @@\n   private String tableName = \"hoodie_test\";\n \n   @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n-  private String tableType = HoodieTableType.MERGE_ON_READ.name();\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMDkzMg==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465330932", "bodyText": "Reverted.", "author": "bvaradar", "createdAt": "2020-08-04T21:07:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODE2OQ=="}], "type": "inlineReview"}, {"oid": "f88d0bf2fbdab85a8f24416b2ed64db5d0e29508", "url": "https://github.com/apache/hudi/commit/f88d0bf2fbdab85a8f24416b2ed64db5d0e29508", "message": "[HUDI-575] minor CR comments", "committedDate": "2020-08-02T16:35:05Z", "type": "forcePushed"}, {"oid": "54e2e256e072d55049284084ea8da25db5fc1aa0", "url": "https://github.com/apache/hudi/commit/54e2e256e072d55049284084ea8da25db5fc1aa0", "message": "Ensure Async Compaction for structured streaming is running in daemon mode", "committedDate": "2020-08-03T07:46:48Z", "type": "forcePushed"}, {"oid": "0a849a8fa55811f80030c6b486bcfe965183de8e", "url": "https://github.com/apache/hudi/commit/0a849a8fa55811f80030c6b486bcfe965183de8e", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-04T16:11:07Z", "type": "forcePushed"}, {"oid": "8d515f9d92482e046a8f8309d80d15b6825164a1", "url": "https://github.com/apache/hudi/commit/8d515f9d92482e046a8f8309d80d15b6825164a1", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-04T16:44:27Z", "type": "forcePushed"}, {"oid": "f3736c2996157285e466140c5775fb3801f42741", "url": "https://github.com/apache/hudi/commit/f3736c2996157285e466140c5775fb3801f42741", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T02:45:16Z", "type": "forcePushed"}, {"oid": "a6992eb75605a88c75c1e2cfdae9b8595ec7b293", "url": "https://github.com/apache/hudi/commit/a6992eb75605a88c75c1e2cfdae9b8595ec7b293", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T05:08:03Z", "type": "forcePushed"}, {"oid": "281b820e82bae3ed6372d1b39a054b128087d6e9", "url": "https://github.com/apache/hudi/commit/281b820e82bae3ed6372d1b39a054b128087d6e9", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T05:58:30Z", "type": "commit"}, {"oid": "281b820e82bae3ed6372d1b39a054b128087d6e9", "url": "https://github.com/apache/hudi/commit/281b820e82bae3ed6372d1b39a054b128087d6e9", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T05:58:30Z", "type": "forcePushed"}]}