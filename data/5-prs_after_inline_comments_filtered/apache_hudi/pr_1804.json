{"pr_number": 1804, "pr_title": "[HUDI-960] Implementation of the HFile base and log file format.", "pr_createdAt": "2020-07-06T21:28:13Z", "pr_url": "https://github.com/apache/hudi/pull/1804", "timeline": [{"oid": "de49867b99a4784655aeb30d57b24f9fd1556ab7", "url": "https://github.com/apache/hudi/commit/de49867b99a4784655aeb30d57b24f9fd1556ab7", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-07-08T04:46:28Z", "type": "forcePushed"}, {"oid": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "url": "https://github.com/apache/hudi/commit/b4d89ff8f80e20231ffaa774a0e550bd2450f798", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-07-08T23:17:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MzU2Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456363566", "bodyText": "java docs", "author": "nsivabalan", "createdAt": "2020-07-17T10:37:50Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+\n+@SuppressWarnings(\"Duplicates\")", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NDg4Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458354886", "bodyText": "Added", "author": "prashantwason", "createdAt": "2020-07-21T20:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MzU2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2ODE4OA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456368188", "bodyText": "can we move all bloom filter related code to a separate class. So that this class is in line w/ ParquetWriter as well. I am not suggesting to introduce an interface, just another class.", "author": "nsivabalan", "createdAt": "2020-07-17T10:48:45Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieHFileWriter.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.fs.HoodieWrapperFileSystem;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * HoodieHFileWriter writes IndexedRecords into an HFile. The record's key is used as the key and the\n+ * AVRO encoded record bytes are saved as the value.\n+ *\n+ * Limitations (compared to columnar formats like Parquet or ORC):\n+ *  1. Records should be added in order of keys\n+ *  2. There are no column stats\n+ */\n+public class HoodieHFileWriter<T extends HoodieRecordPayload, R extends IndexedRecord>\n+    implements HoodieFileWriter<R> {\n+  private static AtomicLong recordIndex = new AtomicLong(1);\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileWriter.class);\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  private final Path file;\n+  private HoodieHFileConfig hfileConfig;\n+  private final HoodieWrapperFileSystem fs;\n+  private final long maxFileSize;\n+  private final String instantTime;\n+  private final SparkTaskContextSupplier sparkTaskContextSupplier;\n+  private HFile.Writer writer;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieHFileWriter(String instantTime, Path file, HoodieHFileConfig hfileConfig, Schema schema,\n+      SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+\n+    Configuration conf = FSUtils.registerFileSystem(file, hfileConfig.getHadoopConf());\n+    this.file = HoodieWrapperFileSystem.convertToHoodiePath(file, conf);\n+    this.fs = (HoodieWrapperFileSystem) this.file.getFileSystem(conf);\n+    this.hfileConfig = hfileConfig;\n+\n+    // We cannot accurately measure the snappy compressed output file size. We are choosing a\n+    // conservative 10%\n+    // TODO - compute this compression ratio dynamically by looking at the bytes written to the\n+    // stream and the actual file size reported by HDFS\n+    // this.maxFileSize = hfileConfig.getMaxFileSize()\n+    //    + Math.round(hfileConfig.getMaxFileSize() * hfileConfig.getCompressionRatio());\n+    this.maxFileSize = hfileConfig.getMaxFileSize();\n+    this.instantTime = instantTime;\n+    this.sparkTaskContextSupplier = sparkTaskContextSupplier;\n+\n+    HFileContext context = new HFileContextBuilder().withBlockSize(hfileConfig.getBlockSize())\n+          .withCompression(hfileConfig.getCompressionAlgorithm())\n+          .build();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    this.writer = HFile.getWriterFactory(conf, cacheConfig).withPath(this.fs, this.file).withFileContext(context).create();\n+\n+    writer.appendFileInfo(KEY_SCHEMA.getBytes(), schema.toString().getBytes());\n+  }\n+\n+  @Override\n+  public void writeAvroWithMetadata(R avroRecord, HoodieRecord record) throws IOException {\n+    String seqId =\n+        HoodieRecord.generateSequenceId(instantTime, sparkTaskContextSupplier.getPartitionIdSupplier().get(), recordIndex.getAndIncrement());\n+    HoodieAvroUtils.addHoodieKeyToRecord((GenericRecord) avroRecord, record.getRecordKey(), record.getPartitionPath(),\n+        file.getName());\n+    HoodieAvroUtils.addCommitMetadataToRecord((GenericRecord) avroRecord, instantTime, seqId);\n+\n+    writeAvro(record.getRecordKey(), (IndexedRecord)avroRecord);\n+  }\n+\n+  @Override\n+  public boolean canWrite() {\n+    return fs.getBytesWritten(file) < maxFileSize;\n+  }\n+\n+  @Override\n+  public void writeAvro(String recordKey, IndexedRecord object) throws IOException {\n+    byte[] value = HoodieAvroUtils.avroToBytes((GenericRecord)object);\n+    KeyValue kv = new KeyValue(recordKey.getBytes(), null, null, value);\n+    writer.append(kv);\n+\n+    if (hfileConfig.useBloomFilter()) {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NDk1Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458354957", "bodyText": "ParquetWriter uses HoodieAvroWriteSupport which has the bloom filter code. I did not want to create another class for such a few lines of code. Also, it may be better for HFile to disable bloom filter in some cases (e.g. RFC-15 HFile will not need bloom filter within the HFile).", "author": "prashantwason", "createdAt": "2020-07-21T20:03:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2ODE4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NjE0MA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456376140", "bodyText": "when I was benchmarking Hfile, found few other cache config params to be useful.\nprefetch on open, cache L1 and drop behind compaction. More details can be found here. Can we add these 3 configs to this class as well", "author": "nsivabalan", "createdAt": "2020-07-17T11:07:43Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieHFileConfig.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.io.compress.Compression;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+\n+public class HoodieHFileConfig {\n+\n+  private Compression.Algorithm compressionAlgorithm;\n+  private int blockSize;\n+  private long maxFileSize;\n+  private Configuration hadoopConf;\n+  private BloomFilter bloomFilter;\n+\n+  public HoodieHFileConfig(Compression.Algorithm compressionAlgorithm, int blockSize, long maxFileSize,", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTAxNQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355015", "bodyText": "Can you please provide the source to your benchmarking so I can add the configs? The source code link in HUDI-432 is broken.", "author": "prashantwason", "createdAt": "2020-07-21T20:03:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NjE0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzNzE4NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475937184", "bodyText": "I think its here. nsivabalan@70ee994 ? @nsivabalan  ?", "author": "vinothchandar", "createdAt": "2020-08-24T22:51:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NjE0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQ3MTMxNg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477471316", "bodyText": "Added the three settings. Please review.", "author": "prashantwason", "createdAt": "2020-08-26T17:33:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NjE0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzODUwOQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477538509", "bodyText": "These are the 3 configs. May I know where these are added\n    conf.set(CacheConfig.PREFETCH_BLOCKS_ON_OPEN_KEY, \"true\");\n    conf.set(\"CACHE_DATA_IN_L1\",\"true\");\n    conf.set(\"hbase.hfile.drop.behind.compaction\", \"false\");", "author": "nsivabalan", "createdAt": "2020-08-26T19:28:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NjE0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NjY5OA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456376698", "bodyText": "can you help me understand whats the usecase for this ?", "author": "nsivabalan", "createdAt": "2020-07-17T11:09:10Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.storage.HoodieHFileReader;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.compress.Compression;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.hbase.util.Pair;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * HoodieHFileDataBlock contains a list of records stored inside an HFile format. It is used with the HFile\n+ * base file format.\n+ */\n+public class HoodieHFileDataBlock extends HoodieDataBlock {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileDataBlock.class);\n+  private static Compression.Algorithm compressionAlgorithm = Compression.Algorithm.GZ;\n+  private static int blockSize = 1 * 1024 * 1024;\n+\n+  public HoodieHFileDataBlock(@Nonnull Map<HeaderMetadataType, String> logBlockHeader,\n+       @Nonnull Map<HeaderMetadataType, String> logBlockFooter,\n+       @Nonnull Option<HoodieLogBlockContentLocation> blockContentLocation, @Nonnull Option<byte[]> content,\n+       FSDataInputStream inputStream, boolean readBlockLazily) {\n+    super(logBlockHeader, logBlockFooter, blockContentLocation, content, inputStream, readBlockLazily);\n+  }\n+\n+  public HoodieHFileDataBlock(HoodieLogFile logFile, FSDataInputStream inputStream, Option<byte[]> content,\n+       boolean readBlockLazily, long position, long blockSize, long blockEndpos, Schema readerSchema,\n+       Map<HeaderMetadataType, String> header, Map<HeaderMetadataType, String> footer) {\n+    super(content, inputStream, readBlockLazily,\n+          Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndpos)), readerSchema, header,\n+          footer);\n+  }\n+\n+  public HoodieHFileDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<HeaderMetadataType, String> header) {\n+    super(records, header, new HashMap<>());\n+  }\n+\n+  @Override\n+  public HoodieLogBlockType getBlockType() {\n+    return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+  }\n+\n+  @Override\n+  protected byte[] serializeRecords() throws IOException {\n+    HFileContext context = new HFileContextBuilder().withBlockSize(blockSize).withCompression(compressionAlgorithm)\n+        .build();\n+    Configuration conf = new Configuration();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    FSDataOutputStream ostream = new FSDataOutputStream(baos, null);\n+\n+    HFile.Writer writer = HFile.getWriterFactory(conf, cacheConfig)\n+        .withOutputStream(ostream).withFileContext(context).create();\n+\n+    // Serialize records into bytes\n+    Map<String, byte[]> recordMap = new TreeMap<>();\n+    Iterator<IndexedRecord> itr = records.iterator();\n+    boolean useIntegerKey = false;\n+    int key = 0;\n+    int keySize = 0;\n+    Field keyField = records.get(0).getSchema().getField(HoodieRecord.RECORD_KEY_METADATA_FIELD);\n+    if (keyField == null) {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTA1NQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355055", "bodyText": "HFile requires written key-values to be sorted by the key. When adding the serialized GenericRecord, the serialized bytes are the \"value\" and the key is to be extracted from the record.\nA HUDI GenericRecord usually has the \"_hoodie_record_key\" field which can be used as the unique key. But in unit tests, the schema might not have _hoodie_record_key. So to support unit tests, a sequential integer is used to order the writes into HFile.", "author": "prashantwason", "createdAt": "2020-07-21T20:03:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NjY5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NzIwNg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456377206", "bodyText": "why another loop rather than writing it in line 121?", "author": "nsivabalan", "createdAt": "2020-07-17T11:10:24Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.storage.HoodieHFileReader;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.compress.Compression;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.hbase.util.Pair;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * HoodieHFileDataBlock contains a list of records stored inside an HFile format. It is used with the HFile\n+ * base file format.\n+ */\n+public class HoodieHFileDataBlock extends HoodieDataBlock {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileDataBlock.class);\n+  private static Compression.Algorithm compressionAlgorithm = Compression.Algorithm.GZ;\n+  private static int blockSize = 1 * 1024 * 1024;\n+\n+  public HoodieHFileDataBlock(@Nonnull Map<HeaderMetadataType, String> logBlockHeader,\n+       @Nonnull Map<HeaderMetadataType, String> logBlockFooter,\n+       @Nonnull Option<HoodieLogBlockContentLocation> blockContentLocation, @Nonnull Option<byte[]> content,\n+       FSDataInputStream inputStream, boolean readBlockLazily) {\n+    super(logBlockHeader, logBlockFooter, blockContentLocation, content, inputStream, readBlockLazily);\n+  }\n+\n+  public HoodieHFileDataBlock(HoodieLogFile logFile, FSDataInputStream inputStream, Option<byte[]> content,\n+       boolean readBlockLazily, long position, long blockSize, long blockEndpos, Schema readerSchema,\n+       Map<HeaderMetadataType, String> header, Map<HeaderMetadataType, String> footer) {\n+    super(content, inputStream, readBlockLazily,\n+          Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndpos)), readerSchema, header,\n+          footer);\n+  }\n+\n+  public HoodieHFileDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<HeaderMetadataType, String> header) {\n+    super(records, header, new HashMap<>());\n+  }\n+\n+  @Override\n+  public HoodieLogBlockType getBlockType() {\n+    return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+  }\n+\n+  @Override\n+  protected byte[] serializeRecords() throws IOException {\n+    HFileContext context = new HFileContextBuilder().withBlockSize(blockSize).withCompression(compressionAlgorithm)\n+        .build();\n+    Configuration conf = new Configuration();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    FSDataOutputStream ostream = new FSDataOutputStream(baos, null);\n+\n+    HFile.Writer writer = HFile.getWriterFactory(conf, cacheConfig)\n+        .withOutputStream(ostream).withFileContext(context).create();\n+\n+    // Serialize records into bytes\n+    Map<String, byte[]> recordMap = new TreeMap<>();\n+    Iterator<IndexedRecord> itr = records.iterator();\n+    boolean useIntegerKey = false;\n+    int key = 0;\n+    int keySize = 0;\n+    Field keyField = records.get(0).getSchema().getField(HoodieRecord.RECORD_KEY_METADATA_FIELD);\n+    if (keyField == null) {\n+      // Missing key metadata field so we should use an integer sequence key\n+      useIntegerKey = true;\n+      keySize = (int) Math.ceil(Math.log(records.size())) + 1;\n+    }\n+    while (itr.hasNext()) {\n+      IndexedRecord record = itr.next();\n+      String recordKey;\n+      if (useIntegerKey) {\n+        recordKey = String.format(\"%\" + keySize + \"s\", key++);\n+      } else {\n+        recordKey = record.get(keyField.pos()).toString();\n+      }\n+      byte[] recordBytes = HoodieAvroUtils.avroToBytes(record);\n+      recordMap.put(recordKey, recordBytes);\n+    }\n+\n+    // Write the records\n+    recordMap.forEach((recordKey, recordBytes) -> {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTA4OA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355088", "bodyText": "Because we need to write to HFile in sorted-by-key order. The loop in 121 is looping over the record Iterator which does not guarantee any order on the records read.", "author": "prashantwason", "createdAt": "2020-07-21T20:03:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NzIwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzOTIyOQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475939229", "bodyText": "@prashantwason we can sort the keys being sent into the data block, using spark partitioning itself? why do we have to sort the records again explicitly?", "author": "vinothchandar", "createdAt": "2020-08-24T22:57:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NzIwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU1MTgwNw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477551807", "bodyText": "@vinothchandar Does not matter where the sorting is performed. It will definitely be referable in the partitioner.\nI traced the entire data path of writes from HoodieWriteClient.upsert(). There are several code paths each using their own partitioner. So could not locate a single place where to implement/ensure the sorting.\nIs there a base class for all Partitioners within HUDI? Maybe we can update that to accept a boolean sort parameter.", "author": "prashantwason", "createdAt": "2020-08-26T19:53:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NzIwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3Nzk5Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456377996", "bodyText": "correct me if I am wrong. we may not have a separate delete block for hfile, since everything is a key, value in bytes. So, we might have to fetch all values and resolve to the latest one to find if the value represents delete or active.", "author": "nsivabalan", "createdAt": "2020-07-17T11:12:34Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java", "diffHunk": "@@ -110,7 +110,7 @@ public long getLogBlockLength() {\n    * Type of the log block WARNING: This enum is serialized as the ordinal. Only add new enums at the end.\n    */\n   public enum HoodieLogBlockType {\n-    COMMAND_BLOCK, DELETE_BLOCK, CORRUPT_BLOCK, AVRO_DATA_BLOCK\n+    COMMAND_BLOCK, DELETE_BLOCK, CORRUPT_BLOCK, AVRO_DATA_BLOCK, HFILE_DATA_BLOCK", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxOTIwMg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458319202", "bodyText": "Yes, a separate DELETE block is not required for HFile. The delete functionality is implemented independent of the data blocks which only save record updates.\nDELETE_BLOCK saves record keys which have been deleted since. While reading the log blocks (HoodieMergedLogRecordScanner), if a DELETE block is encountered then we save a EmptyPayload which represents a delete marker for the record. Such records wont be written out (compaction) or processed (RealtimeRecordReader) thereby representing a delete.\n\n\nSo, we might have to fetch all values and resolve to the latest one to find if the value represents delete or active.\nDeleted records are never saved. Only deleted keys are saved within the DELETE block.", "author": "prashantwason", "createdAt": "2020-07-21T18:56:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3Nzk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3OTU0Mg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456379542", "bodyText": "do you think we introduce a config to say whether a particular file format supports single key look up. Not all file formats might support look up by Key. So, for those, we can keep this as unsupported and for Hfile kind of formats, we can have impl.", "author": "nsivabalan", "createdAt": "2020-07-17T11:16:19Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieFileReader.java", "diffHunk": "@@ -34,7 +35,17 @@\n \n   public Set<String> filterRowKeys(Set<String> candidateRowKeys);\n \n-  public Iterator<R> getRecordIterator(Schema schema) throws IOException;\n+  public Iterator<R> getRecordIterator(Schema readerSchema) throws IOException;\n+\n+  default Iterator<R> getRecordIterator() throws IOException {\n+    return getRecordIterator(getSchema());\n+  }\n+\n+  public Option<R> getRecordByKey(String key, Schema readerSchema) throws IOException;", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTE4OA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355188", "bodyText": "We can introduce that config but how will it be used?\nI feel the lookup by key is only useful for internal features (like RFC-15 or RFC-08) rather than a generic API for HUDI. HUDI record keys tend to be UUDIs which are large and looking them up is not a common usecase.", "author": "prashantwason", "createdAt": "2020-07-21T20:03:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3OTU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzMjk2NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475832964", "bodyText": "I think. throwing UnSupportedException is actually better. nothing can fundamentally change whether a format supports this or not", "author": "vinothchandar", "createdAt": "2020-08-24T19:06:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3OTU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4MTUwNQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456381505", "bodyText": "minor. why define this outside the try block", "author": "nsivabalan", "createdAt": "2020-07-17T11:21:07Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);\n+    FSDataInputStream fsdis = new FSDataInputStream(bis);\n+    this.reader = HFile.createReader(FSUtils.getFs(\"hoodie\", conf), path, new FSDataInputStreamWrapper(fsdis),\n+        content.length, new CacheConfig(conf), conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTI0NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355244", "bodyText": "Updated.", "author": "prashantwason", "createdAt": "2020-07-21T20:03:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4MTUwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NDM4NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456384384", "bodyText": "not required in this patch. but we should think if we can decide whether to read entire set and filter or just do random seeks based on candidate set size to be filtered for.", "author": "nsivabalan", "createdAt": "2020-07-17T11:28:44Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);\n+    FSDataInputStream fsdis = new FSDataInputStream(bis);\n+    this.reader = HFile.createReader(FSUtils.getFs(\"hoodie\", conf), path, new FSDataInputStreamWrapper(fsdis),\n+        content.length, new CacheConfig(conf), conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      return new String[] { new String(fileInfo.get(KEY_MIN_RECORD.getBytes())),\n+          new String(fileInfo.get(KEY_MAX_RECORD.getBytes()))};\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read min/max record key out of file information block correctly from path\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Schema getSchema() {\n+    if (schema == null) {\n+      try {\n+        Map<byte[], byte[]> fileInfo = reader.loadFileInfo();\n+        schema = new Schema.Parser().parse(new String(fileInfo.get(KEY_SCHEMA.getBytes())));\n+      } catch (IOException e) {\n+        throw new HoodieException(\"Could not read schema of file from path\", e);\n+      }\n+    }\n+\n+    return schema;\n+  }\n+\n+  @Override\n+  public BloomFilter readBloomFilter() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      ByteBuffer serializedFilter = reader.getMetaBlock(KEY_BLOOM_FILTER_META_BLOCK, false);\n+      byte[] filterBytes = new byte[serializedFilter.remaining()];\n+      serializedFilter.get(filterBytes); // read the bytes that were written\n+      return BloomFilterFactory.fromString(new String(filterBytes),\n+          new String(fileInfo.get(KEY_BLOOM_FILTER_TYPE_CODE.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read bloom filter from \" + path, e);\n+    }\n+  }\n+\n+  @Override\n+  public Set<String> filterRowKeys(Set candidateRowKeys) {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTMwOA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355308", "bodyText": "I have added a comment to capture the potential for optimization.", "author": "prashantwason", "createdAt": "2020-07-21T20:04:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NDM4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NTMyMQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456385321", "bodyText": "is this where exception will be thrown if file does not exists or do we check file exists check elsewhere prior to this step ?", "author": "nsivabalan", "createdAt": "2020-07-17T11:31:04Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);\n+    FSDataInputStream fsdis = new FSDataInputStream(bis);\n+    this.reader = HFile.createReader(FSUtils.getFs(\"hoodie\", conf), path, new FSDataInputStreamWrapper(fsdis),", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTM2Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355367", "bodyText": "Exception will be thrown here.", "author": "prashantwason", "createdAt": "2020-07-21T20:04:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NTMyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NjA4Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456386087", "bodyText": "curious. why named it as readNextRecord. felt \"getRecord\" or \"getRecordFromCell\" would be more apt.", "author": "nsivabalan", "createdAt": "2020-07-17T11:33:04Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);\n+    FSDataInputStream fsdis = new FSDataInputStream(bis);\n+    this.reader = HFile.createReader(FSUtils.getFs(\"hoodie\", conf), path, new FSDataInputStreamWrapper(fsdis),\n+        content.length, new CacheConfig(conf), conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      return new String[] { new String(fileInfo.get(KEY_MIN_RECORD.getBytes())),\n+          new String(fileInfo.get(KEY_MAX_RECORD.getBytes()))};\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read min/max record key out of file information block correctly from path\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Schema getSchema() {\n+    if (schema == null) {\n+      try {\n+        Map<byte[], byte[]> fileInfo = reader.loadFileInfo();\n+        schema = new Schema.Parser().parse(new String(fileInfo.get(KEY_SCHEMA.getBytes())));\n+      } catch (IOException e) {\n+        throw new HoodieException(\"Could not read schema of file from path\", e);\n+      }\n+    }\n+\n+    return schema;\n+  }\n+\n+  @Override\n+  public BloomFilter readBloomFilter() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      ByteBuffer serializedFilter = reader.getMetaBlock(KEY_BLOOM_FILTER_META_BLOCK, false);\n+      byte[] filterBytes = new byte[serializedFilter.remaining()];\n+      serializedFilter.get(filterBytes); // read the bytes that were written\n+      return BloomFilterFactory.fromString(new String(filterBytes),\n+          new String(fileInfo.get(KEY_BLOOM_FILTER_TYPE_CODE.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read bloom filter from \" + path, e);\n+    }\n+  }\n+\n+  @Override\n+  public Set<String> filterRowKeys(Set candidateRowKeys) {\n+    try {\n+      List<Pair<String, R>> allRecords = readAllRecords();\n+      Set<String> rowKeys = new HashSet<>();\n+      allRecords.forEach(t -> {\n+        if (candidateRowKeys.contains(t.getFirst())) {\n+          rowKeys.add(t.getFirst());\n+        }\n+      });\n+      return rowKeys;\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to read row keys from \" + path, e);\n+    }\n+  }\n+\n+  public List<Pair<String, R>> readAllRecords(Schema writerSchema, Schema readerSchema) throws IOException {\n+    List<Pair<String, R>> recordList = new LinkedList<>();\n+    try {\n+      HFileScanner scanner = reader.getScanner(false, false);\n+      if (scanner.seekTo()) {\n+        do {\n+          Cell c = scanner.getKeyValue();\n+          byte[] keyBytes = Arrays.copyOfRange(c.getRowArray(), c.getRowOffset(), c.getRowOffset() + c.getRowLength());\n+          R record = readNextRecord(c, writerSchema, readerSchema);\n+          recordList.add(new Pair<>(new String(keyBytes), record));\n+        } while (scanner.next());\n+      }\n+\n+      return recordList;\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Error reading hfile \" + path + \" as a dataframe\", e);\n+    }\n+  }\n+\n+  public List<Pair<String, R>> readAllRecords() throws IOException {\n+    Schema schema = new Schema.Parser().parse(new String(reader.loadFileInfo().get(\"schema\".getBytes())));\n+    return readAllRecords(schema, schema);\n+  }\n+\n+  @Override\n+  public Iterator getRecordIterator(Schema readerSchema) throws IOException {\n+    final HFileScanner scanner = reader.getScanner(false, false);\n+    return new Iterator<R>() {\n+      private R next = null;\n+      private boolean eof = false;\n+\n+      @Override\n+      public boolean hasNext() {\n+        try {\n+          // To handle when hasNext() is called multiple times for idempotency and/or the first time\n+          if (this.next == null && !this.eof) {\n+            if (!scanner.isSeeked() && scanner.seekTo()) {\n+                this.next = (R)readNextRecord(scanner.getKeyValue(), getSchema(), readerSchema);\n+            }\n+          }\n+          return this.next != null;\n+        } catch (IOException io) {\n+          throw new HoodieIOException(\"unable to read next record from hfile \", io);\n+        }\n+      }\n+\n+      @Override\n+      public R next() {\n+        try {\n+          // To handle case when next() is called before hasNext()\n+          if (this.next == null) {\n+            if (!hasNext()) {\n+              throw new HoodieIOException(\"No more records left to read from hfile\");\n+            }\n+          }\n+          R retVal = this.next;\n+          if (scanner.next()) {\n+            this.next = (R)readNextRecord(scanner.getKeyValue(), getSchema(), readerSchema);\n+          } else {\n+            this.next = null;\n+            this.eof = true;\n+          }\n+          return retVal;\n+        } catch (IOException io) {\n+          throw new HoodieIOException(\"unable to read next record from parquet file \", io);\n+        }\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public Option getRecordByKey(String key, Schema readerSchema) throws IOException {\n+    HFileScanner scanner = reader.getScanner(false, false);\n+    KeyValue kv = new KeyValue(key.getBytes(), null, null, null);\n+    if (scanner.seekTo(kv) == 0) {\n+      Cell c = scanner.getKeyValue();\n+      byte[] keyBytes = Arrays.copyOfRange(c.getRowArray(), c.getRowOffset(), c.getRowOffset() + c.getRowLength());\n+      R record = readNextRecord(c, getSchema(), readerSchema);\n+      return Option.of(record);\n+    }\n+\n+    return Option.empty();\n+  }\n+\n+  private R readNextRecord(Cell c, Schema writerSchema, Schema readerSchema) throws IOException {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTQ0OQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355449", "bodyText": "getRecordFromCell is good.", "author": "prashantwason", "createdAt": "2020-07-21T20:04:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NjA4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NjU4MQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456386581", "bodyText": "minor. why using hard coded string. we should use constant for \"schema\".", "author": "nsivabalan", "createdAt": "2020-07-17T11:34:12Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);\n+    FSDataInputStream fsdis = new FSDataInputStream(bis);\n+    this.reader = HFile.createReader(FSUtils.getFs(\"hoodie\", conf), path, new FSDataInputStreamWrapper(fsdis),\n+        content.length, new CacheConfig(conf), conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      return new String[] { new String(fileInfo.get(KEY_MIN_RECORD.getBytes())),\n+          new String(fileInfo.get(KEY_MAX_RECORD.getBytes()))};\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read min/max record key out of file information block correctly from path\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Schema getSchema() {\n+    if (schema == null) {\n+      try {\n+        Map<byte[], byte[]> fileInfo = reader.loadFileInfo();\n+        schema = new Schema.Parser().parse(new String(fileInfo.get(KEY_SCHEMA.getBytes())));\n+      } catch (IOException e) {\n+        throw new HoodieException(\"Could not read schema of file from path\", e);\n+      }\n+    }\n+\n+    return schema;\n+  }\n+\n+  @Override\n+  public BloomFilter readBloomFilter() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      ByteBuffer serializedFilter = reader.getMetaBlock(KEY_BLOOM_FILTER_META_BLOCK, false);\n+      byte[] filterBytes = new byte[serializedFilter.remaining()];\n+      serializedFilter.get(filterBytes); // read the bytes that were written\n+      return BloomFilterFactory.fromString(new String(filterBytes),\n+          new String(fileInfo.get(KEY_BLOOM_FILTER_TYPE_CODE.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read bloom filter from \" + path, e);\n+    }\n+  }\n+\n+  @Override\n+  public Set<String> filterRowKeys(Set candidateRowKeys) {\n+    try {\n+      List<Pair<String, R>> allRecords = readAllRecords();\n+      Set<String> rowKeys = new HashSet<>();\n+      allRecords.forEach(t -> {\n+        if (candidateRowKeys.contains(t.getFirst())) {\n+          rowKeys.add(t.getFirst());\n+        }\n+      });\n+      return rowKeys;\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to read row keys from \" + path, e);\n+    }\n+  }\n+\n+  public List<Pair<String, R>> readAllRecords(Schema writerSchema, Schema readerSchema) throws IOException {\n+    List<Pair<String, R>> recordList = new LinkedList<>();\n+    try {\n+      HFileScanner scanner = reader.getScanner(false, false);\n+      if (scanner.seekTo()) {\n+        do {\n+          Cell c = scanner.getKeyValue();\n+          byte[] keyBytes = Arrays.copyOfRange(c.getRowArray(), c.getRowOffset(), c.getRowOffset() + c.getRowLength());\n+          R record = readNextRecord(c, writerSchema, readerSchema);\n+          recordList.add(new Pair<>(new String(keyBytes), record));\n+        } while (scanner.next());\n+      }\n+\n+      return recordList;\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Error reading hfile \" + path + \" as a dataframe\", e);\n+    }\n+  }\n+\n+  public List<Pair<String, R>> readAllRecords() throws IOException {\n+    Schema schema = new Schema.Parser().parse(new String(reader.loadFileInfo().get(\"schema\".getBytes())));", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTQ5Mw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355493", "bodyText": "There is already a constant defined. Corrected.", "author": "prashantwason", "createdAt": "2020-07-21T20:04:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NjU4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NzQ1NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456387454", "bodyText": "why do we need this seekable. HFile reader by default support look up by key right. Can you help me understand", "author": "nsivabalan", "createdAt": "2020-07-17T11:36:27Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTUzMg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355532", "bodyText": "This constructor is creating an HFile.Reader from a byte array (bytes from a HFile Data Block saved in a log file). HFile.createReader constructor requires a FSDataInputStreamWrapper which requires a IO stream implementing \"Seekable\" interface.\nIn other words, this is required for creating a HFile.reader out of an in-memory byte array and is not related to the internals of the HFile reading logic.", "author": "prashantwason", "createdAt": "2020-07-21T20:04:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4NzQ1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4ODEyNQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456388125", "bodyText": "why cacheBlocks and pread args are hard coded? shouldn't we configurize it.", "author": "nsivabalan", "createdAt": "2020-07-17T11:38:14Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);\n+    FSDataInputStream fsdis = new FSDataInputStream(bis);\n+    this.reader = HFile.createReader(FSUtils.getFs(\"hoodie\", conf), path, new FSDataInputStreamWrapper(fsdis),\n+        content.length, new CacheConfig(conf), conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      return new String[] { new String(fileInfo.get(KEY_MIN_RECORD.getBytes())),\n+          new String(fileInfo.get(KEY_MAX_RECORD.getBytes()))};\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read min/max record key out of file information block correctly from path\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Schema getSchema() {\n+    if (schema == null) {\n+      try {\n+        Map<byte[], byte[]> fileInfo = reader.loadFileInfo();\n+        schema = new Schema.Parser().parse(new String(fileInfo.get(KEY_SCHEMA.getBytes())));\n+      } catch (IOException e) {\n+        throw new HoodieException(\"Could not read schema of file from path\", e);\n+      }\n+    }\n+\n+    return schema;\n+  }\n+\n+  @Override\n+  public BloomFilter readBloomFilter() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      ByteBuffer serializedFilter = reader.getMetaBlock(KEY_BLOOM_FILTER_META_BLOCK, false);\n+      byte[] filterBytes = new byte[serializedFilter.remaining()];\n+      serializedFilter.get(filterBytes); // read the bytes that were written\n+      return BloomFilterFactory.fromString(new String(filterBytes),\n+          new String(fileInfo.get(KEY_BLOOM_FILTER_TYPE_CODE.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read bloom filter from \" + path, e);\n+    }\n+  }\n+\n+  @Override\n+  public Set<String> filterRowKeys(Set candidateRowKeys) {\n+    try {\n+      List<Pair<String, R>> allRecords = readAllRecords();\n+      Set<String> rowKeys = new HashSet<>();\n+      allRecords.forEach(t -> {\n+        if (candidateRowKeys.contains(t.getFirst())) {\n+          rowKeys.add(t.getFirst());\n+        }\n+      });\n+      return rowKeys;\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to read row keys from \" + path, e);\n+    }\n+  }\n+\n+  public List<Pair<String, R>> readAllRecords(Schema writerSchema, Schema readerSchema) throws IOException {\n+    List<Pair<String, R>> recordList = new LinkedList<>();\n+    try {\n+      HFileScanner scanner = reader.getScanner(false, false);", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTU5MQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355591", "bodyText": "The usecase here is very limited - read and return all records once. So I feel the parameters should be specific to optimizing this usecase.\nCaching the blocks does not have any value as we are reading the blocks once and wont be reading again.\n\n@param pread\n\n     Use positional read rather than seek+read if true (pread is better\n\n\n\n     for random reads, seek+read is better scanning).\n\n\n\npread is false based on the above code comment in org.apache.hadoop.hbase.io.hfile.HFileReaderV3", "author": "prashantwason", "createdAt": "2020-07-21T20:04:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM4ODEyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM5MDY5NQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456390695", "bodyText": "why move to next element here itself. shouldn't we rely on hasNext() everytime to move to to next record as per the Iterator interface contract?", "author": "nsivabalan", "createdAt": "2020-07-17T11:44:42Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PositionedReadable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Pair;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieFileReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+  private Schema schema;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  public HoodieHFileReader(byte[] content) throws IOException {\n+    Configuration conf = new Configuration();\n+    Path path = new Path(\"hoodie\");\n+    SeekableByteArrayInputStream bis = new SeekableByteArrayInputStream(content);\n+    FSDataInputStream fsdis = new FSDataInputStream(bis);\n+    this.reader = HFile.createReader(FSUtils.getFs(\"hoodie\", conf), path, new FSDataInputStreamWrapper(fsdis),\n+        content.length, new CacheConfig(conf), conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      return new String[] { new String(fileInfo.get(KEY_MIN_RECORD.getBytes())),\n+          new String(fileInfo.get(KEY_MAX_RECORD.getBytes()))};\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read min/max record key out of file information block correctly from path\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Schema getSchema() {\n+    if (schema == null) {\n+      try {\n+        Map<byte[], byte[]> fileInfo = reader.loadFileInfo();\n+        schema = new Schema.Parser().parse(new String(fileInfo.get(KEY_SCHEMA.getBytes())));\n+      } catch (IOException e) {\n+        throw new HoodieException(\"Could not read schema of file from path\", e);\n+      }\n+    }\n+\n+    return schema;\n+  }\n+\n+  @Override\n+  public BloomFilter readBloomFilter() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      ByteBuffer serializedFilter = reader.getMetaBlock(KEY_BLOOM_FILTER_META_BLOCK, false);\n+      byte[] filterBytes = new byte[serializedFilter.remaining()];\n+      serializedFilter.get(filterBytes); // read the bytes that were written\n+      return BloomFilterFactory.fromString(new String(filterBytes),\n+          new String(fileInfo.get(KEY_BLOOM_FILTER_TYPE_CODE.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read bloom filter from \" + path, e);\n+    }\n+  }\n+\n+  @Override\n+  public Set<String> filterRowKeys(Set candidateRowKeys) {\n+    try {\n+      List<Pair<String, R>> allRecords = readAllRecords();\n+      Set<String> rowKeys = new HashSet<>();\n+      allRecords.forEach(t -> {\n+        if (candidateRowKeys.contains(t.getFirst())) {\n+          rowKeys.add(t.getFirst());\n+        }\n+      });\n+      return rowKeys;\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to read row keys from \" + path, e);\n+    }\n+  }\n+\n+  public List<Pair<String, R>> readAllRecords(Schema writerSchema, Schema readerSchema) throws IOException {\n+    List<Pair<String, R>> recordList = new LinkedList<>();\n+    try {\n+      HFileScanner scanner = reader.getScanner(false, false);\n+      if (scanner.seekTo()) {\n+        do {\n+          Cell c = scanner.getKeyValue();\n+          byte[] keyBytes = Arrays.copyOfRange(c.getRowArray(), c.getRowOffset(), c.getRowOffset() + c.getRowLength());\n+          R record = readNextRecord(c, writerSchema, readerSchema);\n+          recordList.add(new Pair<>(new String(keyBytes), record));\n+        } while (scanner.next());\n+      }\n+\n+      return recordList;\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Error reading hfile \" + path + \" as a dataframe\", e);\n+    }\n+  }\n+\n+  public List<Pair<String, R>> readAllRecords() throws IOException {\n+    Schema schema = new Schema.Parser().parse(new String(reader.loadFileInfo().get(\"schema\".getBytes())));\n+    return readAllRecords(schema, schema);\n+  }\n+\n+  @Override\n+  public Iterator getRecordIterator(Schema readerSchema) throws IOException {\n+    final HFileScanner scanner = reader.getScanner(false, false);\n+    return new Iterator<R>() {\n+      private R next = null;\n+      private boolean eof = false;\n+\n+      @Override\n+      public boolean hasNext() {\n+        try {\n+          // To handle when hasNext() is called multiple times for idempotency and/or the first time\n+          if (this.next == null && !this.eof) {\n+            if (!scanner.isSeeked() && scanner.seekTo()) {\n+                this.next = (R)readNextRecord(scanner.getKeyValue(), getSchema(), readerSchema);\n+            }\n+          }\n+          return this.next != null;\n+        } catch (IOException io) {\n+          throw new HoodieIOException(\"unable to read next record from hfile \", io);\n+        }\n+      }\n+\n+      @Override\n+      public R next() {\n+        try {\n+          // To handle case when next() is called before hasNext()\n+          if (this.next == null) {\n+            if (!hasNext()) {\n+              throw new HoodieIOException(\"No more records left to read from hfile\");\n+            }\n+          }\n+          R retVal = this.next;\n+          if (scanner.next()) {\n+            this.next = (R)readNextRecord(scanner.getKeyValue(), getSchema(), readerSchema);", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTYyNw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355627", "bodyText": "As I understand it, the iterator interface contract is:\nhasNext: Does another record exist?\nnext: Return the next record.\nWhether the next record is retrieved in hasNext or next is upto the implementation. If we retrieve the next record in hasNext, we need to additionally deal with idempotency - hasNext called multiple times before next called.\nThere could be a better implementation for sure but I have copied this from ParquetReaderIterator and looked ok to me.", "author": "prashantwason", "createdAt": "2020-07-21T20:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM5MDY5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQzODE5NQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456438195", "bodyText": "I see lot of similarities between this and HoodieParquetInputFormat. Is there a way to re-use the code in anyway.", "author": "nsivabalan", "createdAt": "2020-07-17T13:22:18Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileInputFormat.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.hadoop.utils.HoodieHiveUtils;\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * HoodieInputFormat for HUDI datasets which store data in HFile base file format.\n+ */\n+@UseFileSplitsFromInputFormat\n+public class HoodieHFileInputFormat extends FileInputFormat<NullWritable, ArrayWritable> implements Configurable {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileInputFormat.class);\n+\n+  protected Configuration conf;\n+\n+  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    return HoodieInputFormatUtils.filterInstantsTimeline(timeline);\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(JobConf job) throws IOException {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTY4NQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355685", "bodyText": "The code reuse is already being done as much as possible.\nThe way InputFormats are implemented in HUDI does not allow any further optimization:\n\nHoodieParquetInputFormat extends MapredParquetInputFormat\nHoodieHFileInputFormat extends FileInputFormat\nHoodieXXXXInputFormat will need to extend its own BaseInputFormat.\n\nHence, its not possible to define a base class for all InputFormats for maximum code reuse.", "author": "prashantwason", "createdAt": "2020-07-21T20:04:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQzODE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTk0MDM5Mw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475940393", "bodyText": "While this is true, we can use more helpers and avoid copying to a large degree?", "author": "vinothchandar", "createdAt": "2020-08-24T23:00:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQzODE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU1MjI0MA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477552240", "bodyText": "yes, thats what I meant, by using some helper class.", "author": "nsivabalan", "createdAt": "2020-08-26T19:54:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQzODE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQzODkwMQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456438901", "bodyText": "minor: fix java docs. // parquet -> hfile", "author": "nsivabalan", "createdAt": "2020-07-17T13:23:33Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.realtime;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.hadoop.HoodieHFileInputFormat;\n+import org.apache.hudi.hadoop.UseFileSplitsFromInputFormat;\n+import org.apache.hudi.hadoop.UseRecordReaderFromInputFormat;\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * HoodieRealtimeInputFormat for HUDI datasets which store data in HFile base file format.\n+ */\n+@UseRecordReaderFromInputFormat\n+@UseFileSplitsFromInputFormat\n+public class HoodieHFileRealtimeInputFormat extends HoodieHFileInputFormat {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileRealtimeInputFormat.class);\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    Stream<FileSplit> fileSplits = Arrays.stream(super.getSplits(job, numSplits)).map(is -> (FileSplit) is);\n+    return HoodieRealtimeInputFormatUtils.getRealtimeSplits(job, fileSplits);\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(JobConf job) throws IOException {\n+    // Call the HoodieInputFormat::listStatus to obtain all latest parquet files, based on commit", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQ0MDYwNg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456440606", "bodyText": "similar comment as above. can we see if we can re-use code if possible.", "author": "nsivabalan", "createdAt": "2020-07-17T13:26:15Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.realtime;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.hadoop.HoodieHFileInputFormat;\n+import org.apache.hudi.hadoop.UseFileSplitsFromInputFormat;\n+import org.apache.hudi.hadoop.UseRecordReaderFromInputFormat;\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * HoodieRealtimeInputFormat for HUDI datasets which store data in HFile base file format.\n+ */\n+@UseRecordReaderFromInputFormat\n+@UseFileSplitsFromInputFormat\n+public class HoodieHFileRealtimeInputFormat extends HoodieHFileInputFormat {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileRealtimeInputFormat.class);\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQ1MjkyNQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r456452925", "bodyText": "can you remind me why sorting is required here? my understanding is that newRecordKeysSorted is already sorted.", "author": "nsivabalan", "createdAt": "2020-07-17T13:47:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+\n+@SuppressWarnings(\"Duplicates\")\n+public class HoodieSortedMergeHandle<T extends HoodieRecordPayload> extends HoodieMergeHandle<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSortedMergeHandle.class);\n+\n+  private Queue<String> newRecordKeysSorted = new PriorityQueue<>();\n+\n+  public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n+       Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, sparkTaskContextSupplier);\n+    newRecordKeysSorted.addAll(keyToNewRecords.keySet());\n+  }\n+\n+  /**\n+   * Called by compactor code path.\n+   */\n+  public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n+      Map<String, HoodieRecord<T>> keyToNewRecordsOrig, String partitionPath, String fileId,\n+      HoodieBaseFile dataFileToBeMerged, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+    super(config, instantTime, hoodieTable, keyToNewRecordsOrig, partitionPath, fileId, dataFileToBeMerged,\n+        sparkTaskContextSupplier);\n+\n+    newRecordKeysSorted.addAll(keyToNewRecords.keySet());\n+  }\n+\n+  /**\n+   * Go through an old record. Here if we detect a newer version shows up, we write the new one to the file.\n+   */\n+  @Override\n+  public void write(GenericRecord oldRecord) {\n+    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+\n+    // To maintain overall sorted order across updates and inserts, write any new inserts whose keys are less than\n+    // the oldRecord's key.\n+    while (!newRecordKeysSorted.isEmpty() && newRecordKeysSorted.peek().compareTo(key) <= 0) {\n+      String keyToPreWrite = newRecordKeysSorted.remove();\n+      if (keyToPreWrite.equals(key)) {\n+        // will be handled as an update later\n+        break;\n+      }\n+\n+      // This is a new insert\n+      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(keyToPreWrite));\n+      if (writtenRecordKeys.contains(keyToPreWrite)) {\n+        throw new HoodieUpsertException(\"Insert/Update not in sorted order\");\n+      }\n+      try {\n+        if (useWriterSchema) {\n+          writeRecord(hoodieRecord, hoodieRecord.getData().getInsertValue(writerSchema));\n+        } else {\n+          writeRecord(hoodieRecord, hoodieRecord.getData().getInsertValue(originalSchema));\n+        }\n+        insertRecordsWritten++;\n+        writtenRecordKeys.add(keyToPreWrite);\n+      } catch (IOException e) {\n+        throw new HoodieUpsertException(\"Failed to write records\", e);\n+      }\n+    }\n+\n+    super.write(oldRecord);\n+  }\n+\n+  @Override\n+  public WriteStatus close() {\n+    // write out any pending records (this can happen when inserts are turned into updates)\n+    newRecordKeysSorted.stream().sorted().forEach(key -> {", "originalCommit": "b4d89ff8f80e20231ffaa774a0e550bd2450f798", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM1NTc5Mg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r458355792", "bodyText": "Yes, sorting is not required here. Leftover from earlier where I was not using PriorityQueue for newRecordKeysSorted.", "author": "prashantwason", "createdAt": "2020-07-21T20:05:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQ1MjkyNQ=="}], "type": "inlineReview"}, {"oid": "b9beb471c6d892a9deb81b614647fc26c7e1989f", "url": "https://github.com/apache/hudi/commit/b9beb471c6d892a9deb81b614647fc26c7e1989f", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-07-06T21:25:59Z", "type": "forcePushed"}, {"oid": "044412ce619287134088184e56cebc25772d8027", "url": "https://github.com/apache/hudi/commit/044412ce619287134088184e56cebc25772d8027", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-07-21T20:02:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTEwOTYxOQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r461109619", "bodyText": "for now, I guess it's okay to assume the records will fit into memory? eventually we need to make this sorting spillable (using rocksDB for eg) for RFC-08 indexing work", "author": "vinothchandar", "createdAt": "2020-07-27T19:11:25Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+\n+/**\n+ * Hoodie merge handle which writes records (new inserts or updates) sorted by their key.\n+ *\n+ * The implementation performs a merge-sort by comparing the key of the record being written to the list of\n+ * keys in newRecordKeys (sorted in-memory).\n+ */\n+public class HoodieSortedMergeHandle<T extends HoodieRecordPayload> extends HoodieMergeHandle<T> {\n+\n+  private Queue<String> newRecordKeysSorted = new PriorityQueue<>();", "originalCommit": "044412ce619287134088184e56cebc25772d8027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwODg3Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r464708877", "bodyText": "The assumption here is that the record keys (for the records being updated) will fit into memory. These record keys are extracted from the keySet of keyToNewRecords which is already a ExternalSpillableMap.", "author": "prashantwason", "createdAt": "2020-08-03T23:12:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTEwOTYxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTExMTc2Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r461111767", "bodyText": "I guess you are holding the entire byte[] corresponding to the HFile in memory and then logging this as  the payload?", "author": "vinothchandar", "createdAt": "2020-07-27T19:15:34Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.storage.HoodieHFileReader;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.compress.Compression;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.hbase.util.Pair;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * HoodieHFileDataBlock contains a list of records stored inside an HFile format. It is used with the HFile\n+ * base file format.\n+ */\n+public class HoodieHFileDataBlock extends HoodieDataBlock {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileDataBlock.class);\n+  private static Compression.Algorithm compressionAlgorithm = Compression.Algorithm.GZ;\n+  private static int blockSize = 1 * 1024 * 1024;\n+\n+  public HoodieHFileDataBlock(@Nonnull Map<HeaderMetadataType, String> logBlockHeader,\n+       @Nonnull Map<HeaderMetadataType, String> logBlockFooter,\n+       @Nonnull Option<HoodieLogBlockContentLocation> blockContentLocation, @Nonnull Option<byte[]> content,\n+       FSDataInputStream inputStream, boolean readBlockLazily) {\n+    super(logBlockHeader, logBlockFooter, blockContentLocation, content, inputStream, readBlockLazily);\n+  }\n+\n+  public HoodieHFileDataBlock(HoodieLogFile logFile, FSDataInputStream inputStream, Option<byte[]> content,\n+       boolean readBlockLazily, long position, long blockSize, long blockEndpos, Schema readerSchema,\n+       Map<HeaderMetadataType, String> header, Map<HeaderMetadataType, String> footer) {\n+    super(content, inputStream, readBlockLazily,\n+          Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndpos)), readerSchema, header,\n+          footer);\n+  }\n+\n+  public HoodieHFileDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<HeaderMetadataType, String> header) {\n+    super(records, header, new HashMap<>());\n+  }\n+\n+  @Override\n+  public HoodieLogBlockType getBlockType() {\n+    return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+  }\n+\n+  @Override\n+  protected byte[] serializeRecords() throws IOException {\n+    HFileContext context = new HFileContextBuilder().withBlockSize(blockSize).withCompression(compressionAlgorithm)\n+        .build();\n+    Configuration conf = new Configuration();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    FSDataOutputStream ostream = new FSDataOutputStream(baos, null);\n+\n+    HFile.Writer writer = HFile.getWriterFactory(conf, cacheConfig)", "originalCommit": "044412ce619287134088184e56cebc25772d8027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTExMjQ5MQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r461112491", "bodyText": "This seems fine for metadata logging . eventually, we may want to think about writing directly to the underlying fs . This needs some changes on log format writing per se.", "author": "vinothchandar", "createdAt": "2020-07-27T19:16:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTExMTc2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwOTc2MQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r464709761", "bodyText": "Yes. This is similar to how the Avro Log Block is being written right now with the records being serialized into ByteArrayOutputStream in-memory.", "author": "prashantwason", "createdAt": "2020-08-03T23:14:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTExMTc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTExMzcxMg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r461113712", "bodyText": "Can we explore use of InlineFileSystem to read HFile as-is. This was you can actually do point lookup on teh log blocks, just like you do on the base file. it will be significantly faster", "author": "vinothchandar", "createdAt": "2020-07-27T19:19:25Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.storage.HoodieHFileReader;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.compress.Compression;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.hbase.util.Pair;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * HoodieHFileDataBlock contains a list of records stored inside an HFile format. It is used with the HFile\n+ * base file format.\n+ */\n+public class HoodieHFileDataBlock extends HoodieDataBlock {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileDataBlock.class);\n+  private static Compression.Algorithm compressionAlgorithm = Compression.Algorithm.GZ;\n+  private static int blockSize = 1 * 1024 * 1024;\n+\n+  public HoodieHFileDataBlock(@Nonnull Map<HeaderMetadataType, String> logBlockHeader,\n+       @Nonnull Map<HeaderMetadataType, String> logBlockFooter,\n+       @Nonnull Option<HoodieLogBlockContentLocation> blockContentLocation, @Nonnull Option<byte[]> content,\n+       FSDataInputStream inputStream, boolean readBlockLazily) {\n+    super(logBlockHeader, logBlockFooter, blockContentLocation, content, inputStream, readBlockLazily);\n+  }\n+\n+  public HoodieHFileDataBlock(HoodieLogFile logFile, FSDataInputStream inputStream, Option<byte[]> content,\n+       boolean readBlockLazily, long position, long blockSize, long blockEndpos, Schema readerSchema,\n+       Map<HeaderMetadataType, String> header, Map<HeaderMetadataType, String> footer) {\n+    super(content, inputStream, readBlockLazily,\n+          Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndpos)), readerSchema, header,\n+          footer);\n+  }\n+\n+  public HoodieHFileDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<HeaderMetadataType, String> header) {\n+    super(records, header, new HashMap<>());\n+  }\n+\n+  @Override\n+  public HoodieLogBlockType getBlockType() {\n+    return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+  }\n+\n+  @Override\n+  protected byte[] serializeRecords() throws IOException {\n+    HFileContext context = new HFileContextBuilder().withBlockSize(blockSize).withCompression(compressionAlgorithm)\n+        .build();\n+    Configuration conf = new Configuration();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    FSDataOutputStream ostream = new FSDataOutputStream(baos, null);\n+\n+    HFile.Writer writer = HFile.getWriterFactory(conf, cacheConfig)\n+        .withOutputStream(ostream).withFileContext(context).create();\n+\n+    // Serialize records into bytes\n+    Map<String, byte[]> recordMap = new TreeMap<>();\n+    Iterator<IndexedRecord> itr = records.iterator();\n+    boolean useIntegerKey = false;\n+    int key = 0;\n+    int keySize = 0;\n+    Field keyField = records.get(0).getSchema().getField(HoodieRecord.RECORD_KEY_METADATA_FIELD);\n+    if (keyField == null) {\n+      // Missing key metadata field so we should use an integer sequence key\n+      useIntegerKey = true;\n+      keySize = (int) Math.ceil(Math.log(records.size())) + 1;\n+    }\n+    while (itr.hasNext()) {\n+      IndexedRecord record = itr.next();\n+      String recordKey;\n+      if (useIntegerKey) {\n+        recordKey = String.format(\"%\" + keySize + \"s\", key++);\n+      } else {\n+        recordKey = record.get(keyField.pos()).toString();\n+      }\n+      byte[] recordBytes = HoodieAvroUtils.avroToBytes(record);\n+      recordMap.put(recordKey, recordBytes);\n+    }\n+\n+    // Write the records\n+    recordMap.forEach((recordKey, recordBytes) -> {\n+      try {\n+        KeyValue kv = new KeyValue(recordKey.getBytes(), null, null, recordBytes);\n+        writer.append(kv);\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"IOException serializing records\", e);\n+      }\n+    });\n+\n+    writer.close();\n+    ostream.flush();\n+    ostream.close();\n+\n+    return baos.toByteArray();\n+  }\n+\n+  @Override\n+  protected void deserializeRecords() throws IOException {\n+    // Get schema from the header\n+    Schema writerSchema = new Schema.Parser().parse(super.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));\n+\n+    // If readerSchema was not present, use writerSchema\n+    if (schema == null) {\n+      schema = writerSchema;\n+    }\n+\n+    // Read the content\n+    HoodieHFileReader reader = new HoodieHFileReader<>(getContent().get());", "originalCommit": "044412ce619287134088184e56cebc25772d8027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNDQ2OQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r464714469", "bodyText": "The log record scanning infrastructure (AbstractHoodieLogRecordScanner and derived classes) do not support this optimization.\nAbstractHoodieLogRecordScanner reads each data block from the log file into memory and then calls HoodieLogblock.getRecords(). Since the contents of the entire log block are already in memory, it is simpler to create a HoodieHFileReader on the byte[].\nBut you have a valid point. If the AbstractHoodieLogRecordScanner could be changed to provide the offset of the data block instead of reading it fully (i.e. it can provide a InLineFsDataInputStream) then HFileReader can do point lookups easily. But this may or may not improve performance because of the following reasons:\n\nIn RFC-15 we need to read all the records anyways since we are creating an updated in-memory state of the change to metadata. So point look does not help much here.\nIn RFC-08, point lookups should help if the number of key lookups are relatively smaller than total keys in the block. (within HFile full file scan may be faster in some cases).\n\nGood idea but probably can be implemented separately.", "author": "prashantwason", "createdAt": "2020-08-03T23:30:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTExMzcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzOTU1OA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475939558", "bodyText": "understood. I had a different view of what RFC-15 would do.\nwe will track this for RFC-08 cc @n3nash", "author": "vinothchandar", "createdAt": "2020-08-24T22:57:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTExMzcxMg=="}], "type": "inlineReview"}, {"oid": "396dc657c587d7431bec5ffd1f7803bf4e801b36", "url": "https://github.com/apache/hudi/commit/396dc657c587d7431bec5ffd1f7803bf4e801b36", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-04T00:13:50Z", "type": "forcePushed"}, {"oid": "b3c7aeca22da427e2559614a41a617f9d5ade42f", "url": "https://github.com/apache/hudi/commit/b3c7aeca22da427e2559614a41a617f9d5ade42f", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-04T01:00:02Z", "type": "forcePushed"}, {"oid": "de706fa0de998865c757a6beaa925ef153da8eea", "url": "https://github.com/apache/hudi/commit/de706fa0de998865c757a6beaa925ef153da8eea", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-04T23:33:37Z", "type": "forcePushed"}, {"oid": "f51ba5aa91d09bc894a7e2d376e1710644b6ee1f", "url": "https://github.com/apache/hudi/commit/f51ba5aa91d09bc894a7e2d376e1710644b6ee1f", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-04T23:57:36Z", "type": "forcePushed"}, {"oid": "96db8f169e8ccdef08531a2ad802c61935951f9e", "url": "https://github.com/apache/hudi/commit/96db8f169e8ccdef08531a2ad802c61935951f9e", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-05T07:14:37Z", "type": "forcePushed"}, {"oid": "7fc6f4f3e07d16139411c16662ddabc92750b17c", "url": "https://github.com/apache/hudi/commit/7fc6f4f3e07d16139411c16662ddabc92750b17c", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-05T07:36:55Z", "type": "forcePushed"}, {"oid": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "url": "https://github.com/apache/hudi/commit/231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-24T18:18:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxNTg3MQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475815871", "bodyText": "not sure if its a good idea to overload two configs like this. we may need to break this builder method up separately.", "author": "vinothchandar", "createdAt": "2020-08-24T18:34:20Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieStorageConfig.java", "diffHunk": "@@ -81,6 +87,7 @@ public Builder fromProperties(Properties props) {\n \n     public Builder limitFileSize(long maxFileSize) {\n       props.setProperty(PARQUET_FILE_MAX_BYTES, String.valueOf(maxFileSize));\n+      props.setProperty(HFILE_FILE_MAX_BYTES, String.valueOf(maxFileSize));", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA2OTcyNA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477069724", "bodyText": "I overloaded for two reasons:\n\nchanging this function name will be backward incompatible\nEach HUDI Write Config can only have one type of base files. So limitFileSIze is the limit of the base file size.\n\nHow about changing the key from HFILE_FILE_MAX_BYTES / PARQUET_FILE_MAX_BYTES to BASE_FILE_MAX_BYTES ?", "author": "prashantwason", "createdAt": "2020-08-26T06:44:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxNTg3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU1Mjg0MQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477552841", "bodyText": "@vinothchandar I see that you split this function into file specific functions. That doable but with more base file formats added, it may be cumbersome and verbose to keep adding the .limitXXXFileSize for specific formats.\nI am ok either way.", "author": "prashantwason", "createdAt": "2020-08-26T19:55:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxNTg3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxNzE3Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475817176", "bodyText": "need to ensure that having this be a map wont affect normal inserts.", "author": "vinothchandar", "createdAt": "2020-08-24T18:36:48Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java", "diffHunk": "@@ -55,7 +56,7 @@\n   private long recordsWritten = 0;\n   private long insertRecordsWritten = 0;\n   private long recordsDeleted = 0;\n-  private Iterator<HoodieRecord<T>> recordIterator;\n+  private Map<String, HoodieRecord<T>> recordMap;", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzMjc0OA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475932748", "bodyText": "okay understood this better. looks good for now", "author": "vinothchandar", "createdAt": "2020-08-24T22:37:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxNzE3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxOTczNw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475819737", "bodyText": "instead, we can just do a streaming sort-merge?", "author": "vinothchandar", "createdAt": "2020-08-24T18:41:26Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+\n+/**\n+ * Hoodie merge handle which writes records (new inserts or updates) sorted by their key.\n+ *\n+ * The implementation performs a merge-sort by comparing the key of the record being written to the list of\n+ * keys in newRecordKeys (sorted in-memory).\n+ */\n+public class HoodieSortedMergeHandle<T extends HoodieRecordPayload> extends HoodieMergeHandle<T> {\n+\n+  private Queue<String> newRecordKeysSorted = new PriorityQueue<>();\n+\n+  public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n+       Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, sparkTaskContextSupplier);\n+    newRecordKeysSorted.addAll(keyToNewRecords.keySet());\n+  }\n+\n+  /**\n+   * Called by compactor code path.\n+   */\n+  public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n+      Map<String, HoodieRecord<T>> keyToNewRecordsOrig, String partitionPath, String fileId,\n+      HoodieBaseFile dataFileToBeMerged, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+    super(config, instantTime, hoodieTable, keyToNewRecordsOrig, partitionPath, fileId, dataFileToBeMerged,\n+        sparkTaskContextSupplier);\n+\n+    newRecordKeysSorted.addAll(keyToNewRecords.keySet());\n+  }\n+\n+  /**\n+   * Go through an old record. Here if we detect a newer version shows up, we write the new one to the file.\n+   */\n+  @Override\n+  public void write(GenericRecord oldRecord) {\n+    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+\n+    // To maintain overall sorted order across updates and inserts, write any new inserts whose keys are less than\n+    // the oldRecord's key.\n+    while (!newRecordKeysSorted.isEmpty() && newRecordKeysSorted.peek().compareTo(key) <= 0) {\n+      String keyToPreWrite = newRecordKeysSorted.remove();", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzNTcwMA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475935700", "bodyText": "cc @prashantwason can you please chime in. I feel we can avoid the queue altogether and just sort merge?", "author": "vinothchandar", "createdAt": "2020-08-24T22:46:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxOTczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA3NDQ1Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477074456", "bodyText": "This is a streaming sort-merge. The logic is as follows:\n\nHold the keys of records which have changed in memory (in a PriorityQueue). This is in addition to the Map whic his already there in HoodieMergeHandle.\nFor each write()\n\nif the key of the record being written < head of PriorityQueue  => Write out this record\nif the key of the record being written > head of PriorityQueue  => Write out all records with smaller keys\n\n\n\nDo you have some other algorithm in mind?", "author": "prashantwason", "createdAt": "2020-08-26T06:54:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxOTczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ4Mzg2MA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478483860", "bodyText": "I am thinking we don't need the map in HoodieMergeHandle or the priorityQueue. The record which have changeed i.e. the input iterator is already sorted. lets call it inputItr\nSo , we can just compare the recordBeingWritten with inputItr.next() and write out the smallest one, if equal, we call the payload to merge.\nThis will avoid any kind of memory overhead", "author": "vinothchandar", "createdAt": "2020-08-27T14:57:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxOTczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY1NTIwMQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478655201", "bodyText": "If the inputItr is sorted then yes all this overhead can be removed.", "author": "prashantwason", "createdAt": "2020-08-27T19:45:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgxOTczNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgyNzgzOQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475827839", "bodyText": "are these configs used at all now?", "author": "vinothchandar", "createdAt": "2020-08-24T18:56:21Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieHFileWriter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.fs.HoodieWrapperFileSystem;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * HoodieHFileWriter writes IndexedRecords into an HFile. The record's key is used as the key and the\n+ * AVRO encoded record bytes are saved as the value.\n+ *\n+ * Limitations (compared to columnar formats like Parquet or ORC):\n+ *  1. Records should be added in order of keys\n+ *  2. There are no column stats\n+ */\n+public class HoodieHFileWriter<T extends HoodieRecordPayload, R extends IndexedRecord>\n+    implements HoodieFileWriter<R> {\n+  private static AtomicLong recordIndex = new AtomicLong(1);\n+\n+  private final Path file;\n+  private HoodieHFileConfig hfileConfig;\n+  private final HoodieWrapperFileSystem fs;\n+  private final long maxFileSize;\n+  private final String instantTime;\n+  private final SparkTaskContextSupplier sparkTaskContextSupplier;\n+  private HFile.Writer writer;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieHFileWriter(String instantTime, Path file, HoodieHFileConfig hfileConfig, Schema schema,\n+      SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+\n+    Configuration conf = FSUtils.registerFileSystem(file, hfileConfig.getHadoopConf());\n+    this.file = HoodieWrapperFileSystem.convertToHoodiePath(file, conf);\n+    this.fs = (HoodieWrapperFileSystem) this.file.getFileSystem(conf);\n+    this.hfileConfig = hfileConfig;\n+\n+    // TODO - compute this compression ratio dynamically by looking at the bytes written to the\n+    // stream and the actual file size reported by HDFS\n+    // this.maxFileSize = hfileConfig.getMaxFileSize()", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzMDI5MA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475830290", "bodyText": "this will pay additional merge costs", "author": "vinothchandar", "createdAt": "2020-08-24T19:01:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/compact/HoodieMergeOnReadTableCompactor.java", "diffHunk": "@@ -135,15 +135,14 @@\n \n     // Compacting is very similar to applying updates to existing file\n     Iterator<List<WriteStatus>> result;\n-    // If the dataFile is present, there is a base parquet file present, perform updates else perform inserts into a\n-    // new base parquet file.\n+    // If the dataFile is present, perform updates else perform inserts into a new base file.\n     if (oldDataFileOpt.isPresent()) {\n       result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n               operation.getFileId(), scanner.getRecords(),\n           oldDataFileOpt.get());\n     } else {\n       result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n-          scanner.iterator());\n+          scanner.getRecords());", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0MjE3Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477542176", "bodyText": "if we have two overloaded methods, then we could use table. requireSortedRecords() and call appropriate methods either w/ iterator or with records.", "author": "nsivabalan", "createdAt": "2020-08-26T19:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzMDI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzMzU3MQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475833571", "bodyText": "throw UnsupportedException", "author": "vinothchandar", "createdAt": "2020-08-24T19:07:26Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetReader.java", "diffHunk": "@@ -77,4 +79,9 @@ public long getTotalRecords() {\n     // TODO Auto-generated method stub\n     return 0;\n   }\n+\n+  @Override\n+  public Option getRecordByKey(String key, Schema schema) throws IOException {\n+    throw new HoodieException(\"HoodieParquetReader does not support reading records by key\");", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQ4NjI4Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477486286", "bodyText": "Done.", "author": "prashantwason", "createdAt": "2020-08-26T17:59:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzMzU3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzNDM2Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475834367", "bodyText": "is this dead code?", "author": "vinothchandar", "createdAt": "2020-08-24T19:08:52Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -83,68 +79,6 @@ protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline tim\n     return timeline;\n   }\n \n-  /**\n-   * Add a field to the existing fields projected.\n-   */\n-  private static Configuration addProjectionField(Configuration conf, String fieldName, int fieldIndex) {", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzNDgzOA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475834838", "bodyText": "is this copy-pasting some other test? room for code re-use?", "author": "vinothchandar", "createdAt": "2020-08-24T19:09:49Z", "path": "hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieHFileInputFormat.java", "diffHunk": "@@ -0,0 +1,426 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop;\n+\n+import org.apache.hudi.avro.model.HoodieCompactionPlan;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.hadoop.testutils.InputFormatTestUtil;\n+import org.apache.hudi.hadoop.utils.HoodieHiveUtils;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+public class TestHoodieHFileInputFormat {", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzNjg3Mw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477536873", "bodyText": "This is the same test as TestHoodieParquetInputFormat.java\nI could not combine the two as the test calls various methods on the InputFormat (HoodieParquetInputFormat or HoodieHFileInputFormat) and we currently do not have a common base class. So this can be combined once we re-work the input formats.", "author": "prashantwason", "createdAt": "2020-08-26T19:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzNDgzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzNTE3Mg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475835172", "bodyText": "this is good. but will add to our integration test runtime by a lot. we should disable this and enabled later. need a JIRA", "author": "vinothchandar", "createdAt": "2020-08-24T19:10:25Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java", "diffHunk": "@@ -115,6 +115,35 @@ public void testParquetDemo() throws Exception {\n     testIncrementalHiveQueryAfterCompaction();\n   }\n \n+  @Test\n+  public void testHFileDemo() throws Exception {", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA3ODAwMA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477078000", "bodyText": "Ok.", "author": "prashantwason", "createdAt": "2020-08-26T07:02:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzNTE3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQ5MTUyMQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477491521", "bodyText": "Filed https://issues.apache.org/jira/browse/HUDI-1229", "author": "prashantwason", "createdAt": "2020-08-26T18:08:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgzNTE3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg5MzM4Ng==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475893386", "bodyText": "@prashantwason it's best we don't import scala methods into java. com.databricks/avro is simply deprecated. We have eequivalent functionality in spark-avro itself. In general we have to fix this test code that reads HFile as a DataSet in a different way .", "author": "vinothchandar", "createdAt": "2020-08-24T21:04:44Z", "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java", "diffHunk": "@@ -45,22 +45,39 @@\n import org.apache.avro.generic.GenericRecord;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.parquet.avro.AvroSchemaConverter;\n import org.apache.parquet.hadoop.ParquetWriter;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+import com.databricks.spark.avro.SchemaConverters;\n+\n+import scala.Function1;", "originalCommit": "231bdacb6cbd12ad7511ffecde9ee1e13a690cc7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkwNzMwOA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475907308", "bodyText": "Figured out a way to fix it for now. tests seem happy", "author": "vinothchandar", "createdAt": "2020-08-24T21:34:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg5MzM4Ng=="}], "type": "inlineReview"}, {"oid": "41337158fd9bbc3b123849abaa6957e352eb1ef9", "url": "https://github.com/apache/hudi/commit/41337158fd9bbc3b123849abaa6957e352eb1ef9", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc", "committedDate": "2020-08-24T21:37:24Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzNjIyNw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475936227", "bodyText": "So, no code calls this atm ? @prashantwason\nThe following code in HoodieMergeOnReadTableCompactor\nif (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    }\n\nends up calling the following.\n protected HoodieMergeHandle getUpdateHandle(String instantTime, String partitionPath, String fileId,\n      Map<String, HoodieRecord<T>> keyToNewRecords, HoodieBaseFile dataFileToBeMerged) {\n    return new HoodieMergeHandle<>(config, instantTime, this, keyToNewRecords,\n            partitionPath, fileId, dataFileToBeMerged, sparkTaskContextSupplier);\n  }", "author": "vinothchandar", "createdAt": "2020-08-24T22:48:13Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+\n+/**\n+ * Hoodie merge handle which writes records (new inserts or updates) sorted by their key.\n+ *\n+ * The implementation performs a merge-sort by comparing the key of the record being written to the list of\n+ * keys in newRecordKeys (sorted in-memory).\n+ */\n+public class HoodieSortedMergeHandle<T extends HoodieRecordPayload> extends HoodieMergeHandle<T> {\n+\n+  private Queue<String> newRecordKeysSorted = new PriorityQueue<>();\n+\n+  public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n+       Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, sparkTaskContextSupplier);\n+    newRecordKeysSorted.addAll(keyToNewRecords.keySet());\n+  }\n+\n+  /**\n+   * Called by compactor code path.\n+   */\n+  public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,", "originalCommit": "41337158fd9bbc3b123849abaa6957e352eb1ef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA3MTM2NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477071364", "bodyText": "We will need to change the HoodieMergeOnReadTableCompactor code to use HoodieSortedMergeHandle for HFile cases.", "author": "prashantwason", "createdAt": "2020-08-26T06:48:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzNjIyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzNDY5Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477534697", "bodyText": "Updated.", "author": "prashantwason", "createdAt": "2020-08-26T19:21:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkzNjIyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTk0MDgwOQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r475940809", "bodyText": "we need to share code somehow. This same large comment need not be in multiple places", "author": "vinothchandar", "createdAt": "2020-08-24T23:01:53Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.realtime;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.hadoop.HoodieHFileInputFormat;\n+import org.apache.hudi.hadoop.UseFileSplitsFromInputFormat;\n+import org.apache.hudi.hadoop.UseRecordReaderFromInputFormat;\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * HoodieRealtimeInputFormat for HUDI datasets which store data in HFile base file format.\n+ */\n+@UseRecordReaderFromInputFormat\n+@UseFileSplitsFromInputFormat\n+public class HoodieHFileRealtimeInputFormat extends HoodieHFileInputFormat {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileRealtimeInputFormat.class);\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    Stream<FileSplit> fileSplits = Arrays.stream(super.getSplits(job, numSplits)).map(is -> (FileSplit) is);\n+    return HoodieRealtimeInputFormatUtils.getRealtimeSplits(job, fileSplits);\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(JobConf job) throws IOException {\n+    // Call the HoodieInputFormat::listStatus to obtain all latest hfiles, based on commit timeline.\n+    return super.listStatus(job);\n+  }\n+\n+  @Override\n+  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    // no specific filtering for Realtime format\n+    return timeline;\n+  }\n+\n+  @Override\n+  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n+      final Reporter reporter) throws IOException {\n+    // Hive on Spark invokes multiple getRecordReaders from different threads in the same spark task (and hence the", "originalCommit": "41337158fd9bbc3b123849abaa6957e352eb1ef9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ba6badbe61574315e4b2327184ea19dfc5ee13e6", "url": "https://github.com/apache/hudi/commit/ba6badbe61574315e4b2327184ea19dfc5ee13e6", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize()", "committedDate": "2020-08-25T00:04:19Z", "type": "forcePushed"}, {"oid": "5891ffdeb92b6482acdb6da4912da3e291b2c9a7", "url": "https://github.com/apache/hudi/commit/5891ffdeb92b6482acdb6da4912da3e291b2c9a7", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize()", "committedDate": "2020-08-25T13:16:12Z", "type": "forcePushed"}, {"oid": "1d972f2438671d360101f385e1f776015e9e7468", "url": "https://github.com/apache/hudi/commit/1d972f2438671d360101f385e1f776015e9e7468", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-25T23:38:01Z", "type": "commit"}, {"oid": "346c798493aa177f5edad9267f9ca0f0a24eefcf", "url": "https://github.com/apache/hudi/commit/346c798493aa177f5edad9267f9ca0f0a24eefcf", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize()", "committedDate": "2020-08-25T23:38:01Z", "type": "forcePushed"}, {"oid": "aa11837b588b919b029e837ad945568d369e6d4b", "url": "https://github.com/apache/hudi/commit/aa11837b588b919b029e837ad945568d369e6d4b", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize()", "committedDate": "2020-08-26T01:08:46Z", "type": "commit"}, {"oid": "aa11837b588b919b029e837ad945568d369e6d4b", "url": "https://github.com/apache/hudi/commit/aa11837b588b919b029e837ad945568d369e6d4b", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize()", "committedDate": "2020-08-26T01:08:46Z", "type": "forcePushed"}, {"oid": "5a59b14de7af988f44ec0bf9e252202f2f2ae158", "url": "https://github.com/apache/hudi/commit/5a59b14de7af988f44ec0bf9e252202f2f2ae158", "message": "Some more improvements\n - Added three new configs for HoodieHFileConfig - prefetchBlocksOnOpen, cacheDataInL1, dropBehindCacheCompaction\n - Throw UnsupportedException in HFileReader.getRecordKeys()\n - Updated HoodieCopyOnWriteTable to create the correct merge handle (HoodieSortedMergeHandle for HFile and HoodieMergeHandle otherwise)", "committedDate": "2020-08-26T19:56:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyOTY5Mg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477529692", "bodyText": "Not sure if we can leak the type of base file to compactor. But did you think about having two overloaded methods here. So for parquet compaction path, iterator will be passed in, where as for hfile compaction, record map will be passed in.", "author": "nsivabalan", "createdAt": "2020-08-26T19:11:41Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java", "diffHunk": "@@ -90,9 +91,10 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa\n    * Called by the compactor code path.\n    */\n   public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-      String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordIterator, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+      String partitionPath, String fileId, Map<String, HoodieRecord<T>> recordMap,", "originalCommit": "aa11837b588b919b029e837ad945568d369e6d4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ4MDU0NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478480544", "bodyText": "Ideally not. the more the compactor can function wihtout knowing the base file type specifics, the better", "author": "vinothchandar", "createdAt": "2020-08-27T14:53:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyOTY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0NTA2OQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477545069", "bodyText": "@vinothchandar : wrt you comment on having two diff configs. I see similar configs at other places too. like bloom index parallelism, we have one config per index type. Initially I thought we will have any one config which will be used by any index type that is being initialized. But I saw that every index has its own set of configs and don't share any.", "author": "nsivabalan", "createdAt": "2020-08-26T19:41:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieStorageConfig.java", "diffHunk": "@@ -94,6 +100,16 @@ public Builder parquetPageSize(int pageSize) {\n       return this;\n     }\n \n+    public Builder hfileMaxFileSize(long maxFileSize) {\n+      props.setProperty(HFILE_FILE_MAX_BYTES, String.valueOf(maxFileSize));", "originalCommit": "aa11837b588b919b029e837ad945568d369e6d4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ3OTQ1Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478479457", "bodyText": "not following. sorry. are you suggesting having a single config or two?\nSo, we need to have a config per usage of HFile. so we can control the base file size for data, metadata, record index separately.\nWe cannot have a generic base.file.size or hfile.size config here, at this level IMO. cc @prashantwason", "author": "vinothchandar", "createdAt": "2020-08-27T14:51:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0NTA2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ5MDk4MA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478490980", "bodyText": "I see that you split this function into file specific functions. That doable but with more base file formats added, it may be cumbersome and verbose to keep adding the .limitXXXFileSize for specific formats.\n\n@prashantwason I think we need to eventually have a config \"per use\" of base file - data, metadata, index - since people may want to control them differently. So, in that sense, this has to kind of change.\nyes the change is backwards compatible to RDD clients (which I thought was okay, since its just uber. if you prefer to not have that, lmk. IMO, its about time, we cleaned these up, given we are moving to having way more base files/tables in the mix)", "author": "vinothchandar", "createdAt": "2020-08-27T15:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0NTA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0OTQwMA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477549400", "bodyText": "minor. why call this as sortedRecordsMap. I don't see any sorting actually", "author": "nsivabalan", "createdAt": "2020-08-26T19:49:14Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.storage.HoodieHFileReader;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.compress.Compression;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.hbase.util.Pair;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * HoodieHFileDataBlock contains a list of records stored inside an HFile format. It is used with the HFile\n+ * base file format.\n+ */\n+public class HoodieHFileDataBlock extends HoodieDataBlock {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileDataBlock.class);\n+  private static Compression.Algorithm compressionAlgorithm = Compression.Algorithm.GZ;\n+  private static int blockSize = 1 * 1024 * 1024;\n+\n+  public HoodieHFileDataBlock(@Nonnull Map<HeaderMetadataType, String> logBlockHeader,\n+       @Nonnull Map<HeaderMetadataType, String> logBlockFooter,\n+       @Nonnull Option<HoodieLogBlockContentLocation> blockContentLocation, @Nonnull Option<byte[]> content,\n+       FSDataInputStream inputStream, boolean readBlockLazily) {\n+    super(logBlockHeader, logBlockFooter, blockContentLocation, content, inputStream, readBlockLazily);\n+  }\n+\n+  public HoodieHFileDataBlock(HoodieLogFile logFile, FSDataInputStream inputStream, Option<byte[]> content,\n+       boolean readBlockLazily, long position, long blockSize, long blockEndpos, Schema readerSchema,\n+       Map<HeaderMetadataType, String> header, Map<HeaderMetadataType, String> footer) {\n+    super(content, inputStream, readBlockLazily,\n+          Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndpos)), readerSchema, header,\n+          footer);\n+  }\n+\n+  public HoodieHFileDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<HeaderMetadataType, String> header) {\n+    super(records, header, new HashMap<>());\n+  }\n+\n+  @Override\n+  public HoodieLogBlockType getBlockType() {\n+    return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+  }\n+\n+  @Override\n+  protected byte[] serializeRecords() throws IOException {\n+    HFileContext context = new HFileContextBuilder().withBlockSize(blockSize).withCompression(compressionAlgorithm)\n+        .build();\n+    Configuration conf = new Configuration();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    FSDataOutputStream ostream = new FSDataOutputStream(baos, null);\n+\n+    HFile.Writer writer = HFile.getWriterFactory(conf, cacheConfig)\n+        .withOutputStream(ostream).withFileContext(context).create();\n+\n+    // Serialize records into bytes\n+    Map<String, byte[]> sortedRecordsMap = new TreeMap<>();", "originalCommit": "aa11837b588b919b029e837ad945568d369e6d4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ4NjM1Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478486357", "bodyText": "its a tree map. its sorted/ordered", "author": "vinothchandar", "createdAt": "2020-08-27T15:00:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0OTQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ4ODc4Mw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478488783", "bodyText": "Does not matter where the sorting is performed. It will definitely be referable in the partitioner.\n\n@prashantwason for some reason I cannot find your comment overlaid here. (are you using the review feature?) . Anywasy, what I meant was just rdd.repartitionAndSort... for the AppendHandle path as well. There is no generic partitioner in Hudi, since the ones we have are all serving different purposes", "author": "vinothchandar", "createdAt": "2020-08-27T15:04:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0OTQwMA=="}], "type": "inlineReview"}, {"oid": "cbad0b447df4e643701783a4c62f1ff988f96423", "url": "https://github.com/apache/hudi/commit/cbad0b447df4e643701783a4c62f1ff988f96423", "message": "Fixing checkstyle", "committedDate": "2020-08-31T14:17:16Z", "type": "commit"}]}