{"pr_number": 2161, "pr_title": "[HUDI-1328] Introduce HoodieFlinkEngineContext to hudi-flink-client", "pr_createdAt": "2020-10-09T16:49:07Z", "pr_url": "https://github.com/apache/hudi/pull/2161", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTAxMQ==", "url": "https://github.com/apache/hudi/pull/2161#discussion_r502729011", "bodyText": "use HoodieException?", "author": "leesf", "createdAt": "2020-10-10T01:35:02Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/common/function/FunctionWrapper.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.common.function;\n+\n+import scala.Tuple2;\n+\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Function wrapper util class, which catches the exception thrown by input function and return a similar function\n+ * with no exception thrown.\n+ */\n+public class FunctionWrapper {\n+\n+  public static <I, O> Function<I, O> throwingMapWrapper(SerializableFunction<I, O> throwingMapFunction) {\n+    return v1 -> {\n+      try {\n+        return throwingMapFunction.apply(v1);\n+      } catch (Exception e) {\n+        throw new RuntimeException(e);", "originalCommit": "5b82c40f0ad533d0b1d4730e91ad4d0133b49c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjczMTcxMg==", "url": "https://github.com/apache/hudi/pull/2161#discussion_r502731712", "bodyText": "use HoodieException?\n\ndone", "author": "wangxianghu", "createdAt": "2020-10-10T01:57:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTAxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTgwNQ==", "url": "https://github.com/apache/hudi/pull/2161#discussion_r502729805", "bodyText": "maybe we would move the methods to super class as default implementation after finishing the flink integration.", "author": "leesf", "createdAt": "2020-10-10T01:41:48Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/common/HoodieFlinkEngineContext.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.common;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.common.function.SerializableConsumer;\n+import org.apache.hudi.client.common.function.SerializableFunction;\n+import org.apache.hudi.client.common.function.SerializablePairFunction;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.util.Option;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.client.common.function.FunctionWrapper.throwingFlatMapWrapper;\n+import static org.apache.hudi.client.common.function.FunctionWrapper.throwingForeachWrapper;\n+import static org.apache.hudi.client.common.function.FunctionWrapper.throwingMapToPairWrapper;\n+import static org.apache.hudi.client.common.function.FunctionWrapper.throwingMapWrapper;\n+\n+/**\n+ * A flink engine implementation of HoodieEngineContext.\n+ */\n+public class HoodieFlinkEngineContext extends HoodieEngineContext {\n+\n+  public HoodieFlinkEngineContext(TaskContextSupplier taskContextSupplier) {\n+    this(new SerializableConfiguration(new Configuration()), taskContextSupplier);\n+  }\n+\n+  public HoodieFlinkEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    super(hadoopConf, taskContextSupplier);\n+  }\n+\n+  @Override\n+  public <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism) {\n+    return data.stream().parallel().map(throwingMapWrapper(func)).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public <I, O> List<O> flatMap(List<I> data, SerializableFunction<I, Stream<O>> func, int parallelism) {\n+    return data.stream().parallel().flatMap(throwingFlatMapWrapper(func)).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public <I> void foreach(List<I> data, SerializableConsumer<I> consumer, int parallelism) {\n+    data.forEach(throwingForeachWrapper(consumer));\n+  }\n+\n+  @Override\n+  public <I, K, V> Map<K, V> mapToPair(List<I> data, SerializablePairFunction<I, K, V> func, Integer parallelism) {\n+    Map<K, V> map = new HashMap<>();\n+    data.stream().map(throwingMapToPairWrapper(func)).forEach(x -> map.put(x._1, x._2));\n+    return map;\n+  }\n+\n+  @Override\n+  public void setProperty(EngineProperty key, String value) {\n+    // no operation for now\n+  }\n+\n+  @Override\n+  public Option<String> getProperty(EngineProperty key) {\n+    // no operation for now\n+    return Option.empty();\n+  }\n+\n+  @Override\n+  public void setJobStatus(String activeModule, String activityDescription) {\n+    // no operation for now\n+  }", "originalCommit": "5b82c40f0ad533d0b1d4730e91ad4d0133b49c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTk2Mg==", "url": "https://github.com/apache/hudi/pull/2161#discussion_r502729962", "bodyText": "maybe we would move the methods to super class as default implementation after finishing the flink integration.\n\nGood idea", "author": "wangxianghu", "createdAt": "2020-10-10T01:42:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjczMDMzNg==", "url": "https://github.com/apache/hudi/pull/2161#discussion_r502730336", "bodyText": "here would always get spark instead of flink?", "author": "leesf", "createdAt": "2020-10-10T01:46:05Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/client/common/TestHoodieFlinkEngineContext.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.common;\n+\n+import org.apache.hudi.common.util.Option;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import scala.Tuple2;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Unit test against HoodieFlinkEngineContext.\n+ */\n+public class TestHoodieFlinkEngineContext {\n+  private HoodieFlinkEngineContext context;\n+\n+  @BeforeEach\n+  public void init() {\n+    context = new HoodieFlinkEngineContext(new DummyTaskContextSupplier());\n+  }\n+\n+  @Test\n+  public void testMap() {\n+    List<Integer> mapList = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);\n+    List<Integer> result = context.map(mapList, x -> x + 1, 2);\n+    result.removeAll(mapList);\n+\n+    Assertions.assertEquals(1, result.size());\n+    Assertions.assertEquals(11, result.get(0));\n+  }\n+\n+  @Test\n+  public void testFlatMap() {\n+    List<String> list1 = Arrays.asList(\"a\", \"b\", \"c\");\n+    List<String> list2 = Arrays.asList(\"d\", \"e\", \"f\");\n+    List<String> list3 = Arrays.asList(\"g\", \"h\", \"i\");\n+\n+    List<List<String>> inputList = new ArrayList<>();\n+    inputList.add(list1);\n+    inputList.add(list2);\n+    inputList.add(list3);\n+\n+    List<String> result = context.flatMap(inputList, Collection::stream, 2);\n+\n+    Assertions.assertEquals(9, result.size());\n+  }\n+\n+  @Test\n+  public void testForeach() {\n+    List<Integer> mapList = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);\n+    List<Integer> result = new ArrayList<>(10);\n+    context.foreach(mapList, result::add, 2);\n+\n+    Assertions.assertEquals(result.size(), mapList.size());\n+    Assertions.assertTrue(result.containsAll(mapList));\n+  }\n+\n+  @Test\n+  public void testMapToPair() {\n+    List<String> mapList = Arrays.asList(\"hudi_flink\", \"hudi_spark\");\n+\n+    Map<String, String> resultMap = context.mapToPair(mapList, x -> {\n+      String[] splits = x.split(\"_\");\n+      return Tuple2.apply(splits[0], splits[1]);\n+    }, 2);\n+\n+    Assertions.assertEquals(\"spark\", resultMap.get(\"hudi\"));", "originalCommit": "5b82c40f0ad533d0b1d4730e91ad4d0133b49c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjczMTQ4Nw==", "url": "https://github.com/apache/hudi/pull/2161#discussion_r502731487", "bodyText": "here would always get spark instead of flink?\n\nIt should be. List is ordered.\nAssertions.assertNotNull(resultMap.get(\"hudi\")) should be better understood", "author": "wangxianghu", "createdAt": "2020-10-10T01:55:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjczMDMzNg=="}], "type": "inlineReview"}, {"oid": "be70afdbbc55bb9a0870a8d3d0cff66ae12a441f", "url": "https://github.com/apache/hudi/commit/be70afdbbc55bb9a0870a8d3d0cff66ae12a441f", "message": "[HUDI-1328] Introduce HoodieFlinkEngineContext to hudi-flink-client", "committedDate": "2020-10-10T01:56:21Z", "type": "commit"}, {"oid": "be70afdbbc55bb9a0870a8d3d0cff66ae12a441f", "url": "https://github.com/apache/hudi/commit/be70afdbbc55bb9a0870a8d3d0cff66ae12a441f", "message": "[HUDI-1328] Introduce HoodieFlinkEngineContext to hudi-flink-client", "committedDate": "2020-10-10T01:56:21Z", "type": "forcePushed"}]}