{"pr_number": 2281, "pr_title": "[HUDI-1418] Set up flink client unit test infra", "pr_createdAt": "2020-11-25T15:58:35Z", "pr_url": "https://github.com/apache/hudi/pull/2281", "timeline": [{"oid": "72cfa3f4bb2029ce3682a0a48c8c221378dfc698", "url": "https://github.com/apache/hudi/commit/72cfa3f4bb2029ce3682a0a48c8c221378dfc698", "message": "[HUDI-1418] set up flink client unit test infra", "committedDate": "2020-11-26T05:50:46Z", "type": "forcePushed"}, {"oid": "29903844aeb9b13067e33f4c65c26c1692014449", "url": "https://github.com/apache/hudi/commit/29903844aeb9b13067e33f4c65c26c1692014449", "message": "[HUDI-1418] set up flink client unit test infra\n\nCo-authored-by: Li Yanjia <liyanjia.gary@bytedance.com>", "committedDate": "2020-12-08T13:05:35Z", "type": "forcePushed"}, {"oid": "5b5e2fd36f4ffa6cb142f9d2d126a0a13eecb3e6", "url": "https://github.com/apache/hudi/commit/5b5e2fd36f4ffa6cb142f9d2d126a0a13eecb3e6", "message": "[HUDI-1418] set up flink client unit test infra\n\nCo-authored-by: Li Yanjia <liyanjia.gary@bytedance.com>", "committedDate": "2020-12-10T08:40:39Z", "type": "forcePushed"}, {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9", "url": "https://github.com/apache/hudi/commit/f58a7b737e9d7f3ca6119950c665b77823dc94f9", "message": "[HUDI-1418] set up flink client unit test infra\n\nCo-authored-by: Li Yanjia <liyanjia.gary@bytedance.com>", "committedDate": "2020-12-14T14:11:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MjgwMg==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544862802", "bodyText": "Hi @garyli1019 , thanks for adding flink unit test infra.\nhow about enhancing HoodieWriteConfig instead of adding HoodieDataSourceConfig, it seems they have lots in common", "author": "wangxianghu", "createdAt": "2020-12-17T07:19:05Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+", "originalCommit": "f58a7b737e9d7f3ca6119950c665b77823dc94f9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544866146", "bodyText": "Most of these options have been defined in DataSourceOptions, I have been planning to move them to hudi-client-common (https://issues.apache.org/jira/browse/HUDI-1438), till then we can reuse these options.\nDo you mind waiting for a while, we can continue this PR when HUDI-1438 is finished\nWDYT \uff1f", "author": "wangxianghu", "createdAt": "2020-12-17T07:26:47Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+\n+  public static final String TABLE_NAME_PROP = HoodieWriteConfig.TABLE_NAME;\n+  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n+  public static final String RECORDKEY_FIELD_PROP = KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY;\n+  public static final String PARTITIONPATH_FIELD_PROP = KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY;\n+\n+  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n+  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n+  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n+  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();", "originalCommit": "f58a7b737e9d7f3ca6119950c665b77823dc94f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4NTM0Nw==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544885347", "bodyText": "I agree that we should move DataSourceOptions to hudi-client-common. Are we looking for moving them intoHoodieWriteConfig? If so I can do it on this PR.\nIMO we need to write tests for every single Flink-related feature actually, otherwise, the features are not trustworthy, so I think we should land this asap and force unit tests for the upcoming Flink features.", "author": "garyli1019", "createdAt": "2020-12-17T08:04:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4ODUyNQ==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544888525", "bodyText": "I agree that we should move DataSourceOptions to hudi-client-common. Are we looking for moving them intoHoodieWriteConfig? If so I can do it on this PR.\nIMO we need to write tests for every single Flink-related feature actually, otherwise, the features are not trustworthy, so I think we should land this asap and force unit tests for the upcoming Flink features.\n\nI think DataSourceOptions should be independent of HoodieWriteConfig just as it is before.\n@liujinhui1994 do you have time for HUDI-1438 recently? if not, do you mind resign it to @garyli1019", "author": "wangxianghu", "createdAt": "2020-12-17T08:10:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk0ODI3MA==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544948270", "bodyText": "@garyli1019 @wangxianghu\nI'm almost done, I will post it as soon as possible", "author": "liujinhui1994", "createdAt": "2020-12-17T09:43:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk1MjE3OA==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544952178", "bodyText": "@liujinhui1994 awesome!", "author": "garyli1019", "createdAt": "2020-12-17T09:49:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI4MTQ3NQ==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r547281475", "bodyText": "hi @liujinhui1994 any update on refactoring DataSourceOptions ticket?", "author": "garyli1019", "createdAt": "2020-12-22T13:36:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk4NTA5NA==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544985094", "bodyText": "There are many fields that have not been used. Can we add them when we would use them?", "author": "yanghua", "createdAt": "2020-12-17T10:38:03Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.minicluster.HdfsTestService;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient HoodieFlinkWriteClient writeClient;", "originalCommit": "f58a7b737e9d7f3ca6119950c665b77823dc94f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI4MDkwMA==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r547280900", "bodyText": "sure, will do", "author": "garyli1019", "createdAt": "2020-12-22T13:35:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk4NTA5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk4NTc0Nw==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544985747", "bodyText": "Since this method has been deprecated. Can we use the normal one?", "author": "yanghua", "createdAt": "2020-12-17T10:39:07Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.minicluster.HdfsTestService;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient HoodieFlinkWriteClient writeClient;\n+  protected transient HoodieTableFileSystemView tableView;\n+  protected transient MiniClusterWithClientResource flinkCluster = null;\n+\n+  // dfs\n+  protected String dfsBasePath;\n+  protected transient HdfsTestService hdfsTestService;\n+  protected transient MiniDFSCluster dfsCluster;\n+  protected transient DistributedFileSystem dfs;\n+\n+  @BeforeEach\n+  public void setTestMethodName(TestInfo testInfo) {\n+    if (testInfo.getTestMethod().isPresent()) {\n+      testMethodName = testInfo.getTestMethod().get().getName();\n+    } else {\n+      testMethodName = \"Unknown\";\n+    }\n+  }\n+\n+  protected void initFlinkMiniCluster() {\n+    flinkCluster = new MiniClusterWithClientResource(\n+        new MiniClusterResourceConfiguration.Builder()\n+            .setNumberSlotsPerTaskManager(2)\n+            .setNumberTaskManagers(1)\n+            .build());\n+  }\n+\n+  protected void initFileSystem() {\n+    hadoopConf = new Configuration();\n+    initFileSystemWithConfiguration(hadoopConf);\n+  }\n+\n+  private void initFileSystemWithConfiguration(Configuration configuration) {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    fs = FSUtils.getFs(basePath, configuration);\n+    if (fs instanceof LocalFileSystem) {\n+      LocalFileSystem lfs = (LocalFileSystem) fs;\n+      // With LocalFileSystem, with checksum disabled, fs.open() returns an inputStream which is FSInputStream\n+      // This causes ClassCastExceptions in LogRecordScanner (and potentially other places) calling fs.open\n+      // So, for the tests, we enforce checksum verification to circumvent the problem\n+      lfs.setVerifyChecksum(true);\n+    }\n+  }\n+\n+  /**\n+   * Initializes an instance of {@link HoodieTableMetaClient} with a special table type specified by\n+   * {@code getTableType()}.\n+   *\n+   * @throws IOException\n+   */\n+  protected void initMetaClient() throws IOException {\n+    initMetaClient(getTableType());\n+  }\n+\n+  protected void initMetaClient(HoodieTableType tableType) throws IOException {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    metaClient = HoodieTestUtils.init(hadoopConf, basePath, tableType);\n+  }\n+\n+\n+  /**\n+   * Cleanups file system.\n+   *\n+   * @throws IOException\n+   */\n+  protected void cleanupFileSystem() throws IOException {\n+    if (fs != null) {\n+      LOG.warn(\"Closing file-system instance used in previous test-run\");\n+      fs.close();\n+      fs = null;\n+    }\n+  }\n+\n+  protected void cleanupFlinkMiniCluster() {\n+    if (flinkCluster != null) {\n+      flinkCluster.after();\n+      flinkCluster = null;\n+    }\n+  }\n+\n+  public static class SimpleSink implements SinkFunction<HoodieRecord> {\n+\n+    // must be static\n+    public static List<HoodieRecord> valuesList = new ArrayList<>();\n+\n+    @Override\n+    public synchronized void invoke(HoodieRecord value) throws Exception {", "originalCommit": "f58a7b737e9d7f3ca6119950c665b77823dc94f9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e3c9a8d84fe176f7374fa4d6228771e6cce66fa1", "url": "https://github.com/apache/hudi/commit/e3c9a8d84fe176f7374fa4d6228771e6cce66fa1", "message": "[HUDI-1418] set up flink client unit test infra\n\nCo-authored-by: Li Yanjia <liyanjia.gary@bytedance.com>", "committedDate": "2020-12-23T15:43:55Z", "type": "commit"}, {"oid": "47e7bff5653047351ea61e4572141254eb9ea568", "url": "https://github.com/apache/hudi/commit/47e7bff5653047351ea61e4572141254eb9ea568", "message": "Move datasource config to HoodieWriteConfig", "committedDate": "2020-12-23T16:24:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2MzQ4MQ==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550163481", "bodyText": "Why do we need to move this code snippet from HoodieClientTestHarness in this PR?", "author": "yanghua", "createdAt": "2020-12-30T11:34:50Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java", "diffHunk": "@@ -52,6 +53,24 @@ protected void initPath() {\n     }\n   }\n \n+  /**\n+   * Initializes a test data generator which used to generate test datas.\n+   *\n+   */\n+  protected void initTestDataGenerator() {", "originalCommit": "47e7bff5653047351ea61e4572141254eb9ea568", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE5MTE4Mg==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550191182", "bodyText": "This can be reuse by both Spark and Flink client test harness and only depend on hudi's code.", "author": "garyli1019", "createdAt": "2020-12-30T13:15:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2MzQ4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NTY4Nw==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550165687", "bodyText": "SimpleTestSinkFunction  sounds better?", "author": "yanghua", "createdAt": "2020-12-30T11:42:52Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient MiniClusterWithClientResource flinkCluster = null;\n+\n+  @BeforeEach\n+  public void setTestMethodName(TestInfo testInfo) {\n+    if (testInfo.getTestMethod().isPresent()) {\n+      testMethodName = testInfo.getTestMethod().get().getName();\n+    } else {\n+      testMethodName = \"Unknown\";\n+    }\n+  }\n+\n+  protected void initFlinkMiniCluster() {\n+    flinkCluster = new MiniClusterWithClientResource(\n+        new MiniClusterResourceConfiguration.Builder()\n+            .setNumberSlotsPerTaskManager(2)\n+            .setNumberTaskManagers(1)\n+            .build());\n+  }\n+\n+  protected void initFileSystem() {\n+    hadoopConf = new Configuration();\n+    initFileSystemWithConfiguration(hadoopConf);\n+  }\n+\n+  private void initFileSystemWithConfiguration(Configuration configuration) {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    fs = FSUtils.getFs(basePath, configuration);\n+    if (fs instanceof LocalFileSystem) {\n+      LocalFileSystem lfs = (LocalFileSystem) fs;\n+      // With LocalFileSystem, with checksum disabled, fs.open() returns an inputStream which is FSInputStream\n+      // This causes ClassCastExceptions in LogRecordScanner (and potentially other places) calling fs.open\n+      // So, for the tests, we enforce checksum verification to circumvent the problem\n+      lfs.setVerifyChecksum(true);\n+    }\n+  }\n+\n+  /**\n+   * Initializes an instance of {@link HoodieTableMetaClient} with a special table type specified by\n+   * {@code getTableType()}.\n+   *\n+   * @throws IOException\n+   */\n+  protected void initMetaClient() throws IOException {\n+    initMetaClient(getTableType());\n+  }\n+\n+  protected void initMetaClient(HoodieTableType tableType) throws IOException {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    metaClient = HoodieTestUtils.init(hadoopConf, basePath, tableType);\n+  }\n+\n+\n+  /**\n+   * Cleanups file system.\n+   *\n+   * @throws IOException\n+   */\n+  protected void cleanupFileSystem() throws IOException {\n+    if (fs != null) {\n+      LOG.warn(\"Closing file-system instance used in previous test-run\");\n+      fs.close();\n+      fs = null;\n+    }\n+  }\n+\n+  protected void cleanupFlinkMiniCluster() {\n+    if (flinkCluster != null) {\n+      flinkCluster.after();\n+      flinkCluster = null;\n+    }\n+  }\n+\n+  public static class SimpleSink implements SinkFunction<HoodieRecord> {", "originalCommit": "47e7bff5653047351ea61e4572141254eb9ea568", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE5MDU0OA==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550190548", "bodyText": "yes, agree", "author": "garyli1019", "createdAt": "2020-12-30T13:13:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NTY4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NjI4OA==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550166288", "bodyText": "Can we reuse the first constructor?", "author": "yanghua", "createdAt": "2020-12-30T11:44:58Z", "path": "hudi-flink/src/main/java/org/apache/hudi/source/JsonStringToHoodieRecordMapFunction.java", "diffHunk": "@@ -40,32 +43,47 @@\n  */\n public class JsonStringToHoodieRecordMapFunction implements MapFunction<String, HoodieRecord> {\n \n-  private final HoodieFlinkStreamer.Config cfg;\n+  private TypedProperties props;\n   private KeyGenerator keyGenerator;\n   private AvroConvertor avroConvertor;\n+  private Option<String> schemaStr = Option.empty();\n+  private String payloadClassName;\n+  private String orderingField;\n \n-  public JsonStringToHoodieRecordMapFunction(HoodieFlinkStreamer.Config cfg) {\n-    this.cfg = cfg;\n+  public JsonStringToHoodieRecordMapFunction(TypedProperties props) {\n+    this.props = props;\n+    init();\n+  }\n+\n+  public JsonStringToHoodieRecordMapFunction(TypedProperties props, String schemaStr) {", "originalCommit": "47e7bff5653047351ea61e4572141254eb9ea568", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE5NDk5OQ==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550194999", "bodyText": "Done.", "author": "garyli1019", "createdAt": "2020-12-30T13:28:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NjI4OA=="}], "type": "inlineReview"}, {"oid": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66", "url": "https://github.com/apache/hudi/commit/3f75fa543b1d3b255963fe3f9e15a1ede0f80c66", "message": "Move datasource config to HoodieWriteConfig", "committedDate": "2020-12-30T13:26:58Z", "type": "commit"}, {"oid": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66", "url": "https://github.com/apache/hudi/commit/3f75fa543b1d3b255963fe3f9e15a1ede0f80c66", "message": "Move datasource config to HoodieWriteConfig", "committedDate": "2020-12-30T13:26:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxNjI4OQ==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550216289", "bodyText": "This line is too long, it would be better if we could break it. But it does not matter, let's refactor it next time.", "author": "yanghua", "createdAt": "2020-12-30T14:38:37Z", "path": "hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "diffHunk": "@@ -104,29 +80,7 @@ public HoodieWriteableTestTable forCommit(String instantTime) {\n     return (HoodieWriteableTestTable) super.forCommit(instantTime);\n   }\n \n-  public String getFileIdWithInserts(String partition) throws Exception {\n-    return getFileIdWithInserts(partition, new HoodieRecord[0]);\n-  }\n-\n-  public String getFileIdWithInserts(String partition, HoodieRecord... records) throws Exception {\n-    return getFileIdWithInserts(partition, Arrays.asList(records));\n-  }\n-\n-  public String getFileIdWithInserts(String partition, List<HoodieRecord> records) throws Exception {\n-    String fileId = UUID.randomUUID().toString();\n-    withInserts(partition, fileId, records);\n-    return fileId;\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId) throws Exception {\n-    return withInserts(partition, fileId, new HoodieRecord[0]);\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, HoodieRecord... records) throws Exception {\n-    return withInserts(partition, fileId, Arrays.asList(records));\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records) throws Exception {\n+  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records, TaskContextSupplier contextSupplier) throws Exception {", "originalCommit": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxNjMwNA==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550216304", "bodyText": "This line is too long, it would be better if we could break it. But it does not matter, let's refactor it next time.", "author": "yanghua", "createdAt": "2020-12-30T14:38:39Z", "path": "hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "diffHunk": "@@ -104,29 +80,7 @@ public HoodieWriteableTestTable forCommit(String instantTime) {\n     return (HoodieWriteableTestTable) super.forCommit(instantTime);\n   }\n \n-  public String getFileIdWithInserts(String partition) throws Exception {\n-    return getFileIdWithInserts(partition, new HoodieRecord[0]);\n-  }\n-\n-  public String getFileIdWithInserts(String partition, HoodieRecord... records) throws Exception {\n-    return getFileIdWithInserts(partition, Arrays.asList(records));\n-  }\n-\n-  public String getFileIdWithInserts(String partition, List<HoodieRecord> records) throws Exception {\n-    String fileId = UUID.randomUUID().toString();\n-    withInserts(partition, fileId, records);\n-    return fileId;\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId) throws Exception {\n-    return withInserts(partition, fileId, new HoodieRecord[0]);\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, HoodieRecord... records) throws Exception {\n-    return withInserts(partition, fileId, Arrays.asList(records));\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records) throws Exception {\n+  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records, TaskContextSupplier contextSupplier) throws Exception {", "originalCommit": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxNzIzMg==", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550217232", "bodyText": "It would be better to split the non-static and static fields, moreover, it is a logger.", "author": "yanghua", "createdAt": "2020-12-30T14:41:31Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);", "originalCommit": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}