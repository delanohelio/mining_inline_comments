{"pr_number": 1449, "pr_title": "[HUDI-698]Add unit test for CleansCommand", "pr_createdAt": "2020-03-26T06:41:49Z", "pr_url": "https://github.com/apache/hudi/pull/1449", "timeline": [{"oid": "5d301c5b96520bb929ccee6657df512a0edb530f", "url": "https://github.com/apache/hudi/commit/5d301c5b96520bb929ccee6657df512a0edb530f", "message": "Add test for cleanCommand", "committedDate": "2020-04-13T15:48:10Z", "type": "commit"}, {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f", "url": "https://github.com/apache/hudi/commit/5d301c5b96520bb929ccee6657df512a0edb530f", "message": "Add test for cleanCommand", "committedDate": "2020-04-13T15:48:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODgwMQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407838801", "bodyText": "add a blank before the number 1?", "author": "yanghua", "createdAt": "2020-04-14T03:05:41Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCleansCommand.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ITTestCleansCommand extends AbstractShellIntegrationTest {\n+  private String tablePath;\n+  private URL propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\");\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+  }\n+\n+  /**\n+   * Test case for cleans run.\n+   */\n+  @Test\n+  public void testRunClean() throws IOException {\n+    // First, there should none of clean instant.\n+    assertEquals(0, metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    // Check properties file exists.\n+    assertNotNull(\"Not found properties file\", propsFilePath);\n+\n+    // Create partition metadata\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans run --sparkMaster local --propsFilePath \" + propsFilePath.toString());\n+    assertTrue(cr.isSuccess());\n+\n+    // After run clean, there should have 1 clean instant\n+    assertEquals(\"Loaded 1 clean and the count should match\",1,", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg0MDI3NQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407840275", "bodyText": "It can be assertNotNull(...)", "author": "yanghua", "createdAt": "2020-04-14T03:11:20Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private URL propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\");\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // Check properties file exists.\n+    assertNotNull(\"Not found properties file\", propsFilePath);\n+\n+    // First, run clean\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath.getPath(), new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().orElse(null);\n+    assertTrue(clean != null);", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b14b63b14de667ecc0ff88c5d149682da15fab6c", "url": "https://github.com/apache/hudi/commit/b14b63b14de667ecc0ff88c5d149682da15fab6c", "message": "add version for download", "committedDate": "2020-04-14T08:09:36Z", "type": "commit"}]}