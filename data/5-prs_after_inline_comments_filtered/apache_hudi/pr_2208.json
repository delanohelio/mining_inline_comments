{"pr_number": 2208, "pr_title": "[HUDI-1040] Make Hudi support Spark 3", "pr_createdAt": "2020-10-26T20:15:00Z", "pr_url": "https://github.com/apache/hudi/pull/2208", "timeline": [{"oid": "649fadd3b28ff464a1c525753094510577f53de8", "url": "https://github.com/apache/hudi/commit/649fadd3b28ff464a1c525753094510577f53de8", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-10-27T15:46:50Z", "type": "forcePushed"}, {"oid": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "url": "https://github.com/apache/hudi/commit/ed2714bb26e11b24707fcf13cebeb5e2116476c5", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-10-28T18:58:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzczNjM0OQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r513736349", "bodyText": "let's file a tracking JIRA for this?", "author": "vinothchandar", "createdAt": "2020-10-28T20:22:49Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkDatasetTestUtils.java", "diffHunk": "@@ -173,4 +176,17 @@ public static InternalRow getInternalRowWithError(String partitionPath) {\n         .withBulkInsertParallelism(2);\n   }\n \n+  private static InternalRow serializeRow(ExpressionEncoder encoder, Row row)\n+      throws InvocationTargetException, IllegalAccessException, NoSuchMethodException, ClassNotFoundException {\n+    // TODO remove reflection if Spark 2.x support is dropped", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMzE5NQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514603195", "bodyText": "What is the need to move the hudi datasource itself to hudi-spark2 ? I think we should leave it under hudi-spark and later if we want to have separate datasource implementations we can create separately under hudi-spark2 and hudi-spark3 modules. Thoughts ?", "author": "umehrot2", "createdAt": "2020-10-29T22:26:58Z", "path": "hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java", "diffHunk": "@@ -18,7 +18,7 @@\n \n package org.apache.hudi.internal;\n \n-import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.DataSourceUtilsForSpark2;", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM5NjY4Mg==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r515396682", "bodyText": "Because hudi-spark depends on hudi-spark2. I cannot also let hudi-spark2 depends on hudi-spark tho copying files is not a clean way.", "author": "zhedoubushishi", "createdAt": "2020-10-30T21:45:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxMDE3OA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516410178", "bodyText": "I had misunderstood that you moved DefaultSource.scala which is the main datasource implementation. But seems like you have moved the internal datasource implementation used for bulk insert v2. So it seems fine to me.", "author": "umehrot2", "createdAt": "2020-11-03T03:27:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMzE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgzNA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514609834", "bodyText": "As discussed internally regarding this in the code review, can you confirm if this is actually converting paths to point to local file system and not HDFS ? Also would be good to explain why you did this for reference in the description.", "author": "umehrot2", "createdAt": "2020-10-29T22:45:32Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieMergeOnReadTestUtils.java", "diffHunk": "@@ -85,7 +85,7 @@\n         .collect(Collectors.toList()));\n \n     return inputPaths.stream().map(path -> {\n-      setInputPath(jobConf, path);\n+      FileInputFormat.setInputPaths(jobConf, path);", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyMDc2NA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516420764", "bodyText": "Me and discussed discussed it internally and this is not a concern anymore.", "author": "umehrot2", "createdAt": "2020-11-03T04:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgzNA=="}], "type": "inlineReview"}, {"oid": "eac8358868c44c1e031c00a11a4814a59a59d979", "url": "https://github.com/apache/hudi/commit/eac8358868c44c1e031c00a11a4814a59a59d979", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-10-30T21:38:41Z", "type": "forcePushed"}, {"oid": "5ad2ff9e28fbd309093ac294636a9d7e2c53280c", "url": "https://github.com/apache/hudi/commit/5ad2ff9e28fbd309093ac294636a9d7e2c53280c", "message": "resolve comments for fixing mor flaky", "committedDate": "2020-11-01T19:13:27Z", "type": "forcePushed"}, {"oid": "5d1870ac01af856e4504b7f0b5af15c9d08f5c9a", "url": "https://github.com/apache/hudi/commit/5d1870ac01af856e4504b7f0b5af15c9d08f5c9a", "message": "resolve comments for fixing mor flaky", "committedDate": "2020-11-01T19:16:29Z", "type": "forcePushed"}, {"oid": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "url": "https://github.com/apache/hudi/commit/0df7521bf7ba087be882da55bc6ce3fd3c717a78", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-01T22:53:34Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQwNjg3MQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516406871", "bodyText": "It might make sense to create Spark2RowSerializer and Spark3RowSerializer similar to the implementations we have created for deserializers.", "author": "umehrot2", "createdAt": "2020-11-03T03:11:31Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkDatasetTestUtils.java", "diffHunk": "@@ -173,4 +176,17 @@ public static InternalRow getInternalRowWithError(String partitionPath) {\n         .withBulkInsertParallelism(2);\n   }\n \n+  private static InternalRow serializeRow(ExpressionEncoder encoder, Row row)\n+      throws InvocationTargetException, IllegalAccessException, NoSuchMethodException, ClassNotFoundException {\n+    // TODO remove reflection if Spark 2.x support is dropped\n+    if (package$.MODULE$.SPARK_VERSION().startsWith(\"2.\")) {\n+      Method spark2method = encoder.getClass().getMethod(\"toRow\", Object.class);\n+      return (InternalRow) spark2method.invoke(encoder, row);", "originalCommit": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NTUwMQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516985501", "bodyText": "The problem here is hudi-spark2 already depends on hudi-common and hudi-client. Say if I create Spark2RowSerializer under hudi-spark2, I also need to make hudi-client depends on hudi-spark2 and as a result, it will bring a dependency loop.", "author": "zhedoubushishi", "createdAt": "2020-11-03T22:11:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQwNjg3MQ=="}], "type": "inlineReview"}, {"oid": "29cfc4783b2106a7f45b594703c3cef1b94f43f5", "url": "https://github.com/apache/hudi/commit/29cfc4783b2106a7f45b594703c3cef1b94f43f5", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-03T21:59:01Z", "type": "forcePushed"}, {"oid": "fe002982d099af08df5f5ba4768a7b6905fa61c8", "url": "https://github.com/apache/hudi/commit/fe002982d099af08df5f5ba4768a7b6905fa61c8", "message": "relocate createRdd", "committedDate": "2020-11-04T06:50:21Z", "type": "forcePushed"}, {"oid": "a6e0240279472cb981c620a9036add70e3efa05d", "url": "https://github.com/apache/hudi/commit/a6e0240279472cb981c620a9036add70e3efa05d", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-04T18:52:15Z", "type": "forcePushed"}, {"oid": "fb9514216e84f36beab82aaa4496af73e3a10693", "url": "https://github.com/apache/hudi/commit/fb9514216e84f36beab82aaa4496af73e3a10693", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-16T19:34:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNTA4Mw==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r528435083", "bodyText": "would be good to have a JIRA with all these follow ups when Spark 2.x support is dropped", "author": "vinothchandar", "createdAt": "2020-11-23T01:11:29Z", "path": "hudi-spark2/src/main/java/org/apache/hudi/DataSourceUtilsForSpark2.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+\n+import java.util.Map;\n+\n+/**\n+ * Utilities used throughout the data source.\n+ * TODO: This file is partially copied from org.apache.hudi.DataSourceUtils.\n+ * Should be removed if Spark 2.x support is dropped.", "originalCommit": "2af55b357babdd0278f8273fc6658185c459f285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4OTMxMw==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r533589313", "bodyText": "I created a new module hudi-spark-common to avoid creating xxxForSpark2 classes.\nSo the current structure of hudi-spark would be:\nhudi-spark-datasource\n|\n-----------------hudi-spark\n|\n-----------------hudi-spark-common\n|\n-----------------hudi-spark2\n|\n-----------------hudi-spark3", "author": "zhedoubushishi", "createdAt": "2020-12-01T17:24:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNTA4Mw=="}], "type": "inlineReview"}, {"oid": "dae14f129534cb1a6c886c2bca87fef8611d0e07", "url": "https://github.com/apache/hudi/commit/dae14f129534cb1a6c886c2bca87fef8611d0e07", "message": "Create hudi-spark-common module and refactor hudi-spark related modules", "committedDate": "2020-12-01T17:26:49Z", "type": "forcePushed"}, {"oid": "1698576919984f94930c624efff3518556370aac", "url": "https://github.com/apache/hudi/commit/1698576919984f94930c624efff3518556370aac", "message": "Create hudi-spark-common module and refactor hudi-spark related modules", "committedDate": "2020-12-01T17:33:01Z", "type": "forcePushed"}, {"oid": "2d55a9972fc4d6016f42d83286f5056fd2dfd3d7", "url": "https://github.com/apache/hudi/commit/2d55a9972fc4d6016f42d83286f5056fd2dfd3d7", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-01T19:52:47Z", "type": "forcePushed"}, {"oid": "286969293d58f6a305b87cb740cf41e8de593ca9", "url": "https://github.com/apache/hudi/commit/286969293d58f6a305b87cb740cf41e8de593ca9", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-03T18:36:05Z", "type": "forcePushed"}, {"oid": "2afb3437bf8e00fdc6eeaa7b488953e26095397c", "url": "https://github.com/apache/hudi/commit/2afb3437bf8e00fdc6eeaa7b488953e26095397c", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-04T05:01:54Z", "type": "forcePushed"}, {"oid": "8ff0ebc8d6d750e59598d60da63997841af0b8a2", "url": "https://github.com/apache/hudi/commit/8ff0ebc8d6d750e59598d60da63997841af0b8a2", "message": "Fix flaky MOR unit test", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "7ea7d3524a9679f60863d4630cb6673ec7845ad3", "url": "https://github.com/apache/hudi/commit/7ea7d3524a9679f60863d4630cb6673ec7845ad3", "message": "Update Spark APIs to make it be compatible with both spark2 & spark3", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "f266e1786c1260aa900695a84685de96b83f41e1", "url": "https://github.com/apache/hudi/commit/f266e1786c1260aa900695a84685de96b83f41e1", "message": "Refactor bulk insert v2 part to make Hudi be able to compile with Spark3", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "9b9b81819425170ae4ccbb74611294ec45826d0f", "url": "https://github.com/apache/hudi/commit/9b9b81819425170ae4ccbb74611294ec45826d0f", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "url": "https://github.com/apache/hudi/commit/d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-08T22:09:43Z", "type": "commit"}, {"oid": "d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "url": "https://github.com/apache/hudi/commit/d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-08T22:09:43Z", "type": "forcePushed"}]}