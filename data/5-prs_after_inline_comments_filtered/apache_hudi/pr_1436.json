{"pr_number": 1436, "pr_title": "[HUDI-711] Refactor exporter main logic", "pr_createdAt": "2020-03-21T23:27:38Z", "pr_url": "https://github.com/apache/hudi/pull/1436", "timeline": [{"oid": "b4969833b62c060049f458e6f40d62f7c0cce40c", "url": "https://github.com/apache/hudi/commit/b4969833b62c060049f458e6f40d62f7c0cce40c", "message": "Refactor exporter main logic\n\n* break main method into multiple readable methods\n* fix bug of passing wrong file list\n* avoid deleting output path when exists\n* throw exception to early abort on multiple cases", "committedDate": "2020-03-21T23:28:58Z", "type": "commit"}, {"oid": "b4969833b62c060049f458e6f40d62f7c0cce40c", "url": "https://github.com/apache/hudi/commit/b4969833b62c060049f458e6f40d62f7c0cce40c", "message": "Refactor exporter main logic\n\n* break main method into multiple readable methods\n* fix bug of passing wrong file list\n* avoid deleting output path when exists\n* throw exception to early abort on multiple cases", "committedDate": "2020-03-21T23:28:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAzODk1Mw==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r396038953", "bodyText": "We don't want to accidentally delete output directory.", "author": "xushiyan", "createdAt": "2020-03-21T23:34:50Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -160,37 +174,36 @@ private void exportAsNonHudi(SparkSession spark, Config cfg, List<String> dataFi\n         ? defaultPartitioner\n         : ReflectionUtils.loadClass(cfg.outputPartitioner);\n \n-    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+    final JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    final BaseFileOnlyView fsView = getBaseFileOnlyView(jsc, cfg);\n+    Iterator<String> exportingFilePaths = jsc\n+        .parallelize(partitions, partitions.size())\n+        .flatMap(partition -> fsView\n+            .getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp)\n+            .map(HoodieBaseFile::getPath).iterator())\n+        .toLocalIterator();\n+\n+    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(exportingFilePaths).toSeq());\n     partitioner.partition(sourceDataset)\n         .format(cfg.outputFormat)\n         .mode(SaveMode.Overwrite)\n         .save(cfg.targetOutputPath);\n   }\n \n-  private void copySnapshot(JavaSparkContext jsc,\n-      FileSystem fs,\n-      Config cfg,\n-      List<String> partitions,\n-      List<String> dataFiles,\n-      String latestCommitTimestamp,\n-      SerializableConfiguration serConf) throws IOException {\n-    // Make sure the output directory is empty\n-    Path outputPath = new Path(cfg.targetOutputPath);\n-    if (fs.exists(outputPath)) {\n-      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-      fs.delete(new Path(cfg.targetOutputPath), true);\n-    }", "originalCommit": "b4969833b62c060049f458e6f40d62f7c0cce40c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAzOTE2OQ==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r396039169", "bodyText": "Fix dataFiles by scanning the current partition.", "author": "xushiyan", "createdAt": "2020-03-21T23:38:04Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -160,37 +174,36 @@ private void exportAsNonHudi(SparkSession spark, Config cfg, List<String> dataFi\n         ? defaultPartitioner\n         : ReflectionUtils.loadClass(cfg.outputPartitioner);\n \n-    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+    final JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    final BaseFileOnlyView fsView = getBaseFileOnlyView(jsc, cfg);\n+    Iterator<String> exportingFilePaths = jsc\n+        .parallelize(partitions, partitions.size())\n+        .flatMap(partition -> fsView\n+            .getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp)\n+            .map(HoodieBaseFile::getPath).iterator())\n+        .toLocalIterator();\n+\n+    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(exportingFilePaths).toSeq());\n     partitioner.partition(sourceDataset)\n         .format(cfg.outputFormat)\n         .mode(SaveMode.Overwrite)\n         .save(cfg.targetOutputPath);\n   }\n \n-  private void copySnapshot(JavaSparkContext jsc,\n-      FileSystem fs,\n-      Config cfg,\n-      List<String> partitions,\n-      List<String> dataFiles,\n-      String latestCommitTimestamp,\n-      SerializableConfiguration serConf) throws IOException {\n-    // Make sure the output directory is empty\n-    Path outputPath = new Path(cfg.targetOutputPath);\n-    if (fs.exists(outputPath)) {\n-      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-      fs.delete(new Path(cfg.targetOutputPath), true);\n-    }\n-\n+  private void exportAsHudi(JavaSparkContext jsc, Config cfg, List<String> partitions, String latestCommitTimestamp) throws IOException {\n+    final BaseFileOnlyView fsView = getBaseFileOnlyView(jsc, cfg);\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n     jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n       // Only take latest version files <= latestCommit.\n-      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n       List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+      Stream<HoodieBaseFile> dataFiles = fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp);\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile.getPath())));", "originalCommit": "b4969833b62c060049f458e6f40d62f7c0cce40c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "45b3ef61135f65629ebc4dba2664dd669de81dd6", "url": "https://github.com/apache/hudi/commit/45b3ef61135f65629ebc4dba2664dd669de81dd6", "message": "add more testcases", "committedDate": "2020-03-22T01:09:39Z", "type": "commit"}, {"oid": "45b3ef61135f65629ebc4dba2664dd669de81dd6", "url": "https://github.com/apache/hudi/commit/45b3ef61135f65629ebc4dba2664dd669de81dd6", "message": "add more testcases", "committedDate": "2020-03-22T01:09:39Z", "type": "forcePushed"}, {"oid": "5bb9e0b111b3d46f6186082c1c8e55571053f8c0", "url": "https://github.com/apache/hudi/commit/5bb9e0b111b3d46f6186082c1c8e55571053f8c0", "message": "Use JavaSparkContext instead of SparkSession", "committedDate": "2020-03-22T15:55:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4NDg1Nw==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r397584857", "bodyText": "use fail assert?", "author": "leesf", "createdAt": "2020-03-25T03:08:03Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -159,18 +161,85 @@ public void testExportAsHudi() throws IOException {\n       assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n     }\n+  }\n+\n+  public static class TestHoodieSnapshotExporterForEarlyAbort extends ExporterTestHarness {\n+\n+    private HoodieSnapshotExporter.Config cfg;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      super.setUp();\n+      cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = OutputFormatValidator.HUDI;\n+    }\n \n     @Test\n-    public void testExportEmptyDataset() throws IOException {\n+    public void testExportWhenTargetPathExists() throws IOException {\n+      // make target output path present\n+      dfs.mkdirs(new Path(targetPath));\n+\n+      // export\n+      Throwable t = null;\n+      try {\n+        new HoodieSnapshotExporter().export(jsc, cfg);", "originalCommit": "5bb9e0b111b3d46f6186082c1c8e55571053f8c0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4NDkyNg==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r397584926", "bodyText": "would move to catch block?", "author": "leesf", "createdAt": "2020-03-25T03:08:23Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -159,18 +161,85 @@ public void testExportAsHudi() throws IOException {\n       assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n     }\n+  }\n+\n+  public static class TestHoodieSnapshotExporterForEarlyAbort extends ExporterTestHarness {\n+\n+    private HoodieSnapshotExporter.Config cfg;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      super.setUp();\n+      cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = OutputFormatValidator.HUDI;\n+    }\n \n     @Test\n-    public void testExportEmptyDataset() throws IOException {\n+    public void testExportWhenTargetPathExists() throws IOException {\n+      // make target output path present\n+      dfs.mkdirs(new Path(targetPath));\n+\n+      // export\n+      Throwable t = null;\n+      try {\n+        new HoodieSnapshotExporter().export(jsc, cfg);\n+      } catch (Exception e) {\n+        t = e;\n+      } finally {\n+        assertNotNull(t);\n+        assertTrue(t instanceof HoodieSnapshotExporterException);\n+        assertEquals(\"The target output path already exists.\", t.getMessage());", "originalCommit": "5bb9e0b111b3d46f6186082c1c8e55571053f8c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5MDAyNg==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r397590026", "bodyText": "@leesf The reason of checking the assertion in finally is to cover the case when exporter does not throw exception, which should be deemed as failed test. Putting in catch block may still pass in that case. \ud83d\ude04", "author": "xushiyan", "createdAt": "2020-03-25T03:28:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4NDkyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5MjEzNw==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r397592137", "bodyText": "let me change these cases to using ExpectedException; something to do with\n@Rule\npublic ExpectedException exceptionRule = ExpectedException.none();\nI'll do a small commit and please see if it looks better. I kinda miss junit 5 \ud83d\ude02", "author": "xushiyan", "createdAt": "2020-03-25T03:37:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4NDkyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4NTA0Mg==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r397585042", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-03-25T03:08:46Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -159,18 +161,85 @@ public void testExportAsHudi() throws IOException {\n       assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n     }\n+  }\n+\n+  public static class TestHoodieSnapshotExporterForEarlyAbort extends ExporterTestHarness {\n+\n+    private HoodieSnapshotExporter.Config cfg;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      super.setUp();\n+      cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = OutputFormatValidator.HUDI;\n+    }\n \n     @Test\n-    public void testExportEmptyDataset() throws IOException {\n+    public void testExportWhenTargetPathExists() throws IOException {\n+      // make target output path present\n+      dfs.mkdirs(new Path(targetPath));\n+\n+      // export\n+      Throwable t = null;\n+      try {\n+        new HoodieSnapshotExporter().export(jsc, cfg);\n+      } catch (Exception e) {\n+        t = e;\n+      } finally {\n+        assertNotNull(t);\n+        assertTrue(t instanceof HoodieSnapshotExporterException);\n+        assertEquals(\"The target output path already exists.\", t.getMessage());\n+      }\n+    }\n+\n+    @Test\n+    public void testExportDatasetWithNoCommit() throws IOException {\n+      // delete commit files\n+      List<Path> commitFiles = Arrays.stream(dfs.listStatus(new Path(sourcePath + \"/.hoodie\")))\n+          .map(FileStatus::getPath)\n+          .filter(filePath -> filePath.getName().endsWith(\".commit\"))\n+          .collect(Collectors.toList());\n+      for (Path p : commitFiles) {\n+        dfs.delete(p, false);\n+      }\n+\n+      // export\n+      Throwable t = null;\n+      try {\n+        new HoodieSnapshotExporter().export(jsc, cfg);\n+      } catch (Exception e) {\n+        t = e;\n+      } finally {\n+        assertNotNull(t);\n+        assertTrue(t instanceof HoodieSnapshotExporterException);\n+        assertEquals(\"No commits present. Nothing to snapshot.\", t.getMessage());", "originalCommit": "5bb9e0b111b3d46f6186082c1c8e55571053f8c0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4NTEyNg==", "url": "https://github.com/apache/hudi/pull/1436#discussion_r397585126", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-03-25T03:09:02Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -159,18 +161,85 @@ public void testExportAsHudi() throws IOException {\n       assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n     }\n+  }\n+\n+  public static class TestHoodieSnapshotExporterForEarlyAbort extends ExporterTestHarness {\n+\n+    private HoodieSnapshotExporter.Config cfg;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      super.setUp();\n+      cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = OutputFormatValidator.HUDI;\n+    }\n \n     @Test\n-    public void testExportEmptyDataset() throws IOException {\n+    public void testExportWhenTargetPathExists() throws IOException {\n+      // make target output path present\n+      dfs.mkdirs(new Path(targetPath));\n+\n+      // export\n+      Throwable t = null;\n+      try {\n+        new HoodieSnapshotExporter().export(jsc, cfg);\n+      } catch (Exception e) {\n+        t = e;\n+      } finally {\n+        assertNotNull(t);\n+        assertTrue(t instanceof HoodieSnapshotExporterException);\n+        assertEquals(\"The target output path already exists.\", t.getMessage());\n+      }\n+    }\n+\n+    @Test\n+    public void testExportDatasetWithNoCommit() throws IOException {\n+      // delete commit files\n+      List<Path> commitFiles = Arrays.stream(dfs.listStatus(new Path(sourcePath + \"/.hoodie\")))\n+          .map(FileStatus::getPath)\n+          .filter(filePath -> filePath.getName().endsWith(\".commit\"))\n+          .collect(Collectors.toList());\n+      for (Path p : commitFiles) {\n+        dfs.delete(p, false);\n+      }\n+\n+      // export\n+      Throwable t = null;\n+      try {\n+        new HoodieSnapshotExporter().export(jsc, cfg);\n+      } catch (Exception e) {\n+        t = e;\n+      } finally {\n+        assertNotNull(t);\n+        assertTrue(t instanceof HoodieSnapshotExporterException);\n+        assertEquals(\"No commits present. Nothing to snapshot.\", t.getMessage());\n+      }\n+\n+      // Check results\n+      assertFalse(dfs.exists(new Path(targetPath)));\n+    }\n+\n+    @Test\n+    public void testExportDatasetWithNoPartition() throws IOException {\n       // delete all source data\n       dfs.delete(new Path(sourcePath + \"/\" + PARTITION_PATH), true);\n \n       // export\n-      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+      Throwable t = null;\n+      try {\n+        new HoodieSnapshotExporter().export(jsc, cfg);\n+      } catch (Exception e) {\n+        t = e;\n+      } finally {\n+        assertNotNull(t);\n+        assertTrue(t instanceof HoodieSnapshotExporterException);\n+        assertEquals(\"The source dataset has 0 partition to snapshot.\", t.getMessage());", "originalCommit": "5bb9e0b111b3d46f6186082c1c8e55571053f8c0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6bb034656370b3360082de5f8e19e2ce52eeb49a", "url": "https://github.com/apache/hudi/commit/6bb034656370b3360082de5f8e19e2ce52eeb49a", "message": "improve unit test for expected exceptions", "committedDate": "2020-03-25T03:53:04Z", "type": "commit"}]}