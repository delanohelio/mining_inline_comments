{"pr_number": 2048, "pr_title": "[HUDI-1072] Introduce REPLACE top level action", "pr_createdAt": "2020-08-27T23:48:41Z", "pr_url": "https://github.com/apache/hudi/pull/2048", "timeline": [{"oid": "d259d1f041753ff2b88c13870b03979734942363", "url": "https://github.com/apache/hudi/commit/d259d1f041753ff2b88c13870b03979734942363", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-08-28T04:05:31Z", "type": "forcePushed"}, {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "url": "https://github.com/apache/hudi/commit/c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-08-28T05:24:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyNjUwMQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479626501", "bodyText": "As discussed, lets retain all the file-groups but perform filtering in the get APIs. THis would avoid correctness issues in filtering and also makes handling incremental file system view easier.", "author": "bvaradar", "createdAt": "2020-08-29T08:40:01Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -173,29 +180,59 @@ protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n     List<HoodieFileGroup> fileGroups = new ArrayList<>();\n     fileIdSet.forEach(pair -> {\n       String fileId = pair.getValue();\n-      HoodieFileGroup group = new HoodieFileGroup(pair.getKey(), fileId, timeline);\n-      if (baseFiles.containsKey(pair)) {\n-        baseFiles.get(pair).forEach(group::addBaseFile);\n-      }\n-      if (logFiles.containsKey(pair)) {\n-        logFiles.get(pair).forEach(group::addLogFile);\n-      }\n+      String partitionPath = pair.getKey();\n+      if (isExcludeFileGroup(partitionPath, fileId)) {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDE3MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364171", "bodyText": "Modified. Please take a look. RocksDB needs to be implemented.", "author": "satishkotha", "createdAt": "2020-09-01T18:56:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyNjUwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyOTcwOQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479629709", "bodyText": "Why are we not tracking dropped fileIds ?", "author": "bvaradar", "createdAt": "2020-08-29T09:18:25Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceStat.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Statistics about a single Hoodie replace operation.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceStat extends HoodieWriteStat {\n+\n+  // records from the 'getFileId()' can be written to multiple new file groups. This list tracks all new fileIds\n+  private List<String> newFileIds;", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDQ2Mg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364462", "bodyText": "its part of HoodieWriteStat#fileId", "author": "satishkotha", "createdAt": "2020-09-01T18:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyOTcwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDE0MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630141", "bodyText": "getAllExcludeFileGroups -> getReplacedFileGroups ?", "author": "bvaradar", "createdAt": "2020-08-29T09:23:53Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java", "diffHunk": "@@ -355,6 +357,18 @@ public RemoteHoodieTableFileSystemView(String server, int port, HoodieTableMetaC\n     }\n   }\n \n+  @Override\n+  public Stream<String> getAllExcludeFileGroups(final String partitionPath) {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDUzNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364537", "bodyText": "changed", "author": "satishkotha", "createdAt": "2020-09-01T18:57:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDE0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDIxMQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630211", "bodyText": "ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL -> REPLACED_FILEGROUPS_FOR_PARTITION_URL\nLets use a single consistent name \"replaced\" instead of exclude everywhere.", "author": "bvaradar", "createdAt": "2020-08-29T09:24:45Z", "path": "hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java", "diffHunk": "@@ -284,6 +284,13 @@ private void registerFileSlicesAPI() {\n       writeValueAsString(ctx, dtos);\n     }, true));\n \n+    app.get(RemoteHoodieTableFileSystemView.ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL, new ViewHandler(ctx -> {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDU3NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364575", "bodyText": "changed", "author": "satishkotha", "createdAt": "2020-09-01T18:57:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDIxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDMxMA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630310", "bodyText": "why is this needed ?", "author": "bvaradar", "createdAt": "2020-08-29T09:25:46Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java", "diffHunk": "@@ -104,4 +104,8 @@ protected SyncableFileSystemView getFileSystemViewWithUnCommittedSlices(HoodieTa\n   protected HoodieTableType getTableType() {\n     return HoodieTableType.COPY_ON_WRITE;\n   }\n+\n+  protected boolean areTimeTravelTestsEnabled() {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDk5NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364995", "bodyText": "RocksDB and Remote FileSystemViews are not implemented yet. So i temporarily disabled those tests. Moved this method to view tests instead of common", "author": "satishkotha", "createdAt": "2020-09-01T18:57:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDMxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTExNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479631115", "bodyText": "Are you planning to address this TODO as part of this PR ?", "author": "bvaradar", "createdAt": "2020-08-29T09:36:10Z", "path": "hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java", "diffHunk": "@@ -102,7 +102,7 @@ public void commit(WriterCommitMessage[] messages) {\n             .flatMap(m -> m.getWriteStatuses().stream().map(m2 -> m2.getStat())).collect(Collectors.toList());\n \n     try {\n-      writeClient.commitStats(instantTime, writeStatList, Option.empty());\n+      writeClient.commitStats(instantTime, writeStatList, Option.empty(), HoodieTimeline.COMMIT_ACTION); //TODO get action type from HoodieWriterCommitMessage", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTA4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365080", "bodyText": "Fixed", "author": "satishkotha", "createdAt": "2020-09-01T18:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTExNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTcyNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479631725", "bodyText": "Rename to getReplacedFileGroups", "author": "bvaradar", "createdAt": "2020-08-29T09:43:46Z", "path": "hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java", "diffHunk": "@@ -89,6 +89,10 @@ public FileSliceHandler(Configuration conf, FileSystemViewManager viewManager) t\n         .collect(Collectors.toList());\n   }\n \n+  public List<String> getExcludeFileGroups(String basePath, String partitionPath) {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTE0NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365144", "bodyText": "Done", "author": "satishkotha", "createdAt": "2020-09-01T18:58:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTcyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjMxMA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479632310", "bodyText": "Please follow the same structure like the one we did for compaction.", "author": "bvaradar", "createdAt": "2020-08-29T09:51:09Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java", "diffHunk": "@@ -77,6 +79,13 @@ public SpillableMapBasedFileSystemView(HoodieTableMetaClient metaClient, HoodieT\n     }\n   }\n \n+  @Override\n+  protected Map<String, Set<String>> createPartitionToExcludeFileGroups() {\n+    // TODO should we create another spillable directory under baseStoreDir?\n+    // the exclude file group is expected to be small, so use parent class in-memory representation\n+    return super.createPartitionToExcludeFileGroups();", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTc0Mg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365742", "bodyText": "Please take a look if i did it correctly. (To be honest, dont fully understand compaction implementation in great detail)", "author": "satishkotha", "createdAt": "2020-09-01T18:59:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjMxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjM4MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479632381", "bodyText": "I think I have mentioned it somewhere else. Lets denote this feature with the consistent term \"Replace\" everywhere.", "author": "bvaradar", "createdAt": "2020-08-29T09:52:04Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java", "diffHunk": "@@ -148,6 +148,9 @@\n    */\n   Stream<HoodieFileGroup> getAllFileGroups(String partitionPath);\n \n+  Stream<String> getAllExcludeFileGroups(String partitionPath);", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTgyNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365825", "bodyText": "Done", "author": "satishkotha", "createdAt": "2020-09-01T18:59:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjM4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzNDgxMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479634813", "bodyText": "Instead of direct listStatus, can you use FileSystemViewAbstraction to get the file-group and then delete each files in it ? THis way, once consolidated metadata becomes available, you can take advantage of that. cc @prashantwason", "author": "bvaradar", "createdAt": "2020-08-29T10:22:24Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,44 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {\n+      try {\n+        HoodieCommitMetadata metadata = HoodieCommitMetadata.fromBytes(\n+            metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n+            HoodieCommitMetadata.class);\n+\n+        metadata.getPartitionToReplaceStats().entrySet().stream().forEach(entry ->\n+            deleteFileGroups(entry.getKey(), entry.getValue().stream().map(e -> e.getFileId()).collect(Collectors.toSet()), instant)\n+        );\n+      } catch (IOException e) {\n+        throw new HoodieCommitException(\"Failed to archive because cannot delete replace files\", e);\n+      }\n+    });\n+  }\n+\n+  private void deleteFileGroups(String partitionPath, Set<String> fileIdsToDelete, HoodieInstant instant) {\n+    try {\n+      FileStatus[] statuses = metaClient.getFs().listStatus(FSUtils.getPartitionPath(metaClient.getBasePath(), partitionPath));", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTk4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365985", "bodyText": "Modified to use FileSystemViews", "author": "satishkotha", "createdAt": "2020-09-01T18:59:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzNDgxMw=="}], "type": "inlineReview"}, {"oid": "1f5f737ee065b08979dca06d3153082806710057", "url": "https://github.com/apache/hudi/commit/1f5f737ee065b08979dca06d3153082806710057", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-08-31T07:33:49Z", "type": "forcePushed"}, {"oid": "849d7880b8b3dd7b32abb8ab313f58b7793c604f", "url": "https://github.com/apache/hudi/commit/849d7880b8b3dd7b32abb8ab313f58b7793c604f", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T00:40:04Z", "type": "forcePushed"}, {"oid": "aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "url": "https://github.com/apache/hudi/commit/aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T00:47:33Z", "type": "forcePushed"}, {"oid": "28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "url": "https://github.com/apache/hudi/commit/28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T17:15:17Z", "type": "forcePushed"}, {"oid": "f04fbc6bc375ec29271a449dd870cb2652f85626", "url": "https://github.com/apache/hudi/commit/f04fbc6bc375ec29271a449dd870cb2652f85626", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T20:22:50Z", "type": "forcePushed"}, {"oid": "087909fe7d2000171a551be12c8ffa79b6ec6250", "url": "https://github.com/apache/hudi/commit/087909fe7d2000171a551be12c8ffa79b6ec6250", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-02T00:59:47Z", "type": "forcePushed"}, {"oid": "f5b19ed6194364a9125f7b7110ed49ae5090b66c", "url": "https://github.com/apache/hudi/commit/f5b19ed6194364a9125f7b7110ed49ae5090b66c", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-02T18:06:39Z", "type": "forcePushed"}, {"oid": "6bd5bce92282f4131c0349083174e3e0b68a2f1d", "url": "https://github.com/apache/hudi/commit/6bd5bce92282f4131c0349083174e3e0b68a2f1d", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-02T18:59:17Z", "type": "forcePushed"}, {"oid": "00b151b7ed0b419146fabb9b85122903aaada57f", "url": "https://github.com/apache/hudi/commit/00b151b7ed0b419146fabb9b85122903aaada57f", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-04T19:41:30Z", "type": "forcePushed"}, {"oid": "6c9793056d885a74c17e3d0275d348d28684e63b", "url": "https://github.com/apache/hudi/commit/6c9793056d885a74c17e3d0275d348d28684e63b", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-04T20:23:41Z", "type": "forcePushed"}, {"oid": "f7e55e9b2db8a3fd906c48af77c3759dca1f065d", "url": "https://github.com/apache/hudi/commit/f7e55e9b2db8a3fd906c48af77c3759dca1f065d", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-10T18:58:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjM2OTQ0MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r486369440", "bodyText": "can't we just call commit(String, JavaRDD, Option.empty()) without having to implement this?", "author": "vinothchandar", "createdAt": "2020-09-10T14:03:10Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,55 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);", "originalCommit": "6c9793056d885a74c17e3d0275d348d28684e63b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTU3OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541578", "bodyText": "rename: to just buildMetdata() , Commit is already  implicit from context.", "author": "vinothchandar", "createdAt": "2020-09-13T15:14:50Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,57 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, String commitActionType) {\n+    return commit(instantTime, writeStatuses, Option.empty(), commitActionType);\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType);\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n+      Option<Map<String, String>> extraMetadata, String commitActionType) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replaceStats = writeStatuses.filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, replaceStats);\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, List<HoodieWriteStat> replaceStats) {\n+    LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n     HoodieTableMetaClient metaClient = createMetaClient(false);\n-    String actionType = metaClient.getCommitActionType();\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n     HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-\n+    HoodieCommitMetadata metadata = CommitUtils.buildWriteActionMetadata(stats, replaceStats, extraMetadata, operationType, config.getSchema(), commitActionType);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTY2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541663", "bodyText": "note to self: lets see if we can simply the commit() overloaded methods.", "author": "vinothchandar", "createdAt": "2020-09-13T15:15:30Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,57 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, String commitActionType) {\n+    return commit(instantTime, writeStatuses, Option.empty(), commitActionType);\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType);\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTg2NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541865", "bodyText": "rename: writeInputRecords()", "author": "vinothchandar", "createdAt": "2020-09-13T15:17:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -95,6 +93,13 @@ public HoodieWriteMetadata execute(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n       saveWorkloadProfileMetadataToInflight(profile, instantTime);\n     }\n \n+    JavaRDD<WriteStatus> writeStatusRDD = processInputRecords(inputRecordsRDD, profile);\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    updateIndexAndCommitIfNeeded(writeStatusRDD, result);\n+    return result;\n+  }\n+\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MjA3Mg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542072", "bodyText": "note to self: see if there is a way to avoid repeating this filtering here again", "author": "vinothchandar", "createdAt": "2020-09-13T15:19:03Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MjI0MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542241", "bodyText": "rename: completeInstant(), its better to stay close to what the method is doing; using just one terminology", "author": "vinothchandar", "createdAt": "2020-09-13T15:20:18Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n-  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> stats) {\n-    String actionType = table.getMetaClient().getCommitActionType();\n+  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n+    String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n-    HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n \n     result.setCommitted(true);\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-    result.setWriteStats(stats);\n+    result.setWriteStats(writeStats);\n+    result.setReplaceStats(replaceStats);\n \n     // Finalize write\n-    finalizeWrite(instantTime, stats, result);\n-\n-    // add in extra metadata\n-    if (extraMetadata.isPresent()) {\n-      extraMetadata.get().forEach(metadata::addMetadata);\n-    }\n-    metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, getSchemaToStoreInCommit());\n-    metadata.setOperationType(operationType);\n+    finalizeWrite(instantTime, writeStats, result);\n \n     try {\n-      activeTimeline.saveAsComplete(new HoodieInstant(true, actionType, instantTime),\n-          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n-      LOG.info(\"Committed \" + instantTime);\n+      HoodieCommitMetadata metadata = writeInstant(writeStats, replaceStats, extraMetadata);\n+      result.setCommitMetadata(Option.of(metadata));\n     } catch (IOException e) {\n       throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n           e);\n     }\n-    result.setCommitMetadata(Option.of(metadata));\n+  }\n+\n+  private HoodieCommitMetadata writeInstant(List<HoodieWriteStat>  writeStats, List<HoodieWriteStat> replaceStats, Option<Map<String, String>> extraMetadata) throws IOException {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Mjc2OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542769", "bodyText": "nts: need to ensure the operation type  is properly set", "author": "vinothchandar", "createdAt": "2020-09-13T15:25:18Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n-  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> stats) {\n-    String actionType = table.getMetaClient().getCommitActionType();\n+  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n+    String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n-    HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n \n     result.setCommitted(true);\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-    result.setWriteStats(stats);\n+    result.setWriteStats(writeStats);\n+    result.setReplaceStats(replaceStats);\n \n     // Finalize write\n-    finalizeWrite(instantTime, stats, result);\n-\n-    // add in extra metadata\n-    if (extraMetadata.isPresent()) {\n-      extraMetadata.get().forEach(metadata::addMetadata);\n-    }\n-    metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, getSchemaToStoreInCommit());\n-    metadata.setOperationType(operationType);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzE5NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487543194", "bodyText": "I think at the HoodieTable level, the API has to be about replacing file groups and not insertOverwrite (which can be limited to the WriteClient level). This way clustering can also use the same method, to build on top.", "author": "vinothchandar", "createdAt": "2020-09-13T15:29:36Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -213,6 +213,12 @@ public abstract HoodieWriteMetadata insertPrepped(JavaSparkContext jsc, String i\n   public abstract HoodieWriteMetadata bulkInsertPrepped(JavaSparkContext jsc, String instantTime,\n       JavaRDD<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> bulkInsertPartitioner);\n \n+  /**\n+   * Logically delete all existing records and Insert a batch of new records into Hoodie table at the supplied instantTime.\n+   */\n+  public abstract HoodieWriteMetadata insertOverwrite(JavaSparkContext jsc, String instantTime,", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2NDIxNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487564215", "bodyText": "On second thoughts, I am okay leaving this as-is for now as well. and reeval when acutally implementing clustering", "author": "vinothchandar", "createdAt": "2020-09-13T18:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzMxNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487543317", "bodyText": "rename: deleteReplacedFileGroups() , to be consistent with our terminology", "author": "vinothchandar", "createdAt": "2020-09-13T15:30:29Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487544876", "bodyText": "you probably dont want to fetch the fs object each time? Also lets delete in parallel, from the get go?  we will invariably need to do this, much like parallelizing cleaning.", "author": "vinothchandar", "createdAt": "2020-09-13T15:45:12Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {\n+      fg.getAllRawFileSlices().forEach(fileSlice -> {\n+        fileSlice.getBaseFile().map(baseFile -> deletePath(baseFile.getFileStatus().getPath(), instant));\n+        fileSlice.getLogFiles().forEach(logFile -> deletePath(logFile.getPath(), instant));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Because we are creating new 'HoodieTable' and FileSystemView objects in this class constructor,\n+   * partition view may not be loaded correctly.\n+   * Reload all partitions modified by REPLACE action\n+   *\n+   * TODO find a better way to pass the FileSystemView to this class.\n+   */\n+  private void ensureReplacedPartitionsLoadedCorrectly(HoodieInstant instant, TableFileSystemView fileSystemView) {\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {\n+      try {\n+        HoodieReplaceCommitMetadata metadata = HoodieReplaceCommitMetadata.fromBytes(\n+            metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n+            HoodieReplaceCommitMetadata.class);\n+\n+        metadata.getPartitionToReplaceStats().keySet().forEach(partition -> fileSystemView.getAllFileGroups(partition));\n+      } catch (IOException e) {\n+        throw new HoodieCommitException(\"Failed to archive because cannot delete replace files\", e);\n+      }\n+    });\n+  }\n+\n+  private boolean deletePath(Path path, HoodieInstant instant) {\n+    try {\n+      LOG.info(\"Deleting \" + path + \" before archiving \" + instant);\n+      metaClient.getFs().delete(path);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwODE4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488208180", "bodyText": "metaclient.getFs() seems to be following singleton pattern, so it doesnt seem expensive to get this. am i reading it incorrectly?\nI can work to setup parallel deletes", "author": "satishkotha", "createdAt": "2020-09-14T20:43:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk5MDc5OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488990799", "bodyText": "for parallel deletes, JavaSparkContext is not exposed to Archive process. Since we anyway want to move this to be part of clean, is it ok if  I defer this to https://issues.apache.org/jira/browse/HUDI-1276?", "author": "satishkotha", "createdAt": "2020-09-15T21:40:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0Nzk4Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489247987", "bodyText": "This could become a performance issue when we are deleting lot of replaced files. HoodieTimelineArchiveLog.archive() method is taking JavaSparkContext. right ?", "author": "bvaradar", "createdAt": "2020-09-16T08:12:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2MTE3OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490461178", "bodyText": "i missed that we already have JavaSparkContext. Implemented parallel clean up. PTAL", "author": "satishkotha", "createdAt": "2020-09-17T18:13:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487545022", "bodyText": "we need to do the deletion in parallel.", "author": "vinothchandar", "createdAt": "2020-09-13T15:46:44Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0NjY2Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489246667", "bodyText": "+1", "author": "bvaradar", "createdAt": "2020-09-16T08:10:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ3NjY4OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490476688", "bodyText": "Implemented parallel execution. PTAL", "author": "satishkotha", "createdAt": "2020-09-17T18:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTExNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487545115", "bodyText": "probably a check that this is a replace instant as well?", "author": "vinothchandar", "createdAt": "2020-09-13T15:47:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwODYzNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488208637", "bodyText": "correct. will add it.", "author": "satishkotha", "createdAt": "2020-09-14T20:44:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTExNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2MTMzMQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490461331", "bodyText": "Implemented parallel deletion. PTAL.", "author": "satishkotha", "createdAt": "2020-09-17T18:14:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTExNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NzUwNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487547505", "bodyText": "This seems like a check for whether the instant is a replacecommit or not. if the instant time is a completed instant and replacecommit type, then we must find the instant here, right?", "author": "vinothchandar", "createdAt": "2020-09-13T16:10:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {\n+      fg.getAllRawFileSlices().forEach(fileSlice -> {\n+        fileSlice.getBaseFile().map(baseFile -> deletePath(baseFile.getFileStatus().getPath(), instant));\n+        fileSlice.getLogFiles().forEach(logFile -> deletePath(logFile.getPath(), instant));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Because we are creating new 'HoodieTable' and FileSystemView objects in this class constructor,\n+   * partition view may not be loaded correctly.\n+   * Reload all partitions modified by REPLACE action\n+   *\n+   * TODO find a better way to pass the FileSystemView to this class.\n+   */\n+  private void ensureReplacedPartitionsLoadedCorrectly(HoodieInstant instant, TableFileSystemView fileSystemView) {\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwODU3MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488208571", "bodyText": "correct, i'll restructure this code", "author": "satishkotha", "createdAt": "2020-09-14T20:43:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NzUwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487547971", "bodyText": "Do we need to ask the file system view for all the replace file groups? this must be in the metadata already right? As long as we can get the HoodieFileGroup objects corresponding to the filegroup ids in the metadata, we can go ahead? What I am suggesting in an alternative and subjectively cleaner replacement for ensureReplaced... above, which seems to make a dummy read to warm up the datastuctures. I prefer to let that happen naturally on its own as opposed to having this \"special\" call", "author": "vinothchandar", "createdAt": "2020-09-13T16:14:41Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjMxOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552318", "bodyText": "As an after thought, I also realize that if we just encoded the entire file group being replaced into the metadata, (as opposed to just encoding the file ids), we can simply delete the file groups without any interaction with tableFileSytemView at all. Probably a simpler solution even?", "author": "vinothchandar", "createdAt": "2020-09-13T16:57:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2NDQwMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487564403", "bodyText": "One more consideration, as I went through the remainder of the PR. if there was an pending compaction for the replaced file group, then the file group metadata we encode may miss new base files produced as a result of the compaction. This scenario needs to be thought thru.", "author": "vinothchandar", "createdAt": "2020-09-13T18:58:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwNzI3OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488207278", "bodyText": "Yes, compaction is the primary reason I only recorded fileId in the replace metadata. When deleting, we can get all file paths (through view or by listing using consolidated metadata) that have same fileId and delete these files.\nThere can be race conditions that compaction might create a new file with replaced fileId after we queried for existing files though. But because FileSystemView#get methods do not include replaced file groups, I think this is unlikely to happen. I'm not sure if there are edge cases with long running compactions.\nPlease suggest any other improvements.", "author": "satishkotha", "createdAt": "2020-09-14T20:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0NTc1MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489245751", "bodyText": "Only commit instants older than oldest pending compaction is allowed to be archived. But, if we encode the entire file-group in the replace metadata, we will have race conditions with pending compactions. So, I guess it is safer to figure out the file-group during the time of archiving when it is guaranteed pending compaction is done.\nRegarding the requirement for ensureReplacedPartitionsLoadedCorrectly, If you look at pending compaction handling in filesystem-view, pending compactions are eagerly loaded whenever we construct the filesystem view. This seems to be the case also for replace metadata. Then, why do we need to trigger loading from outside ?", "author": "bvaradar", "createdAt": "2020-09-16T08:08:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5MTc0NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490491745", "bodyText": "So, this is tricky to explain. In FileSystemView, only metadata seems to be eagerly loaded. file groups are not eagerly loaded. i.e., fetchAllStoredFileGroups() returns empty.  For replace instants, we need to get List for all replaced fileId. Because fetchAllStoredFileGroups() is empty, its also returning empty list of FileSlices. So we dont delete replaced files.\nI think instead of creating new HoodieTable in constructor. passing that from callers would help workaround this problem. But that is somewhat involved change because of test dependencies. Also, it might be better to refresh partition content in case new files are created by compaction or other process and somehow that is not reflected in table views. This might be safer option.\nLet me know if you want me to work on passing in HoodieTable to HoodieTimelineArchiveLog constructor.", "author": "satishkotha", "createdAt": "2020-09-17T19:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4OTkwOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490589908", "bodyText": "@satishkotha Here is the plan as we discussed.\n\nChange the signature of fileSystemView.getReplacedFileGroupsBeforeOrOn to also take in partitionId\nIn HoodieTimelineArchiveLog.deleteReplacedFileGroups, read the replace metadata (which we are already doing) and for each partition, call fileSystemView.getReplacedFileGroupsBeforeOrOn().\n(2) must be done in such a way that we are calling the fileSystemView.getReplacedFileGroupsBeforeOrOn in parallel.\n\nThis should allow for lazy loading semantics to be retained at file-system view.", "author": "bvaradar", "createdAt": "2020-09-17T22:09:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMTM0MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490621340", "bodyText": "Done. PTAL.", "author": "satishkotha", "createdAt": "2020-09-17T23:44:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODI4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487548280", "bodyText": "you can just call startCommitWithTime(instantTime) from here?", "author": "vinothchandar", "createdAt": "2020-09-13T16:17:26Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -576,7 +592,8 @@ public String startCommit() {\n       rollbackPendingCommits();\n     }\n     String instantTime = HoodieActiveTimeline.createNewInstantTime();\n-    startCommit(instantTime);\n+    HoodieTableMetaClient metaClient = createMetaClient(true);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwOTY4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488209685", "bodyText": "sure", "author": "satishkotha", "createdAt": "2020-09-14T20:46:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODI4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODM1NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487548355", "bodyText": "can we pass in the metaClient from caller. This seems to introduce additional creations, which all list .hoodie again", "author": "vinothchandar", "createdAt": "2020-09-13T16:18:17Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,15 +603,23 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, metaClient.getCommitActionType());\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  public void startCommitWithTime(String instantTime, String actionType) {\n     // NOTE : Need to ensure that rollback is done before a new commit is started\n     if (rollbackPending) {\n       // Only rollback inflight commit/delta-commits. Do not touch compaction commits\n       rollbackPendingCommits();\n     }\n-    startCommit(instantTime);\n+    startCommit(instantTime, actionType);\n   }\n \n-  private void startCommit(String instantTime) {\n+  private void startCommit(String instantTime, String actionType) {\n     LOG.info(\"Generate a new instant time \" + instantTime);\n     HoodieTableMetaClient metaClient = createMetaClient(true);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIxMjkyOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488212928", "bodyText": "Good point. Will do.", "author": "satishkotha", "createdAt": "2020-09-14T20:52:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODM1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0OTc3Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487549773", "bodyText": "typo: getReplaceStats", "author": "vinothchandar", "createdAt": "2020-09-13T16:32:12Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java", "diffHunk": "@@ -94,6 +94,14 @@ public void setWriteStats(List<HoodieWriteStat> writeStats) {\n     this.writeStats = Option.of(writeStats);\n   }\n \n+  public Option<List<HoodieWriteStat>> getReplacetats() {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIxNTgzNA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488215834", "bodyText": "This is not needed anymore given we are tracking replaced files as boolean in WriteStatus. I removed this.", "author": "satishkotha", "createdAt": "2020-09-14T20:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0OTc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTQ2Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487551467", "bodyText": "why limit to just base files. we may have log files without base files. i.e insert to log files code path", "author": "vinothchandar", "createdAt": "2020-09-13T16:49:36Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n+    // do necessary inserts into new file groups\n+    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    return writeStatuses.union(replaceStatuses);\n+  }\n+\n+  private JavaRDD<WriteStatus> getAllReplaceWriteStatus(WorkloadProfile profile) {\n+    JavaRDD<String> partitions = jsc.parallelize(new ArrayList<>(profile.getPartitionPaths()));\n+    JavaRDD<WriteStatus> replaceStatuses = partitions.flatMap(partition ->\n+        getAllExistingFileIds(partition).map(fileId -> getReplaceWriteStatus(partition, fileId)).iterator());\n+\n+    return replaceStatuses;\n+  }\n+\n+  private Stream<String> getAllExistingFileIds(String partitionPath) {\n+    // because new commit is not complete. it is safe to mark all base files as old files\n+    return table.getBaseFileOnlyView().getAllBaseFiles(partitionPath).map(baseFile -> baseFile.getFileId());", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIxNjE5OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488216199", "bodyText": "My bad, missed that insert into log files case. I fixed it now. thanks for finding this bug.", "author": "satishkotha", "createdAt": "2020-09-14T20:58:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTQ2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTY4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487551680", "bodyText": "Seeing such large values in the metadata can be bit confusing. can we set it to -1 instead for now", "author": "vinothchandar", "createdAt": "2020-09-13T16:51:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n+    // do necessary inserts into new file groups\n+    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    return writeStatuses.union(replaceStatuses);\n+  }\n+\n+  private JavaRDD<WriteStatus> getAllReplaceWriteStatus(WorkloadProfile profile) {\n+    JavaRDD<String> partitions = jsc.parallelize(new ArrayList<>(profile.getPartitionPaths()));\n+    JavaRDD<WriteStatus> replaceStatuses = partitions.flatMap(partition ->\n+        getAllExistingFileIds(partition).map(fileId -> getReplaceWriteStatus(partition, fileId)).iterator());\n+\n+    return replaceStatuses;\n+  }\n+\n+  private Stream<String> getAllExistingFileIds(String partitionPath) {\n+    // because new commit is not complete. it is safe to mark all base files as old files\n+    return table.getBaseFileOnlyView().getAllBaseFiles(partitionPath).map(baseFile -> baseFile.getFileId());\n+  }\n+\n+  private WriteStatus getReplaceWriteStatus(String partitionPath, String fileId) {\n+    // mark file as 'replaced' in metadata. the actual file will be removed later by cleaner to provide snapshot isolation\n+    WriteStatus status = new WriteStatus(false, 0.0);\n+    status.setReplacedFileId(true);\n+    status.setFileId(fileId);\n+    status.setTotalErrorRecords(0);\n+    status.setPartitionPath(partitionPath);\n+    HoodieWriteStat replaceStat = new HoodieWriteStat();\n+    status.setStat(replaceStat);\n+    replaceStat.setPartitionPath(partitionPath);\n+    replaceStat.setFileId(fileId);\n+    replaceStat.setPath(table.getBaseFileOnlyView().getLatestBaseFile(partitionPath, fileId).get().getPath());\n+    status.getStat().setNumDeletes(Integer.MAX_VALUE);//token to indicate all rows are deleted", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjE3MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552170", "bodyText": "So, this creates a dependency on the workloadProfile for doing insert overwrite. While we always provide a WorkloadProfile for now, in the future we would like to remove this need for caching data in memory and building the profile.\nCan we try to reimplement this such that\n\nprocessInputRecords(..) just writes the new records and returns WriteStatus for the new file groups alone.\nDuring commit time, after we collect the WriteStatus, we can obtain the replaceStatuses based on the partitions that were actually written to during step above.\n\nThis also gives us a cleaner solution for avoiding the boolean flag we discussed. API is also consistent now, that writeClient.insertOverwrite() only returns the WriteStatus for the new file group IDs.", "author": "vinothchandar", "createdAt": "2020-09-13T16:56:18Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4NzkzNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488987937", "bodyText": "I refactored it and removed boolean from WriteStatus. PTAL", "author": "satishkotha", "createdAt": "2020-09-15T21:36:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjE3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjYwOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552608", "bodyText": "please keep test/naming to base and log files. and not leak parquet to the test? Also can you please see if this test can be authored by reusing existing helpers. Its often bit hard to read and reuse the exisiting helpers, but hte more one-offs we introduce, the worse this situation becomes.", "author": "vinothchandar", "createdAt": "2020-09-13T17:00:59Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -880,6 +880,89 @@ public void testDeletesWithDeleteApi() throws Exception {\n     testDeletes(client, updateBatch3.getRight(), 10, file1, \"007\", 140, keysSoFar);\n   }\n \n+  /**\n+   * Test scenario of writing more file groups than existing number of file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlingWithMoreRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(1000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing fewer file groups than existing number of file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlingWithFewerRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(3000, 1000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlinWithSimilarNumberOfRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(3000, 3000);\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records.\n+   *  2) Do write2 (insert overwrite) with 'batch2RecordsCount' number of records.\n+   *\n+   *  Verify that all records in step1 are overwritten\n+   */\n+  private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int batch2RecordsCount) throws Exception {\n+    final String testPartitionPath = \"americas\";\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000);\n+    HoodieWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n+\n+    // Do Inserts\n+    String commitTime1 = \"001\";\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime1, batch1RecordsCount);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyParquetFileData(commitTime1, inserts1, statuses);\n+\n+    // Do Insert Overwrite\n+    String commitTime2 = \"002\";\n+    client.startCommitWithTime(commitTime2, HoodieTimeline.REPLACE_COMMIT_ACTION);\n+    List<HoodieRecord> inserts2 = dataGen.generateInserts(commitTime2, batch2RecordsCount);\n+    List<HoodieRecord> insertsAndUpdates2 = new ArrayList<>();\n+    insertsAndUpdates2.addAll(inserts2);\n+    JavaRDD<HoodieRecord> insertAndUpdatesRDD2 = jsc.parallelize(insertsAndUpdates2, 2);\n+    statuses = client.insertOverwrite(insertAndUpdatesRDD2, commitTime2).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> replacedBuckets = statuses.stream().filter(s -> s.isReplacedFileId())\n+        .map(s -> s.getFileId()).collect(Collectors.toSet());\n+    assertEquals(batch1Buckets, replacedBuckets);\n+    List<WriteStatus> newBuckets = statuses.stream().filter(s -> !(s.isReplacedFileId()))\n+        .collect(Collectors.toList());\n+    verifyParquetFileData(commitTime2, inserts2, newBuckets);\n+  }\n+\n+  /**\n+   * Verify data in parquet files matches expected records and commit time.\n+   */\n+  private void verifyParquetFileData(String commitTime, List<HoodieRecord> expectedRecords, List<WriteStatus> allStatus) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NjE1Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488346156", "bodyText": "renamed it for now. I'll look into if there are any other helpers for doing this.", "author": "satishkotha", "createdAt": "2020-09-15T02:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjYwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjY4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552685", "bodyText": "rename: isReplaced", "author": "vinothchandar", "createdAt": "2020-09-13T17:01:51Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java", "diffHunk": "@@ -52,6 +52,9 @@\n \n   private HoodieWriteStat stat = null;\n \n+  // if true, indicates the fileId in this WriteStatus is being replaced\n+  private boolean isReplacedFileId;", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Mjc4MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552781", "bodyText": "now that replace also is replacecommit. should we just leave the getCommitsAndCompactionTimeline() be?", "author": "vinothchandar", "createdAt": "2020-09-13T17:03:07Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjgyOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552828", "bodyText": "nit: remove extra line?", "author": "vinothchandar", "createdAt": "2020-09-13T17:03:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n+    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);\n   }\n \n   /**\n    * Adds the provided statuses into the file system view, and also caches it inside this object.\n    */\n   protected List<HoodieFileGroup> addFilesToView(FileStatus[] statuses) {\n     HoodieTimer timer = new HoodieTimer().startTimer();\n+", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NTEzOQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487555139", "bodyText": "please use HoodieTimer to time code segments.", "author": "vinothchandar", "createdAt": "2020-09-13T17:27:05Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -196,6 +205,32 @@ protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n     return fileGroups;\n   }\n \n+  /**\n+   * Get replaced instant for each file group by looking at all commit instants.\n+   */\n+  private void resetFileGroupsReplaced(HoodieTimeline timeline) {\n+    Instant indexStartTime = Instant.now();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NjEzMA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556130", "bodyText": "I think its sufficient todo this reset in the init() method above, much like pendingCompaction and bootstreap handling. This method is simply used to refresh the timeline i.e the instants that are visible.", "author": "vinothchandar", "createdAt": "2020-09-13T17:36:31Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n+    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NjYyNg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556626", "bodyText": "rename: getReplaceInstant()", "author": "vinothchandar", "createdAt": "2020-09-13T17:41:51Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -727,6 +775,21 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract Stream<HoodieFileGroup> fetchAllStoredFileGroups();\n \n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for new file groups replaced.\n+   */\n+  protected abstract void addReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract Option<HoodieInstant> getReplacedInstant(final HoodieFileGroupId fileGroupId);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Njg5OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556898", "bodyText": "can you just call isFileGroupReplacedBeforeOrOn(fileGroup, max(instants)) ? without having to implement this again", "author": "vinothchandar", "createdAt": "2020-09-13T17:44:44Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -880,6 +957,30 @@ private FileSlice fetchMergedFileSlice(HoodieFileGroup fileGroup, FileSlice file\n         .fromJavaOptional(fetchLatestFileSlices(partitionPath).filter(fs -> fs.getFileId().equals(fileId)).findFirst());\n   }\n \n+  private boolean isFileGroupReplaced(HoodieFileGroup fileGroup) {\n+    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    return hoodieInstantOption.isPresent();\n+  }\n+\n+  private boolean isFileGroupReplacedBeforeAny(HoodieFileGroup fileGroup, List<String> instants) {\n+    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    if (!hoodieInstantOption.isPresent()) {\n+      return false;\n+    }\n+\n+    return HoodieTimeline.compareTimestamps(instants.stream().max(Comparator.naturalOrder()).get(),", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzA0OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557049", "bodyText": "deltacommit is not here. huh. file a \"code cleanup\" JIRA for later?", "author": "vinothchandar", "createdAt": "2020-09-13T17:45:40Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java", "diffHunk": "@@ -22,5 +22,5 @@\n  * The supported action types.\n  */\n public enum ActionType {\n-  commit, savepoint, compaction, clean, rollback\n+  commit, savepoint, compaction, clean, rollback, replacecommit", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI2ODc3Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488268777", "bodyText": "Filed HUDI-1281", "author": "satishkotha", "createdAt": "2020-09-14T22:31:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzA0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzE5MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557190", "bodyText": "please add a simple unit tests for this . testing for e.g that the schema is set, op type is set etc", "author": "vinothchandar", "createdAt": "2020-09-13T17:46:52Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Helper class to generate compaction plan from FileGroup/FileSlice abstraction.\n+ */\n+public class CommitUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(CommitUtils.class);\n+\n+  public static HoodieCommitMetadata buildWriteActionMetadata(List<HoodieWriteStat> writeStats,", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0MzM2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488343363", "bodyText": "Added", "author": "satishkotha", "createdAt": "2020-09-15T02:23:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzM3Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557376", "bodyText": "nit: extra line", "author": "vinothchandar", "createdAt": "2020-09-13T17:48:56Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java", "diffHunk": "@@ -46,11 +46,12 @@\n   public static final String SCHEMA_KEY = \"schema\";\n   private static final Logger LOG = LogManager.getLogger(HoodieCommitMetadata.class);\n   protected Map<String, List<HoodieWriteStat>> partitionToWriteStats;\n+", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzQzMg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557432", "bodyText": "rename: getCompletedReplaceTimeline()  current naming gives the impression that its either completed or replacecommit", "author": "vinothchandar", "createdAt": "2020-09-13T17:49:49Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java", "diffHunk": "@@ -113,6 +112,18 @@ public HoodieDefaultTimeline getCommitsAndCompactionTimeline() {\n     return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n   }\n \n+  @Override\n+  public HoodieDefaultTimeline getWriteActionTimeline() {\n+    Set<String> validActions = CollectionUtils.createSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION, REPLACE_COMMIT_ACTION);\n+    return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n+  }\n+\n+  @Override\n+  public HoodieTimeline getCompletedAndReplaceTimeline() {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557894", "bodyText": "these are the file groups being replaced? I thought we were going to just track the file ids?", "author": "vinothchandar", "createdAt": "2020-09-13T17:54:22Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.PropertyAccessor;\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * All the metadata that gets stored along with a commit.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n+  private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n+  protected Map<String, List<HoodieWriteStat>> partitionToReplaceStats;", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODAwNA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558004", "bodyText": "if these are the file groups being replaced, then does this contain all the file slices (see my comment around deleting the replaced file groups in timeline archive log)", "author": "vinothchandar", "createdAt": "2020-09-13T17:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NDg1OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488344859", "bodyText": "I changed it to List to include only fileIds. I'm inclined against storing all file slices because they can evolve between metadata creation and archival/clean. Let me know if this understanding is incorrect", "author": "satishkotha", "createdAt": "2020-09-15T02:28:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODA1NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558055", "bodyText": "rename: fgIdToReplaceInstants", "author": "vinothchandar", "createdAt": "2020-09-13T17:55:59Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java", "diffHunk": "@@ -64,6 +64,11 @@\n    */\n   protected Map<HoodieFileGroupId, BootstrapBaseFileMapping> fgIdToBootstrapBaseFile;\n \n+  /**\n+   * Track replace time for replaced file groups.\n+   */\n+  protected Map<HoodieFileGroupId, HoodieInstant> fgIdToReplaceInstant;", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODI3NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558275", "bodyText": "for tests, I suggest using the HoodieWritableTestTable etc instead of introducing new methods. Also please check other utilities to avoid writing a new method here", "author": "vinothchandar", "createdAt": "2020-09-13T17:58:44Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java", "diffHunk": "@@ -366,6 +378,14 @@ public static void createCompactionRequestedFile(String basePath, String instant\n     createEmptyFile(basePath, commitFile, configuration);\n   }\n \n+  public static void createDataFile(String basePath, String partitionPath, String instantTime, String fileID, Configuration configuration)", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODMzNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558335", "bodyText": "why is this change necessayr?", "author": "vinothchandar", "createdAt": "2020-09-13T17:59:21Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java", "diffHunk": "@@ -378,9 +398,9 @@ public static void createCompactionAuxiliaryMetadata(String basePath, HoodieInst\n         new Path(basePath + \"/\" + HoodieTableMetaClient.AUXILIARYFOLDER_NAME + \"/\" + instant.getFileName());\n     FileSystem fs = FSUtils.getFs(basePath, configuration);\n     try (FSDataOutputStream os = fs.create(commitFile, true)) {\n-      HoodieCompactionPlan workload = new HoodieCompactionPlan();\n+      HoodieCompactionPlan workload = HoodieCompactionPlan.newBuilder().setVersion(1).build();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4Mzg5Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488283893", "bodyText": "Version is not being set by default, so reading test plan generated here is failing. So I explicitly set version.", "author": "satishkotha", "createdAt": "2020-09-14T22:58:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODMzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODQ1MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558451", "bodyText": "there is nothing specific about replace in this method? should we move this to the test class itself. inline?", "author": "vinothchandar", "createdAt": "2020-09-13T18:00:24Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java", "diffHunk": "@@ -200,6 +199,14 @@ public static void createInflightCommitFiles(String basePath, String... instantT\n     }\n   }\n \n+  public static HoodieWriteStat createReplaceStat(final String partitionPath, final String fileId1) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4NDY1NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488284655", "bodyText": "Done.", "author": "satishkotha", "createdAt": "2020-09-14T23:00:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODQ1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558507", "bodyText": "see earlier comment on whether we need this method.", "author": "vinothchandar", "createdAt": "2020-09-13T18:01:05Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java", "diffHunk": "@@ -133,6 +137,21 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+  /**\n+   * Timeline to just include commits (commit/deltacommit), replace and compaction actions.\n+   *\n+   * @return\n+   */\n+  HoodieDefaultTimeline getWriteActionTimeline();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2MzM4NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487563384", "bodyText": "To expand, I am wondering if we should just include replacecommit within getCommitsAndCompactionTimeline(). Most of its callers are around compaction/savepoint/restore etc. So we may not be seeing some cases here.\nThings work for now, since filterCompletedInstants() etc are including replace commit in the timeline when filtering for queries. Semantically, if replace is a commit level action that can add new data to the timeline, then we should just treat it like delta commit IMO.\nWould anything break if we did do that?", "author": "vinothchandar", "createdAt": "2020-09-13T18:49:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NTYwNA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488345604", "bodyText": "I removed and included getCommitsAndCompactionTimeline. I think we will run into some edge cases for MOR tables where something would break. But don't have concrete examples. We can run it for a while on MOR table and see if works.", "author": "satishkotha", "createdAt": "2020-09-15T02:31:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558663", "bodyText": "@bvaradar if you can take a pass at these, that would be great", "author": "vinothchandar", "createdAt": "2020-09-13T18:02:44Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java", "diffHunk": "@@ -371,6 +371,47 @@ void removeBootstrapBaseFileMapping(Stream<BootstrapBaseFileMapping> bootstrapBa\n         schemaHelper.getPrefixForSliceViewByPartitionFile(partitionPath, fileId)).map(Pair::getValue)).findFirst());\n   }\n \n+  @Override\n+  protected void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODkyNg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558926", "bodyText": "General question I have around incremental file system view and rocksDB like persistent file system view storage is whether we will keep this list updated. i.e when the archival/cleaning runs, how do we ensure the deleted replaced file groups are no longer tracked inside rocksdb.\nI guess the lines below, are doing a bulk delete and insert to achieve the same?", "author": "vinothchandar", "createdAt": "2020-09-13T18:05:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NTg0OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488345849", "bodyText": "Yes,  we do delete and insert. Definitely, would be helpful to have someone with more experience review this.", "author": "satishkotha", "createdAt": "2020-09-15T02:32:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTA5OTk5OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489099999", "bodyText": "This part looks good. Going through rest of the code", "author": "bvaradar", "createdAt": "2020-09-16T01:00:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1OTI3NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487559275", "bodyText": "better approach for these situations generally is to introduce a commitStats(.) that does not take the last argument and deal with it internally inside WriteClient.\nThis way the code will remain more readable, without having to reference replace stats in bulk_insert, which have nothing to do with each other. hope that makes sense", "author": "vinothchandar", "createdAt": "2020-09-13T18:09:20Z", "path": "hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java", "diffHunk": "@@ -102,7 +104,9 @@ public void commit(WriterCommitMessage[] messages) {\n             .flatMap(m -> m.getWriteStatuses().stream().map(m2 -> m2.getStat())).collect(Collectors.toList());\n \n     try {\n-      writeClient.commitStats(instantTime, writeStatList, Option.empty());\n+      writeClient.commitStats(instantTime, writeStatList, Option.empty(),", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4NDk2NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488284965", "bodyText": "Sure. Added new method", "author": "satishkotha", "createdAt": "2020-09-14T23:01:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1OTI3NQ=="}], "type": "inlineReview"}, {"oid": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "url": "https://github.com/apache/hudi/commit/4ac518737b4388ab4c510f517312072a4b3b6dc0", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-15T01:22:25Z", "type": "forcePushed"}, {"oid": "6c6f370bd9800636aee60d02cb8ceaa3776a6715", "url": "https://github.com/apache/hudi/commit/6c6f370bd9800636aee60d02cb8ceaa3776a6715", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-15T02:20:22Z", "type": "forcePushed"}, {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9", "url": "https://github.com/apache/hudi/commit/067c285f5dc762a3753ad4162d02957662daa1d9", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-15T21:35:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIyNTM1MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489225350", "bodyText": "Is this needed ?", "author": "bvaradar", "createdAt": "2020-09-16T07:34:38Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteResult.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Result of a write operation.\n+ */\n+public class HoodieWriteResult implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long RANDOM_SEED = 9038412832L;", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI1NDI5MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489254290", "bodyText": "Move this ReplaceHelper class ?", "author": "bvaradar", "createdAt": "2020-09-16T08:23:07Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -370,4 +430,17 @@ private IndexedRecord convertToAvroRecord(HoodieTimeline commitTimeline, HoodieI\n     avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, \"\");\n     return avroMetaData;\n   }\n+\n+  public static org.apache.hudi.avro.model.HoodieReplaceCommitMetadata convertReplaceCommitMetadata(", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2MDk1Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489260956", "bodyText": "nit: Can you add @OverRide annotation ?", "author": "bvaradar", "createdAt": "2020-09-16T08:33:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.PropertyAccessor;\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * All the metadata that gets stored along with a commit.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n+  private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n+  protected Map<String, List<String>> partitionToReplaceFileIds;\n+\n+  // for ser/deser\n+  public HoodieReplaceCommitMetadata() {\n+    this(false);\n+  }\n+\n+  public HoodieReplaceCommitMetadata(boolean compacted) {\n+    super(compacted);\n+    partitionToReplaceFileIds = new HashMap<>();\n+  }\n+\n+  public void setPartitionToReplaceFileIds(Map<String, List<String>> partitionToReplaceFileIds) {\n+    this.partitionToReplaceFileIds = partitionToReplaceFileIds;\n+  }\n+\n+  public void addReplaceFileId(String partitionPath, String fileId) {\n+    if (!partitionToReplaceFileIds.containsKey(partitionPath)) {\n+      partitionToReplaceFileIds.put(partitionPath, new ArrayList<>());\n+    }\n+    partitionToReplaceFileIds.get(partitionPath).add(fileId);\n+  }\n+\n+  public List<String> getReplaceFileIds(String partitionPath) {\n+    return partitionToReplaceFileIds.get(partitionPath);\n+  }\n+\n+  public Map<String, List<String>> getPartitionToReplaceFileIds() {\n+    return partitionToReplaceFileIds;\n+  }\n+\n+  public String toJsonString() throws IOException {", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489278233", "bodyText": "@satishkotha : We need to handle case when we restore a .replace instant for incremental timeline file system view.\nAbout the implementation -\nIn addRestoreInstant(), we need to look at HoodieRestoreMetadata.instantsToRollback and for each instants which are .replace types, we need to remove the replace file-group mapping kept in the file-system view.  We would need a reverse mapping of instant to file-group-id and also a way to identify which of the entries in HoodieRestoreMetadata.instantsToRollback is replace metadata. Currently, we only store commit timestamps in HoodieRestoreMetadata.instantsToRollback.\nI think it would be useful if we add an additional field in HoodieRestoreCommitMetadata and HoodieRollbackCommitMetadata to store both the timestamp and commit-action-type and use it here.\nSince, we only read committed replace actions, rollback is fine though.", "author": "bvaradar", "createdAt": "2020-09-16T08:59:08Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java", "diffHunk": "@@ -251,6 +262,28 @@ private void addRollbackInstant(HoodieTimeline timeline, HoodieInstant instant)\n     LOG.info(\"Done Syncing rollback instant (\" + instant + \")\");\n   }\n \n+  /**\n+   * Add newly found REPLACE instant.\n+   *\n+   * @param timeline Hoodie Timeline\n+   * @param instant REPLACE Instant\n+   */\n+  private void addReplaceInstant(HoodieTimeline timeline, HoodieInstant instant) throws IOException {", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI4Mzk2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489283963", "bodyText": "Also, can you also add test cases for incremental file-system view for both addReplaceInstant and removeReplaceInstant in TestIncrementalFSViewSync ?", "author": "bvaradar", "createdAt": "2020-09-16T09:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMwMzEyMg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r492303122", "bodyText": "I need to understand this flow a bit more. But, have a question on why we need to track commit-action-type and timestamp. Today, HoodieRollbackMetadata tracks successFiles, deletedFiles etc.  Do you think we can add replacedFileIds also there? This will be empty for regular commits. But for replace commits, it will have some content.  If this value is present, we can remove corresponding fileIds from View#replacedFileGroups. Let me know if i'm missing anything with this approach.", "author": "satishkotha", "createdAt": "2020-09-21T19:42:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMjE1OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493002158", "bodyText": "Discussed with @satishkotha. We will track commit action type along with instant since we have introduced one another type of \"commit\" -> replace. Restore can perform custom handling only for this action type. It is safer to let Incremental file-system view to revert replace mappings (in memory or rocksdb) by providing the replace instant time as it  isrobust in case of partial restore failures.", "author": "bvaradar", "createdAt": "2020-09-22T20:07:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg4MjI5MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493882291", "bodyText": "@bvaradar Made the change and added basic test. Please take a look. If the general approach looks good. I'll add more complex tests.", "author": "satishkotha", "createdAt": "2020-09-23T20:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}], "type": "inlineReview"}, {"oid": "694656aa63d93a18fc9aa719b5b65bf2a082f455", "url": "https://github.com/apache/hudi/commit/694656aa63d93a18fc9aa719b5b65bf2a082f455", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-17T17:18:04Z", "type": "forcePushed"}, {"oid": "33a5a878d7a07f1797b33d32c336b33a66e4d727", "url": "https://github.com/apache/hudi/commit/33a5a878d7a07f1797b33d32c336b33a66e4d727", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-17T17:37:52Z", "type": "forcePushed"}, {"oid": "9de32eef15c6fff7296ea0683a5143dc954c0d90", "url": "https://github.com/apache/hudi/commit/9de32eef15c6fff7296ea0683a5143dc954c0d90", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-17T23:01:04Z", "type": "forcePushed"}, {"oid": "a546978c03f71ee3beadd5ea7f65abb7635b407f", "url": "https://github.com/apache/hudi/commit/a546978c03f71ee3beadd5ea7f65abb7635b407f", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-18T17:08:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMzUxMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493003513", "bodyText": "@satishkotha  : As discussed, All the replace filtering needs to move to getXXX() apis as the fetch APIs are only responsible for fetching file slices/base-files from different types of storage.", "author": "bvaradar", "createdAt": "2020-09-22T20:09:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -738,7 +799,9 @@ private String formatPartitionKey(String partitionStr) {\n    * @param commitsToReturn Commits\n    */\n   Stream<FileSlice> fetchLatestFileSliceInRange(List<String> commitsToReturn) {\n-    return fetchAllStoredFileGroups().map(fileGroup -> fileGroup.getLatestFileSliceInRange(commitsToReturn))\n+    return fetchAllStoredFileGroups()\n+        .filter(fileGroup -> !isFileGroupReplacedBeforeAny(fileGroup, commitsToReturn))", "originalCommit": "a546978c03f71ee3beadd5ea7f65abb7635b407f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg4MTE1MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493881151", "bodyText": "Changed. Please take a look.", "author": "satishkotha", "createdAt": "2020-09-23T20:39:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMzUxMw=="}], "type": "inlineReview"}, {"oid": "1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "url": "https://github.com/apache/hudi/commit/1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-23T18:43:57Z", "type": "forcePushed"}, {"oid": "7f3de1b7355303d56aa8f50809cbb4afc256472e", "url": "https://github.com/apache/hudi/commit/7f3de1b7355303d56aa8f50809cbb4afc256472e", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-23T19:11:44Z", "type": "forcePushed"}, {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "url": "https://github.com/apache/hudi/commit/980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-23T20:39:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk1NjI1Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493956253", "bodyText": "@satishkotha : Dont we need to use instant time when checking for replaced-file here ?", "author": "bvaradar", "createdAt": "2020-09-23T23:41:53Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -425,10 +459,14 @@ protected HoodieBaseFile addBootstrapBaseFileIfPresent(HoodieFileGroupId fileGro\n       readLock.lock();\n       String partitionPath = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partitionPath);\n-      return fetchHoodieFileGroup(partitionPath, fileId).map(fileGroup -> fileGroup.getAllBaseFiles()\n-          .filter(baseFile -> HoodieTimeline.compareTimestamps(baseFile.getCommitTime(), HoodieTimeline.EQUALS,\n-              instantTime)).filter(df -> !isBaseFileDueToPendingCompaction(df)).findFirst().orElse(null))\n-          .map(df -> addBootstrapBaseFileIfPresent(new HoodieFileGroupId(partitionPath, fileId), df));\n+      if (isFileGroupReplaced(partitionPath, fileId)) {", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk1NzU2MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493957561", "bodyText": "same case here,, we need to use the maxInstantTime passed here instead of the timeline's maxInstant.", "author": "bvaradar", "createdAt": "2020-09-23T23:46:26Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -554,14 +608,16 @@ protected HoodieBaseFile addBootstrapBaseFileIfPresent(HoodieFileGroupId fileGro\n       readLock.lock();\n       String partition = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partition);\n-      return fetchAllStoredFileGroups(partition).map(fileGroup -> {\n-        Option<FileSlice> fileSlice = fileGroup.getLatestFileSliceBeforeOrOn(maxInstantTime);\n-        // if the file-group is under construction, pick the latest before compaction instant time.\n-        if (fileSlice.isPresent()) {\n-          fileSlice = Option.of(fetchMergedFileSlice(fileGroup, fileSlice.get()));\n-        }\n-        return fileSlice;\n-      }).filter(Option::isPresent).map(Option::get).map(this::addBootstrapBaseFileIfPresent);\n+      return fetchAllStoredFileGroups(partition)\n+          .filter(fg -> !isFileGroupReplaced(fg.getFileGroupId()))", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4NzgyOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493987828", "bodyText": "Doc needs fixing.", "author": "bvaradar", "createdAt": "2020-09-24T01:30:57Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Helper class to generate compaction plan from FileGroup/FileSlice abstraction.", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5MTg3Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493991876", "bodyText": "rename removeReplacedFileIdsAtInstants ?", "author": "bvaradar", "createdAt": "2020-09-24T01:47:31Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -727,6 +795,26 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract Stream<HoodieFileGroup> fetchAllStoredFileGroups();\n \n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for new file groups replaced.\n+   */\n+  protected abstract void addReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Remove file groups that are replaced in any of the specified instants.\n+   */\n+  protected abstract void removeReplacedFileIds(Set<String> instants);", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5MzUzNg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493993536", "bodyText": "this is no longer needed and can be removed ?", "author": "bvaradar", "createdAt": "2020-09-24T01:54:16Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -91,40 +92,47 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType, Collections.emptyMap());\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n+      Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType) {\n+    return commitStats(instantTime, stats, extraMetadata, commitActionType, Collections.emptyMap());\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n     HoodieTableMetaClient metaClient = createMetaClient(false);", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwNzI4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494007285", "bodyText": "We are creating metaclient and loading timeline once here and in the function called in the next line. Can you make sure you create metaclient only once without loading timeline.", "author": "bvaradar", "createdAt": "2020-09-24T02:49:20Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,24 +602,39 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDcyNzQ4NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494727484", "bodyText": "This is not calling function in the next line (calling method after that). So we only create meta client once. Please double check and let me know if i'm misinterpreting your suggestion.", "author": "satishkotha", "createdAt": "2020-09-25T03:31:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwNzI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTMyOQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494731329", "bodyText": "My bad. Got confused with the method naming :)", "author": "bvaradar", "createdAt": "2020-09-25T03:48:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwNzI4NQ=="}], "type": "inlineReview"}, {"oid": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "url": "https://github.com/apache/hudi/commit/c281f2eb0d311c7b54ac850b831cea6d48a47502", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-25T03:27:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTc2OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494731768", "bodyText": "Does this have to be non-static ? Can it be moved to CommitUtils ?", "author": "bvaradar", "createdAt": "2020-09-25T03:50:07Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,24 +602,39 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, metaClient.getCommitActionType(), metaClient);\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  public void startCommitWithTime(String instantTime, String actionType) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, actionType, metaClient);\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  private void startCommitWithTime(String instantTime, String actionType, HoodieTableMetaClient metaClient) {\n     // NOTE : Need to ensure that rollback is done before a new commit is started\n     if (rollbackPending) {\n       // Only rollback inflight commit/delta-commits. Do not touch compaction commits\n       rollbackPendingCommits();\n     }\n-    startCommit(instantTime);\n+    startCommit(instantTime, actionType, metaClient);\n   }\n \n-  private void startCommit(String instantTime) {\n-    LOG.info(\"Generate a new instant time \" + instantTime);\n-    HoodieTableMetaClient metaClient = createMetaClient(true);\n+  private void startCommit(String instantTime, String actionType, HoodieTableMetaClient metaClient) {", "originalCommit": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEzNzk4OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r495137989", "bodyText": "This is a private method. Do you want to make this public static?  Personally, I think having all startCommit methods in HoodieWriteClient makes more sense because user workflow is\n\nwriteClient#startCommit\nwriteClient#upsert\nwriteClient#commit\n\nBut if you have a strong preference to make this part of CommitUtils, I can move it. let me know.", "author": "satishkotha", "createdAt": "2020-09-25T17:40:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcwMDQzOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r495700438", "bodyText": "Lets leave it for now If needed, we can refactor later.", "author": "bvaradar", "createdAt": "2020-09-28T05:44:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTc2OA=="}], "type": "inlineReview"}, {"oid": "d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "url": "https://github.com/apache/hudi/commit/d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-28T17:23:44Z", "type": "forcePushed"}, {"oid": "aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "url": "https://github.com/apache/hudi/commit/aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-28T20:11:48Z", "type": "commit"}, {"oid": "aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "url": "https://github.com/apache/hudi/commit/aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-28T20:11:48Z", "type": "forcePushed"}]}