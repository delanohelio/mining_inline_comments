{"pr_number": 2206, "pr_title": "[HUDI-1105] Adding dedup support for Bulk Insert w/ Rows", "pr_createdAt": "2020-10-25T17:09:37Z", "pr_url": "https://github.com/apache/hudi/pull/2206", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r511623934", "bodyText": "@bvaradar : Is there some other option to go about deduping multiple Rows. Bcoz, in Bulksert with Rows, we don't have any HoodiePayload. Hence we have to operate on \"Row\"s only. So, have to group by keys and reduce by a function. In this patch, I have designed the function as this interface.\nSo, two questions.\na. Is there a better option.\nb. Even if we go with this option, I am getting task not serializable when executing this, since avro Schema is also sent along with Row. Also, wondering this might have any performance complications.\nCaused by: java.io.NotSerializableException: org.apache.avro.Schema$RecordSchema\nSerialization stack:\n\t- object not serializable (class: org.apache.avro.Schema$RecordSchema, value: {\"type\":\"record\",\"name\":\"trip\",\"namespace\":\"example.schema\",\"fields\":[{\"name\":\"_row_key\",\"type\":\"string\"},{\"name\":\"partition\",\"type\":\"string\"},{\"name\":\"ts\",\"type\":[\"long\",\"null\"]}]})\n\t- field (class: org.apache.hudi.TestHoodieDatasetBulkInsertHelper, name: schema, type: class org.apache.avro.Schema)\n\t- object (class org.apache.hudi.TestHoodieDatasetBulkInsertHelper, org.apache.hudi.TestHoodieDatasetBulkInsertHelper@8d7718e)\n\t- field (class: org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow, name: this$0, type: class org.apache.hudi.TestHoodieDatasetBulkInsertHelper)\n\t- object (class org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow, org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow@3436d3d7)\n\t- element of array (index: 0)\n\t- array (class [Ljava.lang.Object;, size 1)\n\t- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)\n\t- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.apache.hudi.SparkRowWriteHelper, functionalInterfaceMethod=org/apache/spark/api/java/function/ReduceFunction.call:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic org/apache/hudi/SparkRowWriteHelper.lambda$deduplicateRows$14bf715c$1:(Lorg/apache/hudi/PreCombineRow;Lorg/apache/spark/sql/Row;Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, instantiatedMethodType=(Lorg/apache/spark/sql/Row;Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, numCaptured=1])\n\t- writeReplace data (class: java.lang.invoke.SerializedLambda)\n\t- object (class org.apache.hudi.SparkRowWriteHelper$$Lambda$306/2078785618, org.apache.hudi.SparkRowWriteHelper$$Lambda$306/2078785618@19d9ba89)\n\t- field (class: org.apache.spark.sql.KeyValueGroupedDataset$$anonfun$reduceGroups$1, name: f$4, type: interface org.apache.spark.api.java.function.ReduceFunction)\n\t- object (class org.apache.spark.sql.KeyValueGroupedDataset$$anonfun$reduceGroups$1, <function2>)\n\t- field (class: org.apache.spark.sql.expressions.ReduceAggregator, name: func, type: interface scala.Function2)\n\t- object (class org.apache.spark.sql.expressions.ReduceAggregator, org.apache.spark.sql.expressions.ReduceAggregator@14af73e1)", "author": "nsivabalan", "createdAt": "2020-10-25T17:15:13Z", "path": "hudi-spark/src/main/java/org/apache/hudi/PreCombineRow.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.spark.sql.Row;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Interface used to preCombine two Spark sql Rows.\n+ */\n+public interface PreCombineRow extends Serializable {", "originalCommit": "d46a16f158358ed35528cd916b7e33ce3127904c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTQ5NzU5Nw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519497597", "bodyText": "What is holding the avro schema? this seems like a member held in the class you wrote?", "author": "vinothchandar", "createdAt": "2020-11-09T00:05:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUwMTAyNA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519501024", "bodyText": "nope. Thats is what is confusing me.\n  class TestPreCombineRow implements PreCombineRow {\n\n    @Override\n    public Row combineTwoRows(Row v1, Row v2) {\n      long tsV1 = v1.getAs(\"ts\");\n      long tsV2 = v2.getAs(\"ts\");\n      return (tsV1 >= tsV2) ? v1 : v2;\n    }\n  }", "author": "nsivabalan", "createdAt": "2020-11-09T00:30:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUwMTYyMg==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519501622", "bodyText": "let me investigate this. might be in my dedup logic.\npublic Dataset<Row> deduplicateRows(Dataset<Row> inputDf, PreCombineRow preCombineRow) {\n    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n    return inputDf.groupByKey(\n        (MapFunction<Row, String>) value -> value.getAs(HoodieRecord.PARTITION_PATH_METADATA_FIELD) + \"+\" + value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD), Encoders.STRING())\n        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> preCombineRow.combineTwoRows(v1, v2)).map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n  }", "author": "nsivabalan", "createdAt": "2020-11-09T00:34:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUwMzY2NA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519503664", "bodyText": "yeah, no idea why the reduce fn is giving an issue. Casting to comprable works, but this may not be good enough right. We need to think about complex fields and not just simple fields. Also, we might have to support user defined preCombine.\ngroupedDataset.reduceGroups((ReduceFunction<Row>) (v1, v2) ->\n    {\n      int compareVal = ((Comparable) v1.get(fieldIndex)).compareTo(v2.get(fieldIndex));\n      if (compareVal >= 0) {\n        return v1;\n      } else {\n        return v2;\n      }\n    });", "author": "nsivabalan", "createdAt": "2020-11-09T00:49:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI2NzU4OQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r520267589", "bodyText": "@nsivabalan could we take the same approach as key generators, extend HoodieRecordPayload with new methods for specifying how two rows should combine, but still honor the avro based impl, by converting row -> avro as needed?", "author": "vinothchandar", "createdAt": "2020-11-10T03:45:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}], "type": "inlineReview"}, {"oid": "15d0db55fb82ef35633ab1807e0327954ec61233", "url": "https://github.com/apache/hudi/commit/15d0db55fb82ef35633ab1807e0327954ec61233", "message": "Fixing dedup support", "committedDate": "2020-12-01T17:06:46Z", "type": "forcePushed"}, {"oid": "01e47cc05db74d78d28a2dee255b8d3cd3f26f22", "url": "https://github.com/apache/hudi/commit/01e47cc05db74d78d28a2dee255b8d3cd3f26f22", "message": "Fixing dedup support", "committedDate": "2021-05-23T04:27:32Z", "type": "forcePushed"}, {"oid": "63ca76bbba456b67e5ce764907935731ff35b146", "url": "https://github.com/apache/hudi/commit/63ca76bbba456b67e5ce764907935731ff35b146", "message": "Fixing build failure", "committedDate": "2021-05-24T06:16:17Z", "type": "forcePushed"}, {"oid": "39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "url": "https://github.com/apache/hudi/commit/39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "message": "Fixing build failure", "committedDate": "2021-05-24T06:33:08Z", "type": "forcePushed"}, {"oid": "5d68e035eef1ef37b2121bafc67e1f81d85aea74", "url": "https://github.com/apache/hudi/commit/5d68e035eef1ef37b2121bafc67e1f81d85aea74", "message": "Some refactoring and fixing tests", "committedDate": "2021-06-07T20:00:50Z", "type": "forcePushed"}, {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "url": "https://github.com/apache/hudi/commit/031b8fa2a947f69815bce9fa181dc98dd972d07e", "message": "Fixing tests", "committedDate": "2021-06-08T02:31:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NDIwNQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664284205", "bodyText": "all these configs need to be redone based ConfigProperty/HoodieConfig", "author": "vinothchandar", "createdAt": "2021-07-06T06:56:33Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -98,6 +98,8 @@\n   public static final String DEFAULT_COMBINE_BEFORE_UPSERT = \"true\";\n   public static final String COMBINE_BEFORE_DELETE_PROP = \"hoodie.combine.before.delete\";\n   public static final String DEFAULT_COMBINE_BEFORE_DELETE = \"true\";\n+  public static final String COMBINE_BEFORE_BULK_INSERT_PROP = \"hoodie.combine.before.bulk.insert\";", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDY5NjEzNw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664696137", "bodyText": "yes.", "author": "nsivabalan", "createdAt": "2021-07-06T16:12:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NDIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NTIyOQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664285229", "bodyText": "So far, we have used one config combine.before.insert to control it for both insert and bulk_insert. Can we keep it the same way? Otherwise, wont it be backwards incompatible, ie a user can be expecting the combine.before.insert continue to take effect for bulk_insert as well and it won't be the case?", "author": "vinothchandar", "createdAt": "2021-07-06T06:58:25Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -306,6 +308,10 @@ public boolean shouldCombineBeforeInsert() {\n     return Boolean.parseBoolean(props.getProperty(COMBINE_BEFORE_INSERT_PROP));\n   }\n \n+  public boolean shouldCombineBeforeBulkInsert() {", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDY5NjQwMw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664696403", "bodyText": "yeah, makes sense to use combine.before.insert only. will remove the new config.", "author": "nsivabalan", "createdAt": "2021-07-06T16:12:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NTIyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NjUzNA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664286534", "bodyText": "I understand that the new config is just used here as of this PR. but from an user standpoint, on the non-row writer path, combine.before.insert was controlling this already. We should just make it consistent.", "author": "vinothchandar", "createdAt": "2021-07-06T07:00:37Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDatasetBulkInsertHelper.java", "diffHunk": "@@ -96,9 +97,15 @@\n                 functions.lit(\"\").cast(DataTypes.StringType))\n             .withColumn(HoodieRecord.FILENAME_METADATA_FIELD,\n                 functions.lit(\"\").cast(DataTypes.StringType));\n+\n+    Dataset<Row> dedupedDf = rowDatasetWithHoodieColumns;\n+    if (config.shouldCombineBeforeBulkInsert()) {", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NjkwOA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664286908", "bodyText": "nit: extra line?", "author": "vinothchandar", "createdAt": "2021-07-06T07:01:14Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzI0MQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664287241", "bodyText": "why the singleton etc? Can't we just use a static method?", "author": "vinothchandar", "createdAt": "2021-07-06T07:01:52Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDcwNDU2Ng==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664704566", "bodyText": "I took inspiration from existing code.\nhttps://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "author": "nsivabalan", "createdAt": "2021-07-06T16:23:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzI0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzczMA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664287730", "bodyText": "lets use reduceByKey(), which we use for RDD path? groupByKey() can hog memory.", "author": "vinothchandar", "createdAt": "2021-07-06T07:02:41Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n+  }\n+\n+  public static SparkRowWriteHelper newInstance() {\n+    return SparkRowWriteHelper.WriteHelperHolder.SPARK_WRITE_HELPER;\n+  }\n+\n+  public Dataset<Row> deduplicateRows(Dataset<Row> inputDf, String preCombineField, boolean isGlobalIndex) {\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+\n+    return inputDf.groupByKey(", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDcwODEwNQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664708105", "bodyText": "I too badly wanted to use one, but unfortunately there is none :(\nhttps://stackoverflow.com/questions/38383207/rolling-your-own-reducebykey-in-spark-dataset\nhttps://stackoverflow.com/questions/57359260/why-there-is-no-reducebykey-in-sparks-dataset\nhence, had to go with groupByKey and then do reduceBy.", "author": "nsivabalan", "createdAt": "2021-07-06T16:28:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzczMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4ODM1OA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664288358", "bodyText": "have you tested with both Spark 2 and 3?  Some of these classes can be different and actually fail?", "author": "vinothchandar", "createdAt": "2021-07-06T07:03:54Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n+  }\n+\n+  public static SparkRowWriteHelper newInstance() {\n+    return SparkRowWriteHelper.WriteHelperHolder.SPARK_WRITE_HELPER;\n+  }\n+\n+  public Dataset<Row> deduplicateRows(Dataset<Row> inputDf, String preCombineField, boolean isGlobalIndex) {\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+\n+    return inputDf.groupByKey(\n+        (MapFunction<Row, String>) value ->\n+            isGlobalIndex ? (value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD)) :\n+                (value.getAs(HoodieRecord.PARTITION_PATH_METADATA_FIELD) + \"+\" + value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD)), Encoders.STRING())\n+        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {\n+          if (((Comparable) v1.getAs(preCombineField)).compareTo(((Comparable) v2.getAs(preCombineField))) >= 0) {\n+            return v1;\n+          } else {\n+            return v2;\n+          }\n+            }\n+        ).map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n+  }\n+\n+  private ExpressionEncoder getEncoder(StructType schema) {\n+    List<Attribute> attributes = JavaConversions.asJavaCollection(schema.toAttributes()).stream()\n+        .map(Attribute::toAttribute).collect(Collectors.toList());\n+    return RowEncoder.apply(schema)", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDcwODY0Mw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664708643", "bodyText": "its been quite sometime I put up this patch :). Will do a round of testing and will update for both spark versions.", "author": "nsivabalan", "createdAt": "2021-07-06T16:29:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4ODM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDg4NDgzMw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664884833", "bodyText": "yes, tested both spark2 and spark3.", "author": "nsivabalan", "createdAt": "2021-07-06T21:18:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4ODM1OA=="}], "type": "inlineReview"}, {"oid": "80ae3446670e4012ef3896477964bb916b71a864", "url": "https://github.com/apache/hudi/commit/80ae3446670e4012ef3896477964bb916b71a864", "message": "Addressing feedback", "committedDate": "2021-07-06T19:42:12Z", "type": "forcePushed"}, {"oid": "0b5cdce9cea59681d7e604c6f6706a4e96a19861", "url": "https://github.com/apache/hudi/commit/0b5cdce9cea59681d7e604c6f6706a4e96a19861", "message": "Addressing feedback", "committedDate": "2021-07-06T21:11:45Z", "type": "forcePushed"}, {"oid": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "url": "https://github.com/apache/hudi/commit/ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "message": "Addressing feedback", "committedDate": "2021-07-06T21:20:22Z", "type": "forcePushed"}, {"oid": "1fa675d9739656c45850d60fa49b66970b18f5ac", "url": "https://github.com/apache/hudi/commit/1fa675d9739656c45850d60fa49b66970b18f5ac", "message": "Addressing feedback", "committedDate": "2021-07-07T04:00:13Z", "type": "forcePushed"}, {"oid": "bf73f313cdfb5e640c16e40eb017151dcd464b41", "url": "https://github.com/apache/hudi/commit/bf73f313cdfb5e640c16e40eb017151dcd464b41", "message": "trigger rebuild", "committedDate": "2021-07-07T15:16:04Z", "type": "commit"}, {"oid": "ab8b925db2387548626d762dae3c8458371b43f3", "url": "https://github.com/apache/hudi/commit/ab8b925db2387548626d762dae3c8458371b43f3", "message": "[HUDI-1156] Remove unused dependencies from HoodieDeltaStreamerWrapper Class (#1927)", "committedDate": "2021-07-07T15:16:04Z", "type": "commit"}, {"oid": "b436ba3ce44ad7d393e98f11bda8e986888f31fa", "url": "https://github.com/apache/hudi/commit/b436ba3ce44ad7d393e98f11bda8e986888f31fa", "message": "Adding Dedup support for BulkInsert with Rows", "committedDate": "2021-07-07T15:25:35Z", "type": "commit"}, {"oid": "280aecde2d0a426616144becf8cfeb75bb86166b", "url": "https://github.com/apache/hudi/commit/280aecde2d0a426616144becf8cfeb75bb86166b", "message": "Fixing dedup support", "committedDate": "2021-07-07T15:29:51Z", "type": "commit"}, {"oid": "5785cf93a8a4214e8d2bad920edaa00e0d775874", "url": "https://github.com/apache/hudi/commit/5785cf93a8a4214e8d2bad920edaa00e0d775874", "message": "Fixing build failure", "committedDate": "2021-07-07T15:34:05Z", "type": "commit"}, {"oid": "28d12341dbe50489570dd13e9b2ac0aa9f41fff3", "url": "https://github.com/apache/hudi/commit/28d12341dbe50489570dd13e9b2ac0aa9f41fff3", "message": "Refactoring and minor fixes", "committedDate": "2021-07-07T15:35:03Z", "type": "commit"}, {"oid": "8f5b6ef1537c2b3f68d1bef667fbfd56ca748599", "url": "https://github.com/apache/hudi/commit/8f5b6ef1537c2b3f68d1bef667fbfd56ca748599", "message": "Some refactoring and fixing tests", "committedDate": "2021-07-07T15:36:48Z", "type": "commit"}, {"oid": "e4813604f6757ecc09eb117e7e35754d564ef004", "url": "https://github.com/apache/hudi/commit/e4813604f6757ecc09eb117e7e35754d564ef004", "message": "Removing PreCombine interface", "committedDate": "2021-07-07T15:41:31Z", "type": "commit"}, {"oid": "c8d6dbb49a3c528190c1464e45fcb9ac137e43ad", "url": "https://github.com/apache/hudi/commit/c8d6dbb49a3c528190c1464e45fcb9ac137e43ad", "message": "Fixing tests", "committedDate": "2021-07-07T15:41:33Z", "type": "commit"}, {"oid": "51ccc2db570a8cc996ff80725f668d7f6158aa24", "url": "https://github.com/apache/hudi/commit/51ccc2db570a8cc996ff80725f668d7f6158aa24", "message": "Addressing feedback", "committedDate": "2021-07-07T15:54:26Z", "type": "commit"}, {"oid": "51ccc2db570a8cc996ff80725f668d7f6158aa24", "url": "https://github.com/apache/hudi/commit/51ccc2db570a8cc996ff80725f668d7f6158aa24", "message": "Addressing feedback", "committedDate": "2021-07-07T15:54:26Z", "type": "forcePushed"}]}