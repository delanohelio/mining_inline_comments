{"pr_number": 1687, "pr_title": " [HUDI-684] Introduced abstraction for writing and reading different types of base file formats.", "pr_createdAt": "2020-05-29T21:40:18Z", "pr_url": "https://github.com/apache/hudi/pull/1687", "timeline": [{"oid": "b2a4c2e54b579e6c58ea9320f8b85625b577084e", "url": "https://github.com/apache/hudi/commit/b2a4c2e54b579e6c58ea9320f8b85625b577084e", "message": "[HUDI-684] Fixed merge conflict due to upstream changes.\n\nAdded extra unit tests for HFile Input format.", "committedDate": "2020-06-17T06:50:47Z", "type": "forcePushed"}, {"oid": "75667356003fe23b5fc4df8b37bcfd8aa256c61e", "url": "https://github.com/apache/hudi/commit/75667356003fe23b5fc4df8b37bcfd8aa256c61e", "message": "[HUDI-684] Fixed integration tests.", "committedDate": "2020-06-18T21:13:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTQ0Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261442", "bodyText": "might as well expose getBaseFileExtension() in HoodieTable.", "author": "nsivabalan", "createdAt": "2020-06-21T22:11:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -115,7 +115,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       oldFilePath = new Path(config.getBasePath() + \"/\" + partitionPath + \"/\" + latestValidFilePath);\n       String relativePath = new Path((partitionPath.isEmpty() ? \"\" : partitionPath + \"/\")\n-          + FSUtils.makeDataFileName(instantTime, writeToken, fileId)).toString();\n+          + FSUtils.makeDataFileName(instantTime, writeToken, fileId,\n+              hoodieTable.getBaseFileFormat().getFileExtension())).toString();", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTY1OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261658", "bodyText": "minor. Do you think we should name this getNewStorageReader(). Just to be cautious to avoid some caller using this more like a getter for Reader. This does not return a singleton, but creates a new reader everytime.", "author": "nsivabalan", "createdAt": "2020-06-21T22:14:46Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieReadHandle.java", "diffHunk": "@@ -56,4 +61,9 @@ protected HoodieBaseFile getLatestDataFile() {\n     return hoodieTable.getBaseFileOnlyView()\n         .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n   }\n+\n+  protected HoodieFileReader getStorageReader() throws IOException {", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0NTM5MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443845391", "bodyText": "Yes, getNewStorageReader() would be more clear.", "author": "prashantwason", "createdAt": "2020-06-22T21:48:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTY1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443262086", "bodyText": "similar to HoodieReadHandle, how about we expose this writer instantiation in HoodieWriteHandle.\nHoodieFileWriter<IndexedRecord> getFileWriter(String instantTime, Path newFilePath){\n     return HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n}", "author": "nsivabalan", "createdAt": "2020-06-21T22:20:11Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -132,7 +133,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       // Create the writer for writing the new version file\n       storageWriter =\n-          HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+          HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NDAzMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443264032", "bodyText": "minor: similar to my suggestion in reader, try to see if we need to name this as getNewFileWriter()", "author": "nsivabalan", "createdAt": "2020-06-21T22:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0ODMzNQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443848335", "bodyText": "Sounds good.", "author": "prashantwason", "createdAt": "2020-06-22T21:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NTUyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443265526", "bodyText": "not sure if my way of thinking is right. correct me if I am wrong. we are making our file formats flexible where in either of base file or log file can be of any format. I do understand for index purposes both base file and log file will be HFile. but wondering if tagging this config to table is the right think to do or should we have two configs, one for base file and one for log file. What in case we want to experiment w/ Hfile format for log files and parquet as base files since we might get indexing capability for log files too.", "author": "nsivabalan", "createdAt": "2020-06-21T23:09:52Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java", "diffHunk": "@@ -115,7 +115,11 @@ public CommitActionExecutor(JavaSparkContext jsc,\n   }\n \n   protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n-    return new HoodieMergeHandle<>(config, instantTime, (HoodieTable<T>)table, recordItr, partitionPath, fileId, sparkTaskContextSupplier);\n+    if (table.requireSortedRecords()) {", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0OTYwMA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443849600", "bodyText": "Your understanding it correct.\nI will take care of this in a separate PR with HFile implementation.", "author": "prashantwason", "createdAt": "2020-06-22T21:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NTUyNg=="}], "type": "inlineReview"}, {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "url": "https://github.com/apache/hudi/commit/f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\nNotable changes:\n1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n4. HiveSyncTool accepts the base file format as a CLI parameter\n5. HoodieDeltaStreamer accepts the base file format as a CLI parameter", "committedDate": "2020-06-22T23:33:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NDUxOA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444444518", "bodyText": "Minor : Rename getFileReader => createFileReader ?", "author": "bvaradar", "createdAt": "2020-06-23T19:05:15Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieReadHandle.java", "diffHunk": "@@ -56,4 +61,9 @@ protected HoodieBaseFile getLatestDataFile() {\n     return hoodieTable.getBaseFileOnlyView()\n         .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n   }\n+\n+  protected HoodieFileReader getNewFileReader() throws IOException {\n+    return HoodieFileReaderFactory.getFileReader(hoodieTable.getHadoopConf(),", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NDY0NA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444654644", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-06-24T05:36:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NDUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NTU3Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444445572", "bodyText": "Similar suggestion on rename getFileWriter => createFileWriter", "author": "bvaradar", "createdAt": "2020-06-23T19:06:52Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -180,4 +183,10 @@ protected int getStageId() {\n   protected long getAttemptId() {\n     return sparkTaskContextSupplier.getAttemptIdSupplier().get();\n   }\n+\n+  protected HoodieFileWriter getNewFileWriter(String instantTime, Path path, HoodieTable<T> hoodieTable,\n+                                              HoodieWriteConfig config, Schema schema,\n+                                              SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+    return HoodieFileWriterFactory.getFileWriter(instantTime, path, hoodieTable, config, schema, sparkTaskContextSupplier);", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NDkwMA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444654900", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-06-24T05:37:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NTU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzQwOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444447409", "bodyText": "Can we move this to FSUtils class ?", "author": "bvaradar", "createdAt": "2020-06-23T19:10:14Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriter.java", "diffHunk": "@@ -33,4 +37,12 @@\n   void close() throws IOException;\n \n   void writeAvro(String key, R oldRecord) throws IOException;\n+\n+  static Configuration registerFileSystem(Path file, Configuration conf) {", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NTc2NQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444655765", "bodyText": "Moved", "author": "prashantwason", "createdAt": "2020-06-24T05:40:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ1OTI4Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444459286", "bodyText": "thanks for cleaning this up", "author": "bvaradar", "createdAt": "2020-06-23T19:33:00Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java", "diffHunk": "@@ -51,7 +49,6 @@\n   private final long maxFileSize;\n   private final HoodieAvroWriteSupport writeSupport;\n   private final String instantTime;\n-  private final Schema schema;", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2MDk0MA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444460940", "bodyText": "Is this done some where else now ?", "author": "bvaradar", "createdAt": "2020-06-23T19:36:19Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -150,11 +148,13 @@ public HoodieWriteMetadata compact(JavaSparkContext jsc, String compactionInstan\n       throw new HoodieUpsertException(\n           \"Error in finding the old file path at commit \" + instantTime + \" for fileId: \" + fileId);\n     } else {\n-      AvroReadSupport.setAvroReadSchema(getHadoopConf(), upsertHandle.getWriterSchema());", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2NTg5OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444465898", "bodyText": "nvm, found it in getRecordIterator", "author": "bvaradar", "createdAt": "2020-06-23T19:46:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2MDk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NDIzOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444474239", "bodyText": "thanks for cleaning this up.", "author": "bvaradar", "createdAt": "2020-06-23T20:02:01Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -356,20 +356,7 @@ public MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompac\n    * @return\n    */\n   public MessageType readSchemaFromLogFile(Path path) throws IOException {\n-    FileSystem fs = metaClient.getRawFs();\n-    Reader reader = HoodieLogFormat.newReader(fs, new HoodieLogFile(path), null);\n-    HoodieAvroDataBlock lastBlock = null;\n-    while (reader.hasNext()) {\n-      HoodieLogBlock block = reader.next();\n-      if (block instanceof HoodieAvroDataBlock) {\n-        lastBlock = (HoodieAvroDataBlock) block;\n-      }\n-    }\n-    reader.close();\n-    if (lastBlock != null) {\n-      return new AvroSchemaConverter().convert(lastBlock.getSchema());\n-    }\n-    return null;\n+    return readSchemaFromLogFile(metaClient.getRawFs(), path);", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444486839", "bodyText": "Can we keep this in hudi-client ?", "author": "bvaradar", "createdAt": "2020-06-23T20:27:11Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetReaderIterator.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.client.utils;\n+package org.apache.hudi.common.util;", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NjYwOA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444656608", "bodyText": "This will be hard as HoodieParquetReader (hudi-common) uses ParquetReaderIterator. And hudi-common cannot add dependency to hudi-client.", "author": "prashantwason", "createdAt": "2020-06-24T05:42:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5MTc1OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444691759", "bodyText": "Got it. hudi-hadoop-mr also needs HoodieFileReader abstraction for reading schema from file and this is reason why all these classes needs to be in hudi-common", "author": "bvaradar", "createdAt": "2020-06-24T07:15:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NzYyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444487626", "bodyText": "HFile => Avro", "author": "bvaradar", "createdAt": "2020-06-23T20:28:45Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieAvroLogFormat.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.functional;\n+\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock.HoodieLogBlockType;\n+\n+/**\n+ * Tests HFile log format {@link HoodieHFileLogFormat}.", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444498080", "bodyText": "Can we avoid this cleanup as this is not directly related to this PR and I am also making changes on these same methods.", "author": "bvaradar", "createdAt": "2020-06-23T20:49:09Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -80,58 +77,6 @@ protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline tim\n     return timeline;\n   }\n \n-  /**", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1Nzk2Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444657966", "bodyText": "This is simply moving some code into the helper class HoodieRealtimeInputFormatUtils. This has no logic change so hopefully it should not conflict with your changes.\nStill, if you want me to revert these changes, do let me know.", "author": "prashantwason", "createdAt": "2020-06-24T05:47:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4NzA5OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444687099", "bodyText": "I agree but knowing where I made changes to the same methods and consolidated in a different place due to other requirements :) it would make merging easier if I this is not done here. Hope, this is ok.", "author": "bvaradar", "createdAt": "2020-06-24T07:05:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2MDMwOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445760309", "bodyText": "Ok. Reverted these changed.", "author": "prashantwason", "createdAt": "2020-06-25T18:38:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444504285", "bodyText": "The output format (MapredParquetOutputFormat  in case of parquet) would have to change depending on storage type here. right ? We need to parameterize that as well", "author": "bvaradar", "createdAt": "2020-06-23T20:57:50Z", "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -146,21 +146,22 @@ private void syncSchema(String tableName, boolean tableExists, boolean useRealTi\n     // Check and sync schema\n     if (!tableExists) {\n       LOG.info(\"Hive table \" + tableName + \" is not found. Creating it\");\n-      if (!useRealTimeInputFormat) {\n-        String inputFormatClassName = cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.HoodieInputFormat.class.getName()\n-            : HoodieParquetInputFormat.class.getName();\n-        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),\n-            ParquetHiveSerDe.class.getName());\n-      } else {\n-        // Custom serde will not work with ALTER TABLE REPLACE COLUMNS\n-        // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive\n-        // /ql/exec/DDLTask.java#L3488\n-        String inputFormatClassName =\n-            cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat.class.getName()\n-                : HoodieParquetRealtimeInputFormat.class.getName();\n-        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),\n-            ParquetHiveSerDe.class.getName());\n+      HoodieFileFormat baseFileFormat = HoodieFileFormat.valueOf(cfg.baseFileFormat.toUpperCase());\n+      String inputFormatClassName = HoodieInputFormatUtils.getInputFormatClassName(baseFileFormat, useRealTimeInputFormat,\n+          new Configuration());\n+\n+      if (baseFileFormat.equals(HoodieFileFormat.PARQUET) && cfg.usePreApacheInputFormat) {\n+        // Parquet input format had an InputFormat class visible under the old naming scheme.\n+        inputFormatClassName = useRealTimeInputFormat\n+            ? com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat.class.getName()\n+            : com.uber.hoodie.hadoop.HoodieInputFormat.class.getName();\n       }\n+\n+      // Custom serde will not work with ALTER TABLE REPLACE COLUMNS\n+      // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive\n+      // /ql/exec/DDLTask.java#L3488\n+      hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1ODc2MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444658761", "bodyText": "HUDI does not implement any custom outputformat yet. So this will be a big task.\nIf my understanding is correct, the OutputFormat will be used when data is written into a HUDI dataset from Hive itself. Do we even support this case?", "author": "prashantwason", "createdAt": "2020-06-24T05:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4NDcyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444684726", "bodyText": "I did not mean to implement output formats. For Parquet, the outputformat is MapredParquetOutputFormat but this will surely be different for  HFile. right ?\nWhat I meant was we need to keep a mapping (that identifies an outputformat.) Yes, we do not use the output format but registering output format as \"MapredParquetOutputFormat\" for non-parquet storage types is  misleading.\nWe can just keep an enum that identifies the output format for each storage type and use it here. Thats all is needed IMO.", "author": "bvaradar", "createdAt": "2020-06-24T07:00:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2NDQ2OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445764469", "bodyText": "Done. I have creates the following functions which currently only support PARQUET but can be extended in future.\nHoodieInputFormatUtils.getOutputFormatClassName()\nHoodieInputFormatUtils.getSerDeClassName()", "author": "prashantwason", "createdAt": "2020-06-25T18:43:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNjA4OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444506088", "bodyText": "Similar functionality needs to be done for Spark SQL Writer. See HoodieSparkSqlWriter.scala", "author": "bvaradar", "createdAt": "2020-06-23T21:01:18Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -177,6 +177,9 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--table-type\"}, description = \"Type of table. COPY_ON_WRITE (or) MERGE_ON_READ\", required = true)\n     public String tableType;\n \n+    @Parameter(names = {\"--base-file-format\"}, description = \"File format for the base files. PARQUET (or) HFILE\", required = false)", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2NTY5Mw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445765693", "bodyText": "Done. The changes are in\nhudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\nhudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala\nhudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "author": "prashantwason", "createdAt": "2020-06-25T18:44:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNjA4OA=="}], "type": "inlineReview"}, {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter", "committedDate": "2020-06-25T18:53:36Z", "type": "commit"}, {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter", "committedDate": "2020-06-25T18:53:36Z", "type": "forcePushed"}]}