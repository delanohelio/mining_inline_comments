{"pr_number": 1810, "pr_title": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync", "pr_createdAt": "2020-07-08T16:13:06Z", "pr_url": "https://github.com/apache/hudi/pull/1810", "timeline": [{"oid": "4c08849c4d7226118202ca4dbeaa63634b16daf3", "url": "https://github.com/apache/hudi/commit/4c08849c4d7226118202ca4dbeaa63634b16daf3", "message": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync", "committedDate": "2020-07-15T04:17:54Z", "type": "commit"}, {"oid": "4c08849c4d7226118202ca4dbeaa63634b16daf3", "url": "https://github.com/apache/hudi/commit/4c08849c4d7226118202ca4dbeaa63634b16daf3", "message": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync", "committedDate": "2020-07-15T04:17:54Z", "type": "forcePushed"}, {"oid": "36979503d9f4d44281136caadc51ab681e1cf056", "url": "https://github.com/apache/hudi/commit/36979503d9f4d44281136caadc51ab681e1cf056", "message": " [HUDI-875] Abstract support hudi-dla-sync", "committedDate": "2020-07-15T09:18:54Z", "type": "commit"}, {"oid": "36979503d9f4d44281136caadc51ab681e1cf056", "url": "https://github.com/apache/hudi/commit/36979503d9f4d44281136caadc51ab681e1cf056", "message": " [HUDI-875] Abstract support hudi-dla-sync", "committedDate": "2020-07-15T09:18:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk3Mzk2MQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r454973961", "bodyText": "since dla meta do not support alter table properties yet, it would be simpler here", "author": "leesf", "createdAt": "2020-07-15T11:12:23Z", "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/DLASyncTool.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import com.beust.jcommander.JCommander;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.dla.util.Utils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.hudi.sync.common.AbstractSyncTool;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Tool to sync a hoodie table with a dla table. Either use it as a api\n+ * DLASyncTool.syncHoodieTable(DLASyncConfig) or as a command line java -cp hoodie-hive.jar DLASyncTool [args]\n+ * <p>\n+ * This utility will get the schema from the latest commit and will sync dla table schema Also this will sync the\n+ * partitions incrementally (all the partitions modified since the last commit)\n+ */\n+@SuppressWarnings(\"WeakerAccess\")\n+public class DLASyncTool extends AbstractSyncTool {\n+\n+  private static final Logger LOG = LogManager.getLogger(DLASyncTool.class);\n+  public static final String SUFFIX_SNAPSHOT_TABLE = \"_rt\";\n+  public static final String SUFFIX_READ_OPTIMIZED_TABLE = \"_ro\";\n+\n+  private final DLASyncConfig cfg;\n+  private final HoodieDLAClient hoodieDLAClient;\n+  private final String snapshotTableName;\n+  private final Option<String> roTableTableName;\n+\n+  public DLASyncTool(Properties properties, FileSystem fs) {\n+    super(properties, fs);\n+    this.hoodieDLAClient = new HoodieDLAClient(Utils.propertiesToConfig(properties), fs);\n+    this.cfg = Utils.propertiesToConfig(properties);\n+    switch (hoodieDLAClient.getTableType()) {\n+      case COPY_ON_WRITE:\n+        this.snapshotTableName = cfg.tableName;\n+        this.roTableTableName = Option.empty();\n+        break;\n+      case MERGE_ON_READ:\n+        this.snapshotTableName = cfg.tableName + SUFFIX_SNAPSHOT_TABLE;\n+        this.roTableTableName = cfg.skipROSuffix ? Option.of(cfg.tableName) :\n+            Option.of(cfg.tableName + SUFFIX_READ_OPTIMIZED_TABLE);\n+        break;\n+      default:\n+        LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+        throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+    }\n+  }\n+\n+  @Override\n+  public void syncHoodieTable() {\n+    try {\n+      switch (hoodieDLAClient.getTableType()) {\n+        case COPY_ON_WRITE:\n+          syncHoodieTable(snapshotTableName, false);\n+          break;\n+        case MERGE_ON_READ:\n+          // sync a RO table for MOR\n+          syncHoodieTable(roTableTableName.get(), false);\n+          // sync a RT table for MOR\n+          syncHoodieTable(snapshotTableName, true);\n+          break;\n+        default:\n+          LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+          throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+      }\n+    } catch (RuntimeException re) {\n+      LOG.error(\"Got runtime exception when dla syncing\", re);\n+    } finally {\n+      hoodieDLAClient.close();\n+    }\n+  }\n+\n+  private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n+    LOG.info(\"Trying to sync hoodie table \" + tableName + \" with base path \" + hoodieDLAClient.getBasePath()\n+        + \" of type \" + hoodieDLAClient.getTableType());\n+    // Check if the necessary table exists\n+    boolean tableExists = hoodieDLAClient.doesTableExist(tableName);\n+    // Get the parquet schema for this table looking at the latest commit\n+    MessageType schema = hoodieDLAClient.getDataSchema();\n+    // Sync schema if needed\n+    syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);\n+\n+    LOG.info(\"Schema sync complete. Syncing partitions for \" + tableName);\n+    // Get the last time we successfully synced partitions\n+    Option<String> lastCommitTimeSynced = Option.empty();\n+    /*if (tableExists) {\n+      lastCommitTimeSynced = hoodieDLAClient.getLastCommitTimeSynced(tableName);\n+    }*/\n+    LOG.info(\"Last commit time synced was found to be \" + lastCommitTimeSynced.orElse(\"null\"));", "originalCommit": "36979503d9f4d44281136caadc51ab681e1cf056", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAxMTAyMw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r455011023", "bodyText": "yes", "author": "lw309637554", "createdAt": "2020-07-15T12:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk3Mzk2MQ=="}], "type": "inlineReview"}, {"oid": "458d7ebfff40114bb4c8536fec25038bf89deed1", "url": "https://github.com/apache/hudi/commit/458d7ebfff40114bb4c8536fec25038bf89deed1", "message": " [HUDI-875] merge master", "committedDate": "2020-08-01T16:05:38Z", "type": "commit"}, {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c", "url": "https://github.com/apache/hudi/commit/fcc3a9c1444f8488164a570d506abe6ab245644c", "message": "Merge branch 'master' into pull/1810", "committedDate": "2020-08-04T04:04:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTU1OA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805558", "bodyText": "can we print a warning around this, so the user knows?\nhere's my take. we can change the code so that --enable-sync and --sync-tool-class-list are the main drivers out of which we derive a Set<String> denoting all the sync tool classes. if --enable-hive-sync is specified, then we simply add the hive sync tool class to this set.. rest of the code just syncs to all sync tools in this set.\nthis way, --enable-hive-sync will be just isolated to the initial command line parsing code. We can apply the same method to datasource as well, if you don't see issues  @lw309637554 @leesf wdyt?", "author": "vinothchandar", "createdAt": "2020-08-04T05:20:06Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -267,9 +267,16 @@ public Operation convert(String value) throws ParameterException {\n         description = \"Should duplicate records from source be dropped/filtered out before insert/bulk-insert\")\n     public Boolean filterDupes = false;\n \n+    //will abandon in the future version, recommended use --enable-sync", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTEzMzA5Mg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465133092", "bodyText": "agree with you ,and  i will do it", "author": "lw309637554", "createdAt": "2020-08-04T15:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTU1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805989", "bodyText": "what if both hive and meta sync are off? we would still emit metrics for meta?", "author": "vinothchandar", "createdAt": "2020-08-04T05:21:33Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs, long syncNs, boolean hiveSync) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+      if (hiveSync) {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(syncNs));\n+      } else {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"metaSyncDuration\"), getDurationInMs(syncNs));", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjU5NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464806594", "bodyText": "should we derive the metric name from the sync tool class. i.e instead of metaSyncDuration, we do dlaSyncDuration?  that seems more usable and understandable", "author": "vinothchandar", "createdAt": "2020-08-04T05:24:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE1NzA1Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465157057", "bodyText": "i have do it , different  sync tool class have its own metrics with name of sync class", "author": "lw309637554", "createdAt": "2020-08-04T15:57:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjMzNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464806335", "bodyText": "is there a way to do this by iterating over the configured sync tool classes? i.e only do it when sync is configured?", "author": "vinothchandar", "createdAt": "2020-08-04T05:23:01Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -442,7 +449,8 @@ private void refreshTimeline() throws IOException {\n     long overallTimeMs = overallTimerContext != null ? overallTimerContext.stop() : 0;\n \n     // Send DeltaStreamer Metrics\n-    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs, true);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, metaSyncTimeMs, false);", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE1ODYwOA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465158608", "bodyText": "ok  , have do this in syncMeta", "author": "lw309637554", "createdAt": "2020-08-04T15:59:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjMzNQ=="}], "type": "inlineReview"}, {"oid": "e35037775016004364176de7f46091ce2b8ba49e", "url": "https://github.com/apache/hudi/commit/e35037775016004364176de7f46091ce2b8ba49e", "message": "Smaller CR feedback", "committedDate": "2020-08-04T05:43:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyOTQ1Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465229457", "bodyText": "lets do HiveSyncTool.class.getName or soemthing?", "author": "vinothchandar", "createdAt": "2020-08-04T17:57:13Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -267,9 +267,16 @@ public Operation convert(String value) throws ParameterException {\n         description = \"Should duplicate records from source be dropped/filtered out before insert/bulk-insert\")\n     public Boolean filterDupes = false;\n \n+    //will abandon in the future version, recommended use --enable-sync\n     @Parameter(names = {\"--enable-hive-sync\"}, description = \"Enable syncing to hive\")\n     public Boolean enableHiveSync = false;\n \n+    @Parameter(names = {\"--enable-sync\"}, description = \"Enable syncing meta\")\n+    public Boolean enableMetaSync = false;\n+\n+    @Parameter(names = {\"--sync-tool-classes\"}, description = \"Meta sync client tool, using comma to separate multi tools\")\n+    public String syncClientToolClass = \"org.apache.hudi.hive.HiveSyncTool\";", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQzNDExMw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465434113", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T02:24:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyOTQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNTg2OA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465415868", "bodyText": "we would add a TODO here once DLA supports alter table properties.", "author": "leesf", "createdAt": "2020-08-05T01:16:41Z", "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/DLASyncTool.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import com.beust.jcommander.JCommander;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.dla.util.Utils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.hudi.sync.common.AbstractSyncTool;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Tool to sync a hoodie table with a dla table. Either use it as a api\n+ * DLASyncTool.syncHoodieTable(DLASyncConfig) or as a command line java -cp hoodie-hive.jar DLASyncTool [args]\n+ * <p>\n+ * This utility will get the schema from the latest commit and will sync dla table schema Also this will sync the\n+ * partitions incrementally (all the partitions modified since the last commit)\n+ */\n+@SuppressWarnings(\"WeakerAccess\")\n+public class DLASyncTool extends AbstractSyncTool {\n+\n+  private static final Logger LOG = LogManager.getLogger(DLASyncTool.class);\n+  public static final String SUFFIX_SNAPSHOT_TABLE = \"_rt\";\n+  public static final String SUFFIX_READ_OPTIMIZED_TABLE = \"_ro\";\n+\n+  private final DLASyncConfig cfg;\n+  private final HoodieDLAClient hoodieDLAClient;\n+  private final String snapshotTableName;\n+  private final Option<String> roTableTableName;\n+\n+  public DLASyncTool(Properties properties, FileSystem fs) {\n+    super(properties, fs);\n+    this.hoodieDLAClient = new HoodieDLAClient(Utils.propertiesToConfig(properties), fs);\n+    this.cfg = Utils.propertiesToConfig(properties);\n+    switch (hoodieDLAClient.getTableType()) {\n+      case COPY_ON_WRITE:\n+        this.snapshotTableName = cfg.tableName;\n+        this.roTableTableName = Option.empty();\n+        break;\n+      case MERGE_ON_READ:\n+        this.snapshotTableName = cfg.tableName + SUFFIX_SNAPSHOT_TABLE;\n+        this.roTableTableName = cfg.skipROSuffix ? Option.of(cfg.tableName) :\n+            Option.of(cfg.tableName + SUFFIX_READ_OPTIMIZED_TABLE);\n+        break;\n+      default:\n+        LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+        throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+    }\n+  }\n+\n+  @Override\n+  public void syncHoodieTable() {\n+    try {\n+      switch (hoodieDLAClient.getTableType()) {\n+        case COPY_ON_WRITE:\n+          syncHoodieTable(snapshotTableName, false);\n+          break;\n+        case MERGE_ON_READ:\n+          // sync a RO table for MOR\n+          syncHoodieTable(roTableTableName.get(), false);\n+          // sync a RT table for MOR\n+          syncHoodieTable(snapshotTableName, true);\n+          break;\n+        default:\n+          LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+          throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+      }\n+    } catch (RuntimeException re) {\n+      LOG.error(\"Got runtime exception when dla syncing\", re);\n+    } finally {\n+      hoodieDLAClient.close();\n+    }\n+  }\n+\n+  private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n+    LOG.info(\"Trying to sync hoodie table \" + tableName + \" with base path \" + hoodieDLAClient.getBasePath()\n+        + \" of type \" + hoodieDLAClient.getTableType());\n+    // Check if the necessary table exists\n+    boolean tableExists = hoodieDLAClient.doesTableExist(tableName);\n+    // Get the parquet schema for this table looking at the latest commit\n+    MessageType schema = hoodieDLAClient.getDataSchema();\n+    // Sync schema if needed\n+    syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);\n+\n+    LOG.info(\"Schema sync complete. Syncing partitions for \" + tableName);\n+    // Get the last time we successfully synced partitions\n+    Option<String> lastCommitTimeSynced = Option.empty();", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0OTA2NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465449064", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:24:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNTg2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNjIxNw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465416217", "bodyText": "please use // TODO here", "author": "leesf", "createdAt": "2020-08-05T01:18:06Z", "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/HoodieDLAClient.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.hive.HiveSyncConfig;\n+import org.apache.hudi.hive.HoodieHiveSyncException;\n+import org.apache.hudi.hive.PartitionValueExtractor;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.DatabaseMetaData;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class HoodieDLAClient extends AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(HoodieDLAClient.class);\n+  private static final String HOODIE_LAST_COMMIT_TIME_SYNC = \"hoodie_last_sync\";\n+  // Make sure we have the dla JDBC driver in classpath\n+  private static final String DRIVER_NAME = \"com.mysql.jdbc.Driver\";\n+  private static final String DLA_ESCAPE_CHARACTER = \"\";\n+  private static final String TBL_PROPERTIES_STR = \"TBLPROPERTIES\";\n+\n+  static {\n+    try {\n+      Class.forName(DRIVER_NAME);\n+    } catch (ClassNotFoundException e) {\n+      throw new IllegalStateException(\"Could not find \" + DRIVER_NAME + \" in classpath. \", e);\n+    }\n+  }\n+\n+  private Connection connection;\n+  private DLASyncConfig dlaConfig;\n+  private PartitionValueExtractor partitionValueExtractor;\n+\n+  public HoodieDLAClient(DLASyncConfig syncConfig, FileSystem fs) {\n+    super(syncConfig.basePath, syncConfig.assumeDatePartitioning, fs);\n+    this.dlaConfig = syncConfig;\n+    try {\n+      this.partitionValueExtractor =\n+          (PartitionValueExtractor) Class.forName(dlaConfig.partitionValueExtractorClass).newInstance();\n+    } catch (Exception e) {\n+      throw new HoodieException(\n+          \"Failed to initialize PartitionValueExtractor class \" + dlaConfig.partitionValueExtractorClass, e);\n+    }\n+    createDLAConnection();\n+  }\n+\n+  private void createDLAConnection() {\n+    if (connection == null) {\n+      try {\n+        Class.forName(DRIVER_NAME);\n+      } catch (ClassNotFoundException e) {\n+        LOG.error(\"Unable to load DLA driver class\", e);\n+        return;\n+      }\n+      try {\n+        this.connection = DriverManager.getConnection(dlaConfig.jdbcUrl, dlaConfig.dlaUser, dlaConfig.dlaPass);\n+        LOG.info(\"Successfully established DLA connection to  \" + dlaConfig.jdbcUrl);\n+      } catch (SQLException e) {\n+        throw new HoodieException(\"Cannot create dla connection \", e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void createTable(String tableName, MessageType storageSchema, String inputFormatClass, String outputFormatClass, String serdeClass) {\n+    try {\n+      String createSQLQuery = HiveSchemaUtil.generateCreateDDL(tableName, storageSchema, toHiveSyncConfig(), inputFormatClass, outputFormatClass, serdeClass);\n+      LOG.info(\"Creating table with \" + createSQLQuery);\n+      updateDLASQL(createSQLQuery);\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Failed to create table \" + tableName, e);\n+    }\n+  }\n+\n+  public Map<String, String> getTableSchema(String tableName) {\n+    if (!doesTableExist(tableName)) {\n+      throw new IllegalArgumentException(\n+          \"Failed to get schema for table \" + tableName + \" does not exist\");\n+    }\n+    Map<String, String> schema = new HashMap<>();\n+    ResultSet result = null;\n+    try {\n+      DatabaseMetaData databaseMetaData = connection.getMetaData();\n+      result = databaseMetaData.getColumns(dlaConfig.databaseName, dlaConfig.databaseName, tableName, null);\n+      while (result.next()) {\n+        String columnName = result.getString(4);\n+        String columnType = result.getString(6);\n+        if (\"DECIMAL\".equals(columnType)) {\n+          int columnSize = result.getInt(\"COLUMN_SIZE\");\n+          int decimalDigits = result.getInt(\"DECIMAL_DIGITS\");\n+          columnType += String.format(\"(%s,%s)\", columnSize, decimalDigits);\n+        }\n+        schema.put(columnName, columnType);\n+      }\n+      return schema;\n+    } catch (SQLException e) {\n+      throw new HoodieException(\"Failed to get table schema for \" + tableName, e);\n+    } finally {\n+      closeQuietly(result, null);\n+    }\n+  }\n+\n+  @Override\n+  public void addPartitionsToTable(String tableName, List<String> partitionsToAdd) {\n+    if (partitionsToAdd.isEmpty()) {\n+      LOG.info(\"No partitions to add for \" + tableName);\n+      return;\n+    }\n+    LOG.info(\"Adding partitions \" + partitionsToAdd.size() + \" to table \" + tableName);\n+    String sql = constructAddPartitions(tableName, partitionsToAdd);\n+    updateDLASQL(sql);\n+  }\n+\n+  public String constructAddPartitions(String tableName, List<String> partitions) {\n+    return constructDLAAddPartitions(tableName, partitions);\n+  }\n+\n+  String generateAbsolutePathStr(Path path) {\n+    String absolutePathStr = path.toString();\n+    if (path.toUri().getScheme() == null) {\n+      absolutePathStr = getDefaultFs() + absolutePathStr;\n+    }\n+    return absolutePathStr.endsWith(\"/\") ? absolutePathStr : absolutePathStr + \"/\";\n+  }\n+\n+  public List<String> constructChangePartitions(String tableName, List<String> partitions) {\n+    List<String> changePartitions = new ArrayList<>();\n+    String useDatabase = \"USE \" + DLA_ESCAPE_CHARACTER + dlaConfig.databaseName + DLA_ESCAPE_CHARACTER;\n+    changePartitions.add(useDatabase);\n+    String alterTable = \"ALTER TABLE \" + DLA_ESCAPE_CHARACTER + tableName + DLA_ESCAPE_CHARACTER;\n+    for (String partition : partitions) {\n+      String partitionClause = getPartitionClause(partition);\n+      Path partitionPath = FSUtils.getPartitionPath(dlaConfig.basePath, partition);\n+      String fullPartitionPathStr = generateAbsolutePathStr(partitionPath);\n+      String changePartition =\n+          alterTable + \" ADD IF NOT EXISTS PARTITION (\" + partitionClause + \") LOCATION '\" + fullPartitionPathStr + \"'\";\n+      changePartitions.add(changePartition);\n+    }\n+    return changePartitions;\n+  }\n+\n+  /**\n+   * Generate Hive Partition from partition values.\n+   *\n+   * @param partition Partition path\n+   * @return\n+   */\n+  public String getPartitionClause(String partition) {\n+    List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);\n+    ValidationUtils.checkArgument(dlaConfig.partitionFields.size() == partitionValues.size(),\n+        \"Partition key parts \" + dlaConfig.partitionFields + \" does not match with partition values \" + partitionValues\n+            + \". Check partition strategy. \");\n+    List<String> partBuilder = new ArrayList<>();\n+    for (int i = 0; i < dlaConfig.partitionFields.size(); i++) {\n+      partBuilder.add(dlaConfig.partitionFields.get(i) + \"='\" + partitionValues.get(i) + \"'\");\n+    }\n+    return partBuilder.stream().collect(Collectors.joining(\",\"));\n+  }\n+\n+  private String constructDLAAddPartitions(String tableName, List<String> partitions) {\n+    StringBuilder alterSQL = new StringBuilder(\"ALTER TABLE \");\n+    alterSQL.append(DLA_ESCAPE_CHARACTER).append(dlaConfig.databaseName)\n+        .append(DLA_ESCAPE_CHARACTER).append(\".\").append(DLA_ESCAPE_CHARACTER)\n+        .append(tableName).append(DLA_ESCAPE_CHARACTER).append(\" ADD IF NOT EXISTS \");\n+    for (String partition : partitions) {\n+      String partitionClause = getPartitionClause(partition);\n+      Path partitionPath = FSUtils.getPartitionPath(dlaConfig.basePath, partition);\n+      String fullPartitionPathStr = generateAbsolutePathStr(partitionPath);\n+      alterSQL.append(\"  PARTITION (\").append(partitionClause).append(\") LOCATION '\").append(fullPartitionPathStr)\n+          .append(\"' \");\n+    }\n+    return alterSQL.toString();\n+  }\n+\n+  private void updateDLASQL(String sql) {\n+    Statement stmt = null;\n+    try {\n+      stmt = connection.createStatement();\n+      LOG.info(\"Executing SQL \" + sql);\n+      stmt.execute(sql);\n+    } catch (SQLException e) {\n+      throw new HoodieException(\"Failed in executing SQL \" + sql, e);\n+    } finally {\n+      closeQuietly(null, stmt);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doesTableExist(String tableName) {\n+    String sql = consutructShowCreateTableSQL(tableName);\n+    Statement stmt = null;\n+    ResultSet rs = null;\n+    try {\n+      stmt = connection.createStatement();\n+      rs = stmt.executeQuery(sql);\n+    } catch (SQLException e) {\n+      return false;\n+    } finally {\n+      closeQuietly(rs, stmt);\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public Option<String> getLastCommitTimeSynced(String tableName) {\n+    String sql = consutructShowCreateTableSQL(tableName);\n+    Statement stmt = null;\n+    ResultSet rs = null;\n+    try {\n+      stmt = connection.createStatement();\n+      rs = stmt.executeQuery(sql);\n+      if (rs.next()) {\n+        String table = rs.getString(2);\n+        Map<String, String> attr = new HashMap<>();\n+        int index = table.indexOf(TBL_PROPERTIES_STR);\n+        if (index != -1) {\n+          String sub = table.substring(index + TBL_PROPERTIES_STR.length());\n+          sub = sub.replaceAll(\"\\\\(\", \"\").replaceAll(\"\\\\)\", \"\").replaceAll(\"'\", \"\");\n+          String[] str = sub.split(\",\");\n+\n+          for (int i = 0; i < str.length; i++) {\n+            String key = str[i].split(\"=\")[0].trim();\n+            String value = str[i].split(\"=\")[1].trim();\n+            attr.put(key, value);\n+          }\n+        }\n+        return Option.ofNullable(attr.getOrDefault(HOODIE_LAST_COMMIT_TIME_SYNC, null));\n+      }\n+      return Option.empty();\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get the last commit time synced from the table\", e);\n+    } finally {\n+      closeQuietly(rs, stmt);\n+    }\n+  }\n+\n+  @Override\n+  public void updateLastCommitTimeSynced(String tableName) {\n+    // dla do not support update tblproperties, so do nothing.", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0OTIyNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465449225", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:25:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNjIxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzA1Mw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417053", "bodyText": "please change to warn", "author": "leesf", "createdAt": "2020-08-05T01:21:08Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public abstract Map<String, String> getTableSchema(String tableName);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0OTM2Mw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465449363", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:25:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzA1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzEzNg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417136", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-08-05T01:21:28Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public abstract Map<String, String> getTableSchema(String tableName);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);\n+    }\n+\n+    try {\n+      if (resultSet != null) {\n+        resultSet.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the resultset opened \", e);", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1MDM1MQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465450351", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:29:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzUzOQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417539", "bodyText": "use HiveSyncTool.class.getName here?", "author": "leesf", "createdAt": "2020-08-05T01:23:05Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -475,12 +480,38 @@ private String startCommit() {\n     throw lastException;\n   }\n \n-  /**\n-   * Sync to Hive.\n-   */\n-  public void syncHiveIfNeeded() {\n+  private void syncMeta(HoodieDeltaStreamerMetrics metrics) {\n+    String syncClientToolClass = cfg.syncClientToolClass;\n+    // for backward compatibility\n     if (cfg.enableHiveSync) {\n-      syncHive();\n+      cfg.enableMetaSync = true;\n+      syncClientToolClass = String.format(\"%s,%s\", cfg.syncClientToolClass, \"org.apache.hudi.hive.HiveSyncTool\");", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1MDk3MQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465450971", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:31:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzUzOQ=="}], "type": "inlineReview"}, {"oid": "fe598ff9af58355469ca08157d7369b7150bde71", "url": "https://github.com/apache/hudi/commit/fe598ff9af58355469ca08157d7369b7150bde71", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T04:29:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2ODQ4Mg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465468482", "bodyText": "do we need the entire class name here? Would that not make for a long metric name? :)\nMay be have a getShortName() method for the AbstractSyncTool class and return \"hive\" and \"dla\" from them?", "author": "vinothchandar", "createdAt": "2020-08-05T04:42:04Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+    }\n+  }\n+\n+  public void updateDeltaStreamerMetaSyncMetrics(String syncClassName, long syncNs) {", "originalCommit": "fe598ff9af58355469ca08157d7369b7150bde71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ5NTgxNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465495815", "bodyText": "ok ,i will do it", "author": "lw309637554", "createdAt": "2020-08-05T06:13:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2ODQ4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTUwMzEyNA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465503124", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T06:32:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2ODQ4Mg=="}], "type": "inlineReview"}, {"oid": "015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "url": "https://github.com/apache/hudi/commit/015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T06:31:48Z", "type": "forcePushed"}, {"oid": "856ef5a9cbf45784932fde3574d3e8e8435f5c26", "url": "https://github.com/apache/hudi/commit/856ef5a9cbf45784932fde3574d3e8e8435f5c26", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T09:55:36Z", "type": "forcePushed"}, {"oid": "3808c3253e6d71c0b7b504046be0c1e9bde969d7", "url": "https://github.com/apache/hudi/commit/3808c3253e6d71c0b7b504046be0c1e9bde969d7", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T13:08:33Z", "type": "commit"}, {"oid": "3808c3253e6d71c0b7b504046be0c1e9bde969d7", "url": "https://github.com/apache/hudi/commit/3808c3253e6d71c0b7b504046be0c1e9bde969d7", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T13:08:33Z", "type": "forcePushed"}, {"oid": "4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "url": "https://github.com/apache/hudi/commit/4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "message": "merge master", "committedDate": "2020-08-06T03:07:13Z", "type": "commit"}, {"oid": "4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "url": "https://github.com/apache/hudi/commit/4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "message": "merge master", "committedDate": "2020-08-06T03:07:13Z", "type": "forcePushed"}]}