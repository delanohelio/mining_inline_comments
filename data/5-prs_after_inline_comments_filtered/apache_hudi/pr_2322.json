{"pr_number": 2322, "pr_title": "[HUDI-1437] support more accurate spark JobGroup for better performan tracking", "pr_createdAt": "2020-12-10T15:23:01Z", "pr_url": "https://github.com/apache/hudi/pull/2322", "timeline": [{"oid": "d7be7c992e707884206c835356068c28b079034b", "url": "https://github.com/apache/hudi/commit/d7be7c992e707884206c835356068c28b079034b", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-11T01:54:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk4MjMwMQ==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r540982301", "bodyText": ",\"Delete replaced file groups\" -> , \"Delete replaced file groups\"", "author": "leesf", "createdAt": "2020-12-11T14:23:32Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/ReplaceArchivalHelper.java", "diffHunk": "@@ -68,7 +68,7 @@\n   public static boolean deleteReplacedFileGroups(HoodieEngineContext context, HoodieTableMetaClient metaClient,\n                                                  TableFileSystemView fileSystemView,\n                                                  HoodieInstant instant, List<String> replacedPartitions) {\n-\n+    context.setJobStatus(ReplaceArchivalHelper.class.getSimpleName(),\"Delete replaced file groups\");", "originalCommit": "d7be7c992e707884206c835356068c28b079034b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU4Nzk3NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r541587974", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-12T14:36:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk4MjMwMQ=="}], "type": "inlineReview"}, {"oid": "ab96d884a4fa8b8ce1657bbed5b9a5b3267bdad7", "url": "https://github.com/apache/hudi/commit/ab96d884a4fa8b8ce1657bbed5b9a5b3267bdad7", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-12T14:36:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg3MDA5MQ==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r541870091", "bodyText": "this would be inneccessary?", "author": "leesf", "createdAt": "2020-12-13T07:52:01Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/queue/IteratorBasedQueueProducer.java", "diffHunk": "@@ -42,9 +42,10 @@ public IteratorBasedQueueProducer(Iterator<I> inputIterator) {\n   @Override\n   public void produce(BoundedInMemoryQueue<I, ?> queue) throws Exception {\n     LOG.info(\"starting to buffer records\");\n+    long start = System.currentTimeMillis();\n     while (inputIterator.hasNext()) {\n       queue.insertRecord(inputIterator.next());\n     }\n-    LOG.info(\"finished buffering records\");\n+    LOG.info(\"finished buffering records, cost = \" + ((System.currentTimeMillis() - start) / 1000));", "originalCommit": "ab96d884a4fa8b8ce1657bbed5b9a5b3267bdad7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTkzOTUzNg==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r541939536", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-13T14:46:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg3MDA5MQ=="}], "type": "inlineReview"}, {"oid": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "url": "https://github.com/apache/hudi/commit/4181d08d22184035dcb6f73a1103fedcb95eaef3", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-13T14:45:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542250943", "bodyText": "Should we clear the jobGroup after the job finish?If not, the next job will reuse the group name.", "author": "pengzhiwei2018", "createdAt": "2020-12-14T09:53:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java", "diffHunk": "@@ -52,6 +52,7 @@ public SparkMarkerBasedRollbackStrategy(HoodieTable<T, JavaRDD<HoodieRecord<T>>,\n       MarkerFiles markerFiles = new MarkerFiles(table, instantToRollback.getTimestamp());\n       List<String> markerFilePaths = markerFiles.allMarkerFilePaths();\n       int parallelism = Math.max(Math.min(markerFilePaths.size(), config.getRollbackParallelism()), 1);\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Marker files rollback\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3OTc0MA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542479740", "bodyText": "@pengzhiwei2018 Since each job should have been set their own job group name, so it should be ok?", "author": "leesf", "createdAt": "2020-12-14T15:35:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk3MTY4NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542971684", "bodyText": "Should we clear the jobGroup after the job finish?If not, the next job will reuse the group name.\n\n@pengzhiwei2018 Some method to do one specific operation of hudi, may be contain not only one spark action. it can reuse the jobGroup. In this scenario , We don\u2019t have to set it so detailed. Also it will be more suitable just like @leesf mention,  follow the original mechanism of Hudi.", "author": "lw309637554", "createdAt": "2020-12-15T01:19:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk5OTgyMQ==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542999821", "bodyText": "@leesf @lw309637554 Maybe we can make all of the spark job in hoodie have a job group name. But if user does some spark action after the hoodie job.The user's  job may reuse the hoodie's job group name.It is hard to control the user's behavior. In my use case, the reused job group name make me really confused in performance profile.", "author": "pengzhiwei2018", "createdAt": "2020-12-15T02:35:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAzODM3Nw==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543038377", "bodyText": "yeah, as your case  you can set job group in you spark job. just like hudi do .", "author": "lw309637554", "createdAt": "2020-12-15T04:31:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5MTk3NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543091974", "bodyText": "may be we can set the jobGroup to \"\" upon writeClient.close() or something?", "author": "vinothchandar", "createdAt": "2020-12-15T06:56:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDIwNDA5NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r544204094", "bodyText": "this option make sense", "author": "lw309637554", "createdAt": "2020-12-16T10:58:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5Mjk0OA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543092948", "bodyText": "change message to : \"Delete invalid files generated during the write operation\" ?", "author": "vinothchandar", "createdAt": "2020-12-15T06:59:06Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -403,6 +403,7 @@ public void finalizeWrite(HoodieEngineContext context, String instantTs, List<Ho\n \n   private void deleteInvalidFilesByPartitions(HoodieEngineContext context, Map<String, List<Pair<String, String>>> invalidFilesByPartition) {\n     // Now delete partially written files\n+    context.setJobStatus(this.getClass().getSimpleName(), \"Delete invalid files by partitions\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDIwNDUwMg==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r544204502", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-16T10:59:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5Mjk0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5MzM4NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543093384", "bodyText": "change to : Obtaining marker files for all created, merged paths", "author": "vinothchandar", "createdAt": "2020-12-15T07:00:03Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/MarkerFiles.java", "diffHunk": "@@ -135,6 +135,7 @@ public boolean doesMarkerDirExist() throws IOException {\n     if (subDirectories.size() > 0) {\n       parallelism = Math.min(subDirectories.size(), parallelism);\n       SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());\n+      context.setJobStatus(this.getClass().getSimpleName(), \"MarkerFiles created and merged data paths\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDIwNDgxMA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r544204810", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-16T11:00:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5MzM4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NDI2Ng==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543094266", "bodyText": "Change to : Compute all comparisons needed between records and files", "author": "vinothchandar", "createdAt": "2020-12-15T07:01:59Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -137,12 +137,14 @@ public SparkHoodieBloomIndex(HoodieWriteConfig config) {\n    */\n   private Map<String, Long> computeComparisonsPerFileGroup(final Map<String, Long> recordsPerPartition,\n                                                            final Map<String, List<BloomIndexFileInfo>> partitionToFileInfo,\n-                                                           JavaPairRDD<String, String> partitionRecordKeyPairRDD) {\n+                                                           JavaPairRDD<String, String> partitionRecordKeyPairRDD,\n+                                                           final HoodieEngineContext context) {\n \n     Map<String, Long> fileToComparisons;\n     if (config.getBloomIndexPruneByRanges()) {\n       // we will just try exploding the input and then count to determine comparisons\n       // FIX(vc): Only do sampling here and extrapolate?\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Explode recordRDD with file comparisons\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NDQ0OA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543094448", "bodyText": "Change to : Building workload profile", "author": "vinothchandar", "createdAt": "2020-12-15T07:02:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -101,6 +101,7 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n \n     WorkloadProfile profile = null;\n     if (isWorkloadProfileNeeded()) {\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Build workload profile\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NTEwMw==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543095103", "bodyText": "Change to : Preparing compaction metadata", "author": "vinothchandar", "createdAt": "2020-12-15T07:03:54Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkRunCompactionActionExecutor.java", "diffHunk": "@@ -76,6 +76,7 @@ public SparkRunCompactionActionExecutor(HoodieSparkEngineContext context,\n       JavaRDD<WriteStatus> statuses = compactor.compact(context, compactionPlan, table, config, instantTime);\n \n       statuses.persist(SparkMemoryUtils.getWriteStatusStorageLevel(config.getProps()));\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Collect compaction metadata status\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NTQ0Mg==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543095442", "bodyText": "Change to:  Rolling back using marker files", "author": "vinothchandar", "createdAt": "2020-12-15T07:04:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java", "diffHunk": "@@ -52,6 +52,7 @@ public SparkMarkerBasedRollbackStrategy(HoodieTable<T, JavaRDD<HoodieRecord<T>>,\n       MarkerFiles markerFiles = new MarkerFiles(table, instantToRollback.getTimestamp());\n       List<String> markerFilePaths = markerFiles.allMarkerFilePaths();\n       int parallelism = Math.max(Math.min(markerFilePaths.size(), config.getRollbackParallelism()), 1);\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Marker files rollback\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "url": "https://github.com/apache/hudi/commit/65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-16T11:03:15Z", "type": "commit"}, {"oid": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "url": "https://github.com/apache/hudi/commit/65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-16T11:03:15Z", "type": "forcePushed"}, {"oid": "9eef5e89c314a0e99cfc41a91be94e457c7279cf", "url": "https://github.com/apache/hudi/commit/9eef5e89c314a0e99cfc41a91be94e457c7279cf", "message": "Merge branch 'master' into HUDI-1437-3", "committedDate": "2020-12-16T11:05:55Z", "type": "commit"}]}