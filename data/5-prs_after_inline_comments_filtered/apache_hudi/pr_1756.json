{"pr_number": 1756, "pr_title": "[HUDI-839] Introducing support for rollbacks using marker files", "pr_createdAt": "2020-06-22T14:48:41Z", "pr_url": "https://github.com/apache/hudi/pull/1756", "timeline": [{"oid": "922b1efdbd2d4e33297076eea5fe47d96a0c261c", "url": "https://github.com/apache/hudi/commit/922b1efdbd2d4e33297076eea5fe47d96a0c261c", "message": "[HUDI-839] Introducing rollback strategy using marker files\n\n  - Initial commit", "committedDate": "2020-06-02T02:47:16Z", "type": "commit"}, {"oid": "470842d988f383f305c812a47e260e6bba51e993", "url": "https://github.com/apache/hudi/commit/470842d988f383f305c812a47e260e6bba51e993", "message": "Adding unit test for MarkerFiles", "committedDate": "2020-06-11T17:10:21Z", "type": "commit"}, {"oid": "751f2baa3a8a562d53855c4d2c9d93daa20fde0c", "url": "https://github.com/apache/hudi/commit/751f2baa3a8a562d53855c4d2c9d93daa20fde0c", "message": "[HUDI-839] Adding unit test for MarkerFiles,RollbackUtils, RollbackActionExecutor for markers and filelisting", "committedDate": "2020-06-22T14:15:36Z", "type": "commit"}, {"oid": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "url": "https://github.com/apache/hudi/commit/e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "message": " [HUDI-839] Merge branch 'master'", "committedDate": "2020-06-22T16:15:00Z", "type": "commit"}, {"oid": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "url": "https://github.com/apache/hudi/commit/e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "message": " [HUDI-839] Merge branch 'master'", "committedDate": "2020-06-22T16:15:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU3NzA5NA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r444577094", "bodyText": "cc @xushiyan I feel just making these two commits or this template code in some TestHelper.. will clean up code by a lot..", "author": "vinothchandar", "createdAt": "2020-06-24T00:20:32Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/action/rollback/TestCopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+public class TestCopyOnWriteRollbackActionExecutor extends HoodieClientTestBase {\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initPath();\n+    initSparkContexts();\n+    //just generate tow partitions\n+    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n+    initFileSystem();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  @Test\n+  public void testCopyOnWriteRollbackActionExecutorForFileListingAsGenerateFile() throws IOException {\n+    // Let's create some commit files and parquet files\n+    String commitTime1 = \"001\";\n+    String commitTime2 = \"002\";\n+    new File(basePath + \"/.hoodie\").mkdirs();\n+    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{\"2015/03/16\", \"2015/03/17\", \"2016/03/15\"},\n+        basePath);\n+    HoodieTestUtils.createCommitFiles(basePath, commitTime1, commitTime2);\n+\n+    // Make commit1\n+    String file11 = HoodieTestUtils.createDataFile(basePath, \"2015/03/16\", commitTime1, \"id11\");\n+    HoodieTestUtils.createNewLogFile(fs, basePath, \"2015/03/16\",\n+        commitTime1, \"id11\", Option.of(3));\n+    String file12 = HoodieTestUtils.createDataFile(basePath, \"2015/03/17\", commitTime1, \"id12\");\n+\n+    // Make commit2\n+    String file21 = HoodieTestUtils.createDataFile(basePath, \"2015/03/16\", commitTime2, \"id21\");\n+    String file22 = HoodieTestUtils.createDataFile(basePath, \"2015/03/17\", commitTime2, \"id22\");\n+    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n+    HoodieTable table = this.getHoodieTable(metaClient, config);\n+    HoodieInstant needRollBackInstant = new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, \"002\");\n+\n+    // execute CopyOnWriteRollbackActionExecutor with filelisting mode\n+    CopyOnWriteRollbackActionExecutor copyOnWriteRollbackActionExecutor = new CopyOnWriteRollbackActionExecutor(jsc, config, table, \"003\", needRollBackInstant, true);\n+    assertFalse(copyOnWriteRollbackActionExecutor.getRollbackStrategy() instanceof MarkerBasedRollbackStrategy);\n+    List<HoodieRollbackStat> hoodieRollbackStats = copyOnWriteRollbackActionExecutor.executeRollback();\n+\n+    // assert hoodieRollbackStats\n+    assertEquals(hoodieRollbackStats.size(), 3);\n+    hoodieRollbackStats.forEach(stat -> {\n+      if (stat.getPartitionPath().equals(\"2015/03/16\")) {\n+        assertEquals(1, stat.getSuccessDeleteFiles().size());\n+        assertEquals(0, stat.getFailedDeleteFiles().size());\n+        assertEquals(null, stat.getCommandBlocksCount());\n+        assertEquals(\"file:\" + HoodieTestUtils.getDataFilePath(basePath, \"2015/03/16\", commitTime2, file21),\n+            stat.getSuccessDeleteFiles().get(0));\n+      } else if (stat.getPartitionPath().equals(\"2015/03/17\")) {\n+        assertEquals(1, stat.getSuccessDeleteFiles().size());\n+        assertEquals(0, stat.getFailedDeleteFiles().size());\n+        assertEquals(null, stat.getCommandBlocksCount());\n+        assertEquals(\"file:\" + HoodieTestUtils.getDataFilePath(basePath, \"2015/03/17\", commitTime2, file22),\n+            stat.getSuccessDeleteFiles().get(0));\n+      } else if (stat.getPartitionPath().equals(\"2015/03/17\")) {\n+        assertEquals(0, stat.getSuccessDeleteFiles().size());\n+        assertEquals(0, stat.getFailedDeleteFiles().size());\n+        assertEquals(null, stat.getCommandBlocksCount());\n+      }\n+    });\n+\n+    assertTrue(HoodieTestUtils.doesCommitExist(basePath, \"001\"));\n+    assertTrue(HoodieTestUtils.doesInflightExist(basePath, \"001\"));\n+    assertFalse(HoodieTestUtils.doesCommitExist(basePath, \"002\"));\n+    assertFalse(HoodieTestUtils.doesInflightExist(basePath, \"002\"));\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/16\", commitTime1, file11)\n+        && HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/17\", commitTime1, file12));\n+    assertFalse(HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/16\", commitTime2, file21)\n+        || HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/17\", commitTime2, file22));\n+  }\n+\n+  private void twoUpsertCommitDataRollBack(boolean isUsingMarkers) throws IOException {\n+    //1. prepare data\n+    HoodieWriteConfig cfg = getConfigBuilder().withRollbackUsingMarkers(isUsingMarkers).build();", "originalCommit": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "37244033e285aa198ec1b420b3040d6c1beb706b", "url": "https://github.com/apache/hudi/commit/37244033e285aa198ec1b420b3040d6c1beb706b", "message": "[HUDI-839] merge latest master patch , add more test for rollback using markers, fix bugs", "committedDate": "2020-06-25T17:24:00Z", "type": "commit"}, {"oid": "37244033e285aa198ec1b420b3040d6c1beb706b", "url": "https://github.com/apache/hudi/commit/37244033e285aa198ec1b420b3040d6c1beb706b", "message": "[HUDI-839] merge latest master patch , add more test for rollback using markers, fix bugs", "committedDate": "2020-06-25T17:24:00Z", "type": "forcePushed"}, {"oid": "97380ea18311668d9eb3ee74998cbd526c8aafa0", "url": "https://github.com/apache/hudi/commit/97380ea18311668d9eb3ee74998cbd526c8aafa0", "message": "[HUDI-839] merge latest master", "committedDate": "2020-06-27T15:56:31Z", "type": "commit"}, {"oid": "713dd3176b3d49901d67d51ef38f0b7a67089360", "url": "https://github.com/apache/hudi/commit/713dd3176b3d49901d67d51ef38f0b7a67089360", "message": "[HUDI-839] clean marker file at pre commit for spark retries", "committedDate": "2020-07-02T08:06:03Z", "type": "commit"}, {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "url": "https://github.com/apache/hudi/commit/9f57d75d73b81a31d7ece46a9134fcf26886dc31", "message": "[HUDI-839] merge latest master", "committedDate": "2020-07-02T13:09:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyMTcwOQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449721709", "bodyText": "I am wondering if we should do this always.. having this logic be rollback dependent can become hard to reason with in the long run", "author": "vinothchandar", "createdAt": "2020-07-04T00:03:12Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -332,9 +333,11 @@ public static SparkConf registerClasses(SparkConf conf) {\n   }\n \n   @Override\n-  protected void postCommit(HoodieCommitMetadata metadata, String instantTime,\n-      Option<Map<String, String>> extraMetadata) {\n+  protected void postCommit(HoodieTable<?> table, HoodieCommitMetadata metadata, String instantTime, Option<Map<String, String>> extraMetadata) {\n     try {\n+      if (!config.getRollBackUsingMarkers()) {", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0NjQxMw==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449946413", "bodyText": "i  also think  it not so good. i think can do not delete markerfiles here.Can delete the unuseful   markerfile in pre-commit. when clean delete the old markerfiles", "author": "lw309637554", "createdAt": "2020-07-06T01:38:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyMTcwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyMTgyMg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449721822", "bodyText": "Reflecting on this, probably good to rename MarkerType to IOType..", "author": "vinothchandar", "createdAt": "2020-07-04T00:04:56Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -278,6 +286,11 @@ public WriteStatus getWriteStatus() {\n     return writeStatus;\n   }\n \n+  @Override\n+  public MarkerFiles.MarkerType getIOType() {", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0NjUzMg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449946532", "bodyText": "to IOType will be better", "author": "lw309637554", "createdAt": "2020-07-06T01:39:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyMTgyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc4NjgyMw==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449786823", "bodyText": "probably need to ensure we are getting the base file format extension from the hoodieTable instance?", "author": "vinothchandar", "createdAt": "2020-07-04T16:34:59Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -113,8 +109,9 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n       partitionMetadata.trySave(getPartitionId());\n \n       oldFilePath = new Path(config.getBasePath() + \"/\" + partitionPath + \"/\" + latestValidFilePath);\n+      String newFileName = FSUtils.makeDataFileName(instantTime, writeToken, fileId);", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0NzM1OA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449947358", "bodyText": "yes, it will be more common use. HUDI will support more base format", "author": "lw309637554", "createdAt": "2020-07-06T01:43:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc4NjgyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc4NzExMA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449787110", "bodyText": "nit: we can just name this create()", "author": "vinothchandar", "createdAt": "2020-07-04T16:38:10Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -97,28 +98,9 @@ public Path makeNewPath(String partitionPath) {\n    *\n    * @param partitionPath Partition path\n    */\n-  protected void createMarkerFile(String partitionPath) {\n-    Path markerPath = makeNewMarkerPath(partitionPath);\n-    try {\n-      LOG.info(\"Creating Marker Path=\" + markerPath);\n-      fs.create(markerPath, false).close();\n-    } catch (IOException e) {\n-      throw new HoodieException(\"Failed to create marker file \" + markerPath, e);\n-    }\n-  }\n-\n-  /**\n-   * THe marker path will be <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename.\n-   */\n-  private Path makeNewMarkerPath(String partitionPath) {\n-    Path markerRootPath = new Path(hoodieTable.getMetaClient().getMarkerFolderPath(instantTime));\n-    Path path = FSUtils.getPartitionPath(markerRootPath, partitionPath);\n-    try {\n-      fs.mkdirs(path); // create a new partition as needed.\n-    } catch (IOException e) {\n-      throw new HoodieIOException(\"Failed to make dir \" + path, e);\n-    }\n-    return new Path(path.toString(), FSUtils.makeMarkerFile(instantTime, writeToken, fileId));\n+  protected void createMarkerFile(String partitionPath, String dataFileName) {\n+    MarkerFiles markerFiles = new MarkerFiles(hoodieTable, instantTime);\n+    markerFiles.createMarkerFile(partitionPath, dataFileName, getIOType());", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDYzMzkzNg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r450633936", "bodyText": "nit: we can just name this create()\n\nyes ,i will be better", "author": "lw309637554", "createdAt": "2020-07-07T06:18:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc4NzExMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNjk1OA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449936958", "bodyText": "these could be replaced with base format from the table object?", "author": "vinothchandar", "createdAt": "2020-07-06T00:30:35Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -54,29 +53,28 @@\n /**\n  * Performs Rollback of Hoodie Tables.\n  */\n-public class RollbackHelper implements Serializable {\n+public class ListingBasedRollbackHelper implements Serializable {\n \n-  private static final Logger LOG = LogManager.getLogger(RollbackHelper.class);\n+  private static final Logger LOG = LogManager.getLogger(ListingBasedRollbackHelper.class);\n \n   private final HoodieTableMetaClient metaClient;\n   private final HoodieWriteConfig config;\n \n-  public RollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config) {\n+  public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config) {\n     this.metaClient = metaClient;\n     this.config = config;\n   }\n \n   /**\n    * Performs all rollback actions that we have collected in parallel.\n    */\n-  public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<RollbackRequest> rollbackRequests) {\n+  public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n \n-    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n     SerializablePathFilter filter = (path) -> {\n-      if (path.toString().contains(basefileExtension)) {\n+      if (path.toString().endsWith(HoodieFileFormat.PARQUET.getFileExtension())) {", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0NzkyMg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449947922", "bodyText": "to be format from the table object will better", "author": "lw309637554", "createdAt": "2020-07-06T01:46:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNjk1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzIxMA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937210", "bodyText": "nit:extra line", "author": "vinothchandar", "createdAt": "2020-07-06T00:32:25Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Performs rollback using marker files generated during the write..\n+ */\n+public class MarkerBasedRollbackStrategy implements BaseRollbackActionExecutor.RollbackStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerBasedRollbackStrategy.class);\n+\n+  private final HoodieTable<?> table;\n+\n+  private final transient JavaSparkContext jsc;\n+\n+  private final HoodieWriteConfig config;\n+\n+  private final String basePath;\n+\n+  private final String instantTime;\n+\n+  public MarkerBasedRollbackStrategy(HoodieTable<?> table, JavaSparkContext jsc, HoodieWriteConfig config, String instantTime) {\n+    this.table = table;\n+    this.jsc = jsc;\n+    this.basePath = table.getMetaClient().getBasePath();\n+    this.config = config;\n+    this.instantTime = instantTime;\n+  }\n+\n+  private HoodieRollbackStat undoMerge(String mergedBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the merged base file:\" + mergedBaseFilePath);\n+    return deleteBaseFile(mergedBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat undoCreate(String createdBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the created base file:\" + createdBaseFilePath);\n+", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzc1Mw==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937753", "bodyText": "these combinations added on Schema evolution test is a bit hard to understand. like why would be testing modes for rollbacks in a schema evolution test? any particular reason?", "author": "vinothchandar", "createdAt": "2020-07-06T00:37:06Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -408,6 +416,16 @@ public void testCopyOnWriteTable() throws Exception {\n     checkReadRecords(\"000\", 2 * numRecords);\n   }\n \n+  @Test\n+  public void testCopyOnWriteTableUsingFileListRollBack() throws Exception {\n+    testCopyOnWriteTable(false);\n+  }\n+\n+  @Test\n+  public void testCopyOnWriteTableUsingMarkersRollBack() throws Exception {", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDYzNjc2OA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r450636768", "bodyText": "yes ,i will remove it", "author": "lw309637554", "createdAt": "2020-07-07T06:26:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzc1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzg5NQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937895", "bodyText": "Similar here.. this test probably does not need to test these two modes? for e.g: what additional testing are we getting over the test schema evolution test by doing this?", "author": "vinothchandar", "createdAt": "2020-07-06T00:38:27Z", "path": "hudi-client/src/test/java/org/apache/hudi/index/TestHoodieIndex.java", "diffHunk": "@@ -328,6 +330,18 @@ public void testSimpleTagLocationAndUpdateWithRollback(IndexType indexType) thro\n     assert (javaRDD.filter(record -> record.getCurrentLocation() != null).collect().size() == 0);\n   }\n \n+  @ParameterizedTest\n+  @EnumSource(value = IndexType.class, names = {\"BLOOM\", \"GLOBAL_BLOOM\", \"SIMPLE\", \"GLOBAL_SIMPLE\"})", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0Nzk0MQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449947941", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-07-06T01:46:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzg5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzkzNw==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937937", "bodyText": "same comment here..", "author": "vinothchandar", "createdAt": "2020-07-06T00:38:59Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java", "diffHunk": "@@ -904,6 +901,19 @@ public void testCleanMarkerDataFilesOnRollback() throws IOException {\n     assertEquals(0, getTotalTempFiles(), \"All temp files are deleted.\");\n   }\n \n+  /**\n+   * Test Cleaning functionality of table.rollback() API.\n+   */\n+  @Test\n+  public void testCleanMarkerDataFilesOnRollbackUsingFileList() throws IOException {", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0Nzk3Mg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449947972", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-07-06T01:46:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODAxNg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449938016", "bodyText": "these may be worth adding in both modes..", "author": "vinothchandar", "createdAt": "2020-07-06T00:39:35Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java", "diffHunk": "@@ -445,10 +442,20 @@ public void testCOWToMORConvertedTableRollback(HoodieFileFormat baseFileFormat)\n \n   @ParameterizedTest\n   @MethodSource(\"argumentsProvider\")\n-  public void testRollbackWithDeltaAndCompactionCommit(HoodieFileFormat baseFileFormat) throws Exception {\n+  public void testCOWToMORConvertedTableRollbackUsingFileList(HoodieFileFormat baseFileFormat) throws Exception {", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0ODA4Mg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449948082", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-07-06T01:47:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODIyMg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449938222", "bodyText": "typo:two", "author": "vinothchandar", "createdAt": "2020-07-06T00:41:11Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/action/rollback/TestCopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+public class TestCopyOnWriteRollbackActionExecutor extends HoodieClientTestBase {\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initPath();\n+    initSparkContexts();\n+    //just generate tow partitions", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0ODU1Nw==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449948557", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-07-06T01:50:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODIyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODUxMw==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449938513", "bodyText": "anyway to share code with the COW test?", "author": "vinothchandar", "createdAt": "2020-07-06T00:43:42Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+\n+public class TestMergeOnReadRollbackActionExecutor extends HoodieClientTestBase {\n+  @Override\n+  protected HoodieTableType getTableType() {\n+    return HoodieTableType.MERGE_ON_READ;\n+  }\n+\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initPath();\n+    initSparkContexts();\n+    //just generate tow partitions\n+    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n+    initFileSystem();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  private void twoUpsertCommitDataRollBack(boolean isUsingMarkers) throws IOException, InterruptedException {", "originalCommit": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0ODMwMg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449948302", "bodyText": "yes  make sense", "author": "lw309637554", "createdAt": "2020-07-06T01:48:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODUxMw=="}], "type": "inlineReview"}, {"oid": "13543363004a757cc7bd7041a7ef81177cef0106", "url": "https://github.com/apache/hudi/commit/13543363004a757cc7bd7041a7ef81177cef0106", "message": "[HUDI-839] using fileformat with table.getBaseFileExtension", "committedDate": "2020-07-07T08:16:26Z", "type": "commit"}, {"oid": "d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "url": "https://github.com/apache/hudi/commit/d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "message": "Merge branch 'master'  into HUDI-839-lw", "committedDate": "2020-07-07T08:30:56Z", "type": "commit"}, {"oid": "d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "url": "https://github.com/apache/hudi/commit/d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "message": "Merge branch 'master'  into HUDI-839-lw", "committedDate": "2020-07-07T08:30:56Z", "type": "forcePushed"}, {"oid": "d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "url": "https://github.com/apache/hudi/commit/d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "message": " [HUDI-839] revert the useless unit tests", "committedDate": "2020-07-16T05:54:23Z", "type": "commit"}, {"oid": "d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "url": "https://github.com/apache/hudi/commit/d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "message": " [HUDI-839] revert the useless unit tests", "committedDate": "2020-07-16T05:54:23Z", "type": "forcePushed"}, {"oid": "f193a1155cf58842bb9d496a691579ba2ed71df7", "url": "https://github.com/apache/hudi/commit/f193a1155cf58842bb9d496a691579ba2ed71df7", "message": "Adding marker directory cleanup to timeline archival process\n\n - Added marker dir deletion after successful commit/rollback, individual files are not deleted during finalize\n - Fail safe for deleting marker directories, now during timeline archival process\n - Added check to ensure completed instants are not rolled back using marker based strategy. This will be incorrect\n - Reworked tests to rollback inflight instants, instead of completed instants whenever necessary\n - Added an unit test for MarkerBasedRollbackStrategy", "committedDate": "2020-07-19T22:41:08Z", "type": "commit"}, {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "url": "https://github.com/apache/hudi/commit/a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "message": "Merge branch 'master' into pull/1756", "committedDate": "2020-07-19T22:52:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTcwOQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456965709", "bodyText": "this PR will change behavior for marker dir deletion, with or without marker based rollback turned on.", "author": "vinothchandar", "createdAt": "2020-07-19T22:54:31Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -332,9 +333,12 @@ public static SparkConf registerClasses(SparkConf conf) {\n   }\n \n   @Override\n-  protected void postCommit(HoodieCommitMetadata metadata, String instantTime,\n-      Option<Map<String, String>> extraMetadata) {\n+  protected void postCommit(HoodieTable<?> table, HoodieCommitMetadata metadata, String instantTime, Option<Map<String, String>> extraMetadata) {\n     try {\n+\n+      // Delete the marker directory for the instant.", "originalCommit": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAwNDcxMQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457004711", "bodyText": "Ack.", "author": "bvaradar", "createdAt": "2020-07-20T03:02:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTcwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTgyOA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456965828", "bodyText": "all of this stuff is now encapsulatd into a  MarkerFiles class", "author": "vinothchandar", "createdAt": "2020-07-19T22:55:31Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -97,28 +98,9 @@ public Path makeNewPath(String partitionPath) {\n    *\n    * @param partitionPath Partition path\n    */\n-  protected void createMarkerFile(String partitionPath) {\n-    Path markerPath = makeNewMarkerPath(partitionPath);\n-    try {\n-      LOG.info(\"Creating Marker Path=\" + markerPath);\n-      fs.create(markerPath, false).close();\n-    } catch (IOException e) {\n-      throw new HoodieException(\"Failed to create marker file \" + markerPath, e);\n-    }\n-  }\n-\n-  /**\n-   * THe marker path will be <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename.\n-   */\n-  private Path makeNewMarkerPath(String partitionPath) {", "originalCommit": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTkzOQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456965939", "bodyText": "quick skim of changes here would be nice.", "author": "vinothchandar", "createdAt": "2020-07-19T22:56:43Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -410,72 +412,54 @@ public void deleteMarkerDir(String instantTs) {\n    * @param consistencyCheckEnabled Consistency Check Enabled\n    * @throws HoodieIOException\n    */\n-  protected void cleanFailedWrites(JavaSparkContext jsc, String instantTs, List<HoodieWriteStat> stats,\n-      boolean consistencyCheckEnabled) throws HoodieIOException {\n+  protected void reconcileAgainstMarkers(JavaSparkContext jsc,\n+                                         String instantTs,\n+                                         List<HoodieWriteStat> stats,\n+                                         boolean consistencyCheckEnabled) throws HoodieIOException {\n     try {\n       // Reconcile marker and data files with WriteStats so that partially written data-files due to failed\n       // (but succeeded on retry) tasks are removed.\n       String basePath = getMetaClient().getBasePath();\n-      FileSystem fs = getMetaClient().getFs();\n-      Path markerDir = new Path(metaClient.getMarkerFolderPath(instantTs));\n+      MarkerFiles markers = new MarkerFiles(this, instantTs);\n \n-      if (!fs.exists(markerDir)) {\n-        // Happens when all writes are appends\n+      if (!markers.doesMarkerDirExist()) {\n+        // can happen if it was an empty write say.\n         return;\n       }\n \n-      final String baseFileExtension = getBaseFileFormat().getFileExtension();\n-      List<String> invalidDataPaths = FSUtils.getAllDataFilesForMarkers(fs, basePath, instantTs, markerDir.toString(),\n-          baseFileExtension);\n-      List<String> validDataPaths = stats.stream().map(w -> String.format(\"%s/%s\", basePath, w.getPath()))\n-          .filter(p -> p.endsWith(baseFileExtension)).collect(Collectors.toList());\n+      // we are not including log appends here, since they are already fail-safe.", "originalCommit": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAxNjEyNA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457016124", "bodyText": "This looks good to me.", "author": "bvaradar", "createdAt": "2020-07-20T03:33:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTkzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjA0Ng==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966046", "bodyText": "during archival. either the commit instant or the corresponding rollback.. any left over marker dir will be deleted. or the archival will fail. there is a test added for this.", "author": "vinothchandar", "createdAt": "2020-07-19T22:57:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -264,6 +275,7 @@ public void archive(List<HoodieInstant> instants) throws HoodieCommitException {\n       List<IndexedRecord> records = new ArrayList<>();\n       for (HoodieInstant hoodieInstant : instants) {\n         try {\n+          deleteAnyLeftOverMarkerFiles(hoodieInstant);", "originalCommit": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAxNjg0Mg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457016842", "bodyText": "Ack", "author": "bvaradar", "createdAt": "2020-07-20T03:35:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjA0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjEwNA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966104", "bodyText": "@umehrot2 we can now add the parallelization changes here. for deletion of marker files.", "author": "vinothchandar", "createdAt": "2020-07-19T22:58:30Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/MarkerFiles.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operates on marker files for a given write action (commit, delta commit, compaction).\n+ */\n+public class MarkerFiles {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerFiles.class);\n+\n+  public static String stripMarkerSuffix(String path) {\n+    return path.substring(0, path.indexOf(HoodieTableMetaClient.MARKER_EXTN));\n+  }\n+\n+  private final String instantTime;\n+  private final FileSystem fs;\n+  private final Path markerDirPath;\n+  private final String basePath;\n+\n+  public MarkerFiles(FileSystem fs, String basePath, String markerFolderPath, String instantTime) {\n+    this.instantTime = instantTime;\n+    this.fs = fs;\n+    this.markerDirPath = new Path(markerFolderPath);\n+    this.basePath = basePath;\n+  }\n+\n+  public MarkerFiles(HoodieTable<?> table, String instantTime) {\n+    this(table.getMetaClient().getFs(),\n+        table.getMetaClient().getBasePath(),\n+        table.getMetaClient().getMarkerFolderPath(instantTime),\n+        instantTime);\n+  }\n+\n+  public void quietDeleteMarkerDir() {\n+    try {\n+      deleteMarkerDir();\n+    } catch (HoodieIOException ioe) {\n+      LOG.warn(\"Error deleting marker directory for instant \" + instantTime, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Delete Marker directory corresponding to an instant.\n+   */\n+  public boolean deleteMarkerDir() {", "originalCommit": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjI3NQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966275", "bodyText": "this is resilient already to attempting to delete an non-existent file.. marker file may not imply the data file is thre.", "author": "vinothchandar", "createdAt": "2020-07-19T23:00:10Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Performs rollback using marker files generated during the write..\n+ */\n+public class MarkerBasedRollbackStrategy implements BaseRollbackActionExecutor.RollbackStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerBasedRollbackStrategy.class);\n+\n+  private final HoodieTable<?> table;\n+\n+  private final transient JavaSparkContext jsc;\n+\n+  private final HoodieWriteConfig config;\n+\n+  private final String basePath;\n+\n+  private final String instantTime;\n+\n+  public MarkerBasedRollbackStrategy(HoodieTable<?> table, JavaSparkContext jsc, HoodieWriteConfig config, String instantTime) {\n+    this.table = table;\n+    this.jsc = jsc;\n+    this.basePath = table.getMetaClient().getBasePath();\n+    this.config = config;\n+    this.instantTime = instantTime;\n+  }\n+\n+  private HoodieRollbackStat undoMerge(String mergedBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the merged base file:\" + mergedBaseFilePath);\n+    return deleteBaseFile(mergedBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat undoCreate(String createdBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the created base file:\" + createdBaseFilePath);\n+    return deleteBaseFile(createdBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat deleteBaseFile(String baseFilePath) throws IOException {", "originalCommit": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjMzMg==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966332", "bodyText": "checked that the log scanner can deal with spurious rollback blocks.. i.e rollbacks logged without any data blocks for that instant", "author": "vinothchandar", "createdAt": "2020-07-19T23:00:46Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Performs rollback using marker files generated during the write..\n+ */\n+public class MarkerBasedRollbackStrategy implements BaseRollbackActionExecutor.RollbackStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerBasedRollbackStrategy.class);\n+\n+  private final HoodieTable<?> table;\n+\n+  private final transient JavaSparkContext jsc;\n+\n+  private final HoodieWriteConfig config;\n+\n+  private final String basePath;\n+\n+  private final String instantTime;\n+\n+  public MarkerBasedRollbackStrategy(HoodieTable<?> table, JavaSparkContext jsc, HoodieWriteConfig config, String instantTime) {\n+    this.table = table;\n+    this.jsc = jsc;\n+    this.basePath = table.getMetaClient().getBasePath();\n+    this.config = config;\n+    this.instantTime = instantTime;\n+  }\n+\n+  private HoodieRollbackStat undoMerge(String mergedBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the merged base file:\" + mergedBaseFilePath);\n+    return deleteBaseFile(mergedBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat undoCreate(String createdBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the created base file:\" + createdBaseFilePath);\n+    return deleteBaseFile(createdBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat deleteBaseFile(String baseFilePath) throws IOException {\n+    Path fullDeletePath = new Path(basePath, baseFilePath);\n+    String partitionPath = FSUtils.getRelativePartitionPath(new Path(basePath), fullDeletePath.getParent());\n+    boolean isDeleted = table.getMetaClient().getFs().delete(fullDeletePath);\n+    return HoodieRollbackStat.newBuilder()\n+        .withPartitionPath(partitionPath)\n+        .withDeletedFileResult(baseFilePath, isDeleted)\n+        .build();\n+  }\n+\n+  private HoodieRollbackStat undoAppend(String appendBaseFilePath, HoodieInstant instantToRollback) throws IOException, InterruptedException {\n+    Path baseFilePathForAppend = new Path(basePath, appendBaseFilePath);\n+    String fileId = FSUtils.getFileIdFromFilePath(baseFilePathForAppend);\n+    String baseCommitTime = FSUtils.getCommitTime(baseFilePathForAppend.getName());\n+    String partitionPath = FSUtils.getRelativePartitionPath(new Path(basePath), new Path(basePath, appendBaseFilePath).getParent());\n+\n+    HoodieLogFormat.Writer writer = null;\n+    try {\n+      Path partitionFullPath = FSUtils.getPartitionPath(basePath, partitionPath);\n+\n+      if (!table.getMetaClient().getFs().exists(partitionFullPath)) {\n+        return HoodieRollbackStat.newBuilder()\n+            .withPartitionPath(partitionPath)\n+            .build();\n+      }\n+      writer = HoodieLogFormat.newWriterBuilder()", "originalCommit": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "url": "https://github.com/apache/hudi/commit/37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "message": "Fix compilation issue", "committedDate": "2020-07-20T00:44:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAwNzA2NA==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457007064", "bodyText": "Is this needed since we already.have shouldRollbackUsingMarkers() ?", "author": "bvaradar", "createdAt": "2020-07-20T03:09:04Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -632,6 +638,10 @@ public FileSystemViewStorageConfig getClientSpecifiedViewStorageConfig() {\n     return clientSpecifiedViewStorageConfig;\n   }\n \n+  public boolean getRollBackUsingMarkers() {", "originalCommit": "37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAxOTYzNQ==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457019635", "bodyText": "This is fine (backwards compatible) as we are not currently writing marker files for appends. right ?", "author": "bvaradar", "createdAt": "2020-07-20T03:43:06Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/MarkerFiles.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operates on marker files for a given write action (commit, delta commit, compaction).\n+ */\n+public class MarkerFiles {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerFiles.class);\n+\n+  public static String stripMarkerSuffix(String path) {\n+    return path.substring(0, path.indexOf(HoodieTableMetaClient.MARKER_EXTN));\n+  }\n+\n+  private final String instantTime;\n+  private final FileSystem fs;\n+  private final Path markerDirPath;\n+  private final String basePath;\n+\n+  public MarkerFiles(FileSystem fs, String basePath, String markerFolderPath, String instantTime) {\n+    this.instantTime = instantTime;\n+    this.fs = fs;\n+    this.markerDirPath = new Path(markerFolderPath);\n+    this.basePath = basePath;\n+  }\n+\n+  public MarkerFiles(HoodieTable<?> table, String instantTime) {\n+    this(table.getMetaClient().getFs(),\n+        table.getMetaClient().getBasePath(),\n+        table.getMetaClient().getMarkerFolderPath(instantTime),\n+        instantTime);\n+  }\n+\n+  public void quietDeleteMarkerDir() {\n+    try {\n+      deleteMarkerDir();\n+    } catch (HoodieIOException ioe) {\n+      LOG.warn(\"Error deleting marker directory for instant \" + instantTime, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Delete Marker directory corresponding to an instant.\n+   */\n+  public boolean deleteMarkerDir() {\n+    try {\n+      boolean result = fs.delete(markerDirPath, true);\n+      if (result) {\n+        LOG.info(\"Removing marker directory at \" + markerDirPath);\n+      } else {\n+        LOG.info(\"No marker directory to delete at \" + markerDirPath);\n+      }\n+      return result;\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(ioe.getMessage(), ioe);\n+    }\n+  }\n+\n+  public boolean doesMarkerDirExist() throws IOException {\n+    return fs.exists(markerDirPath);\n+  }\n+\n+  public List<String> createdAndMergedDataPaths() throws IOException {\n+    List<String> dataFiles = new LinkedList<>();\n+    FSUtils.processFiles(fs, markerDirPath.toString(), (status) -> {\n+      String pathStr = status.getPath().toString();\n+      if (pathStr.contains(HoodieTableMetaClient.MARKER_EXTN) && !pathStr.endsWith(IOType.APPEND.name())) {", "originalCommit": "37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1MTY2Mw==", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457651663", "bodyText": "yes. we will be performing an upgrade anyway to 0.6.0., which will list the inflight instant at the time of upgrade and then subsequently, write compatible, corresponding marker files", "author": "vinothchandar", "createdAt": "2020-07-20T19:47:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAxOTYzNQ=="}], "type": "inlineReview"}, {"oid": "3495237d4b41f5e9c6bbb79869e2fc9880833dd8", "url": "https://github.com/apache/hudi/commit/3495237d4b41f5e9c6bbb79869e2fc9880833dd8", "message": "Fix compilation issue & 1 unit test failure", "committedDate": "2020-07-20T07:51:12Z", "type": "forcePushed"}, {"oid": "74df857678c8d3c92a18fb1e225fbe27be4da555", "url": "https://github.com/apache/hudi/commit/74df857678c8d3c92a18fb1e225fbe27be4da555", "message": "Fix compilation issue & 1 unit test failure", "committedDate": "2020-07-20T19:40:06Z", "type": "forcePushed"}, {"oid": "c6afcb1b90c733975ced4b3ac42eab2677ecc463", "url": "https://github.com/apache/hudi/commit/c6afcb1b90c733975ced4b3ac42eab2677ecc463", "message": "Fix compilation issue & 1 unit test failure", "committedDate": "2020-07-20T21:59:04Z", "type": "forcePushed"}, {"oid": "6371ace3fdc1ecfe2edb5988c7593b78018c6fa3", "url": "https://github.com/apache/hudi/commit/6371ace3fdc1ecfe2edb5988c7593b78018c6fa3", "message": "Fix compilation issue & 1 unit test failure", "committedDate": "2020-07-20T22:13:32Z", "type": "forcePushed"}, {"oid": "551ce84f5de2f9a861940740adf2cfdca117310f", "url": "https://github.com/apache/hudi/commit/551ce84f5de2f9a861940740adf2cfdca117310f", "message": "Fix compilation issue & 1 unit test failure", "committedDate": "2020-07-20T23:34:18Z", "type": "commit"}, {"oid": "551ce84f5de2f9a861940740adf2cfdca117310f", "url": "https://github.com/apache/hudi/commit/551ce84f5de2f9a861940740adf2cfdca117310f", "message": "Fix compilation issue & 1 unit test failure", "committedDate": "2020-07-20T23:34:18Z", "type": "forcePushed"}, {"oid": "e7ab8f374f1ca32f1b006c4122e929d0fe417039", "url": "https://github.com/apache/hudi/commit/e7ab8f374f1ca32f1b006c4122e929d0fe417039", "message": "Fix small bug in test utils for casing test file correctly", "committedDate": "2020-07-21T03:20:27Z", "type": "commit"}, {"oid": "f36b234eaec7c9e7f577f63f631c69fc1f5d6065", "url": "https://github.com/apache/hudi/commit/f36b234eaec7c9e7f577f63f631c69fc1f5d6065", "message": "Remove unused method on HoodieWriteConfig", "committedDate": "2020-07-21T04:14:02Z", "type": "commit"}]}