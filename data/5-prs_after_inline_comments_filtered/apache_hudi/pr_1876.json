{"pr_number": 1876, "pr_title": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets", "pr_createdAt": "2020-07-25T00:59:17Z", "pr_url": "https://github.com/apache/hudi/pull/1876", "timeline": [{"oid": "b65bbde335ebd64bab5e8b2a2b093f4b54714ce1", "url": "https://github.com/apache/hudi/commit/b65bbde335ebd64bab5e8b2a2b093f4b54714ce1", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>", "committedDate": "2020-07-25T01:00:52Z", "type": "forcePushed"}, {"oid": "06dea397a3f6867f0192da91e53c93267a0560ea", "url": "https://github.com/apache/hudi/commit/06dea397a3f6867f0192da91e53c93267a0560ea", "message": "Code review comments, fixing tests", "committedDate": "2020-07-27T15:32:10Z", "type": "forcePushed"}, {"oid": "14cc2c9f52eff574ca0b0b46cdc9d2692e6f5353", "url": "https://github.com/apache/hudi/commit/14cc2c9f52eff574ca0b0b46cdc9d2692e6f5353", "message": "Code review comments, fixing tests", "committedDate": "2020-07-28T17:26:12Z", "type": "forcePushed"}, {"oid": "1c2b44b8d6f332271527d31d0c9e827799c0a531", "url": "https://github.com/apache/hudi/commit/1c2b44b8d6f332271527d31d0c9e827799c0a531", "message": "Code review comments, fixing tests", "committedDate": "2020-07-28T20:56:09Z", "type": "forcePushed"}, {"oid": "f746c719f752ee8712f8f16a3678d039235a123f", "url": "https://github.com/apache/hudi/commit/f746c719f752ee8712f8f16a3678d039235a123f", "message": "Code review comments, fixing tests", "committedDate": "2020-07-28T23:23:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyNjE5NQ==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r462526195", "bodyText": "[typo] FULL_BOOTRAP_INPUT_PROVIDER -> FULL_BOOTSTRAP_INPUT_PROVIDER", "author": "zhedoubushishi", "createdAt": "2020-07-29T19:11:05Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieBootstrapConfig.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.bootstrap.BootstrapMode;\n+import org.apache.hudi.client.bootstrap.selector.RecordMetadataOnlyBootstrapModeSelector;\n+import org.apache.hudi.client.bootstrap.translator.IdentityBootstrapPathTranslator;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Bootstrap specific configs.\n+ */\n+public class HoodieBootstrapConfig extends DefaultHoodieConfig {\n+\n+  public static final String SOURCE_BASE_PATH_PROP = \"hoodie.bootstrap.base.path\";\n+  public static final String BOOTSTRAP_MODE_SELECTOR = \"hoodie.bootstrap.mode.selector\";\n+  public static final String FULL_BOOTRAP_INPUT_PROVIDER = \"hoodie.bootstrap.full.input.provider\";", "originalCommit": "f746c719f752ee8712f8f16a3678d039235a123f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExNjUzNw==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r464116537", "bodyText": "done", "author": "vinothchandar", "createdAt": "2020-08-02T19:49:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyNjE5NQ=="}], "type": "inlineReview"}, {"oid": "5e60097e290f839889aaf009128f1072e5ae42f7", "url": "https://github.com/apache/hudi/commit/5e60097e290f839889aaf009128f1072e5ae42f7", "message": "Code review comments, fixing tests", "committedDate": "2020-07-29T21:39:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg4OTA4Mw==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r463889083", "bodyText": "@vinothchandar I think we should move this to hudi-spark module. You had this comment on my pull request: https://github.com/apache/hudi/pull/1702/files#r444571139 . Please check out the changes I have done in this class.\nBasically we were running into some schema compatibility issues, in particular with the RecordDataBootstrapProvider. The issue was happening because here we are reading avro schema using parquet utils whereas when we will later perform an upsert etc. then we will get avro schema via the regular path where we use spark-avro to convert spark schema to avro schema. The avro schema obtained from these two different approaches has compatibility issues.\nThus to maintain compatibility what I had to do is:\n\nread parquet schema\nusing spark's parquet-spark schema convertor convert it to spark schema\nusing spark-avro convert spark schema to avro schema\n\nThat is why I had to introduce spark-avro in hudi-client. If you agree with the above suggestion, and do not want spark-avro to be added to hudi-client then I would suggest moving this class to hudi-spark.", "author": "umehrot2", "createdAt": "2020-07-31T23:46:11Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/bootstrap/BootstrapSourceSchemaProvider.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.bootstrap;\n+\n+import org.apache.hudi.avro.model.HoodieFileStatus;\n+import org.apache.hudi.common.bootstrap.FileStatusUtils;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+/**\n+ * Bootstrap Schema Provider. Schema provided in config is used. If not available, use schema from Parquet\n+ */\n+public class BootstrapSourceSchemaProvider {\n+\n+  protected final HoodieWriteConfig writeConfig;\n+\n+  public BootstrapSourceSchemaProvider(HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+  }\n+\n+  /**\n+   * Main API to select avro schema for bootstrapping.\n+   * @param jsc Java Spark Context\n+   * @param partitions  List of partitions with files within them\n+   * @return Avro Schema\n+   */\n+  public final Schema getBootstrapSchema(JavaSparkContext jsc, List<Pair<String, List<HoodieFileStatus>>> partitions) {\n+    if (writeConfig.getSchema() != null) {\n+      // Use schema specified by user if set\n+      return Schema.parse(writeConfig.getSchema());\n+    }\n+    return getBootstrapSourceSchema(jsc, partitions);\n+  }\n+\n+  /**\n+   * Select a random file to be used to generate avro schema.\n+   * Override this method to get custom schema selection.\n+   * @param jsc Java Spark Context\n+   * @param partitions  List of partitions with files within them\n+   * @return Avro Schema\n+   */\n+  protected Schema getBootstrapSourceSchema(JavaSparkContext jsc,\n+      List<Pair<String, List<HoodieFileStatus>>> partitions) {\n+    return partitions.stream().flatMap(p -> p.getValue().stream())\n+        .map(fs -> {\n+          try {\n+            Path filePath = FileStatusUtils.toPath(fs.getPath());\n+            return ParquetUtils.readAvroSchema(jsc.hadoopConfiguration(), filePath);", "originalCommit": "5e60097e290f839889aaf009128f1072e5ae42f7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg4OTg5NQ==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r463889895", "bodyText": "Can you move this to a more common class, or make it public ? I am using this method to generate test data for all my test cases https://github.com/apache/hudi/pull/1702/files#diff-1ba0a227dedf7dfe4aa1b666df01f918", "author": "umehrot2", "createdAt": "2020-07-31T23:50:52Z", "path": "hudi-spark/src/test/java/org/apache/hudi/client/TestBootstrap.java", "diffHunk": "@@ -0,0 +1,586 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.avro.model.HoodieFileStatus;\n+import org.apache.hudi.client.bootstrap.BootstrapMode;\n+import org.apache.hudi.client.bootstrap.RecordDataBootstrapInputProvider;\n+import org.apache.hudi.client.bootstrap.selector.BootstrapModeSelector;\n+import org.apache.hudi.client.bootstrap.selector.RecordDataBootstrapModeSelector;\n+import org.apache.hudi.client.bootstrap.selector.RecordMetadataOnlyBootstrapModeSelector;\n+import org.apache.hudi.common.bootstrap.FileStatusUtils;\n+import org.apache.hudi.common.bootstrap.index.BootstrapIndex;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.RawTripTestPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetReaderIterator;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieBootstrapConfig;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.io.DoubleWritable;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.apache.hudi.keygen.NonpartitionedKeyGenerator;\n+import org.apache.hudi.keygen.SimpleKeyGenerator;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieMergeOnReadTestUtils;\n+import org.apache.parquet.avro.AvroParquetReader;\n+import org.apache.parquet.avro.AvroReadSupport;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.api.java.UDF1;\n+import org.apache.spark.sql.types.DataTypes;\n+\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+import java.io.IOException;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Spliterators;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+import static java.util.stream.Collectors.mapping;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.generateGenericRecord;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.apache.spark.sql.functions.callUDF;\n+\n+/**\n+ * Tests Bootstrap Client functionality.\n+ */\n+public class TestBootstrap extends HoodieClientTestBase {\n+\n+  //FIXME(bootstrap): why is this test so darn slow?\n+\n+  public static final String TRIP_HIVE_COLUMN_TYPES = \"double,string,string,string,double,double,double,double,\"\n+      + \"struct<amount:double,currency:string>,array<struct<amount:double,currency:string>>,boolean\";\n+\n+  @TempDir\n+  public java.nio.file.Path tmpFolder;\n+\n+  protected String srcPath = null;\n+\n+  private HoodieParquetInputFormat roInputFormat;\n+  private JobConf roJobConf;\n+\n+  private HoodieParquetRealtimeInputFormat rtInputFormat;\n+  private JobConf rtJobConf;\n+  private SparkSession spark;\n+\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    srcPath = tmpFolder.toAbsolutePath().toString() + \"/data\";\n+    initPath();\n+    spark = SparkSession.builder()\n+        .appName(\"Bootstrap test\")\n+        .master(\"local[2]\")\n+        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+        .getOrCreate();\n+    jsc = new JavaSparkContext(spark.sparkContext());\n+    sqlContext = spark.sqlContext();\n+    hadoopConf = spark.sparkContext().hadoopConfiguration();\n+    initTestDataGenerator();\n+    initMetaClient();\n+    // initialize parquet input format\n+    reloadInputFormats();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws IOException {\n+    cleanupClients();\n+    cleanupTestDataGenerator();\n+  }\n+\n+  private void reloadInputFormats() {\n+    // initialize parquet input format\n+    roInputFormat = new HoodieParquetInputFormat();\n+    roJobConf = new JobConf(jsc.hadoopConfiguration());\n+    roInputFormat.setConf(roJobConf);\n+\n+    rtInputFormat = new HoodieParquetRealtimeInputFormat();\n+    rtJobConf = new JobConf(jsc.hadoopConfiguration());\n+    rtInputFormat.setConf(rtJobConf);\n+  }\n+\n+  public Schema generateNewDataSetAndReturnSchema(double timestamp, int numRecords, List<String> partitionPaths,\n+      String srcPath) throws Exception {\n+    boolean isPartitioned = partitionPaths != null && !partitionPaths.isEmpty();\n+    Dataset<Row> df = generateTestRawTripDataset(timestamp, numRecords, partitionPaths, jsc, sqlContext);\n+    df.printSchema();\n+    if (isPartitioned) {\n+      df.write().partitionBy(\"datestr\").format(\"parquet\").mode(SaveMode.Overwrite).save(srcPath);\n+    } else {\n+      df.write().format(\"parquet\").mode(SaveMode.Overwrite).save(srcPath);\n+    }\n+    String filePath = FileStatusUtils.toPath(FSUtils.getAllLeafFoldersWithFiles(metaClient.getFs(), srcPath,\n+        (status) -> status.getName().endsWith(\".parquet\")).stream().findAny().map(p -> p.getValue().stream().findAny())\n+        .orElse(null).get().getPath()).toString();\n+    ParquetFileReader reader = ParquetFileReader.open(metaClient.getHadoopConf(), new Path(filePath));\n+    MessageType schema = reader.getFooter().getFileMetaData().getSchema();\n+    return new AvroSchemaConverter().convert(schema);\n+  }\n+\n+  @Test\n+  public void testMetadataBootstrapUnpartitionedCOW() throws Exception {\n+    testBootstrapCommon(false, false, EffectiveMode.METADATA_BOOTSTRAP_MODE);\n+  }\n+\n+  @Test\n+  public void testMetadataBootstrapWithUpdatesCOW() throws Exception {\n+    testBootstrapCommon(true, false, EffectiveMode.METADATA_BOOTSTRAP_MODE);\n+  }\n+\n+  private enum EffectiveMode {\n+    FULL_BOOTSTRAP_MODE,\n+    METADATA_BOOTSTRAP_MODE,\n+    MIXED_BOOTSTRAP_MODE\n+  }\n+\n+  private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, EffectiveMode mode) throws Exception {\n+    if (deltaCommit) {\n+      metaClient = HoodieTestUtils.init(basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    int totalRecords = 100;\n+    String keyGeneratorClass = partitioned ? SimpleKeyGenerator.class.getCanonicalName()\n+        : NonpartitionedKeyGenerator.class.getCanonicalName();\n+    final String bootstrapModeSelectorClass;\n+    final String bootstrapCommitInstantTs;\n+    final boolean checkNumRawFiles;\n+    final boolean isBootstrapIndexCreated;\n+    final int numInstantsAfterBootstrap;\n+    final List<String> bootstrapInstants;\n+    switch (mode) {\n+      case FULL_BOOTSTRAP_MODE:\n+        bootstrapModeSelectorClass = RecordDataBootstrapModeSelector.class.getCanonicalName();\n+        bootstrapCommitInstantTs = HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS;\n+        checkNumRawFiles = false;\n+        isBootstrapIndexCreated = false;\n+        numInstantsAfterBootstrap = 1;\n+        bootstrapInstants = Arrays.asList(bootstrapCommitInstantTs);\n+        break;\n+      case METADATA_BOOTSTRAP_MODE:\n+        bootstrapModeSelectorClass = RecordMetadataOnlyBootstrapModeSelector.class.getCanonicalName();\n+        bootstrapCommitInstantTs = HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS;\n+        checkNumRawFiles = true;\n+        isBootstrapIndexCreated = true;\n+        numInstantsAfterBootstrap = 1;\n+        bootstrapInstants = Arrays.asList(bootstrapCommitInstantTs);\n+        break;\n+      default:\n+        bootstrapModeSelectorClass = TestRandomBootstapModeSelector.class.getName();\n+        bootstrapCommitInstantTs = HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS;\n+        checkNumRawFiles = false;\n+        isBootstrapIndexCreated = true;\n+        numInstantsAfterBootstrap = 2;\n+        bootstrapInstants = Arrays.asList(HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS,\n+            HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS);\n+        break;\n+    }\n+    List<String> partitions = Arrays.asList(\"2020/04/01\", \"2020/04/02\", \"2020/04/03\");\n+    double timestamp = new Double(Instant.now().toEpochMilli()).longValue();\n+    Schema schema = generateNewDataSetAndReturnSchema(timestamp, totalRecords, partitions, srcPath);\n+    HoodieWriteConfig config = getConfigBuilder(schema.toString())\n+        .withAutoCommit(true)\n+        .withSchema(schema.toString())\n+        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n+            .withMaxNumDeltaCommitsBeforeCompaction(1)\n+            .build())\n+        .withBootstrapConfig(HoodieBootstrapConfig.newBuilder()\n+            .withBootstrapSourceBasePath(srcPath)\n+            .withBootstrapKeyGenClass(keyGeneratorClass)\n+            .withFullBootstrapInputProvider(FullTestBootstrapInputProvider.class.getName())\n+            .withBootstrapParallelism(3)\n+            .withBootstrapModeSelector(bootstrapModeSelectorClass).build())\n+        .build();\n+    HoodieWriteClient client = new HoodieWriteClient(jsc, config);\n+    client.bootstrap(Option.empty());\n+    checkBootstrapResults(totalRecords, schema, bootstrapCommitInstantTs, checkNumRawFiles, numInstantsAfterBootstrap,\n+        numInstantsAfterBootstrap, timestamp, timestamp, deltaCommit, bootstrapInstants);\n+\n+    // Rollback Bootstrap\n+    FSUtils.deleteInstantFile(metaClient.getFs(), metaClient.getMetaPath(), new HoodieInstant(State.COMPLETED,\n+        deltaCommit ? HoodieTimeline.DELTA_COMMIT_ACTION : HoodieTimeline.COMMIT_ACTION, bootstrapCommitInstantTs));\n+    client.rollBackInflightBootstrap();\n+    metaClient.reloadActiveTimeline();\n+    assertEquals(0, metaClient.getCommitsTimeline().countInstants());\n+    assertEquals(0L, FSUtils.getAllLeafFoldersWithFiles(metaClient.getFs(), basePath,\n+        (status) -> status.getName().endsWith(\".parquet\")).stream().flatMap(f -> f.getValue().stream()).count());\n+\n+    BootstrapIndex index = BootstrapIndex.getBootstrapIndex(metaClient);\n+    assertFalse(index.isIndexAvailable());\n+\n+    // Run bootstrap again\n+    client = new HoodieWriteClient(jsc, config);\n+    client.bootstrap(Option.empty());\n+\n+    metaClient.reloadActiveTimeline();\n+    index = BootstrapIndex.getBootstrapIndex(metaClient);\n+    if (isBootstrapIndexCreated) {\n+      assertTrue(index.isIndexAvailable());\n+    } else {\n+      assertFalse(index.isIndexAvailable());\n+    }\n+\n+    checkBootstrapResults(totalRecords, schema, bootstrapCommitInstantTs, checkNumRawFiles, numInstantsAfterBootstrap,\n+        numInstantsAfterBootstrap, timestamp, timestamp, deltaCommit, bootstrapInstants);\n+\n+    // Upsert case\n+    double updateTimestamp = new Double(Instant.now().toEpochMilli()).longValue();\n+    String updateSPath = tmpFolder.toAbsolutePath().toString() + \"/data2\";\n+    generateNewDataSetAndReturnSchema(updateTimestamp, totalRecords, partitions, updateSPath);\n+    JavaRDD<HoodieRecord> updateBatch =\n+        generateInputBatch(jsc, FSUtils.getAllLeafFoldersWithFiles(metaClient.getFs(), updateSPath,\n+            (status) -> status.getName().endsWith(\"parquet\")), schema);\n+    String newInstantTs = client.startCommit();\n+    client.upsert(updateBatch, newInstantTs);\n+    checkBootstrapResults(totalRecords, schema, newInstantTs, false, numInstantsAfterBootstrap + 1,\n+        updateTimestamp, deltaCommit ? timestamp : updateTimestamp, deltaCommit);\n+\n+    if (deltaCommit) {\n+      Option<String> compactionInstant = client.scheduleCompaction(Option.empty());\n+      assertTrue(compactionInstant.isPresent());\n+      client.compact(compactionInstant.get());\n+      checkBootstrapResults(totalRecords, schema, compactionInstant.get(), checkNumRawFiles,\n+          numInstantsAfterBootstrap + 2, 2, updateTimestamp, updateTimestamp, !deltaCommit,\n+          Arrays.asList(compactionInstant.get()));\n+    }\n+  }\n+\n+  @Test\n+  public void testMetadataBootstrapWithUpdatesMOR() throws Exception {\n+    testBootstrapCommon(true, true, EffectiveMode.METADATA_BOOTSTRAP_MODE);\n+  }\n+\n+  @Test\n+  public void testFullBoostrapOnlyCOW() throws Exception {\n+    testBootstrapCommon(true, false, EffectiveMode.FULL_BOOTSTRAP_MODE);\n+  }\n+\n+  @Test\n+  public void testFullBootstrapWithUpdatesMOR() throws Exception {\n+    testBootstrapCommon(true, true, EffectiveMode.FULL_BOOTSTRAP_MODE);\n+  }\n+\n+  @Test\n+  public void testMetaAndFullBoostrapCOW() throws Exception {\n+    testBootstrapCommon(true, false, EffectiveMode.MIXED_BOOTSTRAP_MODE);\n+  }\n+\n+  @Test\n+  public void testMetadataAndFullBootstrapWithUpdatesMOR() throws Exception {\n+    testBootstrapCommon(true, true, EffectiveMode.MIXED_BOOTSTRAP_MODE);\n+  }\n+\n+  private void checkBootstrapResults(int totalRecords, Schema schema, String maxInstant, boolean checkNumRawFiles,\n+      int expNumInstants, double expTimestamp, double expROTimestamp, boolean isDeltaCommit) throws Exception {\n+    checkBootstrapResults(totalRecords, schema, maxInstant, checkNumRawFiles, expNumInstants, expNumInstants,\n+        expTimestamp, expROTimestamp, isDeltaCommit, Arrays.asList(maxInstant));\n+  }\n+\n+  private void checkBootstrapResults(int totalRecords, Schema schema, String instant, boolean checkNumRawFiles,\n+      int expNumInstants, int numVersions, double expTimestamp, double expROTimestamp, boolean isDeltaCommit,\n+      List<String> instantsWithValidRecords) throws Exception {\n+    metaClient.reloadActiveTimeline();\n+    assertEquals(expNumInstants, metaClient.getCommitsTimeline().filterCompletedInstants().countInstants());\n+    assertEquals(instant, metaClient.getActiveTimeline()\n+        .getCommitsTimeline().filterCompletedInstants().lastInstant().get().getTimestamp());\n+\n+    Dataset<Row> bootstrapped = sqlContext.read().format(\"parquet\").load(basePath);\n+    Dataset<Row> original = sqlContext.read().format(\"parquet\").load(srcPath);\n+    bootstrapped.registerTempTable(\"bootstrapped\");\n+    original.registerTempTable(\"original\");\n+    if (checkNumRawFiles) {\n+      List<HoodieFileStatus> files = FSUtils.getAllLeafFoldersWithFiles(metaClient.getFs(), srcPath,\n+          (status) -> status.getName().endsWith(\".parquet\"))\n+          .stream().flatMap(x -> x.getValue().stream()).collect(Collectors.toList());\n+      assertEquals(files.size() * numVersions,\n+          sqlContext.sql(\"select distinct _hoodie_file_name from bootstrapped\").count());\n+    }\n+\n+    if (!isDeltaCommit) {\n+      String predicate = String.join(\", \",\n+          instantsWithValidRecords.stream().map(p -> \"\\\"\" + p + \"\\\"\").collect(Collectors.toList()));\n+      assertEquals(totalRecords, sqlContext.sql(\"select * from bootstrapped where _hoodie_commit_time IN \"\n+          + \"(\" + predicate + \")\").count());\n+      Dataset<Row> missingOriginal = sqlContext.sql(\"select a._row_key from original a where a._row_key not \"\n+          + \"in (select _hoodie_record_key from bootstrapped)\");\n+      assertEquals(0, missingOriginal.count());\n+      Dataset<Row> missingBootstrapped = sqlContext.sql(\"select a._hoodie_record_key from bootstrapped a \"\n+          + \"where a._hoodie_record_key not in (select _row_key from original)\");\n+      assertEquals(0, missingBootstrapped.count());\n+      //sqlContext.sql(\"select * from bootstrapped\").show(10, false);\n+    }\n+\n+    // RO Input Format Read\n+    reloadInputFormats();\n+    List<GenericRecord> records = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(\n+        jsc.hadoopConfiguration(),\n+        FSUtils.getAllPartitionPaths(metaClient.getFs(), basePath, false).stream()\n+            .map(f -> basePath + \"/\" + f).collect(Collectors.toList()),\n+        basePath, roJobConf, false, schema, TRIP_HIVE_COLUMN_TYPES, false, new ArrayList<>());\n+    assertEquals(totalRecords, records.size());\n+    Set<String> seenKeys = new HashSet<>();\n+    for (GenericRecord r : records) {\n+      assertEquals(r.get(\"_row_key\").toString(), r.get(\"_hoodie_record_key\").toString(), \"Record :\" + r);\n+      assertEquals(expROTimestamp, ((DoubleWritable)r.get(\"timestamp\")).get(), 0.1, \"Record :\" + r);\n+      assertFalse(seenKeys.contains(r.get(\"_hoodie_record_key\").toString()));\n+      seenKeys.add(r.get(\"_hoodie_record_key\").toString());\n+    }\n+    assertEquals(totalRecords, seenKeys.size());\n+\n+    //RT Input Format Read\n+    reloadInputFormats();\n+    seenKeys = new HashSet<>();\n+    records = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(\n+        jsc.hadoopConfiguration(),\n+        FSUtils.getAllPartitionPaths(metaClient.getFs(), basePath, false).stream()\n+            .map(f -> basePath + \"/\" + f).collect(Collectors.toList()),\n+        basePath, rtJobConf, true, schema,  TRIP_HIVE_COLUMN_TYPES, false, new ArrayList<>());\n+    assertEquals(totalRecords, records.size());\n+    for (GenericRecord r : records) {\n+      assertEquals(r.get(\"_row_key\").toString(), r.get(\"_hoodie_record_key\").toString(), \"Realtime Record :\" + r);\n+      assertEquals(expTimestamp, ((DoubleWritable)r.get(\"timestamp\")).get(),0.1, \"Realtime Record :\" + r);\n+      assertFalse(seenKeys.contains(r.get(\"_hoodie_record_key\").toString()));\n+      seenKeys.add(r.get(\"_hoodie_record_key\").toString());\n+    }\n+    assertEquals(totalRecords, seenKeys.size());\n+\n+    // RO Input Format Read - Project only Hoodie Columns\n+    reloadInputFormats();\n+    records = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(\n+        jsc.hadoopConfiguration(),\n+        FSUtils.getAllPartitionPaths(metaClient.getFs(), basePath, false).stream()\n+            .map(f -> basePath + \"/\" + f).collect(Collectors.toList()),\n+        basePath, roJobConf, false, schema, TRIP_HIVE_COLUMN_TYPES,\n+        true, HoodieRecord.HOODIE_META_COLUMNS);\n+    assertEquals(totalRecords, records.size());\n+    seenKeys = new HashSet<>();\n+    for (GenericRecord r : records) {\n+      assertFalse(seenKeys.contains(r.get(\"_hoodie_record_key\").toString()));\n+      seenKeys.add(r.get(\"_hoodie_record_key\").toString());\n+    }\n+    assertEquals(totalRecords, seenKeys.size());\n+\n+    //RT Input Format Read - Project only Hoodie Columns\n+    reloadInputFormats();\n+    seenKeys = new HashSet<>();\n+    records = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(\n+        jsc.hadoopConfiguration(),\n+        FSUtils.getAllPartitionPaths(metaClient.getFs(), basePath, false).stream()\n+            .map(f -> basePath + \"/\" + f).collect(Collectors.toList()),\n+        basePath, rtJobConf, true, schema,  TRIP_HIVE_COLUMN_TYPES, true,\n+        HoodieRecord.HOODIE_META_COLUMNS);\n+    assertEquals(totalRecords, records.size());\n+    for (GenericRecord r : records) {\n+      assertFalse(seenKeys.contains(r.get(\"_hoodie_record_key\").toString()));\n+      seenKeys.add(r.get(\"_hoodie_record_key\").toString());\n+    }\n+    assertEquals(totalRecords, seenKeys.size());\n+\n+    // RO Input Format Read - Project only non-hoodie column\n+    reloadInputFormats();\n+    records = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(\n+        jsc.hadoopConfiguration(),\n+        FSUtils.getAllPartitionPaths(metaClient.getFs(), basePath, false).stream()\n+            .map(f -> basePath + \"/\" + f).collect(Collectors.toList()),\n+        basePath, roJobConf, false, schema, TRIP_HIVE_COLUMN_TYPES, true,\n+        Arrays.asList(\"_row_key\"));\n+    assertEquals(totalRecords, records.size());\n+    seenKeys = new HashSet<>();\n+    for (GenericRecord r : records) {\n+      assertFalse(seenKeys.contains(r.get(\"_row_key\").toString()));\n+      seenKeys.add(r.get(\"_row_key\").toString());\n+    }\n+    assertEquals(totalRecords, seenKeys.size());\n+\n+    //RT Input Format Read - Project only non-hoodie column\n+    reloadInputFormats();\n+    seenKeys = new HashSet<>();\n+    records = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(\n+        jsc.hadoopConfiguration(),\n+        FSUtils.getAllPartitionPaths(metaClient.getFs(), basePath, false).stream()\n+            .map(f -> basePath + \"/\" + f).collect(Collectors.toList()),\n+        basePath, rtJobConf, true, schema,  TRIP_HIVE_COLUMN_TYPES, true,\n+        Arrays.asList(\"_row_key\"));\n+    assertEquals(totalRecords, records.size());\n+    for (GenericRecord r : records) {\n+      assertFalse(seenKeys.contains(r.get(\"_row_key\").toString()));\n+      seenKeys.add(r.get(\"_row_key\").toString());\n+    }\n+    assertEquals(totalRecords, seenKeys.size());\n+  }\n+\n+  public static class FullTestBootstrapInputProvider extends RecordDataBootstrapInputProvider {\n+\n+    public FullTestBootstrapInputProvider(TypedProperties props, JavaSparkContext jsc) {\n+      super(props, jsc);\n+    }\n+\n+    @Override\n+    public JavaRDD<HoodieRecord> generateInputRecordRDD(String tableName, String sourceBasePath,\n+        List<Pair<String, List<HoodieFileStatus>>> partitionPaths) {\n+      String filePath = FileStatusUtils.toPath(partitionPaths.stream().flatMap(p -> p.getValue().stream())\n+          .findAny().get().getPath()).toString();\n+      ParquetFileReader reader = null;\n+      try {\n+        reader = ParquetFileReader.open(jsc.hadoopConfiguration(), new Path(filePath));\n+      } catch (IOException e) {\n+        throw new HoodieIOException(e.getMessage(), e);\n+      }\n+      MessageType parquetSchema = reader.getFooter().getFileMetaData().getSchema();\n+      Schema schema =  new AvroSchemaConverter().convert(parquetSchema);\n+      return generateInputBatch(jsc, partitionPaths, schema);\n+    }\n+  }\n+\n+  private static JavaRDD<HoodieRecord> generateInputBatch(JavaSparkContext jsc,\n+      List<Pair<String, List<HoodieFileStatus>>> partitionPaths, Schema writerSchema) {\n+    List<Pair<String, Path>> fullFilePathsWithPartition = partitionPaths.stream().flatMap(p -> p.getValue().stream()\n+        .map(x -> Pair.of(p.getKey(), FileStatusUtils.toPath(x.getPath())))).collect(Collectors.toList());\n+    return jsc.parallelize(fullFilePathsWithPartition.stream().flatMap(p -> {\n+      try {\n+        Configuration conf = jsc.hadoopConfiguration();\n+        AvroReadSupport.setAvroReadSchema(conf, writerSchema);\n+        Iterator<GenericRecord> recIterator = new ParquetReaderIterator(\n+            AvroParquetReader.<GenericRecord>builder(p.getValue()).withConf(conf).build());\n+        return StreamSupport.stream(Spliterators.spliteratorUnknownSize(recIterator, 0), false).map(gr -> {\n+          try {\n+            String key = gr.get(\"_row_key\").toString();\n+            String pPath = p.getKey();\n+            return new HoodieRecord<>(new HoodieKey(key, pPath), new RawTripTestPayload(gr.toString(), key, pPath,\n+                HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA));\n+          } catch (IOException e) {\n+            throw new HoodieIOException(e.getMessage(), e);\n+          }\n+        });\n+      } catch (IOException ioe) {\n+        throw new HoodieIOException(ioe.getMessage(), ioe);\n+      }\n+    }).collect(Collectors.toList()));\n+  }\n+\n+  public static class TestRandomBootstapModeSelector extends BootstrapModeSelector {\n+\n+    private int currIdx = new Random().nextInt(2);\n+\n+    public TestRandomBootstapModeSelector(HoodieWriteConfig writeConfig) {\n+      super(writeConfig);\n+    }\n+\n+    @Override\n+    public Map<BootstrapMode, List<String>> select(List<Pair<String, List<HoodieFileStatus>>> partitions) {\n+      List<Pair<BootstrapMode, String>> selections = new ArrayList<>();\n+      partitions.stream().forEach(p -> {\n+        final BootstrapMode mode;\n+        if (currIdx == 0) {\n+          mode = BootstrapMode.RECORD_METADATA_ONLY_BOOTSTRAP;\n+        } else {\n+          mode = BootstrapMode.RECORD_DATA_BOOTSTRAP;\n+        }\n+        currIdx = (currIdx + 1) % 2;\n+        selections.add(Pair.of(mode, p.getKey()));\n+      });\n+      return selections.stream().collect(Collectors.groupingBy(Pair::getKey, mapping(Pair::getValue, toList())));\n+    }\n+  }\n+\n+  public HoodieWriteConfig.Builder getConfigBuilder(String schemaStr) {\n+    HoodieWriteConfig.Builder builder = getConfigBuilder(schemaStr, IndexType.BLOOM)\n+        .withExternalSchemaTrasformation(true);\n+    TypedProperties properties = new TypedProperties();\n+    properties.setProperty(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n+    properties.setProperty(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"datestr\");\n+    builder = builder.withProps(properties);\n+    return builder;\n+  }\n+\n+  private static Dataset<Row> generateTestRawTripDataset(double timestamp, int numRecords, List<String> partitionPaths,", "originalCommit": "5e60097e290f839889aaf009128f1072e5ae42f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTAyMA==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r464679020", "bodyText": "feel free to take this on, in your PR? that way you can make sure it works for you.", "author": "vinothchandar", "createdAt": "2020-08-03T21:45:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg4OTg5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5Mjc5Mg==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r464692792", "bodyText": "No worries. I will take it up.", "author": "umehrot2", "createdAt": "2020-08-03T22:22:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg4OTg5NQ=="}], "type": "inlineReview"}, {"oid": "f9aadfc92dd6d15e0ca087f84a85ca90357320e3", "url": "https://github.com/apache/hudi/commit/f9aadfc92dd6d15e0ca087f84a85ca90357320e3", "message": "Code review comments, fixing tests", "committedDate": "2020-07-31T19:39:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkwNDIyMA==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r463904220", "bodyText": "Can we move this to hudi-spark or hudi-client instead which has spark as a dependency ? For https://issues.apache.org/jira/browse/HUDI-999 I am parallelizing this using spark context, and as we have discussed earlier we do not want spark dependency in hudi-common.", "author": "umehrot2", "createdAt": "2020-08-01T01:34:31Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java", "diffHunk": "@@ -516,4 +528,73 @@ public static Configuration registerFileSystem(Path file, Configuration conf) {\n     return returnConf;\n   }\n \n+  /**\n+   * Get the FS implementation for this table.\n+   * @param path  Path String\n+   * @param hadoopConf  Serializable Hadoop Configuration\n+   * @param consistencyGuardConfig Consistency Guard Config\n+   * @return HoodieWrapperFileSystem\n+   */\n+  public static HoodieWrapperFileSystem getFs(String path, SerializableConfiguration hadoopConf,\n+      ConsistencyGuardConfig consistencyGuardConfig) {\n+    FileSystem fileSystem = FSUtils.getFs(path, hadoopConf.newCopy());\n+    //Preconditions.checkArgument(!(fileSystem instanceof HoodieWrapperFileSystem),\n+    //    \"File System not expected to be that of HoodieWrapperFileSystem\");\n+    return new HoodieWrapperFileSystem(fileSystem,\n+        consistencyGuardConfig.isConsistencyCheckEnabled()\n+            ? new FailSafeConsistencyGuard(fileSystem, consistencyGuardConfig)\n+            : new NoOpConsistencyGuard());\n+  }\n+\n+  /**\n+   * Returns leaf folders with files under a path.\n+   * @param fs  File System\n+   * @param basePathStr Base Path to look for leaf folders\n+   * @param filePathFilter  Filters to skip directories/paths\n+   * @return list of partition paths with files under them.\n+   * @throws IOException\n+   */\n+  public static List<Pair<String, List<HoodieFileStatus>>> getAllLeafFoldersWithFiles(FileSystem fs, String basePathStr,\n+      PathFilter filePathFilter) throws IOException {", "originalCommit": "f9aadfc92dd6d15e0ca087f84a85ca90357320e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE1NTYzOQ==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r464155639", "bodyText": "Done. will move to a BootstrapUtils. not sure why this was in common. good call", "author": "vinothchandar", "createdAt": "2020-08-03T01:46:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkwNDIyMA=="}], "type": "inlineReview"}, {"oid": "bdf9ea883787a81c98164c63059135899d912632", "url": "https://github.com/apache/hudi/commit/bdf9ea883787a81c98164c63059135899d912632", "message": "Cleaning up naming, addressing CR comments", "committedDate": "2020-08-02T16:30:45Z", "type": "forcePushed"}, {"oid": "72e19bd928dad6af203066dc19c424dccb7c1f90", "url": "https://github.com/apache/hudi/commit/72e19bd928dad6af203066dc19c424dccb7c1f90", "message": "Cleaning up naming, addressing CR comments", "committedDate": "2020-08-03T03:28:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYyMDEyMw==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r464620123", "bodyText": "Do you think it's better to relocate this class as a separate class? Like HDFSParquetImporter, HoodieCompactor?", "author": "zhedoubushishi", "createdAt": "2020-08-03T19:34:12Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -627,6 +672,128 @@ private HoodieInstant fetchNextCompactionInstant() throws InterruptedException {\n   }\n \n   public DeltaSyncService getDeltaSyncService() {\n-    return deltaSyncService;\n+    return deltaSyncService.get();\n+  }\n+\n+  /**\n+   * Performs bootstrap from a non-hudi source.\n+   */\n+  public static class BootstrapExecutor  implements Serializable {", "originalCommit": "72e19bd928dad6af203066dc19c424dccb7c1f90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3ODI1Ng==", "url": "https://github.com/apache/hudi/pull/1876#discussion_r464678256", "bodyText": "moved. I was mulling the same.", "author": "vinothchandar", "createdAt": "2020-08-03T21:44:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYyMDEyMw=="}], "type": "inlineReview"}, {"oid": "7827712ba6672661486d3a5ab9cb6e388dc2b2dc", "url": "https://github.com/apache/hudi/commit/7827712ba6672661486d3a5ab9cb6e388dc2b2dc", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>", "committedDate": "2020-08-03T20:31:55Z", "type": "forcePushed"}, {"oid": "f29c9fa8424d721b7ef430a48631a1c6b7b9c080", "url": "https://github.com/apache/hudi/commit/f29c9fa8424d721b7ef430a48631a1c6b7b9c080", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>", "committedDate": "2020-08-03T21:42:33Z", "type": "forcePushed"}, {"oid": "92b1ddbf4b5e9585e5c68ff7b49ca3c005d5fe57", "url": "https://github.com/apache/hudi/commit/92b1ddbf4b5e9585e5c68ff7b49ca3c005d5fe57", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>", "committedDate": "2020-08-03T22:39:55Z", "type": "forcePushed"}, {"oid": "38dc3899d5c40e4b99ba03538182f3cb5ddad9cf", "url": "https://github.com/apache/hudi/commit/38dc3899d5c40e4b99ba03538182f3cb5ddad9cf", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>", "committedDate": "2020-08-03T22:50:03Z", "type": "forcePushed"}, {"oid": "18ee1e598b07e843554eeccfbaf367a7bee6fda4", "url": "https://github.com/apache/hudi/commit/18ee1e598b07e843554eeccfbaf367a7bee6fda4", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>", "committedDate": "2020-08-03T23:43:29Z", "type": "forcePushed"}, {"oid": "5e5f4a92d8195146e9bbd97bce43c705013a8256", "url": "https://github.com/apache/hudi/commit/5e5f4a92d8195146e9bbd97bce43c705013a8256", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>", "committedDate": "2020-08-04T01:42:33Z", "type": "commit"}, {"oid": "5e5f4a92d8195146e9bbd97bce43c705013a8256", "url": "https://github.com/apache/hudi/commit/5e5f4a92d8195146e9bbd97bce43c705013a8256", "message": "[HUDI-242] Support for RFC-12/Bootstrapping of external datasets to hudi\n\n - [HUDI-418] Bootstrap Index Implementation using HFile with unit-test\n - [HUDI-421] FileSystem View Changes to support Bootstrap with unit-tests\n - [HUDI-424] Implement Query Side Integration for querying tables containing bootstrap file slices\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-421] Bootstrap Write Client with tests\n - [HUDI-425] Added HoodieDeltaStreamer support\n - [HUDI-899] Add a knob to change partition-path style while performing metadata bootstrap\n - [HUDI-900] Metadata Bootstrap Key Generator needs to handle complex keys correctly\n - [HUDI-424] Simplify Record reader implementation\n - [HUDI-423] Implement upsert functionality for handling updates to these bootstrap file slices\n - [HUDI-420] Hoodie Demo working with hive and sparkSQL. Also, Hoodie CLI working with bootstrap tables\n\nCo-authored-by: Mehrotra <uditme@amazon.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>", "committedDate": "2020-08-04T01:42:33Z", "type": "forcePushed"}]}