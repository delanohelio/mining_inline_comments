{"pr_number": 1834, "pr_title": "[HUDI-1013] Adding Bulk Insert V2 implementation", "pr_createdAt": "2020-07-15T13:32:12Z", "pr_url": "https://github.com/apache/hudi/pull/1834", "timeline": [{"oid": "a5f608ca3be3922e0e8ca58b556aa406638526ec", "url": "https://github.com/apache/hudi/commit/a5f608ca3be3922e0e8ca58b556aa406638526ec", "message": "Fixing fileId generation within HoodieBulkInsertDataInternalWriter", "committedDate": "2020-07-19T23:52:54Z", "type": "forcePushed"}, {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40", "url": "https://github.com/apache/hudi/commit/e5d4939fc7136e846c3123d2084939f75c6fed40", "message": "Bulk Insert Dataset Base Implementation using Datasource to improve performance", "committedDate": "2020-07-22T11:19:34Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNTkyMg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r458735922", "bodyText": "can we keep this in hudi-spark?", "author": "vinothchandar", "createdAt": "2020-07-22T11:53:27Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieInternalWriteStatus.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+/**\n+ * Hoodie's internal write status used in datasource implementation of bulk insert.\n+ */\n+public class HoodieInternalWriteStatus implements Serializable {", "originalCommit": "e5d4939fc7136e846c3123d2084939f75c6fed40", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTkzOTg2Ng==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r461939866", "bodyText": "so, this needs to be a separate class, because?", "author": "vinothchandar", "createdAt": "2020-07-28T22:56:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNTkyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4Mzc0MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466383740", "bodyText": "will address all feedback together.", "author": "nsivabalan", "createdAt": "2020-08-06T12:42:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNTkyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNjk3OQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r458736979", "bodyText": "we should probably assert that this is not null?", "author": "vinothchandar", "createdAt": "2020-07-22T11:55:28Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -670,7 +670,9 @@ public Builder withPath(String basePath) {\n     }\n \n     public Builder withSchema(String schemaStr) {\n-      props.setProperty(AVRO_SCHEMA, schemaStr);\n+      if (null != schemaStr) {", "originalCommit": "e5d4939fc7136e846c3123d2084939f75c6fed40", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2NjExMQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468766111", "bodyText": "@bvaradar  @nsivabalan why would this be null", "author": "vinothchandar", "createdAt": "2020-08-11T18:03:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNjk3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg1NzIwMQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468857201", "bodyText": "For Bulk Insert V2, we are passing null in createHoodieConfig(...). May be we can change createHoodieConfig() to not call withSchema() for bulk insert V2.", "author": "bvaradar", "createdAt": "2020-08-11T20:51:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNjk3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MDk3OQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468980979", "bodyText": "I did that.", "author": "vinothchandar", "createdAt": "2020-08-12T03:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNjk3OQ=="}], "type": "inlineReview"}, {"oid": "15382beed32f8db95a72d3d5e8fe6bf0d09fcc3a", "url": "https://github.com/apache/hudi/commit/15382beed32f8db95a72d3d5e8fe6bf0d09fcc3a", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow and HoodieRowCreateHandle", "committedDate": "2020-07-24T02:02:50Z", "type": "forcePushed"}, {"oid": "3de1e77efae8e645ffe48176349f86e417d64692", "url": "https://github.com/apache/hudi/commit/3de1e77efae8e645ffe48176349f86e417d64692", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter", "committedDate": "2020-07-24T05:01:33Z", "type": "forcePushed"}, {"oid": "af27762c4ec3291107e14d3ad38d7e07b66f67c3", "url": "https://github.com/apache/hudi/commit/af27762c4ec3291107e14d3ad38d7e07b66f67c3", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter, HoodieDataSourceInternalWriter", "committedDate": "2020-07-24T06:16:07Z", "type": "forcePushed"}, {"oid": "1a11d1c84346383a9b8ad6bfed989cdd1c8b44d8", "url": "https://github.com/apache/hudi/commit/1a11d1c84346383a9b8ad6bfed989cdd1c8b44d8", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter, HoodieDataSourceInternalWriter", "committedDate": "2020-07-24T06:19:05Z", "type": "forcePushed"}, {"oid": "332f78af7c43f8efe78925a9b428cacef4232e9a", "url": "https://github.com/apache/hudi/commit/332f78af7c43f8efe78925a9b428cacef4232e9a", "message": "Adding more java docs and minor fixes in tests", "committedDate": "2020-08-06T11:47:05Z", "type": "forcePushed"}, {"oid": "c9570086ecb6866268b200ae6f41b4ca67065af3", "url": "https://github.com/apache/hudi/commit/c9570086ecb6866268b200ae6f41b4ca67065af3", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance", "committedDate": "2020-08-06T11:48:23Z", "type": "forcePushed"}, {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "url": "https://github.com/apache/hudi/commit/dacd635367b59e7d6b8de91f6b785b337dd851eb", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance", "committedDate": "2020-08-06T11:59:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NTEyMA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466385120", "bodyText": "Note to reviewer: if this statement fails, we had to consider it as global error and not as per record error since we don't have record key yet. This is different from how HoodieRecord write happens. So, in these cases, rowCreateHandle will throw exception and caller is expected to close the rowCreateHandle.", "author": "nsivabalan", "createdAt": "2020-08-06T12:45:21Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieRowCreateHandle.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.model.HoodieInternalRow;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriter;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriterFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * Create handle with InternalRow for datasource implemention of bulk insert.\n+ */\n+public class HoodieRowCreateHandle implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(HoodieRowCreateHandle.class);\n+  private static final AtomicLong SEQGEN = new AtomicLong(1);\n+  private final String instantTime;\n+  private final int taskPartitionId;\n+  private final long taskId;\n+  private final long taskEpochId;\n+  private final HoodieTable table;\n+  private final HoodieWriteConfig writeConfig;\n+  private final HoodieInternalRowFileWriter fileWriter;\n+  private final String partitionPath;\n+  private final Path path;\n+  private final String fileId;\n+  private final FileSystem fs;\n+  private final HoodieInternalWriteStatus writeStatus;\n+  private final HoodieTimer currTimer;\n+\n+  public HoodieRowCreateHandle(HoodieTable table, HoodieWriteConfig writeConfig, String partitionPath, String fileId,\n+      String instantTime, int taskPartitionId, long taskId, long taskEpochId,\n+      StructType structType) {\n+    this.partitionPath = partitionPath;\n+    this.table = table;\n+    this.writeConfig = writeConfig;\n+    this.instantTime = instantTime;\n+    this.taskPartitionId = taskPartitionId;\n+    this.taskId = taskId;\n+    this.taskEpochId = taskEpochId;\n+    this.fileId = fileId;\n+    this.currTimer = new HoodieTimer();\n+    this.currTimer.startTimer();\n+    this.fs = table.getMetaClient().getFs();\n+    this.path = makeNewPath(partitionPath);\n+    this.writeStatus = new HoodieInternalWriteStatus(!table.getIndex().isImplicitWithStorage(),\n+        writeConfig.getWriteStatusFailureFraction());\n+    writeStatus.setPartitionPath(partitionPath);\n+    writeStatus.setFileId(fileId);\n+    try {\n+      HoodiePartitionMetadata partitionMetadata =\n+          new HoodiePartitionMetadata(\n+              fs,\n+              instantTime,\n+              new Path(writeConfig.getBasePath()),\n+              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath));\n+      partitionMetadata.trySave(taskPartitionId);\n+      createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));\n+      this.fileWriter = createNewFileWriter(path, table, writeConfig, structType);\n+    } catch (IOException e) {\n+      throw new HoodieInsertException(\"Failed to initialize file writer for path \" + path, e);\n+    }\n+    LOG.info(\"New handle created for partition :\" + partitionPath + \" with fileId \" + fileId);\n+  }\n+\n+  /**\n+   * Writes an {@link InternalRow} to the underlying HoodieInternalRowFileWriter. Before writing, value for meta columns are computed as required\n+   * and wrapped in {@link HoodieInternalRow}. {@link HoodieInternalRow} is what gets written to HoodieInternalRowFileWriter.\n+   * @param record instance of {@link InternalRow} that needs to be written to the fileWriter.\n+   * @throws IOException\n+   */\n+  public void write(InternalRow record) throws IOException {\n+    try {\n+      String partitionPath = record.getUTF8String(HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NTQ1NQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466385455", "bodyText": "Note to reviewer: these methods are copied from HoodieWriteHandle for now.", "author": "nsivabalan", "createdAt": "2020-08-06T12:46:00Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieRowCreateHandle.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.model.HoodieInternalRow;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriter;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriterFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * Create handle with InternalRow for datasource implemention of bulk insert.\n+ */\n+public class HoodieRowCreateHandle implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(HoodieRowCreateHandle.class);\n+  private static final AtomicLong SEQGEN = new AtomicLong(1);\n+  private final String instantTime;\n+  private final int taskPartitionId;\n+  private final long taskId;\n+  private final long taskEpochId;\n+  private final HoodieTable table;\n+  private final HoodieWriteConfig writeConfig;\n+  private final HoodieInternalRowFileWriter fileWriter;\n+  private final String partitionPath;\n+  private final Path path;\n+  private final String fileId;\n+  private final FileSystem fs;\n+  private final HoodieInternalWriteStatus writeStatus;\n+  private final HoodieTimer currTimer;\n+\n+  public HoodieRowCreateHandle(HoodieTable table, HoodieWriteConfig writeConfig, String partitionPath, String fileId,\n+      String instantTime, int taskPartitionId, long taskId, long taskEpochId,\n+      StructType structType) {\n+    this.partitionPath = partitionPath;\n+    this.table = table;\n+    this.writeConfig = writeConfig;\n+    this.instantTime = instantTime;\n+    this.taskPartitionId = taskPartitionId;\n+    this.taskId = taskId;\n+    this.taskEpochId = taskEpochId;\n+    this.fileId = fileId;\n+    this.currTimer = new HoodieTimer();\n+    this.currTimer.startTimer();\n+    this.fs = table.getMetaClient().getFs();\n+    this.path = makeNewPath(partitionPath);\n+    this.writeStatus = new HoodieInternalWriteStatus(!table.getIndex().isImplicitWithStorage(),\n+        writeConfig.getWriteStatusFailureFraction());\n+    writeStatus.setPartitionPath(partitionPath);\n+    writeStatus.setFileId(fileId);\n+    try {\n+      HoodiePartitionMetadata partitionMetadata =\n+          new HoodiePartitionMetadata(\n+              fs,\n+              instantTime,\n+              new Path(writeConfig.getBasePath()),\n+              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath));\n+      partitionMetadata.trySave(taskPartitionId);\n+      createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));\n+      this.fileWriter = createNewFileWriter(path, table, writeConfig, structType);\n+    } catch (IOException e) {\n+      throw new HoodieInsertException(\"Failed to initialize file writer for path \" + path, e);\n+    }\n+    LOG.info(\"New handle created for partition :\" + partitionPath + \" with fileId \" + fileId);\n+  }\n+\n+  /**\n+   * Writes an {@link InternalRow} to the underlying HoodieInternalRowFileWriter. Before writing, value for meta columns are computed as required\n+   * and wrapped in {@link HoodieInternalRow}. {@link HoodieInternalRow} is what gets written to HoodieInternalRowFileWriter.\n+   * @param record instance of {@link InternalRow} that needs to be written to the fileWriter.\n+   * @throws IOException\n+   */\n+  public void write(InternalRow record) throws IOException {\n+    try {\n+      String partitionPath = record.getUTF8String(HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(\n+          HoodieRecord.PARTITION_PATH_METADATA_FIELD)).toString();\n+      String seqId = HoodieRecord.generateSequenceId(instantTime, taskPartitionId, SEQGEN.getAndIncrement());\n+      String recordKey = record.getUTF8String(HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(\n+          HoodieRecord.RECORD_KEY_METADATA_FIELD)).toString();\n+      HoodieInternalRow internalRow = new HoodieInternalRow(instantTime, seqId, recordKey, partitionPath, path.getName(),\n+          record);\n+      try {\n+        fileWriter.writeRow(recordKey, internalRow);\n+        writeStatus.markSuccess(recordKey);\n+      } catch (Throwable t) {\n+        writeStatus.markFailure(recordKey, t);\n+      }\n+    } catch (Throwable ge) {\n+      writeStatus.setGlobalError(ge);\n+      throw ge;\n+    }\n+  }\n+\n+  /**\n+   * @returns {@code true} if this handle can take in more writes. else {@code false}.\n+   */\n+  public boolean canWrite() {\n+    return fileWriter.canWrite();\n+  }\n+\n+  /**\n+   * Closes the {@link HoodieRowCreateHandle} and returns an instance of {@link HoodieInternalWriteStatus} containing the stats and\n+   * status of the writes to this handle.\n+   * @return the {@link HoodieInternalWriteStatus} containing the stats and status of the writes to this handle.\n+   * @throws IOException\n+   */\n+  public HoodieInternalWriteStatus close() throws IOException {\n+    fileWriter.close();\n+    HoodieWriteStat stat = new HoodieWriteStat();\n+    stat.setPartitionPath(partitionPath);\n+    stat.setNumWrites(writeStatus.getTotalRecords());\n+    stat.setNumDeletes(0);\n+    stat.setNumInserts(writeStatus.getTotalRecords());\n+    stat.setPrevCommit(HoodieWriteStat.NULL_COMMIT);\n+    stat.setFileId(fileId);\n+    stat.setPath(new Path(writeConfig.getBasePath()), path);\n+    long fileSizeInBytes = FSUtils.getFileSize(table.getMetaClient().getFs(), path);\n+    stat.setTotalWriteBytes(fileSizeInBytes);\n+    stat.setFileSizeInBytes(fileSizeInBytes);\n+    stat.setTotalWriteErrors(writeStatus.getFailedRowsSize());\n+    HoodieWriteStat.RuntimeStats runtimeStats = new HoodieWriteStat.RuntimeStats();\n+    runtimeStats.setTotalCreateTime(currTimer.endTimer());\n+    stat.setRuntimeStats(runtimeStats);\n+    writeStatus.setStat(stat);\n+    return writeStatus;\n+  }\n+\n+  public String getFileName() {\n+    return path.getName();\n+  }\n+\n+  private Path makeNewPath(String partitionPath) {\n+    Path path = FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath);\n+    try {\n+      fs.mkdirs(path); // create a new partition as needed.\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to make dir \" + path, e);\n+    }\n+    HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();\n+    return new Path(path.toString(), FSUtils.makeDataFileName(instantTime, getWriteToken(), fileId,\n+        tableConfig.getBaseFileFormat().getFileExtension()));\n+  }\n+\n+  /**\n+   * Creates an empty marker file corresponding to storage writer path.\n+   *\n+   * @param partitionPath Partition path\n+   */\n+  private void createMarkerFile(String partitionPath, String dataFileName) {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NTgwMQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466385801", "bodyText": "Note to reviewer: moved this file to hudi-spark, as this needs access to AvroConversionUtils", "author": "nsivabalan", "createdAt": "2020-08-06T12:46:37Z", "path": "hudi-client/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -1,88 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.keygen;\n-\n-import org.apache.hudi.common.config.TypedProperties;\n-import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.exception.HoodieKeyException;\n-\n-import org.apache.avro.generic.GenericRecord;\n-\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Base class for all the built-in key generators. Contains methods structured for\n- * code reuse amongst them.\n- */\n-public abstract class BuiltinKeyGenerator extends KeyGenerator {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4Njg5Mw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466386893", "bodyText": "Note to reviewer: introduced these new apis for Row based KeyGen. All Built in generators have implemented these. If any user has custom key generator, they don't need to implement these apis if not for \"bulk_insert_dataset\". But if they wish to use \"bulk_insert_dataset\", they might have to give implementations to these methods.", "author": "nsivabalan", "createdAt": "2020-08-06T12:48:36Z", "path": "hudi-client/src/main/java/org/apache/hudi/keygen/KeyGenerator.java", "diffHunk": "@@ -51,4 +53,32 @@ protected KeyGenerator(TypedProperties config) {\n     throw new UnsupportedOperationException(\"Bootstrap not supported for key generator. \"\n         + \"Please override this method in your custom key generator.\");\n   }\n+\n+  /**\n+   * Initializes {@link KeyGenerator} for {@link Row} based operations.\n+   * @param structType structype of the dataset.\n+   * @param structName struct name of the dataset.\n+   * @param recordNamespace record namespace of the dataset.\n+   */\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2OTU5Mg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468769592", "bodyText": "can this be just passed to the getRecordKey() methods or overload a constructor? sticking a random init() method here is not very desirable.\nOverall, this ties the KeyGenerator tightly with Spark. for e.g when we do flink, writing a key generator would require a Spark dependency for a flink job. This need more thought.\ncc @bvaradar @leesf @nsivabalan", "author": "vinothchandar", "createdAt": "2020-08-11T18:09:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4Njg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2NjkyMQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468866921", "bodyText": "yes, we could do that. since HoodieDatasetBulkInsertHelper is the only class calls into getRecordKey(row) and getPartitionPath(Row), it should have access to structype and other args.", "author": "nsivabalan", "createdAt": "2020-08-11T21:10:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4Njg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4ODMyMw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466388323", "bodyText": "Note to reviewer: as mentioned above, if there is some error parsing partition path or record key, it will result in global error for the handle and not per record/row error.\nI couldn't repro/test per record error. I tried writing a different datatype to one of the data column expecting the write to fail, but it didn't fail. So, as of now, there are no tests for per record failures. Same applies to RowFileWriter, InternalWriter etc.", "author": "nsivabalan", "createdAt": "2020-08-06T12:51:03Z", "path": "hudi-client/src/test/java/org/apache/hudi/io/TestHoodieRowCreateHandle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestHarness;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.UUID;\n+\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getInternalRowWithError;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.toInternalRows;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+/**\n+ * Unit tests {@link HoodieRowCreateHandle}.\n+ */\n+public class TestHoodieRowCreateHandle extends HoodieClientTestHarness {\n+\n+  private static final Random RANDOM = new Random();\n+\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initSparkContexts(\"TestHoodieRowCreateHandle\");\n+    initPath();\n+    initFileSystem();\n+    initTestDataGenerator();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  @Test\n+  public void testRowCreateHandle() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieTable.create(metaClient, cfg, hadoopConf);\n+    List<String> fileNames = new ArrayList<>();\n+    List<String> fileAbsPaths = new ArrayList<>();\n+\n+    Dataset<Row> totalInputRows = null;\n+    // one round per partition\n+    for (int i = 0; i < 5; i++) {\n+      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[i % 3];\n+\n+      // init some args\n+      String fileId = UUID.randomUUID().toString();\n+      String instantTime = \"000\";\n+\n+      HoodieRowCreateHandle handle = new HoodieRowCreateHandle(table, cfg, partitionPath, fileId, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n+      int size = 10 + RANDOM.nextInt(1000);\n+      // Generate inputs\n+      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+      if (totalInputRows == null) {\n+        totalInputRows = inputRows;\n+      } else {\n+        totalInputRows = totalInputRows.union(inputRows);\n+      }\n+\n+      // issue writes\n+      HoodieInternalWriteStatus writeStatus = writeAndGetWriteStatus(inputRows, handle);\n+\n+      fileAbsPaths.add(basePath + \"/\" + writeStatus.getStat().getPath());\n+      fileNames.add(handle.getFileName());\n+      // verify output\n+      assertOutput(writeStatus, size, fileId, partitionPath, instantTime, totalInputRows, fileNames, fileAbsPaths);\n+    }\n+  }\n+\n+  /**\n+   * Issue some corrupted or wrong schematized InternalRow after few valid InternalRows so that global error is thrown. write batch 1 of valid records write batch 2 of invalid records Global Error\n+   * should be thrown.\n+   */\n+  @Test\n+  public void testGlobalFailure() throws IOException {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQwMDkyNA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469400924", "bodyText": "@nsivabalan is there a jira tracking this?", "author": "vinothchandar", "createdAt": "2020-08-12T16:50:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4ODMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4OTU1NA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466389554", "bodyText": "Note to reviewer: Can't leverage HoodieTestDataGenerator since each record is expected to be in certain format (meta columns followed by data columns). Hence introduced a new schema for testing \"bulk insert dataset\"", "author": "nsivabalan", "createdAt": "2020-08-06T12:53:20Z", "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieDatasetTestUtils.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+import static org.apache.hudi.common.testutils.FileSystemTestUtils.RANDOM;\n+\n+/**\n+ * Dataset test utils.\n+ */\n+public class HoodieDatasetTestUtils {\n+\n+  public static final StructType STRUCT_TYPE = new StructType(new StructField[] {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQwMTE5Mw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469401193", "bodyText": "we need to clean all this up and make things more generic. its okay for now,", "author": "vinothchandar", "createdAt": "2020-08-12T16:51:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4OTU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM5MDQ4Mw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466390483", "bodyText": "Note to reviewer: Have unified code across Simple and Complex key gens.", "author": "nsivabalan", "createdAt": "2020-08-06T12:54:58Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  private List<String> recordKeyFields;", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODU2NTgzOQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468565839", "bodyText": "you mean having all the variables here? why did we need that change?  Not sure if simple and complex should share though", "author": "vinothchandar", "createdAt": "2020-08-11T13:09:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM5MDQ4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM5MTQzMA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466391430", "bodyText": "Note to reviewer: had to move this to hudi-spark as we need to access AvroConversionUtils for Row to GenericRecord converter function.", "author": "nsivabalan", "createdAt": "2020-08-06T12:56:31Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MTMzMw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466441333", "bodyText": "Note to reviewer: returning -1 only in case  of partition path. So, that  getNestedFieldVal(Row row, List positions) will return DEFAULT_PARTITION_PATH if partition path field is not found.", "author": "nsivabalan", "createdAt": "2020-08-06T14:10:56Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/RowKeyGeneratorHelper.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import scala.Option;\n+\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH;\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH_SEPARATOR;\n+import static org.apache.hudi.keygen.KeyGenUtils.EMPTY_RECORDKEY_PLACEHOLDER;\n+import static org.apache.hudi.keygen.KeyGenUtils.NULL_RECORDKEY_PLACEHOLDER;\n+\n+/**\n+ * Helper class to fetch fields from Row.\n+ */\n+public class RowKeyGeneratorHelper {\n+\n+  /**\n+   * Generates record key for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param recordKeyFields record key fields as a list\n+   * @param recordKeyPositions record key positions for the corresponding record keys in {@code recordKeyFields}\n+   * @param prefixFieldName {@code true} if field name need to be prefixed in the returned result. {@code false} otherwise.\n+   * @return the record key thus generated\n+   */\n+  public static String getRecordKeyFromRow(Row row, List<String> recordKeyFields, Map<String, List<Integer>> recordKeyPositions, boolean prefixFieldName) {\n+    AtomicBoolean keyIsNullOrEmpty = new AtomicBoolean(true);\n+    String toReturn = IntStream.range(0, recordKeyFields.size()).mapToObj(idx -> {\n+      String field = recordKeyFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = recordKeyPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple field\n+        Integer fieldPos = fieldPositions.get(0);\n+        if (row.isNullAt(fieldPos)) {\n+          val = NULL_RECORDKEY_PLACEHOLDER;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = EMPTY_RECORDKEY_PLACEHOLDER;\n+          } else {\n+            keyIsNullOrEmpty.set(false);\n+          }\n+        }\n+      } else { // nested fields\n+        val = getNestedFieldVal(row, recordKeyPositions.get(field)).toString();\n+        if (!val.contains(NULL_RECORDKEY_PLACEHOLDER) && !val.contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          keyIsNullOrEmpty.set(false);\n+        }\n+      }\n+      return prefixFieldName ? (field + \":\" + val) : val;\n+    }).collect(Collectors.joining(\",\"));\n+    if (keyIsNullOrEmpty.get()) {\n+      throw new HoodieKeyException(\"recordKey value: \\\"\" + toReturn + \"\\\" for fields: \\\"\" + Arrays.toString(recordKeyFields.toArray()) + \"\\\" cannot be null or empty.\");\n+    }\n+    return toReturn;\n+  }\n+\n+  /**\n+   * Generates partition path for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param partitionPathFields partition path fields as a list\n+   * @param hiveStylePartitioning {@code true} if hive style partitioning is set. {@code false} otherwise\n+   * @param partitionPathPositions partition path positions for the corresponding fields in {@code partitionPathFields}\n+   * @return the generated partition path for the row\n+   */\n+  public static String getPartitionPathFromRow(Row row, List<String> partitionPathFields, boolean hiveStylePartitioning, Map<String, List<Integer>> partitionPathPositions) {\n+    return IntStream.range(0, partitionPathFields.size()).mapToObj(idx -> {\n+      String field = partitionPathFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = partitionPathPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple\n+        Integer fieldPos = fieldPositions.get(0);\n+        // for partition path, if field is not found, index will be set to -1\n+        if (fieldPos == -1 || row.isNullAt(fieldPos)) {\n+          val = DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = DEFAULT_PARTITION_PATH;\n+          }\n+        }\n+        if (hiveStylePartitioning) {\n+          val = field + \"=\" + val;\n+        }\n+      } else { // nested\n+        Object nestedVal = getNestedFieldVal(row, partitionPathPositions.get(field));\n+        if (nestedVal.toString().contains(NULL_RECORDKEY_PLACEHOLDER) || nestedVal.toString().contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          val = hiveStylePartitioning ? field + \"=\" + DEFAULT_PARTITION_PATH : DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = hiveStylePartitioning ? field + \"=\" + nestedVal.toString() : nestedVal.toString();\n+        }\n+      }\n+      return val;\n+    }).collect(Collectors.joining(DEFAULT_PARTITION_PATH_SEPARATOR));\n+  }\n+\n+  /**\n+   * Fetch the field value located at the positions requested for.\n+   * @param row instance of {@link Row} of interest\n+   * @param positions tree style positions where the leaf node need to be fetched and returned\n+   * @return the field value as per the positions requested for.\n+   */\n+  public static Object getNestedFieldVal(Row row, List<Integer> positions) {\n+    if (positions.size() == 1 && positions.get(0) == -1) {\n+      return DEFAULT_PARTITION_PATH;\n+    }\n+    int index = 0;\n+    int totalCount = positions.size();\n+    Row valueToProcess = row;\n+    Object toReturn = null;\n+\n+    while (index < totalCount) {\n+      if (index < totalCount - 1) {\n+        if (valueToProcess.isNullAt(positions.get(index))) {\n+          toReturn = NULL_RECORDKEY_PLACEHOLDER;\n+          break;\n+        }\n+        valueToProcess = (Row) valueToProcess.get(positions.get(index));\n+      } else { // last index\n+        if (valueToProcess.getAs(positions.get(index)).toString().isEmpty()) {\n+          toReturn = EMPTY_RECORDKEY_PLACEHOLDER;\n+          break;\n+        }\n+        toReturn = valueToProcess.getAs(positions.get(index));\n+      }\n+      index++;\n+    }\n+    return toReturn;\n+  }\n+\n+  /**\n+   * Generate the tree style positions for the field requested for as per the defined struct type.\n+   * @param structType schema of interest\n+   * @param field field of interest for which the positions are requested for\n+   * @param isRecordKey {@code true} if the field requested for is a record key. {@code false} incase of a partition path.\n+   * @return the positions of the field as per the struct type.\n+   */\n+  public static List<Integer> getNestedFieldIndices(StructType structType, String field, boolean isRecordKey) {\n+    String[] slices = field.split(\"\\\\.\");\n+    List<Integer> positions = new ArrayList<>();\n+    int index = 0;\n+    int totalCount = slices.length;\n+    while (index < totalCount) {\n+      String slice = slices[index];\n+      Option<Object> curIndexOpt = structType.getFieldIndex(slice);\n+      if (curIndexOpt.isDefined()) {\n+        int curIndex = (int) curIndexOpt.get();\n+        positions.add(curIndex);\n+        final StructField nestedField = structType.fields()[curIndex];\n+        if (index < totalCount - 1) {\n+          if (!(nestedField.dataType() instanceof StructType)) {\n+            if (isRecordKey) {\n+              throw new HoodieKeyException(\"Nested field should be of type StructType \" + nestedField);\n+            } else {\n+              positions = Collections.singletonList(-1);", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MTk3MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466441971", "bodyText": "Note to reviewer: getNestedFieldIndices(StructType structType, String field, boolean isRecordKey) will return -1 for partitionPathIndices if partition path field is not found.", "author": "nsivabalan", "createdAt": "2020-08-06T14:11:51Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/RowKeyGeneratorHelper.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import scala.Option;\n+\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH;\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH_SEPARATOR;\n+import static org.apache.hudi.keygen.KeyGenUtils.EMPTY_RECORDKEY_PLACEHOLDER;\n+import static org.apache.hudi.keygen.KeyGenUtils.NULL_RECORDKEY_PLACEHOLDER;\n+\n+/**\n+ * Helper class to fetch fields from Row.\n+ */\n+public class RowKeyGeneratorHelper {\n+\n+  /**\n+   * Generates record key for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param recordKeyFields record key fields as a list\n+   * @param recordKeyPositions record key positions for the corresponding record keys in {@code recordKeyFields}\n+   * @param prefixFieldName {@code true} if field name need to be prefixed in the returned result. {@code false} otherwise.\n+   * @return the record key thus generated\n+   */\n+  public static String getRecordKeyFromRow(Row row, List<String> recordKeyFields, Map<String, List<Integer>> recordKeyPositions, boolean prefixFieldName) {\n+    AtomicBoolean keyIsNullOrEmpty = new AtomicBoolean(true);\n+    String toReturn = IntStream.range(0, recordKeyFields.size()).mapToObj(idx -> {\n+      String field = recordKeyFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = recordKeyPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple field\n+        Integer fieldPos = fieldPositions.get(0);\n+        if (row.isNullAt(fieldPos)) {\n+          val = NULL_RECORDKEY_PLACEHOLDER;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = EMPTY_RECORDKEY_PLACEHOLDER;\n+          } else {\n+            keyIsNullOrEmpty.set(false);\n+          }\n+        }\n+      } else { // nested fields\n+        val = getNestedFieldVal(row, recordKeyPositions.get(field)).toString();\n+        if (!val.contains(NULL_RECORDKEY_PLACEHOLDER) && !val.contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          keyIsNullOrEmpty.set(false);\n+        }\n+      }\n+      return prefixFieldName ? (field + \":\" + val) : val;\n+    }).collect(Collectors.joining(\",\"));\n+    if (keyIsNullOrEmpty.get()) {\n+      throw new HoodieKeyException(\"recordKey value: \\\"\" + toReturn + \"\\\" for fields: \\\"\" + Arrays.toString(recordKeyFields.toArray()) + \"\\\" cannot be null or empty.\");\n+    }\n+    return toReturn;\n+  }\n+\n+  /**\n+   * Generates partition path for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param partitionPathFields partition path fields as a list\n+   * @param hiveStylePartitioning {@code true} if hive style partitioning is set. {@code false} otherwise\n+   * @param partitionPathPositions partition path positions for the corresponding fields in {@code partitionPathFields}\n+   * @return the generated partition path for the row\n+   */\n+  public static String getPartitionPathFromRow(Row row, List<String> partitionPathFields, boolean hiveStylePartitioning, Map<String, List<Integer>> partitionPathPositions) {\n+    return IntStream.range(0, partitionPathFields.size()).mapToObj(idx -> {\n+      String field = partitionPathFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = partitionPathPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple\n+        Integer fieldPos = fieldPositions.get(0);\n+        // for partition path, if field is not found, index will be set to -1\n+        if (fieldPos == -1 || row.isNullAt(fieldPos)) {\n+          val = DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = DEFAULT_PARTITION_PATH;\n+          }\n+        }\n+        if (hiveStylePartitioning) {\n+          val = field + \"=\" + val;\n+        }\n+      } else { // nested\n+        Object nestedVal = getNestedFieldVal(row, partitionPathPositions.get(field));\n+        if (nestedVal.toString().contains(NULL_RECORDKEY_PLACEHOLDER) || nestedVal.toString().contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          val = hiveStylePartitioning ? field + \"=\" + DEFAULT_PARTITION_PATH : DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = hiveStylePartitioning ? field + \"=\" + nestedVal.toString() : nestedVal.toString();\n+        }\n+      }\n+      return val;\n+    }).collect(Collectors.joining(DEFAULT_PARTITION_PATH_SEPARATOR));\n+  }\n+\n+  /**\n+   * Fetch the field value located at the positions requested for.\n+   * @param row instance of {@link Row} of interest\n+   * @param positions tree style positions where the leaf node need to be fetched and returned\n+   * @return the field value as per the positions requested for.\n+   */\n+  public static Object getNestedFieldVal(Row row, List<Integer> positions) {\n+    if (positions.size() == 1 && positions.get(0) == -1) {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MjcxNA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466442714", "bodyText": "Note to reviewer: no changes here. just moved code to a private method for re-use", "author": "nsivabalan", "createdAt": "2020-08-06T14:12:51Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -129,45 +134,54 @@ public String getPartitionPath(GenericRecord record) {\n     if (partitionVal == null) {\n       partitionVal = 1L;\n     }\n+    try {\n+      return getPartitionPath(partitionVal);\n+    } catch (Exception e) {\n+      throw new HoodieDeltaStreamerException(\"Unable to parse input partition field :\" + partitionVal, e);\n+    }\n+  }\n \n+  /**\n+   * Parse and fetch partition path based on data type.\n+   *\n+   * @param partitionVal partition path object value fetched from record/row\n+   * @return the parsed partition path based on data type\n+   * @throws ParseException on any parse exception\n+   */\n+  private String getPartitionPath(Object partitionVal) throws ParseException {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MTE1Mg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468781152", "bodyText": "need to look at this line-by-line again and see if its all good.", "author": "vinothchandar", "createdAt": "2020-08-11T18:30:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MjcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0NTk4Mg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466445982", "bodyText": "Note to reviewer: here is the only place where we test abort for Datasource path. We couldn't test it elsewhere (TestHoodieRowCreateHandle, TestHoodieInternalRowParquetWriter, TestHoodieBulkInsertDataInternalWriter)", "author": "nsivabalan", "createdAt": "2020-08-06T14:17:46Z", "path": "hudi-spark/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.testutils.HoodieClientTestHarness;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.toInternalRows;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalWriter}.\n+ */\n+public class TestHoodieDataSourceInternalWriter extends HoodieClientTestHarness {\n+\n+  private static final Random RANDOM = new Random();\n+\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initSparkContexts(\"TestHoodieDataSourceInternalWriter\");\n+    initPath();\n+    initFileSystem();\n+    initTestDataGenerator();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  @Test\n+  public void testDataSourceWriter() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n+        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, WriteOperationType.BULK_INSERT_DATASET);\n+    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n+\n+    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n+    List<String> partitionPathsAbs = new ArrayList<>();\n+    for (String partitionPath : partitionPaths) {\n+      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n+    }\n+\n+    int size = 10 + RANDOM.nextInt(1000);\n+    int batches = 5;\n+    Dataset<Row> totalInputRows = null;\n+\n+    for (int j = 0; j < batches; j++) {\n+      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n+      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+      writeRows(inputRows, writer);\n+      if (totalInputRows == null) {\n+        totalInputRows = inputRows;\n+      } else {\n+        totalInputRows = totalInputRows.union(inputRows);\n+      }\n+    }\n+\n+    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n+    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n+    commitMessages.add(commitMetadata);\n+    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n+    metaClient.reloadActiveTimeline();\n+    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n+    // verify output\n+    assertOutput(totalInputRows, result, instantTime);\n+    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n+  }\n+\n+  @Test\n+  public void testMultipleDataSourceWrites() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    int partitionCounter = 0;\n+\n+    // execute N rounds\n+    for (int i = 0; i < 5; i++) {\n+      String instantTime = \"00\" + i;\n+      // init writer\n+      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n+          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, WriteOperationType.BULK_INSERT_DATASET);\n+\n+      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n+      Dataset<Row> totalInputRows = null;\n+      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n+\n+      int size = 10 + RANDOM.nextInt(1000);\n+      int batches = 5; // one batch per partition\n+\n+      for (int j = 0; j < batches; j++) {\n+        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n+        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+        writeRows(inputRows, writer);\n+        if (totalInputRows == null) {\n+          totalInputRows = inputRows;\n+        } else {\n+          totalInputRows = totalInputRows.union(inputRows);\n+        }\n+      }\n+\n+      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n+      commitMessages.add(commitMetadata);\n+      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n+      metaClient.reloadActiveTimeline();\n+\n+      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n+\n+      // verify output\n+      assertOutput(totalInputRows, result, instantTime);\n+      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n+    }\n+  }\n+\n+  @Test\n+  public void testLargeWrites() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    int partitionCounter = 0;\n+\n+    // execute N rounds\n+    for (int i = 0; i < 3; i++) {\n+      String instantTime = \"00\" + i;\n+      // init writer\n+      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n+          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, WriteOperationType.BULK_INSERT_DATASET);\n+\n+      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n+      Dataset<Row> totalInputRows = null;\n+      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n+\n+      int size = 10000 + RANDOM.nextInt(10000);\n+      int batches = 3; // one batch per partition\n+\n+      for (int j = 0; j < batches; j++) {\n+        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n+        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+        writeRows(inputRows, writer);\n+        if (totalInputRows == null) {\n+          totalInputRows = inputRows;\n+        } else {\n+          totalInputRows = totalInputRows.union(inputRows);\n+        }\n+      }\n+\n+      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n+      commitMessages.add(commitMetadata);\n+      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n+      metaClient.reloadActiveTimeline();\n+\n+      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n+\n+      // verify output\n+      assertOutput(totalInputRows, result, instantTime);\n+      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n+    }\n+  }\n+\n+  /**\n+   * Tests that DataSourceWriter.abort() will abort the written records of interest write and commit batch1 write and abort batch2 Read of entire dataset should show only records from batch1.\n+   * commit batch1\n+   * abort batch2\n+   * verify only records from batch1 is available to read\n+   */\n+  @Test\n+  public void testAbort() throws IOException {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0Njk5Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466446997", "bodyText": "Note to reviewer: I am yet to add tests to these new methods. Got these as part of rebase. Also, I notice few other test classes for each key generators after rebasing. Will add tests by tmrw to those new test classes.", "author": "nsivabalan", "createdAt": "2020-08-06T14:19:16Z", "path": "hudi-spark/src/test/java/org/apache/hudi/keygen/TestTimestampBasedKeyGenerator.java", "diffHunk": "@@ -116,20 +165,26 @@ public void testScalar() throws IOException {\n \n     // timezone is GMT\n     properties = getBaseKeyConfig(\"SCALAR\", \"yyyy-MM-dd hh\", \"GMT\", \"days\");\n-    HoodieKey hk5 = new TimestampBasedKeyGenerator(properties).getKey(baseRecord);\n+    TimestampBasedKeyGenerator keyGen = new TimestampBasedKeyGenerator(properties);\n+    HoodieKey hk5 = keyGen.getKey(baseRecord);\n     assertEquals(hk5.getPartitionPath(), \"2024-10-04 12\");\n+\n+    // test w/ Row\n+    baseRow = genericRecordToRow(baseRecord);\n+    keyGen.initializeRowKeyGenerator(structType, testStructName, testNamespace);\n+    assertEquals(\"2024-10-04 12\", keyGen.getPartitionPathFromRow(baseRow));\n   }\n \n   @Test\n   public void test_ExpectsMatch_SingleInputFormat_ISO8601WithMsZ_OutputTimezoneAsUTC() throws IOException {\n     baseRecord.put(\"createTime\", \"2020-04-01T13:01:33.428Z\");\n     properties = this.getBaseKeyConfig(\n-      \"DATE_STRING\",\n-      \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",\n-      \"\",\n-      \"\",\n-      \"yyyyMMddHH\",\n-      \"GMT\");\n+        \"DATE_STRING\",", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQwMTc2Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469401767", "bodyText": "@nsivabalan this is done?", "author": "vinothchandar", "createdAt": "2020-08-12T16:52:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0Njk5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3NTQ5OQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468175499", "bodyText": "one issue we need to think about is how we abstract the key generators out, so that even flink etc can use tthis? ideally we need to templatize GenericRecord, Row. this needs more thought. potentially beyond the scope of this PR", "author": "vinothchandar", "createdAt": "2020-08-10T20:45:18Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java", "diffHunk": "@@ -54,12 +51,17 @@ public String getPartitionPath(GenericRecord record) {\n   }\n \n   @Override\n-  public List<String> getRecordKeyFields() {\n-    return recordKeyFields;\n+  public List<String> getPartitionPathFields() {\n+    return new ArrayList<>();\n   }\n \n   @Override\n-  public List<String> getPartitionPathFields() {\n-    return new ArrayList<>();\n+  public String getRecordKeyFromRow(Row row) {\n+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), getRecordKeyPositions(), true);\n+  }\n+\n+  @Override\n+  public String getPartitionPathFromRow(Row row) {", "originalCommit": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODUxMjc0OQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468512749", "bodyText": "yes, makes sense. we will revisit after 0.6.0 release.", "author": "nsivabalan", "createdAt": "2020-08-11T11:33:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3NTQ5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQwMjE2Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469402167", "bodyText": "follow up JIRA for 0.6.1 please :)", "author": "vinothchandar", "createdAt": "2020-08-12T16:52:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3NTQ5OQ=="}], "type": "inlineReview"}, {"oid": "06e969339fa52f9bd3ffb77ab26b5677143b458a", "url": "https://github.com/apache/hudi/commit/06e969339fa52f9bd3ffb77ab26b5677143b458a", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance", "committedDate": "2020-08-11T06:47:48Z", "type": "forcePushed"}, {"oid": "d491a32d85239b30b4321023a5001979b12c6abb", "url": "https://github.com/apache/hudi/commit/d491a32d85239b30b4321023a5001979b12c6abb", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance", "committedDate": "2020-08-11T11:18:57Z", "type": "commit"}, {"oid": "d491a32d85239b30b4321023a5001979b12c6abb", "url": "https://github.com/apache/hudi/commit/d491a32d85239b30b4321023a5001979b12c6abb", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance", "committedDate": "2020-08-11T11:18:57Z", "type": "forcePushed"}, {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "url": "https://github.com/apache/hudi/commit/5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures", "committedDate": "2020-08-11T16:06:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NDg3Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468744877", "bodyText": "note to self: need to understand this better and see if we can simplify", "author": "vinothchandar", "createdAt": "2020-08-11T17:26:32Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2ODMwNw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468868307", "bodyText": "responded elsewhere. we could move this to getRecordKey(Row) and getPartitionPath(Row) if need be.", "author": "nsivabalan", "createdAt": "2020-08-11T21:13:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NDg3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NTk1Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468745957", "bodyText": "as far as I can tell, this is private and set to null by default and not assigned anywhere else. so we will never pass if (null != ..) check. I think this should be if (null ==converterFn) if the intention was lazy initialization.", "author": "vinothchandar", "createdAt": "2020-08-11T17:28:23Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4NTc1MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468885750", "bodyText": "hmmm, not sure on this. I will reconcile w/ Balaji on this.", "author": "nsivabalan", "createdAt": "2020-08-11T21:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NTk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMDgxMw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468900813", "bodyText": "guess, this could be a bug. just now realizing, we don't have tests for this. we have tests for all built in key generators, but not for this. Will get it done by tonight. sorry to have missed.", "author": "nsivabalan", "createdAt": "2020-08-11T22:30:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NTk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0OTY0MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468749640", "bodyText": "got this when compiling\nError:(433, 16) overloaded method value commit with alternatives:\n  (x$1: String,x$2: java.util.List[org.apache.hudi.common.model.HoodieWriteStat],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean <and>\n  (x$1: String,x$2: org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean\n cannot be applied to (String, org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus], org.apache.hudi.common.util.Option[java.util.HashMap[String,String]])\n        client.commit(instantTime, writeStatuses,", "author": "vinothchandar", "createdAt": "2020-08-11T17:34:45Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -95,20 +95,20 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n       Option<Map<String, String>> extraMetadata) {\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n-    return commit(instantTime, writeStatuses, extraMetadata, metaClient.getCommitActionType());\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStat(instantTime, stats, extraMetadata);\n   }\n \n-  private boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata, String actionType) {\n-\n+  // fixme(bulkinsertv2) this name is ughh\n+  public boolean commitStat(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1MDI0MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468750240", "bodyText": "Looks like we cannot avoid a new public API. so might as well rename", "author": "vinothchandar", "createdAt": "2020-08-11T17:35:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0OTY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468755967", "bodyText": "note to self : check if this is indeed correct. i was expecting us to do something like row.setNullAt(i-5)", "author": "vinothchandar", "createdAt": "2020-08-11T17:45:44Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.model;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Internal Row implementation for Hoodie Row. It wraps an {@link InternalRow} and keeps meta columns locally. But the {@link InternalRow}\n+ * does include the meta columns as well just that {@link HoodieInternalRow} will intercept queries for meta columns and serve from its\n+ * copy rather than fetching from {@link InternalRow}.\n+ */\n+public class HoodieInternalRow extends InternalRow {\n+\n+  private String commitTime;\n+  private String commitSeqNumber;\n+  private String recordKey;\n+  private String partitionPath;\n+  private String fileName;\n+  private InternalRow row;\n+\n+  public HoodieInternalRow(String commitTime, String commitSeqNumber, String recordKey, String partitionPath,\n+      String fileName, InternalRow row) {\n+    this.commitTime = commitTime;\n+    this.commitSeqNumber = commitSeqNumber;\n+    this.recordKey = recordKey;\n+    this.partitionPath = partitionPath;\n+    this.fileName = fileName;\n+    this.row = row;\n+  }\n+\n+  @Override\n+  public int numFields() {\n+    return row.numFields();\n+  }\n+\n+  @Override\n+  public void setNullAt(int i) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = null;\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = null;\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = null;\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = null;\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = null;\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.setNullAt(i);", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg3OTE4MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468879180", "bodyText": "even I had the same doubt when I start reviewing this at first. thats why added some java docs for this class. row will have meta columns as well. just that meta columns will not be fetched from the row but from instance variables in this class. @bvaradar did some analysis before arriving at this", "author": "nsivabalan", "createdAt": "2020-08-11T21:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MDM5NA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468970394", "bodyText": "I think this is because row already has these metafields per se in the schema", "author": "vinothchandar", "createdAt": "2020-08-12T02:34:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2MTA4NQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468761085", "bodyText": "rename to getMetaColumnVal", "author": "vinothchandar", "createdAt": "2020-08-11T17:54:22Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.model;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Internal Row implementation for Hoodie Row. It wraps an {@link InternalRow} and keeps meta columns locally. But the {@link InternalRow}\n+ * does include the meta columns as well just that {@link HoodieInternalRow} will intercept queries for meta columns and serve from its\n+ * copy rather than fetching from {@link InternalRow}.\n+ */\n+public class HoodieInternalRow extends InternalRow {\n+\n+  private String commitTime;\n+  private String commitSeqNumber;\n+  private String recordKey;\n+  private String partitionPath;\n+  private String fileName;\n+  private InternalRow row;\n+\n+  public HoodieInternalRow(String commitTime, String commitSeqNumber, String recordKey, String partitionPath,\n+      String fileName, InternalRow row) {\n+    this.commitTime = commitTime;\n+    this.commitSeqNumber = commitSeqNumber;\n+    this.recordKey = recordKey;\n+    this.partitionPath = partitionPath;\n+    this.fileName = fileName;\n+    this.row = row;\n+  }\n+\n+  @Override\n+  public int numFields() {\n+    return row.numFields();\n+  }\n+\n+  @Override\n+  public void setNullAt(int i) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = null;\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = null;\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = null;\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = null;\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = null;\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.setNullAt(i);\n+    }\n+  }\n+\n+  @Override\n+  public void update(int i, Object value) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = value.toString();\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = value.toString();\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = value.toString();\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = value.toString();\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = value.toString();\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.update(i, value);\n+    }\n+  }\n+\n+  private String getHoodieColumnVal(int ordinal) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468768261", "bodyText": "should we be hardcoding these?", "author": "vinothchandar", "createdAt": "2020-08-11T18:06:51Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieRowParquetWriteSupport.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.HoodieDynamicBoundedBloomFilter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.HashMap;\n+\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MAX_RECORD_KEY_FOOTER;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MIN_RECORD_KEY_FOOTER;\n+\n+/**\n+ * Hoodie Write Support for directly writing Row to Parquet.\n+ */\n+public class HoodieRowParquetWriteSupport extends ParquetWriteSupport {\n+\n+  private Configuration hadoopConf;\n+  private BloomFilter bloomFilter;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieRowParquetWriteSupport(Configuration conf, StructType structType, BloomFilter bloomFilter) {\n+    super();\n+    Configuration hadoopConf = new Configuration(conf);\n+    hadoopConf.set(\"spark.sql.parquet.writeLegacyFormat\", \"false\");", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4NTE5NQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468885195", "bodyText": "Nope. we need to fix this. The built in ParquetWriteSupport expects these two params to be set. I will double check once again to ensure this.", "author": "nsivabalan", "createdAt": "2020-08-11T21:51:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkzNDkyMg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468934922", "bodyText": "Check lines 94 to 104 https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala . Or was your ask just about hardcoding these configs.", "author": "nsivabalan", "createdAt": "2020-08-12T00:19:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MTQ4Mw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468981483", "bodyText": "yes. why we are hardcoding this. any ideas @bvaradar ?", "author": "vinothchandar", "createdAt": "2020-08-12T03:19:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MDYwMw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468770603", "bodyText": "this is a misleading name. Need to rename this. its unclear if it refers to a hoodie dataset or a spark dataset framework.", "author": "vinothchandar", "createdAt": "2020-08-11T18:11:13Z", "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieDatasetTestUtils.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+import static org.apache.hudi.common.testutils.FileSystemTestUtils.RANDOM;\n+\n+/**\n+ * Dataset test utils.\n+ */\n+public class HoodieDatasetTestUtils {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MTUyNw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468771527", "bodyText": "Need to understand why this is needed. so, we pick a different mode for the writer path I believe. We should use a config and not overload further if possible.", "author": "vinothchandar", "createdAt": "2020-08-11T18:12:50Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -35,6 +35,7 @@\n   // bulk insert\n   BULK_INSERT(\"bulk_insert\"),\n   BULK_INSERT_PREPPED(\"bulk_insert_prepped\"),\n+  BULK_INSERT_DATASET(\"bulk_insert_dataset\"),", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4NTg5Mg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468985892", "bodyText": "removing this. it was easy enough.", "author": "vinothchandar", "createdAt": "2020-08-12T03:37:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MTUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MjY1OQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468772659", "bodyText": "another general rule of thumb. we could always review our own diffs again before submitting to make sure whitespace changes are all intentional. cc @nsivabalan .", "author": "vinothchandar", "createdAt": "2020-08-11T18:14:46Z", "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -267,26 +258,26 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doDeleteOperation(HoodieWriteClient client, JavaRDD<HoodieKey> hoodieKeys,\n-                                                       String instantTime) {\n+      String instantTime) {\n     return client.delete(hoodieKeys, instantTime);\n   }\n \n   public static HoodieRecord createHoodieRecord(GenericRecord gr, Comparable orderingVal, HoodieKey hKey,\n-                                                String payloadClass) throws IOException {\n+      String payloadClass) throws IOException {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MzM4MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468773381", "bodyText": "is thre a way to avoid using positions and use names instead?", "author": "vinothchandar", "createdAt": "2020-08-11T18:15:58Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MTgxNw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468881817", "bodyText": "yes, you could do that. I vaguely remember running into some issues and then I went with positions. Don't remember exactly. Might have to code it up to check.", "author": "nsivabalan", "createdAt": "2020-08-11T21:43:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MzM4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkzMjU3OA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468932578", "bodyText": "StructType is just the schema and for recordKey fields and partition paths, we parse the structType and store the chain of positions (if nested). don't think we get away without storing positions.", "author": "nsivabalan", "createdAt": "2020-08-12T00:10:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MzM4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Mzk5MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468773991", "bodyText": "Also, this being in BuiltinKeyGenerator and not KeyGenerator is a problem and will break all the custom key generators out there when they turn on row based writing, correct? should we move this up?", "author": "vinothchandar", "createdAt": "2020-08-11T18:17:09Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MjY1Mw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468882653", "bodyText": "When I was doing the rebase, I saw getRecordKeyFieldNames in KeyGenerator was throwing UnsupportedOperationException. Hence went with the same for these methods too. Before rebase, we had this in KeyGenerator only. So didn't want to move this w/o consulting w/ you.", "author": "nsivabalan", "createdAt": "2020-08-11T21:45:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Mzk5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMTQxOA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468901418", "bodyText": "on 2nd thought, yes, it makes sense to move this to KeyGenerator. and thats why we had the default impl of re-using getRecord(). So that all existing customers can still leverage bulk insert w/ dataset.", "author": "nsivabalan", "createdAt": "2020-08-11T22:32:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Mzk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Njc1Ng==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468776756", "bodyText": "might be good to assert this out in the constructor itself", "author": "vinothchandar", "createdAt": "2020-08-11T18:22:02Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java", "diffHunk": "@@ -55,21 +51,22 @@ public SimpleKeyGenerator(TypedProperties props, String partitionPathField) {\n \n   @Override\n   public String getRecordKey(GenericRecord record) {\n-    return KeyGenUtils.getRecordKey(record, recordKeyField);\n+    return KeyGenUtils.getRecordKey(record, getRecordKeyFields().get(0));", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MzY2MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468883660", "bodyText": "We wanted to have the same behavior as getKey(). we don't throw exception in constructor if record key is not found. we throw only when getKey(GenericRecord record) is called.", "author": "nsivabalan", "createdAt": "2020-08-11T21:47:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Njc1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MzkwMQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468783901", "bodyText": "is the .get(0) really fine?", "author": "vinothchandar", "createdAt": "2020-08-11T18:35:18Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -177,4 +191,26 @@ private long convertLongTimeToMillis(Long partitionVal) {\n     }\n     return MILLISECONDS.convert(partitionVal, timeUnit);\n   }\n+\n+  @Override\n+  public String getRecordKey(Row row) {\n+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), getRecordKeyPositions(), false);\n+  }\n+\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    Object fieldVal = null;\n+    Object partitionPathFieldVal =  RowKeyGeneratorHelper.getNestedFieldVal(row, getPartitionPathPositions().get(getPartitionPathFields().get(0)));", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4NDM1MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468884350", "bodyText": "yes, this extends from SimpleKeyGenerator. Also,  we have a special case for partition path if incase we don't find the field. couldn't find a better way to do it. position will return -1 and when parsing for actual Row, we will return DEFAULT_PARTITION_PATH.", "author": "nsivabalan", "createdAt": "2020-08-11T21:49:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MzkwMQ=="}], "type": "inlineReview"}, {"oid": "95a71fe33accac60ec54c0c312691e229646fa47", "url": "https://github.com/apache/hudi/commit/95a71fe33accac60ec54c0c312691e229646fa47", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures", "committedDate": "2020-08-12T00:57:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MzY4MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468973681", "bodyText": "This line is different from what I see before this patch. It is an optimization, but just to be safe, we can keep it as is.", "author": "nsivabalan", "createdAt": "2020-08-12T02:47:39Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java", "diffHunk": "@@ -22,30 +22,27 @@\n import org.apache.hudi.common.config.TypedProperties;\n \n import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n-import java.util.stream.Collectors;\n \n /**\n- * Key generator for deletes using global indices. Global index deletes do not require partition value\n- * so this key generator avoids using partition value for generating HoodieKey.\n+ * Key generator for deletes using global indices. Global index deletes do not require partition value so this key generator avoids using partition value for generating HoodieKey.\n  */\n public class GlobalDeleteKeyGenerator extends BuiltinKeyGenerator {\n \n   private static final String EMPTY_PARTITION = \"\";\n \n-  protected final List<String> recordKeyFields;\n-\n   public GlobalDeleteKeyGenerator(TypedProperties config) {\n     super(config);\n-    this.recordKeyFields = Arrays.stream(config.getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY()).split(\",\")).map(String::trim).collect(Collectors.toList());\n+    this.recordKeyFields = Arrays.asList(config.getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY()).split(\",\"));", "originalCommit": "95a71fe33accac60ec54c0c312691e229646fa47", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTI2OQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468975269", "bodyText": "note to reviewer: removed the outer try catch and moved it to the caller. Except that, no other code changes.", "author": "nsivabalan", "createdAt": "2020-08-12T02:53:53Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -125,49 +130,58 @@ public TimestampBasedKeyGenerator(TypedProperties config, String partitionPathFi\n \n   @Override\n   public String getPartitionPath(GenericRecord record) {\n-    Object partitionVal = HoodieAvroUtils.getNestedFieldVal(record, partitionPathField, true);\n+    Object partitionVal = HoodieAvroUtils.getNestedFieldVal(record, getPartitionPathFields().get(0), true);\n     if (partitionVal == null) {\n       partitionVal = 1L;\n     }\n+    try {\n+      return getPartitionPath(partitionVal);\n+    } catch (Exception e) {\n+      throw new HoodieDeltaStreamerException(\"Unable to parse input partition field :\" + partitionVal, e);\n+    }\n+  }\n \n+  /**\n+   * Parse and fetch partition path based on data type.\n+   *\n+   * @param partitionVal partition path object value fetched from record/row\n+   * @return the parsed partition path based on data type\n+   * @throws ParseException on any parse exception\n+   */\n+  private String getPartitionPath(Object partitionVal) throws ParseException {\n     DateTimeFormatter partitionFormatter = DateTimeFormat.forPattern(outputDateFormat);\n     if (this.outputDateTimeZone != null) {\n       partitionFormatter = partitionFormatter.withZone(outputDateTimeZone);\n     }\n-\n-    try {\n-      long timeMs;\n-      if (partitionVal instanceof Double) {\n-        timeMs = convertLongTimeToMillis(((Double) partitionVal).longValue());\n-      } else if (partitionVal instanceof Float) {\n-        timeMs = convertLongTimeToMillis(((Float) partitionVal).longValue());\n-      } else if (partitionVal instanceof Long) {\n-        timeMs = convertLongTimeToMillis((Long) partitionVal);\n-      } else if (partitionVal instanceof CharSequence) {\n-        DateTime parsedDateTime = inputFormatter.parseDateTime(partitionVal.toString());\n-        if (this.outputDateTimeZone == null) {\n-          // Use the timezone that came off the date that was passed in, if it had one\n-          partitionFormatter = partitionFormatter.withZone(parsedDateTime.getZone());\n-        }\n-\n-        timeMs = inputFormatter.parseDateTime(partitionVal.toString()).getMillis();\n-      } else {\n-        throw new HoodieNotSupportedException(\n-            \"Unexpected type for partition field: \" + partitionVal.getClass().getName());\n+    long timeMs;", "originalCommit": "95a71fe33accac60ec54c0c312691e229646fa47", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NjAzNA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469296034", "bodyText": "there could be some bug here. if field is not found in structType. I fixed it in RowKeyGeneratorHelper for nested fields, but missed it here.\nsomething like\n .forEach(f ->\n        {\n          if (structType.getFieldIndex(f).isDefined()) {\n            recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n          } else {\n            throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n          }\n        });", "author": "nsivabalan", "createdAt": "2020-08-12T14:20:49Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));", "originalCommit": "95a71fe33accac60ec54c0c312691e229646fa47", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NjQ0NA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469296444", "bodyText": "same here. please note that for recordKey, we throw an exception, where as for partitionpath, we might need to return DEFAULT_PARTITION_PATH.\n     .forEach(f -> {\n              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\t            if (structType.getFieldIndex(f).isDefined()) {\n              partitionPathPositions.put(f,\n                  Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n            } else {\n              partitionPathPositions.put(f, Collections.singletonList(-1));\n            }\n          });", "author": "nsivabalan", "createdAt": "2020-08-12T14:21:24Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,", "originalCommit": "95a71fe33accac60ec54c0c312691e229646fa47", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NzE1Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469297157", "bodyText": "would be good to add a precondition check here that init has been called.", "author": "nsivabalan", "createdAt": "2020-08-12T14:22:22Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "originalCommit": "95a71fe33accac60ec54c0c312691e229646fa47", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NzIxOQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469297219", "bodyText": "would be good to add a precondition check here that init has been called.", "author": "nsivabalan", "createdAt": "2020-08-12T14:22:28Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {\n+      converterFn = AvroConversionHelper.createConverterToAvro(structType, structName, recordNamespace);\n+    }\n+    GenericRecord genericRecord = (GenericRecord) converterFn.apply(row);\n+    return getKey(genericRecord).getRecordKey();\n+  }\n+\n+  /**\n+   * Fetch partition path from {@link Row}.\n+   * @param row instance of {@link Row} from which partition path is requested\n+   * @return the partition path of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    if (null != converterFn) {", "originalCommit": "95a71fe33accac60ec54c0c312691e229646fa47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5ODcyNw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469298727", "bodyText": "if you plan to add such precondition, ensure all built in key generators call it since they might override getRecordKey(Row) and getPartitionPath(Row).\n  protected void preConditionCheckForRowInit(){\n    if(!isRowInitCalled()){\n      throw new IllegalStateException(\"KeyGenerator#initializeRowKeyGenerator should have been invoked before this method \");\n    }\n  }", "author": "nsivabalan", "createdAt": "2020-08-12T14:24:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NzIxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5OTUwOQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469299509", "bodyText": "Can we switch this to Exception to be in sync up with GenericRecord behavior.", "author": "nsivabalan", "createdAt": "2020-08-12T14:25:30Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -177,4 +191,26 @@ private long convertLongTimeToMillis(Long partitionVal) {\n     }\n     return MILLISECONDS.convert(partitionVal, timeUnit);\n   }\n+\n+  @Override\n+  public String getRecordKey(Row row) {\n+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), getRecordKeyPositions(), false);\n+  }\n+\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    Object fieldVal = null;\n+    Object partitionPathFieldVal =  RowKeyGeneratorHelper.getNestedFieldVal(row, getPartitionPathPositions().get(getPartitionPathFields().get(0)));\n+    try {\n+      if (partitionPathFieldVal.toString().contains(DEFAULT_PARTITION_PATH) || partitionPathFieldVal.toString().contains(NULL_RECORDKEY_PLACEHOLDER)\n+          || partitionPathFieldVal.toString().contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+        fieldVal = 1L;\n+      } else {\n+        fieldVal = partitionPathFieldVal;\n+      }\n+      return getPartitionPath(fieldVal);\n+    } catch (ParseException e) {", "originalCommit": "95a71fe33accac60ec54c0c312691e229646fa47", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "06c1370f3805ba3667ea0f61f414c21787762772", "url": "https://github.com/apache/hudi/commit/06c1370f3805ba3667ea0f61f414c21787762772", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures", "committedDate": "2020-08-12T15:23:44Z", "type": "commit"}, {"oid": "06c1370f3805ba3667ea0f61f414c21787762772", "url": "https://github.com/apache/hudi/commit/06c1370f3805ba3667ea0f61f414c21787762772", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures", "committedDate": "2020-08-12T15:23:44Z", "type": "forcePushed"}, {"oid": "383a74578defdcb82a5dddde079e65899f6d60f0", "url": "https://github.com/apache/hudi/commit/383a74578defdcb82a5dddde079e65899f6d60f0", "message": "Some fixes to key generators and adding more tests for Row apis", "committedDate": "2020-08-12T16:06:11Z", "type": "commit"}, {"oid": "fefd4b7618aa47b8f2c3e9a28b898b5a713d88e6", "url": "https://github.com/apache/hudi/commit/fefd4b7618aa47b8f2c3e9a28b898b5a713d88e6", "message": "Cleaning up config placements, naming", "committedDate": "2020-08-12T20:39:33Z", "type": "commit"}, {"oid": "c8745f856c4407f2eb17143237ae3a7655a43cfb", "url": "https://github.com/apache/hudi/commit/c8745f856c4407f2eb17143237ae3a7655a43cfb", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs", "committedDate": "2020-08-13T04:11:08Z", "type": "forcePushed"}, {"oid": "5dc8182ec308dba7ffd04ef159bd3041ede1b117", "url": "https://github.com/apache/hudi/commit/5dc8182ec308dba7ffd04ef159bd3041ede1b117", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs", "committedDate": "2020-08-13T04:38:05Z", "type": "forcePushed"}, {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "url": "https://github.com/apache/hudi/commit/865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs", "committedDate": "2020-08-13T05:16:14Z", "type": "commit"}, {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "url": "https://github.com/apache/hudi/commit/865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs", "committedDate": "2020-08-13T05:16:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTkyMjExOA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469922118", "bodyText": "may I know where is the structType being used ? AvroConversionHelper.createConverterToAvro used row.Schema() and so we may not need it. probably we should rename this to boolean positionMapInitialized.", "author": "nsivabalan", "createdAt": "2020-08-13T12:42:55Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -85,71 +84,40 @@ public final HoodieKey getKey(GenericRecord record) {\n     }).collect(Collectors.toList());\n   }\n \n-  @Override\n-  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n-    // parse simple feilds\n-    getRecordKeyFields().stream()\n-        .filter(f -> !(f.contains(\".\")))\n-        .forEach(f -> {\n-          if (structType.getFieldIndex(f).isDefined()) {\n-            recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n-          } else {\n-            throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n-          }\n-        });\n-    // parse nested fields\n-    getRecordKeyFields().stream()\n-        .filter(f -> f.contains(\".\"))\n-        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n-    // parse simple fields\n-    if (getPartitionPathFields() != null) {\n-      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+  void buildFieldPositionMapIfNeeded(StructType structType) {\n+    if (this.structType == null) {\n+      // parse simple fields\n+      getRecordKeyFields().stream()\n+          .filter(f -> !(f.contains(\".\")))\n           .forEach(f -> {\n             if (structType.getFieldIndex(f).isDefined()) {\n-              partitionPathPositions.put(f,\n-                  Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n+              recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n             } else {\n-              partitionPathPositions.put(f, Collections.singletonList(-1));\n+              throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n             }\n           });\n       // parse nested fields\n-      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n-          .forEach(f -> partitionPathPositions.put(f,\n-              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n-    }\n-    this.structName = structName;\n-    this.structType = structType;\n-    this.recordNamespace = recordNamespace;\n-  }\n-\n-  /**\n-   * Fetch record key from {@link Row}.\n-   *\n-   * @param row instance of {@link Row} from which record key is requested.\n-   * @return the record key of interest from {@link Row}.\n-   */\n-  @Override\n-  public String getRecordKey(Row row) {\n-    if (null == converterFn) {\n-      converterFn = AvroConversionHelper.createConverterToAvro(structType, structName, recordNamespace);\n+      getRecordKeyFields().stream()\n+          .filter(f -> f.contains(\".\"))\n+          .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+      // parse simple fields\n+      if (getPartitionPathFields() != null) {\n+        getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+            .forEach(f -> {\n+              if (structType.getFieldIndex(f).isDefined()) {\n+                partitionPathPositions.put(f,\n+                    Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n+              } else {\n+                partitionPathPositions.put(f, Collections.singletonList(-1));\n+              }\n+            });\n+        // parse nested fields\n+        getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+            .forEach(f -> partitionPathPositions.put(f,\n+                RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+      }\n+      this.structType = structType;", "originalCommit": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}