{"pr_number": 1924, "pr_title": "[HUDI-999][Performance] Parallelize fetching of bootstrap source data files/partitions", "pr_createdAt": "2020-08-06T02:08:25Z", "pr_url": "https://github.com/apache/hudi/pull/1924", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIxNjQ2OA==", "url": "https://github.com/apache/hudi/pull/1924#discussion_r466216468", "bodyText": "we had some very similar code for marker dir listing? can we see if we can reuse some code here across them?", "author": "vinothchandar", "createdAt": "2020-08-06T07:56:44Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/bootstrap/BootstrapUtils.java", "diffHunk": "@@ -41,37 +48,87 @@\n    * Returns leaf folders with files under a path.\n    * @param fs  File System\n    * @param basePathStr Base Path to look for leaf folders\n-   * @param filePathFilter  Filters to skip directories/paths\n+   * @param jsc Java spark context\n    * @return list of partition paths with files under them.\n    * @throws IOException\n    */\n   public static List<Pair<String, List<HoodieFileStatus>>> getAllLeafFoldersWithFiles(FileSystem fs, String basePathStr,\n-                                                                                      PathFilter filePathFilter) throws IOException {\n+      JavaSparkContext jsc) throws IOException {\n     final Path basePath = new Path(basePathStr);\n     final Map<Integer, List<String>> levelToPartitions = new HashMap<>();\n     final Map<String, List<HoodieFileStatus>> partitionToFiles = new HashMap<>();\n-    FSUtils.processFiles(fs, basePathStr, (status) -> {\n-      if (status.isFile() && filePathFilter.accept(status.getPath())) {\n-        String relativePath = FSUtils.getRelativePartitionPath(basePath, status.getPath().getParent());\n-        List<HoodieFileStatus> statusList = partitionToFiles.get(relativePath);\n-        if (null == statusList) {\n-          Integer level = (int) relativePath.chars().filter(ch -> ch == '/').count();\n-          List<String> dirs = levelToPartitions.get(level);\n-          if (null == dirs) {\n-            dirs = new ArrayList<>();\n-            levelToPartitions.put(level, dirs);\n+    PathFilter filePathFilter = getFilePathFilter();\n+    PathFilter metaPathFilter = getExcludeMetaPathFilter();\n+\n+    FileStatus[] topLevelStatuses = fs.listStatus(new Path(basePathStr));\n+    List<String> subDirectories = new ArrayList<>();\n+\n+    List<Pair<HoodieFileStatus, Pair<Integer, String>>> result = new ArrayList<>();", "originalCommit": "3ffe5600fb6b9ae27680e33c0ffb16436571ff11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3OTQ0Mw==", "url": "https://github.com/apache/hudi/pull/1924#discussion_r466779443", "bodyText": "Only the outer structure is a bit similar in terms of first listing and taking action on top level files, and then using spark context to perform the same action on sub-directories in parallel. But the inner logic is different and values being collected are different.\nIf we really want to re-use the common outer logic, it would require exploring extracting out the inner logic into serializable functions that would work fine with spark context as well. So, to not over-complicate this PR I can explore this separately if its okay. I have created a new Jira https://issues.apache.org/jira/browse/HUDI-1158 where I have listed the two optimizations we discussed about w.r.t to parallel listing behavior:\n\nThe parallelization should be at leaf partition directory level and not just at the top directory level\nExtract out common code paths", "author": "umehrot2", "createdAt": "2020-08-07T02:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIxNjQ2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg1MDkwMw==", "url": "https://github.com/apache/hudi/pull/1924#discussion_r466850903", "bodyText": "Sounds good . @umehrot2", "author": "vinothchandar", "createdAt": "2020-08-07T06:41:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIxNjQ2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIxNzE2OA==", "url": "https://github.com/apache/hudi/pull/1924#discussion_r466217168", "bodyText": "can we just use the table's base file format here?", "author": "vinothchandar", "createdAt": "2020-08-06T07:57:57Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/bootstrap/BootstrapUtils.java", "diffHunk": "@@ -41,37 +48,87 @@\n    * Returns leaf folders with files under a path.\n    * @param fs  File System\n    * @param basePathStr Base Path to look for leaf folders\n-   * @param filePathFilter  Filters to skip directories/paths\n+   * @param jsc Java spark context\n    * @return list of partition paths with files under them.\n    * @throws IOException\n    */\n   public static List<Pair<String, List<HoodieFileStatus>>> getAllLeafFoldersWithFiles(FileSystem fs, String basePathStr,\n-                                                                                      PathFilter filePathFilter) throws IOException {\n+      JavaSparkContext jsc) throws IOException {\n     final Path basePath = new Path(basePathStr);\n     final Map<Integer, List<String>> levelToPartitions = new HashMap<>();\n     final Map<String, List<HoodieFileStatus>> partitionToFiles = new HashMap<>();\n-    FSUtils.processFiles(fs, basePathStr, (status) -> {\n-      if (status.isFile() && filePathFilter.accept(status.getPath())) {\n-        String relativePath = FSUtils.getRelativePartitionPath(basePath, status.getPath().getParent());\n-        List<HoodieFileStatus> statusList = partitionToFiles.get(relativePath);\n-        if (null == statusList) {\n-          Integer level = (int) relativePath.chars().filter(ch -> ch == '/').count();\n-          List<String> dirs = levelToPartitions.get(level);\n-          if (null == dirs) {\n-            dirs = new ArrayList<>();\n-            levelToPartitions.put(level, dirs);\n+    PathFilter filePathFilter = getFilePathFilter();\n+    PathFilter metaPathFilter = getExcludeMetaPathFilter();\n+\n+    FileStatus[] topLevelStatuses = fs.listStatus(new Path(basePathStr));\n+    List<String> subDirectories = new ArrayList<>();\n+\n+    List<Pair<HoodieFileStatus, Pair<Integer, String>>> result = new ArrayList<>();\n+\n+    for (FileStatus topLevelStatus: topLevelStatuses) {\n+      if (topLevelStatus.isFile() && filePathFilter.accept(topLevelStatus.getPath())) {\n+        String relativePath = FSUtils.getRelativePartitionPath(basePath, topLevelStatus.getPath().getParent());\n+        Integer level = (int) relativePath.chars().filter(ch -> ch == '/').count();\n+        HoodieFileStatus hoodieFileStatus = FileStatusUtils.fromFileStatus(topLevelStatus);\n+        result.add(Pair.of(hoodieFileStatus, Pair.of(level, relativePath)));\n+      } else if (metaPathFilter.accept(topLevelStatus.getPath())) {\n+        subDirectories.add(topLevelStatus.getPath().toString());\n+      }\n+    }\n+\n+    if (subDirectories.size() > 0) {\n+      result.addAll(jsc.parallelize(subDirectories, subDirectories.size()).flatMap(directory -> {\n+        PathFilter pathFilter = getFilePathFilter();\n+        Path path = new Path(directory);\n+        FileSystem fileSystem = path.getFileSystem(new Configuration());\n+        RemoteIterator<LocatedFileStatus> itr = fileSystem.listFiles(path, true);\n+        List<Pair<HoodieFileStatus, Pair<Integer, String>>> res = new ArrayList<>();\n+        while (itr.hasNext()) {\n+          FileStatus status = itr.next();\n+          if (pathFilter.accept(status.getPath())) {\n+            String relativePath = FSUtils.getRelativePartitionPath(new Path(basePathStr), status.getPath().getParent());\n+            Integer level = (int) relativePath.chars().filter(ch -> ch == '/').count();\n+            HoodieFileStatus hoodieFileStatus = FileStatusUtils.fromFileStatus(status);\n+            res.add(Pair.of(hoodieFileStatus, Pair.of(level, relativePath)));\n           }\n-          dirs.add(relativePath);\n-          statusList = new ArrayList<>();\n-          partitionToFiles.put(relativePath, statusList);\n         }\n-        statusList.add(FileStatusUtils.fromFileStatus(status));\n+        return res.iterator();\n+      }).collect());\n+    }\n+\n+    result.forEach(val -> {\n+      String relativePath = val.getRight().getRight();\n+      List<HoodieFileStatus> statusList = partitionToFiles.get(relativePath);\n+      if (null == statusList) {\n+        Integer level = val.getRight().getLeft();\n+        List<String> dirs = levelToPartitions.get(level);\n+        if (null == dirs) {\n+          dirs = new ArrayList<>();\n+          levelToPartitions.put(level, dirs);\n+        }\n+        dirs.add(relativePath);\n+        statusList = new ArrayList<>();\n+        partitionToFiles.put(relativePath, statusList);\n       }\n-      return true;\n-    }, true);\n+      statusList.add(val.getLeft());\n+    });\n+\n     OptionalInt maxLevelOpt = levelToPartitions.keySet().stream().mapToInt(x -> x).max();\n     int maxLevel = maxLevelOpt.orElse(-1);\n     return maxLevel >= 0 ? levelToPartitions.get(maxLevel).stream()\n-        .map(d -> Pair.of(d, partitionToFiles.get(d))).collect(Collectors.toList()) : new ArrayList<>();\n+            .map(d -> Pair.of(d, partitionToFiles.get(d))).collect(Collectors.toList()) : new ArrayList<>();\n+  }\n+\n+  private static PathFilter getFilePathFilter() {\n+    return (path) -> {\n+      // TODO: Needs to be abstracted out when supporting different formats\n+      // TODO: Remove hoodieFilter\n+      return path.getName().endsWith(HoodieFileFormat.PARQUET.getFileExtension());", "originalCommit": "3ffe5600fb6b9ae27680e33c0ffb16436571ff11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3NDgxMA==", "url": "https://github.com/apache/hudi/pull/1924#discussion_r466774810", "bodyText": "Done.", "author": "umehrot2", "createdAt": "2020-08-07T01:43:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIxNzE2OA=="}], "type": "inlineReview"}, {"oid": "301059d8c95349c0c29bd152120fb91822f5c7d0", "url": "https://github.com/apache/hudi/commit/301059d8c95349c0c29bd152120fb91822f5c7d0", "message": "[RFC-12] Parallelize fetching of source data files/partitions", "committedDate": "2020-08-07T01:40:12Z", "type": "commit"}, {"oid": "301059d8c95349c0c29bd152120fb91822f5c7d0", "url": "https://github.com/apache/hudi/commit/301059d8c95349c0c29bd152120fb91822f5c7d0", "message": "[RFC-12] Parallelize fetching of source data files/partitions", "committedDate": "2020-08-07T01:40:12Z", "type": "forcePushed"}]}