{"pr_number": 2196, "pr_title": "[HUDI-1349]spark sql support overwrite use replace action", "pr_createdAt": "2020-10-22T02:30:21Z", "pr_url": "https://github.com/apache/hudi/pull/2196", "timeline": [{"oid": "ce7aa10fea97363fc24f14ec881761c5a9ffbd58", "url": "https://github.com/apache/hudi/commit/ce7aa10fea97363fc24f14ec881761c5a9ffbd58", "message": "[HUDI-1349]spark sql support overwrite use replace action", "committedDate": "2020-10-22T07:14:10Z", "type": "forcePushed"}, {"oid": "8170a29b994ac4304783decb4aee47b6d0d26fbc", "url": "https://github.com/apache/hudi/commit/8170a29b994ac4304783decb4aee47b6d0d26fbc", "message": "[HUDI-1349]spark sql support overwrite use replace action", "committedDate": "2020-10-22T13:05:21Z", "type": "forcePushed"}, {"oid": "fc15e303e7c40fcff8f560e19183089766da43e6", "url": "https://github.com/apache/hudi/commit/fc15e303e7c40fcff8f560e19183089766da43e6", "message": "[HUDI-1349] spark sql support overwrite use replace action", "committedDate": "2020-10-22T14:00:42Z", "type": "forcePushed"}, {"oid": "28a335ccf774e473ff1a1d42d4459b2f25ed04dd", "url": "https://github.com/apache/hudi/commit/28a335ccf774e473ff1a1d42d4459b2f25ed04dd", "message": "[HUDI-1349] spark sql support overwrite use replace action", "committedDate": "2020-10-23T16:17:10Z", "type": "forcePushed"}, {"oid": "17bfea4716c4ece0b45c8867f848bc937939cd1c", "url": "https://github.com/apache/hudi/commit/17bfea4716c4ece0b45c8867f848bc937939cd1c", "message": "[HUDI-1349] spark sql support overwrite use replace action", "committedDate": "2020-10-30T16:01:27Z", "type": "forcePushed"}, {"oid": "ec48e375bf0f3967fe33bcddfafc9dda9bccc7e7", "url": "https://github.com/apache/hudi/commit/ec48e375bf0f3967fe33bcddfafc9dda9bccc7e7", "message": "[HUDI-1349] spark sql support overwrite use insert_overwrite_table", "committedDate": "2020-11-15T15:20:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQzMDg1Nw==", "url": "https://github.com/apache/hudi/pull/2196#discussion_r526430857", "bodyText": "This can be time consuming for large tables. Is it possible to parallelize this?\n\nget all partitions and then\nenginecontext.parallelize(#paritions)\nfor each partition, get all latest file ids\n\nI think we dont need to add new filesystem apis with this approach", "author": "satishkotha", "createdAt": "2020-11-18T21:25:33Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  public SparkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,\n+                                                       HoodieWriteConfig config, HoodieTable table,\n+                                                       String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(context, config, table, instantTime, inputRecordsRDD, WriteOperationType.INSERT_OVERWRITE_TABLE);\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(JavaRDD<WriteStatus> writeStatuses) {\n+    Map<String, List<String>> result =  table.getSliceView().getLatestFileSlices().distinct()", "originalCommit": "305f3208abf34f57c47c5795da7ebb49ad66f430", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg0OTkxNw==", "url": "https://github.com/apache/hudi/pull/2196#discussion_r533849917", "bodyText": "Can we use function style here instead of needing to case to PairFunction ? Something like partitionPathRdd.mapToPair( () -> ...) ?", "author": "n3nash", "createdAt": "2020-12-02T02:15:12Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFunction;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  public SparkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,\n+                                                       HoodieWriteConfig config, HoodieTable table,\n+                                                       String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(context, config, table, instantTime, inputRecordsRDD, WriteOperationType.INSERT_OVERWRITE_TABLE);\n+  }\n+\n+  protected List<String> getAllExistingFileIds(String partitionPath) {\n+    return table.getSliceView().getLatestFileSlices(partitionPath)\n+        .map(fg -> fg.getFileId()).distinct().collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(JavaRDD<WriteStatus> writeStatuses) {\n+    Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();\n+    try {\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(table.getMetaClient().getFs(),\n+          table.getMetaClient().getBasePath(), false);\n+      JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+      if (partitionPaths != null && partitionPaths.size() > 0) {\n+        context.setJobStatus(this.getClass().getSimpleName(), \"Getting ExistingFileIds of all partitions\");\n+        JavaRDD<String> partitionPathRdd = jsc.parallelize(partitionPaths, partitionPaths.size());\n+        partitionToExistingFileIds = partitionPathRdd.mapToPair((PairFunction<String, String, List<String>>)", "originalCommit": "0da0ca747eb297f21e3e51cbd571cc8a680b18b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI3Njc2OQ==", "url": "https://github.com/apache/hudi/pull/2196#discussion_r534276769", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-02T15:52:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg0OTkxNw=="}], "type": "inlineReview"}, {"oid": "1b099ab17b0245ad164649d3ab95537adf8edf21", "url": "https://github.com/apache/hudi/commit/1b099ab17b0245ad164649d3ab95537adf8edf21", "message": "[HUDI-1349] spark sql support overwrite use insert_overwrite_table", "committedDate": "2020-12-02T15:04:35Z", "type": "commit"}, {"oid": "1b099ab17b0245ad164649d3ab95537adf8edf21", "url": "https://github.com/apache/hudi/commit/1b099ab17b0245ad164649d3ab95537adf8edf21", "message": "[HUDI-1349] spark sql support overwrite use insert_overwrite_table", "committedDate": "2020-12-02T15:04:35Z", "type": "forcePushed"}]}