{"pr_number": 1360, "pr_title": "[HUDI-344][RFC-09] Hudi Dataset Snapshot Exporter", "pr_createdAt": "2020-02-27T04:23:51Z", "pr_url": "https://github.com/apache/hudi/pull/1360", "timeline": [{"oid": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "url": "https://github.com/apache/hudi/commit/e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "message": "first commit", "committedDate": "2020-02-27T04:16:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzUwNg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386007506", "bodyText": "Suggested change", "author": "xushiyan", "createdAt": "2020-02-29T06:50:00Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzY5OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386007699", "bodyText": "I think we can omit the short param like -opf as it would create confusion due to the uncommonly recognized acronyms. The full names will work really well.", "author": "xushiyan", "createdAt": "2020-02-29T06:53:56Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODE3NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008175", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                String basePath = null;\n          \n          \n            \n                String sourceBasePath = null;", "author": "xushiyan", "createdAt": "2020-02-29T07:04:40Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODE4MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008180", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                String outputPath = null;\n          \n          \n            \n                String targetBasePath = null;", "author": "xushiyan", "createdAt": "2020-02-29T07:04:56Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODM2Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008366", "bodyText": "You may take the call but I would say without these local vars using the cfg variable directly to refer to those params looks quite readable", "author": "xushiyan", "createdAt": "2020-02-29T07:08:47Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODczMQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008731", "bodyText": "you can invoke .parquet() to be specific", "author": "xushiyan", "createdAt": "2020-02-29T07:16:02Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .repartition(new Column(outputPartitionField))\n+              .write()\n+              .format(outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(targetBasePath);\n+        } else {\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .write()\n+              .format(outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(targetBasePath);", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTI4Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386009283", "bodyText": "I think you'd also need to partition the output dir\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          .write()\n          \n          \n            \n                          .write()\n          \n          \n            \n                          .partitionBy(outputPartitionField)", "author": "xushiyan", "createdAt": "2020-02-29T07:27:52Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .repartition(new Column(outputPartitionField))\n+              .write()", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386009385", "bodyText": "it would be good to have a switch to differentiate the cases and refactor each case to different methods for readability.\nNote that though we only have 'hudi' and 'parquet' for now, there could be more cases in future when needs rise.", "author": "xushiyan", "createdAt": "2020-02-29T07:30:14Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA3MzYzNQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386073635", "bodyText": "Here, exporter not only supports the conversion of hudi data sets to parquet, but also supports all data types currently supported by spark, such as json or jdbc or avro conversion. The .format () method is the key. Of course, in Exporter, it is only divided into hudi type or not. If it is, then you can copy the relevant files to the specified directory. If not, you can read and convert it. I don't think \"switch cases\" are needed here.", "author": "OpenOpened", "createdAt": "2020-03-01T03:34:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5MzU3Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386093576", "bodyText": "@OpenOpened Understand that the format() can provide some flexibility there but i would argue against it for 2 reasons\n\nwe are providing this as a user-input argument to establish a contract with value passed to our API. We don't want to rely it on the internal logic where spark takes that and does the conversion. Imagine if user passes an invalid value like \"foo\" and then Spark API throws an error which will expose the internal implementation which user should not care about.\nWe want to be explicit on what the API supports and does not. switch makes it clear and the code more readable. And the default: case will be the ideal place to throw \"Unsupported\" exception to user.\nIn addition, the export() method is sort of lengthy. For better readability, I would suggest break it into multiple methods. (Handling different conversion type could be a good separation)", "author": "xushiyan", "createdAt": "2020-03-01T09:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI2MTQ5Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386261497", "bodyText": "If you use the swich cases code, it might look something like this:\nswich (uotput) { case \"hudi\":exportToHudi();break; case \"json\":exportToJson();break; ..... }\nFor the implementation part of the method, it is possible to just transform the format() parameters of spark, which is a bit redundant. We can use a compromise solution, using the Spark DataSource lookup method to verify the correctness of the user input format.", "author": "OpenOpened", "createdAt": "2020-03-02T08:53:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3OTMwOA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386279308", "bodyText": "I uploaded the latest code and gave a \"foo\" output format test case; -)", "author": "OpenOpened", "createdAt": "2020-03-02T09:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI4MDU2NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386280565", "bodyText": "In addition. DataSource lookup method and can handle the case when the user customizes the DataSrouce implementation.", "author": "OpenOpened", "createdAt": "2020-03-02T09:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4MzY2MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386783660", "bodyText": "I got your concern. The major point I'd to emphasize here is we want to be explicit on what we support for this utility to the users.\nThe spark format() API is powerful and reduces the code; on the down side, it makes the code difficult to understand: we claim supporting parquet and hudi, but internally we rely on spark API support. I imagine whatever claimed in the configuration docs should be associated to the code for ease of understanding; that's what I intend the switch for. With fall-through switch cases, you can still use format() and achieve explicitness.\nThough saying that, I don't mind with your current approach and change the Exporter's docs saying support all Spark write format. A little bit worried about spark decoupling in future, but maybe not big issue for now.", "author": "xushiyan", "createdAt": "2020-03-03T03:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}], "type": "inlineReview"}, {"oid": "d6ffad986b20067b2708e212d00575345a039dff", "url": "https://github.com/apache/hudi/commit/d6ffad986b20067b2708e212d00575345a039dff", "message": "code optimize", "committedDate": "2020-03-01T03:38:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5Mzg2Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386093867", "bodyText": "When we export for non-hudi case, I think we should remove the _hoodie_* metadata columns.", "author": "xushiyan", "createdAt": "2020-03-01T09:56:17Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NDk1Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386094957", "bodyText": "With partitionBy() I think you left out the repartition() before write", "author": "xushiyan", "createdAt": "2020-03-01T10:13:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5Mzg2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095272", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")\n          \n          \n            \n                SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")", "author": "xushiyan", "createdAt": "2020-03-01T10:17:50Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(cfg.targetOutputPath);\n+        } else {\n+          write.format(cfg.outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(cfg.targetOutputPath);\n+        }\n+      } else {\n+        // No transformation is needed for output format \"HUDI\", just copy the original files.\n+\n+        // Make sure the output directory is empty\n+        Path outputPath = new Path(cfg.targetOutputPath);\n+        if (fs.exists(outputPath)) {\n+          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n+        }\n+\n+        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+          // Only take latest version files <= latestCommit.\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+          // also need to copy over partition metadata\n+          Path partitionMetaFile =\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+          if (fs1.exists(partitionMetaFile)) {\n+            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+          }\n+\n+          return filePaths.iterator();\n+        }).foreach(tuple -> {\n+          String partition = tuple._1();\n+          Path sourceFilePath = new Path(tuple._2());\n+          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+\n+          if (!ifs.exists(toPartitionPath)) {\n+            ifs.mkdirs(toPartitionPath);\n+          }\n+          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+              ifs.getConf());\n+        });\n+\n+        // Also copy the .commit files\n+        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+        FileStatus[] commitFilesToCopy =\n+            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+                return true;\n+              } else {\n+                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                    HoodieTimeline.LESSER_OR_EQUAL);\n+              }\n+            });\n+        for (FileStatus commitStatus : commitFilesToCopy) {\n+          Path targetFilePath =\n+              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+          if (!fs.exists(targetFilePath.getParent())) {\n+            fs.mkdirs(targetFilePath.getParent());\n+          }\n+          if (fs.exists(targetFilePath)) {\n+            LOG.error(\n+                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+          }\n+          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+        }\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+  }\n+\n+  public static void main(String[] args) throws IOException {\n+    // Take input configs\n+    final Config cfg = new Config();\n+    new JCommander(cfg, null, args);\n+\n+    // Create a spark job to do the snapshot export\n+    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI1NzQ3OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386257479", "bodyText": "if remove master() method, spark test unable to work properly.\norg.apache.spark.SparkException: A master URL must be set in your configuration", "author": "OpenOpened", "createdAt": "2020-03-02T08:44:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc3OTM2Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386779367", "bodyText": "ok but wouldn't this fail when running on production where we need to pass different master url?", "author": "xushiyan", "createdAt": "2020-03-03T03:22:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODMxOTAyOQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r388319029", "bodyText": "It's my fault. Thank you for pointing it out", "author": "OpenOpened", "createdAt": "2020-03-05T14:15:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTM4OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095388", "bodyText": "This seems not used", "author": "xushiyan", "createdAt": "2020-03-01T10:19:31Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test utils for data source tests.\n+ */\n+public class DataSourceTestUtils {\n+\n+  public static Option<String> convertToString(HoodieRecord record) {\n+    try {\n+      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n+      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n+      // Remove the last } bracket\n+      str = str.substring(0, str.length() - 1);\n+      return Option.of(str + \", \\\"partition\\\": \\\"\" + record.getPartitionPath() + \"\\\"}\");\n+    } catch (IOException e) {\n+      return Option.empty();\n+    }\n+  }\n+\n+  public static List<String> convertToStringList(List<HoodieRecord> records) {\n+    return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static List<String> convertKeysToStringList(List<HoodieKey> keys) {", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI1ODQ2Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386258466", "bodyText": "DataSourceTestUtils actually exists under the hudi-spark test module, but we can't access it. I think we can move to the hudi-common package in the future to reuse the code as possible.", "author": "OpenOpened", "createdAt": "2020-03-02T08:47:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTM4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTkyNw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095927", "bodyText": "Any reason of not extending HoodieCommonTestHarness ?", "author": "xushiyan", "createdAt": "2020-03-01T10:28:21Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestHoodieSnapshotExporter {", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3Nzk5Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386277992", "bodyText": "There was little or no connection, so I chose not to inherit it. I have modified the latest code.", "author": "OpenOpened", "createdAt": "2020-03-02T09:28:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTkyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjE4Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386096182", "bodyText": "This assumes what data column exists in TestRawTripPayload, which may easily break. I would suggest\n\nmaking this a private test utils to the Exporter test class as it is very specific to it (not generic enough to be a standalone test utils) and\ntry to make the source data work for the case instead of wrangling with json string representation", "author": "xushiyan", "createdAt": "2020-03-01T10:32:13Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test utils for data source tests.\n+ */\n+public class DataSourceTestUtils {\n+\n+  public static Option<String> convertToString(HoodieRecord record) {\n+    try {\n+      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n+      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n+      // Remove the last } bracket\n+      str = str.substring(0, str.length() - 1);", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjY0OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386096648", "bodyText": "Notice this is from original test for Snapshot copier, not sure about the historical issue for skipping it. I would slightly favor fixing it while you're at this stage so that we can claim the new \"Exporter\" is better tested than the old \"Copier\" :)", "author": "xushiyan", "createdAt": "2020-03-01T10:39:10Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestHoodieSnapshotExporter {\n+  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n+\n+  private SparkSession spark = null;\n+  private HoodieTestDataGenerator dataGen = null;\n+  private String basePath = null;\n+  private String outputPath = null;\n+  private String rootPath = null;\n+  private FileSystem fs = null;\n+  private Map commonOpts;\n+  private HoodieSnapshotExporter.Config cfg;\n+  private JavaSparkContext jsc = null;\n+\n+  @Before\n+  public void initialize() throws IOException {\n+    spark = SparkSession.builder()\n+        .appName(\"Hoodie Datasource test\")\n+        .master(\"local[2]\")\n+        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+        .getOrCreate();\n+    jsc = new JavaSparkContext(spark.sparkContext());\n+    dataGen = new HoodieTestDataGenerator();\n+    TemporaryFolder folder = new TemporaryFolder();\n+    folder.create();\n+    basePath = folder.getRoot().getAbsolutePath();\n+    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n+    commonOpts = new HashMap();\n+\n+    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n+    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n+    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n+    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n+    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n+    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n+\n+\n+    cfg = new HoodieSnapshotExporter.Config();\n+\n+    cfg.sourceBasePath = basePath;\n+    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n+    cfg.outputFormat = \"json\";\n+    cfg.outputPartitionField = \"partition\";\n+\n+  }\n+\n+  @After\n+  public void cleanup() throws Exception {\n+    if (spark != null) {\n+      spark.stop();\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotExporter() throws IOException {\n+    // Insert Operation\n+    List<String> records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100));\n+    Dataset<Row> inputDF = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(records, 2));\n+    inputDF.write().format(\"hudi\")\n+        .options(commonOpts)\n+        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n+        .mode(SaveMode.Overwrite)\n+        .save(basePath);\n+    long sourceCount = inputDF.count();\n+\n+    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n+    hoodieSnapshotExporter.export(spark, cfg);\n+\n+    long targetCount = spark.read().json(outputPath).count();\n+\n+    assertTrue(sourceCount == targetCount);\n+\n+    // Test snapshotPrefix\n+    long filterCount = inputDF.where(\"partition == '2015/03/16'\").count();\n+    cfg.snapshotPrefix = \"2015/03/16\";\n+    hoodieSnapshotExporter.export(spark, cfg);\n+    long targetFilterCount = spark.read().json(outputPath).count();\n+    assertTrue(filterCount == targetFilterCount);\n+\n+  }\n+\n+  // for testEmptySnapshotCopy\n+  public void init() throws IOException {\n+    TemporaryFolder folder = new TemporaryFolder();\n+    folder.create();\n+    rootPath = \"file://\" + folder.getRoot().getAbsolutePath();\n+    basePath = rootPath + \"/\" + HoodieTestUtils.RAW_TRIPS_TEST_NAME;\n+    outputPath = rootPath + \"/output\";\n+\n+    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();\n+    fs = FSUtils.getFs(basePath, hadoopConf);\n+    HoodieTestUtils.init(hadoopConf, basePath);\n+  }\n+\n+  @Test\n+  public void testEmptySnapshotCopy() throws IOException {\n+    init();\n+    // There is no real data (only .hoodie directory)\n+    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n+    assertFalse(fs.exists(new Path(outputPath)));\n+\n+    // Do the snapshot\n+    HoodieSnapshotCopier copier = new HoodieSnapshotCopier();\n+    copier.snapshot(jsc, basePath, outputPath, true);\n+\n+    // Nothing changed; we just bail out\n+    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n+    assertFalse(fs.exists(new Path(outputPath + \"/_SUCCESS\")));\n+  }\n+\n+  // TODO - uncomment this after fixing test failures\n+  // @Test", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3Njk2Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386276963", "bodyText": "I'm not quite sure what the comments here mean and what the implications might be. It may take officials to sort it out.", "author": "OpenOpened", "createdAt": "2020-03-02T09:26:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjY0OA=="}], "type": "inlineReview"}, {"oid": "e917358edc3c65252a2783b761c24a74b7aa04f3", "url": "https://github.com/apache/hudi/commit/e917358edc3c65252a2783b761c24a74b7aa04f3", "message": "code optimize", "committedDate": "2020-03-02T09:14:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc2Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386786763", "bodyText": "you need startswith() to be safe. There could be a column named \"total_hoodie_counts\"", "author": "xushiyan", "createdAt": "2020-03-03T03:55:53Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());", "originalCommit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4ODA2MQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386788061", "bodyText": "This is optional but actually i'd even prefer having an immutable list of _hoodie_* reserved field names so we can be explicit on what we are removing while exporting.", "author": "xushiyan", "createdAt": "2020-03-03T04:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc4Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386786787", "bodyText": "i think you need both repartition() and partitionBy()", "author": "xushiyan", "createdAt": "2020-03-03T03:56:01Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n+        } else {\n+          write = reader.write();\n+        }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);", "originalCommit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "76133ce9788df7bc57406066811fb5e14d40a17c", "url": "https://github.com/apache/hudi/commit/76133ce9788df7bc57406066811fb5e14d40a17c", "message": "code optimize", "committedDate": "2020-03-05T14:13:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389328760", "bodyText": "the description here, I feel a little bit wierd after reading the code below, it means only export source Hudi dataset which contains the snapshotPrefix? would be changed to Snapshot prefix or directory under the source Hudi dataset to be exported? cc @OpenOpened @xushiyan", "author": "leesf", "createdAt": "2020-03-08T02:27:03Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")", "originalCommit": "76133ce9788df7bc57406066811fb5e14d40a17c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM2NTU4Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389365583", "bodyText": "@leesf thanks for catching this. I missed this one: this param is meant to segregate output not input. It was meant to be used in case for multiple exports with the same target path but wanted to be separated from each other (e.g., due to different export date). It actually overlaps with the target base path; users can simply change the target base paths to achieve this. So in conclusion we can just remove this param. @OpenOpened sounds good?", "author": "xushiyan", "createdAt": "2020-03-08T12:40:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ0NDkxMg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389444912", "bodyText": "I don't think we can delete this parameter. We rely on the metadata file .hoodie metadata in the root directory of the datasource to find things like commitime, valid parquet files, etc. If you point directly to the folder that needs to be exported, such as ROOT/2015/03/16 for the test case, an exception will be thrown Hoodie table not found in path / tmp / junit4184871464195097137 / 2015/03/16 / .hoodie.  I agree with @leesf, modify the comment of --snapshot-perfix.", "author": "OpenOpened", "createdAt": "2020-03-09T03:02:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ0OTk3Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389449973", "bodyText": "I see there're some gaps...this param snapshot-prefix is meant to let users export to a specific output directory. For example, --target-base-path=/mytable/ --snapshot-prefix=2020/03/03 and the output data will reside in /mytable/2020/03/03/. After removing --snapshot-prefix, the users will simply set --target-base-path=/mytable/2020/03/03/. This is a redundant param that should be removed from the RFC document too.\nIt is not meant to use on --source-base-path. Users will give the right base path to the hudi dataset to export like --source-base-path=/myhuditable/", "author": "xushiyan", "createdAt": "2020-03-09T03:32:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ1NTQ5OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389455499", "bodyText": "Talked with @xushiyan  offline, the RFC9 means we only export whole hudi dataset, thus may not support exporting ROOT/2015/03/16, thus we could remove --snapshot-prefix and modify the code accordingly. If we need export specified partitions later, we could bring it back and may need more consideration(maybe not use contains, maybe equals or regular expression). WDYT @OpenOpened", "author": "leesf", "createdAt": "2020-03-09T04:08:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODg5OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389328898", "bodyText": "one minor question, would we support config commit time ? it is optional, and get lastest commit by default or get the specified commit time. cc @xushiyan @OpenOpened", "author": "leesf", "createdAt": "2020-03-08T02:30:26Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();", "originalCommit": "76133ce9788df7bc57406066811fb5e14d40a17c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM2NTA3Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389365076", "bodyText": "Per the RFC, we aim to just get the latest commit time.", "author": "xushiyan", "createdAt": "2020-03-08T12:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODg5OA=="}], "type": "inlineReview"}, {"oid": "8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "url": "https://github.com/apache/hudi/commit/8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "message": "code optimize", "committedDate": "2020-03-09T03:08:10Z", "type": "commit"}, {"oid": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "url": "https://github.com/apache/hudi/commit/aa5e7798322ce0701776d8b44b608d601f61f0a7", "message": "remove --snapshot-prefix flag", "committedDate": "2020-03-09T05:58:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ4MjIwMw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389482203", "bodyText": "nit: remove extra line?", "author": "vinothchandar", "createdAt": "2020-03-09T06:33:49Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java", "diffHunk": "@@ -52,6 +52,7 @@\n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n  */\n+", "originalCommit": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "45c0872ce680ad54312cfdeb4c8dd6f70525513c", "url": "https://github.com/apache/hudi/commit/45c0872ce680ad54312cfdeb4c8dd6f70525513c", "message": "remove extra line", "committedDate": "2020-03-09T06:42:11Z", "type": "commit"}]}