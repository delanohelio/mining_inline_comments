{"pr_number": 2283, "pr_title": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "pr_createdAt": "2020-11-26T07:51:11Z", "pr_url": "https://github.com/apache/hudi/pull/2283", "timeline": [{"oid": "2038dda8bfdd03c0e6e57f575a51773fe570c76b", "url": "https://github.com/apache/hudi/commit/2038dda8bfdd03c0e6e57f575a51773fe570c76b", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql", "committedDate": "2020-11-30T15:26:42Z", "type": "forcePushed"}, {"oid": "63e88099492dae817e428cc973d0d8c452dcb039", "url": "https://github.com/apache/hudi/commit/63e88099492dae817e428cc973d0d8c452dcb039", "message": "[HUDI-1415] refactor same code", "committedDate": "2020-12-06T12:23:28Z", "type": "forcePushed"}, {"oid": "0ff1f77a5b6dde711f2c2dad03d70a581e2532c2", "url": "https://github.com/apache/hudi/commit/0ff1f77a5b6dde711f2c2dad03d70a581e2532c2", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code", "committedDate": "2020-12-06T12:26:45Z", "type": "forcePushed"}, {"oid": "98e229343ed140f252d5058813855ce689030396", "url": "https://github.com/apache/hudi/commit/98e229343ed140f252d5058813855ce689030396", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case", "committedDate": "2020-12-06T14:36:44Z", "type": "forcePushed"}, {"oid": "a5e896a04d6fb4227874067ffd47073d0e23ab71", "url": "https://github.com/apache/hudi/commit/a5e896a04d6fb4227874067ffd47073d0e23ab71", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2020-12-07T07:53:30Z", "type": "forcePushed"}, {"oid": "c826eb838a35d4ea1bb21823b33c50e1c4a9d893", "url": "https://github.com/apache/hudi/commit/c826eb838a35d4ea1bb21823b33c50e1c4a9d893", "message": "add log for test case", "committedDate": "2020-12-10T03:25:33Z", "type": "forcePushed"}, {"oid": "bc59a67f3d01dd17beb24592957ede089a5fa9a8", "url": "https://github.com/apache/hudi/commit/bc59a67f3d01dd17beb24592957ede089a5fa9a8", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2020-12-10T03:25:32Z", "type": "forcePushed"}, {"oid": "63cbf0148a033fe511d49b381691d126f78f8828", "url": "https://github.com/apache/hudi/commit/63cbf0148a033fe511d49b381691d126f78f8828", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2021-01-07T15:53:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzU3MA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247570", "bodyText": "since this is abstract class and not every implementation will have some concrete override, can we make this empty here so that HoodieDLAClient does not need to do a no op override.", "author": "nsivabalan", "createdAt": "2021-01-30T13:42:11Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -75,6 +76,8 @@ public abstract void createTable(String tableName, MessageType storageSchema,\n \n   public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n \n+  public abstract void updateTableProperties(String tableName, Map<String, String> tableProperties);", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzY3NA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247674", "bodyText": "I understand it's not part of this diff. But wondering if you can add some java docs to this class in general. I realized we don't have any one. (at line 41 ish) .", "author": "nsivabalan", "createdAt": "2021-01-30T13:43:29Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -63,7 +63,8 @@ public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning,\n   }\n \n   public abstract void createTable(String tableName, MessageType storageSchema,\n-                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+                                   String inputFormatClass, String outputFormatClass,", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODM1NA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248354", "bodyText": "minor. \"Failed to update...\". remove extra \"get\"", "author": "nsivabalan", "createdAt": "2021-01-30T13:49:41Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {\n+      return;\n+    }\n+    try {\n+      Table table = client.getTable(syncConfig.databaseName, tableName);\n+      for (Map.Entry<String, String> entry: tableProperties.entrySet()) {\n+        table.putToParameters(entry.getKey(), entry.getValue());\n+      }\n+      client.alter_table(syncConfig.databaseName, tableName, table);\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get update table properties for table: \"", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM0NzM2Ng==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r570347366", "bodyText": "Thanks for your correct.", "author": "pengzhiwei2018", "createdAt": "2021-02-04T16:07:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODM1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODUyOA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248528", "bodyText": "Would be nice if you write a test for the actual problem you faced as per the title/desc. And ensure that the test fails w/o this patch and succeeds with this patch.", "author": "nsivabalan", "createdAt": "2021-01-30T13:51:09Z", "path": "hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -249,6 +255,54 @@ public void testBasicSync(boolean useJdbc, boolean useSchemaFromCommitMetadata)\n         \"The last commit that was sycned should be 100\");\n   }\n \n+  @ParameterizedTest\n+  @MethodSource({\"useJdbcAndSchemaFromCommitMetadata\"})\n+  public void testSyncWithProperties(boolean useJdbc, boolean useSchemaFromCommitMetadata) throws Exception {", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI1MjQ0Ng==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567252446", "bodyText": "Thanks @nsivabalan  for these nice suggestions, I will spend some time to process.", "author": "pengzhiwei2018", "createdAt": "2021-01-30T14:28:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODUyOA=="}], "type": "inlineReview"}, {"oid": "4b568770a4559f3b3c46694e45a13cf2673277d9", "url": "https://github.com/apache/hudi/commit/4b568770a4559f3b3c46694e45a13cf2673277d9", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2021-01-30T14:37:53Z", "type": "forcePushed"}, {"oid": "618ac88270a3e1745fade239ff5d742cec7bfe66", "url": "https://github.com/apache/hudi/commit/618ac88270a3e1745fade239ff5d742cec7bfe66", "message": "fix some code review", "committedDate": "2021-02-19T02:46:32Z", "type": "forcePushed"}, {"oid": "c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "url": "https://github.com/apache/hudi/commit/c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review", "committedDate": "2021-02-19T03:20:42Z", "type": "forcePushed"}, {"oid": "71363392e720bcc8d32c3c90c601b020f3d6366c", "url": "https://github.com/apache/hudi/commit/71363392e720bcc8d32c3c90c601b020f3d6366c", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review", "committedDate": "2021-02-19T03:22:40Z", "type": "forcePushed"}, {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "url": "https://github.com/apache/hudi/commit/872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-02-20T11:40:45Z", "type": "forcePushed"}, {"oid": "ba70819d3bfa1443c7c4b1f7b21ed89be76b76d6", "url": "https://github.com/apache/hudi/commit/ba70819d3bfa1443c7c4b1f7b21ed89be76b76d6", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-07T06:08:02Z", "type": "forcePushed"}, {"oid": "6343d09682dbcfb6e9716312889d876178d6349b", "url": "https://github.com/apache/hudi/commit/6343d09682dbcfb6e9716312889d876178d6349b", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-12T12:19:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTI4NA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435284", "bodyText": "Can you improve the javadoc ? It has missing properties and descriptions.", "author": "umehrot2", "createdAt": "2021-04-15T22:42:49Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTg0Mw==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435843", "bodyText": "nit: tableProperties.isEmpty() ?", "author": "umehrot2", "createdAt": "2021-04-15T22:44:20Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614457527", "bodyText": "Can't we sync this while creating the table itself, like you are doing for serde properties ?", "author": "umehrot2", "createdAt": "2021-04-15T23:20:55Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -164,7 +165,13 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-\n+    // Sync the table properties if need\n+    if (cfg.tableProperties != null) {\n+      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n+      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n+      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n+          + cfg.tableProperties);\n+    }", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyNTg1Mg==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615525852", "bodyText": "Well, the tableProperties may change if the schema has changed. So we need to update the table properties  by a separate interface.", "author": "pengzhiwei2018", "createdAt": "2021-04-19T03:46:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNjI4ODQ1Nw==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r616288457", "bodyText": "@pengzhiwei2018 I understand that. In the syncSchema() there is a check for schema difference. Perhaps we should move this inside that to avoid this being run every time. And for the first time when table is created, we can do it as part of create table. Thoughts ?", "author": "umehrot2", "createdAt": "2021-04-20T01:55:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNjI5NzczOQ==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r616297739", "bodyText": "Yeah, +1 for this.", "author": "pengzhiwei2018", "createdAt": "2021-04-20T02:25:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MTMxNQ==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614461315", "bodyText": "Can you update the toString() in this class ?", "author": "umehrot2", "createdAt": "2021-04-15T23:31:26Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java", "diffHunk": "@@ -88,6 +88,12 @@\n   @Parameter(names = {\"--verify-metadata-file-listing\"}, description = \"Verify file listing from Hudi's metadata against file system\")\n   public Boolean verifyMetadataFileListing = HoodieMetadataConfig.DEFAULT_METADATA_VALIDATE;\n \n+  @Parameter(names = {\"--table-properties\"}, description = \"Table properties to hive table\")\n+  public String tableProperties;\n+\n+  @Parameter(names = {\"--serde-properties\"}, description = \"Serde properties to hive table\")\n+  public String serdeProperties;\n+", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyNjAxMg==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615526012", "bodyText": "Yes, thanks for remind me.", "author": "pengzhiwei2018", "createdAt": "2021-04-19T03:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MTMxNQ=="}], "type": "inlineReview"}, {"oid": "33acbd66ba6204c3bc47a150d82b64337c3c5bd3", "url": "https://github.com/apache/hudi/commit/33acbd66ba6204c3bc47a150d82b64337c3c5bd3", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-19T02:37:47Z", "type": "forcePushed"}, {"oid": "1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "url": "https://github.com/apache/hudi/commit/1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-19T07:57:07Z", "type": "forcePushed"}, {"oid": "348cf1f629913dc193c17b3377b85351c2596ed9", "url": "https://github.com/apache/hudi/commit/348cf1f629913dc193c17b3377b85351c2596ed9", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-19T07:58:00Z", "type": "forcePushed"}, {"oid": "8bc3097f3683a62429b5e2d2b54833c87773fc5e", "url": "https://github.com/apache/hudi/commit/8bc3097f3683a62429b5e2d2b54833c87773fc5e", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T02:39:53Z", "type": "forcePushed"}, {"oid": "173d21ee455b8ec1bca690d6aecc5c3396bbf7aa", "url": "https://github.com/apache/hudi/commit/173d21ee455b8ec1bca690d6aecc5c3396bbf7aa", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T02:45:16Z", "type": "forcePushed"}, {"oid": "657bf34ca752c67ad4ae902d4458471e6e72601d", "url": "https://github.com/apache/hudi/commit/657bf34ca752c67ad4ae902d4458471e6e72601d", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T02:58:12Z", "type": "forcePushed"}, {"oid": "5b865ec35c296b1182b8a90ffbbbcd5a783aeacc", "url": "https://github.com/apache/hudi/commit/5b865ec35c296b1182b8a90ffbbbcd5a783aeacc", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T03:30:46Z", "type": "forcePushed"}, {"oid": "fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "url": "https://github.com/apache/hudi/commit/fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T05:04:54Z", "type": "commit"}, {"oid": "fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "url": "https://github.com/apache/hudi/commit/fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T05:04:54Z", "type": "forcePushed"}]}