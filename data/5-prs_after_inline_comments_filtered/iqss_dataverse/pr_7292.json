{"pr_number": 7292, "pr_title": "Iqss/7140 google cloud archiver", "pr_createdAt": "2020-10-02T17:47:46Z", "pr_url": "https://github.com/IQSS/dataverse/pull/7292", "timeline": [{"oid": "bf6ca00a8d1f4d0e1e941c3ac7bd40dcc29c7a59", "url": "https://github.com/IQSS/dataverse/commit/bf6ca00a8d1f4d0e1e941c3ac7bd40dcc29c7a59", "message": "add google archiver and dependencies", "committedDate": "2020-10-02T14:26:17Z", "type": "commit"}, {"oid": "ee08e9c2e2ec518248e5bb252ab7f7f1cf41876d", "url": "https://github.com/IQSS/dataverse/commit/ee08e9c2e2ec518248e5bb252ab7f7f1cf41876d", "message": "documentation", "committedDate": "2020-10-02T17:32:57Z", "type": "commit"}, {"oid": "5dfbae394f875edf2ce8c724f9e3bcce45b8f7e0", "url": "https://github.com/IQSS/dataverse/commit/5dfbae394f875edf2ce8c724f9e3bcce45b8f7e0", "message": "update DuraCloud archiver with enhancements from Google archiver", "committedDate": "2020-10-02T17:33:33Z", "type": "commit"}, {"oid": "fed5e456bd6381e3758d637cf69b5aec00641843", "url": "https://github.com/IQSS/dataverse/commit/fed5e456bd6381e3758d637cf69b5aec00641843", "message": "typos", "committedDate": "2020-10-02T17:37:14Z", "type": "commit"}, {"oid": "4d1b4b00bc90389947192340ff9a8a226a0b457e", "url": "https://github.com/IQSS/dataverse/commit/4d1b4b00bc90389947192340ff9a8a226a0b457e", "message": "capitalization", "committedDate": "2020-10-02T17:39:08Z", "type": "commit"}, {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155", "url": "https://github.com/IQSS/dataverse/commit/3eb0ebcab2facd9d79c40821f2903b3ae1c72155", "message": "for example", "committedDate": "2020-10-02T17:40:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3ODI5OA==", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498978298", "bodyText": "Do failures like this show up in the UI? If so, do they need to be translatable into languages other than English?", "author": "pdurbin", "createdAt": "2020-10-02T18:16:59Z", "path": "src/main/java/edu/harvard/iq/dataverse/engine/command/impl/GoogleCloudSubmitToArchiveCommand.java", "diffHunk": "@@ -0,0 +1,228 @@\n+package edu.harvard.iq.dataverse.engine.command.impl;\n+\n+import edu.harvard.iq.dataverse.DOIDataCiteRegisterService;\n+import edu.harvard.iq.dataverse.DataCitation;\n+import edu.harvard.iq.dataverse.Dataset;\n+import edu.harvard.iq.dataverse.DatasetVersion;\n+import edu.harvard.iq.dataverse.DatasetLock.Reason;\n+import edu.harvard.iq.dataverse.authorization.Permission;\n+import edu.harvard.iq.dataverse.authorization.users.ApiToken;\n+import edu.harvard.iq.dataverse.engine.command.Command;\n+import edu.harvard.iq.dataverse.engine.command.DataverseRequest;\n+import edu.harvard.iq.dataverse.engine.command.RequiredPermissions;\n+import edu.harvard.iq.dataverse.util.bagit.BagGenerator;\n+import edu.harvard.iq.dataverse.util.bagit.OREMap;\n+import edu.harvard.iq.dataverse.workflow.step.Failure;\n+import edu.harvard.iq.dataverse.workflow.step.WorkflowStepResult;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.PipedInputStream;\n+import java.io.PipedOutputStream;\n+import java.nio.charset.Charset;\n+import java.security.DigestInputStream;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.util.Map;\n+import java.util.logging.Logger;\n+\n+import org.apache.commons.codec.binary.Hex;\n+import com.google.auth.oauth2.ServiceAccountCredentials;\n+import com.google.cloud.storage.Blob;\n+import com.google.cloud.storage.Bucket;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageOptions;\n+\n+@RequiredPermissions(Permission.PublishDataset)\n+public class GoogleCloudSubmitToArchiveCommand extends AbstractSubmitToArchiveCommand implements Command<DatasetVersion> {\n+\n+    private static final Logger logger = Logger.getLogger(GoogleCloudSubmitToArchiveCommand.class.getName());\n+    private static final String GOOGLECLOUD_BUCKET = \":GoogleCloudBucket\";\n+    private static final String GOOGLECLOUD_PROJECT = \":GoogleCloudProject\";\n+\n+    public GoogleCloudSubmitToArchiveCommand(DataverseRequest aRequest, DatasetVersion version) {\n+        super(aRequest, version);\n+    }\n+\n+    @Override\n+    public WorkflowStepResult performArchiveSubmission(DatasetVersion dv, ApiToken token, Map<String, String> requestedSettings) {\n+        logger.fine(\"In GoogleCloudSubmitToArchiveCommand...\");\n+        String bucketName = requestedSettings.get(GOOGLECLOUD_BUCKET);\n+        String projectName = requestedSettings.get(GOOGLECLOUD_PROJECT);\n+        logger.fine(\"Project: \" + projectName + \" Bucket: \" + bucketName);\n+        if (bucketName != null && projectName != null) {\n+            Storage storage;\n+            try {\n+                FileInputStream fis = new FileInputStream(System.getProperty(\"dataverse.files.directory\") + System.getProperty(\"file.separator\")+ \"googlecloudkey.json\");\n+                storage = StorageOptions.newBuilder()\n+                        .setCredentials(ServiceAccountCredentials.fromStream(fis))\n+                        .setProjectId(projectName)\n+                        .build()\n+                        .getService();\n+                Bucket bucket = storage.get(bucketName);\n+\n+                Dataset dataset = dv.getDataset();\n+                if (dataset.getLockFor(Reason.finalizePublication) == null) {\n+\n+                    String spaceName = dataset.getGlobalId().asString().replace(':', '-').replace('/', '-')\n+                            .replace('.', '-').toLowerCase();\n+\n+                    DataCitation dc = new DataCitation(dv);\n+                    Map<String, String> metadata = dc.getDataCiteMetadata();\n+                    String dataciteXml = DOIDataCiteRegisterService.getMetadataFromDvObject(\n+                            dv.getDataset().getGlobalId().asString(), metadata, dv.getDataset());\n+                    String blobIdString = null;\n+                    MessageDigest messageDigest = MessageDigest.getInstance(\"MD5\");\n+                    try (PipedInputStream dataciteIn = new PipedInputStream(); DigestInputStream digestInputStream = new DigestInputStream(dataciteIn, messageDigest)) {\n+                        // Add datacite.xml file\n+\n+                        new Thread(new Runnable() {\n+                            public void run() {\n+                                try (PipedOutputStream dataciteOut = new PipedOutputStream(dataciteIn)) {\n+\n+                                    dataciteOut.write(dataciteXml.getBytes(Charset.forName(\"utf-8\")));\n+                                    dataciteOut.close();\n+                                } catch (Exception e) {\n+                                    logger.severe(\"Error creating datacite.xml: \" + e.getMessage());\n+                                    // TODO Auto-generated catch block\n+                                    e.printStackTrace();\n+                                    throw new RuntimeException(\"Error creating datacite.xml: \" + e.getMessage());\n+                                }\n+                            }\n+                        }).start();\n+                        //Have seen broken pipe in PostPublishDataset workflow without this delay\n+                        int i=0;\n+                        while(digestInputStream.available()<=0 && i<100) {\n+                            Thread.sleep(10);\n+                            i++;\n+                        }\n+                        Blob dcXml = bucket.create(spaceName + \"/datacite.v\" + dv.getFriendlyVersionNumber()+\".xml\", digestInputStream, \"text/xml\", Bucket.BlobWriteOption.doesNotExist());\n+                        String checksum = dcXml.getMd5ToHexString();\n+                        logger.fine(\"Content: datacite.xml added with checksum: \" + checksum);\n+                        String localchecksum = Hex.encodeHexString(digestInputStream.getMessageDigest().digest());\n+                        if (!checksum.equals(localchecksum)) {\n+                            logger.severe(checksum + \" not equal to \" + localchecksum);\n+                            return new Failure(\"Error in transferring DataCite.xml file to GoogleCloud\",\n+                                    \"GoogleCloud Submission Failure: incomplete metadata transfer\");", "originalCommit": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk5MjMwMg==", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498992302", "bodyText": "They don't appear anywhere except the log AFAIK, even if you run them as part of a post-publish workflow instead of the API. (Since they run asynchronously, the API just returns an OK, it's launched response.\nThe /api/datasets/{id}/actions/:publish also runs archiving, synchronously, if configured, and it does have internationalizable success/failure messages.", "author": "qqmyers", "createdAt": "2020-10-02T18:45:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3ODI5OA=="}], "type": "inlineReview"}, {"oid": "3e5eecfae790dce775b194e5ca34ca0e61cbef50", "url": "https://github.com/IQSS/dataverse/commit/3e5eecfae790dce775b194e5ca34ca0e61cbef50", "message": "adding settings to master list", "committedDate": "2020-10-02T19:47:05Z", "type": "commit"}, {"oid": "ede2a221389839a4e6f70bcdacf6b4b58f45205e", "url": "https://github.com/IQSS/dataverse/commit/ede2a221389839a4e6f70bcdacf6b4b58f45205e", "message": "add formatting", "committedDate": "2020-10-02T19:48:54Z", "type": "commit"}, {"oid": "a33b9b167a1c0b825966a55515388dcfbfdbd9dc", "url": "https://github.com/IQSS/dataverse/commit/a33b9b167a1c0b825966a55515388dcfbfdbd9dc", "message": "simplify links", "committedDate": "2020-10-02T19:54:52Z", "type": "commit"}, {"oid": "4aeccfdf861cb3fb4e5710e8248a3ad5fe658ccb", "url": "https://github.com/IQSS/dataverse/commit/4aeccfdf861cb3fb4e5710e8248a3ad5fe658ccb", "message": "add release notes", "committedDate": "2020-10-02T20:07:47Z", "type": "commit"}, {"oid": "558f008218f0dee1b0d956ee763c5700b24d88e9", "url": "https://github.com/IQSS/dataverse/commit/558f008218f0dee1b0d956ee763c5700b24d88e9", "message": "Merge branch 'IQSS/7140-GoogleCloudArchiver' of https://github.com/QualitativeDataRepository/dataverse into IQSS/7140-GoogleCloudArchiver", "committedDate": "2020-10-02T20:08:53Z", "type": "commit"}, {"oid": "c257a1ea5e2960459bf5bd9325994afee55f7a7c", "url": "https://github.com/IQSS/dataverse/commit/c257a1ea5e2960459bf5bd9325994afee55f7a7c", "message": "Update doc/sphinx-guides/source/installation/config.rst\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>", "committedDate": "2020-10-05T17:29:09Z", "type": "commit"}, {"oid": "57e6b426f48d1a0952694a451391d51fc8e85af6", "url": "https://github.com/IQSS/dataverse/commit/57e6b426f48d1a0952694a451391d51fc8e85af6", "message": "Update doc/sphinx-guides/source/installation/config.rst\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>", "committedDate": "2020-10-05T17:29:28Z", "type": "commit"}, {"oid": "babb316d729817421c93421d8e5edda29e2dac90", "url": "https://github.com/IQSS/dataverse/commit/babb316d729817421c93421d8e5edda29e2dac90", "message": "typo fixes", "committedDate": "2020-10-05T17:47:41Z", "type": "commit"}, {"oid": "f2b1b44c993d9cf196f580385f9f7d634353f36b", "url": "https://github.com/IQSS/dataverse/commit/f2b1b44c993d9cf196f580385f9f7d634353f36b", "message": "Merge branch 'IQSS/7140-GoogleCloudArchiver' of https://github.com/QualitativeDataRepository/dataverse.git into IQSS/7140-GoogleCloudArchiver", "committedDate": "2020-10-05T17:56:41Z", "type": "commit"}, {"oid": "71dd1b9b685fefb1b9e11f5083bf91f1e4f51fc0", "url": "https://github.com/IQSS/dataverse/commit/71dd1b9b685fefb1b9e11f5083bf91f1e4f51fc0", "message": "Merge remote-tracking branch 'IQSS/develop' into IQSS/7140-GoogleCloudArchiver", "committedDate": "2020-10-06T15:17:29Z", "type": "commit"}, {"oid": "ed90534d0150709ecba8882bc764955295a99433", "url": "https://github.com/IQSS/dataverse/commit/ed90534d0150709ecba8882bc764955295a99433", "message": "Merge remote-tracking branch 'IQSS/develop' into IQSS/7140-GoogleCloudArchiver", "committedDate": "2020-10-08T18:36:13Z", "type": "commit"}, {"oid": "ab29f15fa9b34b0806ad79f463a3f4339fed9d72", "url": "https://github.com/IQSS/dataverse/commit/ab29f15fa9b34b0806ad79f463a3f4339fed9d72", "message": "typo", "committedDate": "2020-10-08T19:27:29Z", "type": "commit"}, {"oid": "af6dc98af51abe8b241dc00d4417e89ec4248e64", "url": "https://github.com/IQSS/dataverse/commit/af6dc98af51abe8b241dc00d4417e89ec4248e64", "message": "expanded directions", "committedDate": "2020-10-08T20:22:10Z", "type": "commit"}, {"oid": "3484bb6b6a2cdd08a2e361d8d31de9dc56840939", "url": "https://github.com/IQSS/dataverse/commit/3484bb6b6a2cdd08a2e361d8d31de9dc56840939", "message": "typos", "committedDate": "2020-10-08T20:25:37Z", "type": "commit"}]}