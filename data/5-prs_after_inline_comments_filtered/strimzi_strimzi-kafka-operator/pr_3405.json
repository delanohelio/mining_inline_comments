{"pr_number": 3405, "pr_title": "Initial Kafka Streams TopicStore.", "pr_createdAt": "2020-07-29T08:30:27Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE0OTI1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462149254", "bodyText": "Not something I am asking you to correct, but was there a reason you capitalized Streams, but not kafka?", "author": "samuel-hawker", "createdAt": "2020-07-29T09:01:48Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,17 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /** The store topic for the kafka Streams based TopicStore */", "originalCommit": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE3NzQwNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462177405", "bodyText": "Typo, both should be capitalized.", "author": "alesj", "createdAt": "2020-07-29T09:49:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE0OTI1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1MDE4Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462150183", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            /**\n          \n          \n            \n             * Configure all things needed for KafkaStreamsTopicStore\n          \n          \n            \n             */\n          \n          \n            \n            /**\n          \n          \n            \n             * Configuration required for KafkaStreamsTopicStore\n          \n          \n            \n             */", "author": "samuel-hawker", "createdAt": "2020-07-29T09:03:23Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */", "originalCommit": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE3NzU1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462177558", "bodyText": "OK", "author": "alesj", "createdAt": "2020-07-29T09:49:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1MDE4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1MTM2Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462151366", "bodyText": "Conventionally, in the rest of the code, fields have been annotated like so:\n/* test */ KafkaStreams streams;\n\nAgain not a big deal, but it makes it slightly easier to navigate and understand when the repo is consistent", "author": "samuel-hawker", "createdAt": "2020-07-29T09:05:25Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<Object> closeables = new ArrayList<>();\n+\n+    KafkaStreams streams; // exposed for test purposes", "originalCommit": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1NzQ2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462157469", "bodyText": "Why do we not just change this to only take Objects of type AutoCloseable ?", "author": "samuel-hawker", "createdAt": "2020-07-29T09:15:36Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<Object> closeables = new ArrayList<>();\n+\n+    KafkaStreams streams; // exposed for test purposes\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+\n+            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+            closeables.add(serviceImpl);\n+\n+            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+            streams = new KafkaStreams(topology, kafkaProperties);\n+            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+            closeables.add(streams);\n+            streams.start();\n+\n+            String appServer = config.get(Config.APPLICATION_SERVER);\n+            String[] hostPort = appServer.split(\":\");\n+            log.info(\"Application server gRPC: '{}'\", appServer);\n+            HostInfo hostInfo = new HostInfo(hostPort[0], Integer.parseInt(hostPort[1]));\n+\n+            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n+\n+            ReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(\n+                    streams,\n+                    hostInfo,\n+                    storeName,\n+                    Serdes.String(),\n+                    new TopicSerde(),\n+                    new DefaultGrpcChannelProvider(),\n+                    true,\n+                    filter\n+            );\n+            closeables.add(store);\n+\n+            ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                    kafkaProperties,\n+                    Serdes.String().serializer(),\n+                    new TopicCommandSerde()\n+            );\n+            closeables.add(producer);\n+\n+            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localService =\n+                    new LocalService<>(WaitForResultService.NAME, serviceImpl);\n+            AsyncBiFunctionService<String, String, Integer> service = new DistributedAsyncBiFunctionService<>(\n+                    streams, hostInfo, storeName, localService, new DefaultGrpcChannelProvider()\n+            );\n+            closeables.add(service);\n+\n+            // gRPC\n+\n+            KeyValueStoreGrpc.KeyValueStoreImplBase kvGrpc = streamsKeyValueStoreGrpcImpl(streams, storeName, filter);\n+            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase fnGrpc = streamsAsyncBiFunctionServiceGrpcImpl(localService);\n+            Lifecycle server = streamsGrpcServer(hostInfo, kvGrpc, fnGrpc);\n+            server.start();\n+            AutoCloseable serverCloseable = server::stop;\n+            closeables.add(serverCloseable);\n+\n+            topicStore = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n+            closeables.add(topicStore);\n+        } catch (Exception e) {\n+            stop(); // stop what we already started for any exception\n+            throw e;\n+        }\n+    }\n+\n+    private KeyValueStoreGrpc.KeyValueStoreImplBase streamsKeyValueStoreGrpcImpl(\n+            KafkaStreams streams,\n+            String storeName,\n+            FilterPredicate<String, Topic> filterPredicate\n+    ) {\n+        return new KeyValueStoreGrpcImplLocalDispatcher(\n+                streams,\n+                KeyValueSerde\n+                        .newRegistry()\n+                        .register(\n+                                storeName,\n+                                Serdes.String(), new TopicSerde()\n+                        ),\n+                filterPredicate\n+        );\n+    }\n+\n+    private AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl(\n+            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localWaitForResultService\n+    ) {\n+        return new AsyncBiFunctionServiceGrpcLocalDispatcher(Collections.singletonList(localWaitForResultService));\n+    }\n+\n+    private Lifecycle streamsGrpcServer(\n+            HostInfo localHost,\n+            KeyValueStoreGrpc.KeyValueStoreImplBase streamsStoreGrpcImpl,\n+            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl\n+    ) {\n+        UnknownStatusDescriptionInterceptor unknownStatusDescriptionInterceptor =\n+                new UnknownStatusDescriptionInterceptor(\n+                        Map.of(\n+                                IllegalArgumentException.class, Status.INVALID_ARGUMENT,\n+                                IllegalStateException.class, Status.FAILED_PRECONDITION,\n+                                InvalidStateStoreException.class, Status.FAILED_PRECONDITION,\n+                                Throwable.class, Status.INTERNAL\n+                        )\n+                );\n+\n+        Server server = ServerBuilder\n+                .forPort(localHost.port())\n+                .addService(\n+                        ServerInterceptors.intercept(\n+                                streamsStoreGrpcImpl,\n+                                unknownStatusDescriptionInterceptor\n+                        )\n+                )\n+                .addService(\n+                        ServerInterceptors.intercept(\n+                                streamsAsyncBiFunctionServiceGrpcImpl,\n+                                unknownStatusDescriptionInterceptor\n+                        )\n+                )\n+                .build();\n+\n+        return new Lifecycle() {\n+            @Override\n+            public void start() {\n+                try {\n+                    server.start();\n+                } catch (IOException e) {\n+                    throw new UncheckedIOException(e);\n+                }\n+            }\n+\n+            @Override\n+            public void stop() {\n+                ConcurrentUtil\n+                        .<Server>consumer(Server::awaitTermination)\n+                        .accept(server.shutdown());\n+            }\n+\n+            @Override\n+            public boolean isRunning() {\n+                return !(server.isShutdown() || server.isTerminated());\n+            }\n+        };\n+    }\n+\n+    public void stop() {\n+        Collections.reverse(closeables);\n+        closeables.forEach(KafkaStreamsConfiguration::close);\n+    }\n+\n+    public TopicStore getTopicStore() {\n+        return topicStore;\n+    }\n+\n+    private static void close(Object service) {", "originalCommit": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE3OTU4Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462179583", "bodyText": "I guess this was a copy/paste, where some proxies got auto-generated/added AutoClosable, but was not part of the interface itself.\nLet me check if this change would work here -- if all references are actually AutoCloseable.", "author": "alesj", "createdAt": "2020-07-29T09:53:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1NzQ2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTA2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462161069", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        .compose(v -> {\n          \n          \n            \n                            // update my_topic\n          \n          \n            \n                            return store.update(updatedTopic);\n          \n          \n            \n                        })\n          \n          \n            \n                         // update my_topic\n          \n          \n            \n                        .compose(v -> store.update(updatedTopic))", "author": "samuel-hawker", "createdAt": "2020-07-29T09:21:44Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreTestBase.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public abstract class TopicStoreTestBase {\n+\n+    protected TopicStore store;\n+\n+    protected abstract boolean canRunTest();\n+\n+    @Test\n+    public void testCrud(VertxTestContext context) {\n+        Assumptions.assumeTrue(canRunTest());\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store.create(topic)\n+            .onComplete(context.succeeding())\n+\n+            // Read the topic\n+            .compose(v -> store.read(new TopicName(topicName)))\n+            .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                // assert topics equal\n+                assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+            })))\n+\n+            // try to create it again: assert an error\n+            .compose(v -> store.create(topic))\n+            .onComplete(context.failing(e -> context.verify(() -> {\n+                assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                failedCreateCompleted.complete();\n+            })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+            .compose(v -> {\n+                // update my_topic\n+                return store.update(updatedTopic);\n+            })", "originalCommit": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTQ3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462161471", "bodyText": "On another note these tests are really well written!! :)", "author": "samuel-hawker", "createdAt": "2020-07-29T09:22:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTA2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MDIzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462180239", "bodyText": "This is an old test, I just abstracted it, so I can re-use it with my Kafka Streams TopicStore. :-)", "author": "alesj", "createdAt": "2020-07-29T09:54:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462161928", "bodyText": "This teardown doesn't do anything.. am I missing something here?", "author": "samuel-hawker", "createdAt": "2020-07-29T09:23:13Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();\n+                })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+                .compose(v -> {\n+                    // update my_topic\n+                    return store2.update(updatedTopic);\n+                })\n+                .onComplete(context.succeeding())\n+\n+                // re-read it and assert equal\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(rereadTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(rereadTopic.getTopicName(), is(updatedTopic.getTopicName()));\n+                    assertThat(rereadTopic.getNumPartitions(), is(updatedTopic.getNumPartitions()));\n+                    assertThat(rereadTopic.getNumReplicas(), is(updatedTopic.getNumReplicas()));\n+                    assertThat(rereadTopic.getConfig(), is(updatedTopic.getConfig()));\n+                })))\n+\n+                // delete it\n+                .compose(v -> store2.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.succeeding())\n+\n+                // assert we can't read it again\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(deletedTopic -> context.verify(() ->\n+                        assertThat(deletedTopic, is(nullValue()))))\n+                )\n+\n+                // delete it again: assert an error\n+                .compose(v -> store1.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.NoSuchEntityExistsException.class));\n+                    async.flag();\n+                })));\n+    }\n+\n+    @AfterEach\n+    public void teardown(VertxTestContext context) {", "originalCommit": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MzAwNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462183007", "bodyText": "It does that Checkpoint::flag ... whatever that does , as I've copied this from previous test.", "author": "alesj", "createdAt": "2020-07-29T09:59:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4NTc5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462185798", "bodyText": "Ah, this is not needed anymore, the checkpont needs to be flagged or it delays a test from finishing, since you aren't waiting for any actual teardown, this is redundant.\nThis is sort of analogous to doing\nfuture = new Future()\nfuture.complete()\nIf you're waiting on nothing it is just not needed.", "author": "samuel-hawker", "createdAt": "2020-07-29T10:03:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE5MTczNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462191735", "bodyText": "So I can remove this as well in \"KafkaStreamsTopicStoreTest\" ?", "author": "alesj", "createdAt": "2020-07-29T10:14:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE5MzU3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462193571", "bodyText": "Removed ...", "author": "alesj", "createdAt": "2020-07-29T10:18:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}], "type": "inlineReview"}, {"oid": "0f05cf18ba167b0fb95ee90d5937708b1c654dbb", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0f05cf18ba167b0fb95ee90d5937708b1c654dbb", "message": "Apply feedback.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-07-29T10:03:15Z", "type": "forcePushed"}, {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3f76fa46b74fdd815a7b4c631704e639821047be", "message": "Apply feedback.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-07-29T10:17:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTI0OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464325249", "bodyText": "This is the name of the store within Kafka Streams?", "author": "tombentley", "createdAt": "2020-08-03T10:20:10Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"topic-store\");", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MTM3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464381373", "bodyText": "Yes, this is there those Topic instances (from TopicStore) are stored.", "author": "alesj", "createdAt": "2020-08-03T12:28:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTI0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTU3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464325575", "bodyText": "Can we include the unit?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static final String TC_STALE_RESULT_TIMEOUT = \"STRIMZI_STALE_RESULT_TIMEOUT\";\n          \n          \n            \n                public static final String TC_STALE_RESULT_TIMEOUT_MS = \"STRIMZI_STALE_RESULT_TIMEOUT_MS\";", "author": "tombentley", "createdAt": "2020-08-03T10:20:55Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -108,6 +108,12 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     public static final String TC_TLS_KEYSTORE_PASSWORD = \"STRIMZI_KEYSTORE_PASSWORD\";\n     public static final String TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = \"STRIMZI_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM\";\n \n+    public static final String TC_STORE_TOPIC = \"STRIMZI_STORE_TOPIC\";\n+    public static final String TC_STORE_NAME = \"STRIMZI_STORE_NAME\";\n+    public static final String TC_APPLICATION_ID = \"STRIMZI_APPLICATION_ID\";\n+    public static final String TC_APPLICATION_SERVER = \"STRIMZI_APPLICATION_SERVER\";\n+    public static final String TC_STALE_RESULT_TIMEOUT = \"STRIMZI_STALE_RESULT_TIMEOUT\";", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MTQ4Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464381483", "bodyText": "Sure.", "author": "alesj", "createdAt": "2020-08-03T12:29:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464325996", "bodyText": "Something like __strimzi_topic_store would be a better default.", "author": "tombentley", "createdAt": "2020-08-03T10:21:48Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MTg5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464381893", "bodyText": "Yeah, since those log topics get autogenerated from the store name ... will fix it.", "author": "alesj", "createdAt": "2020-08-03T12:29:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0Nzk3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516747973", "bodyText": "I got confused here. This is the topic for storing \"topics\" information so, because it's an internal topic in the infrastructure, @tombentley asked to rename it as __strimzi_topic_store while I see this name used for the store name which is not a topic, right?", "author": "ppatierno", "createdAt": "2020-11-03T15:22:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0MzcwNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516843706", "bodyText": "This is the name of the entry topic in our Streams topology.\nWhere we construct a flow for a Kafka message (that is sent from KafkaStreamsTopicStore), so it ends up in KVStore.\nAh, I see I renamed STORE_NAME (which is the name of our KVStore), but forgot to rename this topic name ... doing it now.", "author": "alesj", "createdAt": "2020-11-03T17:38:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0NDY0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516844645", "bodyText": "@tombentley but it's gonna be \"__strimzi_store_topic\", since it's a topic not a store.\n(you suggested \"__strimzi_topic_store\", which I used for the STORE_NAME)", "author": "alesj", "createdAt": "2020-11-03T17:39:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg2NDA0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516864043", "bodyText": "But what it stores is topics. It seems weird to call a topic \"...topic\", because obviously it's a topic. I'd be happy with __strimzi_topic_metadata or something if we want to distinguish from the kv store name.", "author": "tombentley", "createdAt": "2020-11-03T18:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5MzQyOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516893429", "bodyText": "This topic doesn't store anything.\nThis is just the name of the topic where out Streams topology \"starts\",\nwhere we send Kafka messages from KafkaStreamsTopicStore instance.", "author": "alesj", "createdAt": "2020-11-03T19:06:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5MzYxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516893618", "bodyText": "Btw, I'm happy with whatever name you decide. :-)", "author": "alesj", "createdAt": "2020-11-03T19:06:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDQyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464330424", "bodyText": "Since we're already representing the topic as JSON I think it would be better to represent the command as JSON too, then this would pretty much be a call to Jackson's JsonMapper.\nI also wonder about including an explicit version.", "author": "tombentley", "createdAt": "2020-08-03T10:31:42Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        try {\n+            ByteArrayOutputStream baos = new ByteArrayOutputStream();", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQyMjQ3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464422470", "bodyText": "Explicit version?", "author": "alesj", "createdAt": "2020-08-03T13:44:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDQyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1NTgxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464455817", "bodyText": "Yeah. I considered it at the time for the TopicSerialization and decided that no version was semantically equivalent to having a version=0. That's still true, of course, so I'm not saying you should do this. IOW, I'm still wondering. Only a need to change the format is really going to force me to decide ;-)", "author": "tombentley", "createdAt": "2020-08-03T14:37:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDQyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDU2MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464330560", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-08-03T10:32:04Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStore.java", "diffHunk": "@@ -20,6 +20,10 @@\n \n     }\n \n+    public static class InvalidStateException extends Exception {", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMTAzNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464331034", "bodyText": "Since TO has to deal with topic names being represented as Kube resource names, it's always best to be explicit about which name you mean.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // Key is topic name -- which is also used for KeyValue store key\n          \n          \n            \n                    // Key is Kafka topic name -- which is also used for KeyValue store key", "author": "tombentley", "createdAt": "2020-08-03T10:33:10Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMTYxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464331614", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-08-03T10:34:29Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key\n+        KStream<String, TopicCommand> topicRequest = builder.stream(\n+                storeTopic,\n+                Consumed.with(Serdes.String(), new TopicCommandSerde())\n+        );\n+\n+        // Data structure holds all topic information\n+        StoreBuilder<KeyValueStore<String /* topic */, Topic>> topicStoreBuilder =\n+                Stores\n+                        .keyValueStoreBuilder(\n+                                Stores.inMemoryKeyValueStore(topicStoreName),\n+                                Serdes.String(), new TopicSerde()\n+                        )\n+                        .withCachingEnabled()\n+                        .withLoggingEnabled(configuration);\n+\n+        builder.addStateStore(topicStoreBuilder);\n+\n+        topicRequest.process(\n+            () -> new TopicCommandTransformer(topicStoreName, dispatcher),\n+            topicStoreName\n+        );\n+\n+        return builder.build(kafkaProperties);\n+    }\n+\n+    private static class TopicCommandTransformer implements Processor<String, TopicCommand> {", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjI0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464332241", "bodyText": "When we throw EEE like this should be have updated the store by side-effect? I assume not, since I think ZK would throw without updating the stored state.", "author": "tombentley", "createdAt": "2020-08-03T10:35:50Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key\n+        KStream<String, TopicCommand> topicRequest = builder.stream(\n+                storeTopic,\n+                Consumed.with(Serdes.String(), new TopicCommandSerde())\n+        );\n+\n+        // Data structure holds all topic information\n+        StoreBuilder<KeyValueStore<String /* topic */, Topic>> topicStoreBuilder =\n+                Stores\n+                        .keyValueStoreBuilder(\n+                                Stores.inMemoryKeyValueStore(topicStoreName),\n+                                Serdes.String(), new TopicSerde()\n+                        )\n+                        .withCachingEnabled()\n+                        .withLoggingEnabled(configuration);\n+\n+        builder.addStateStore(topicStoreBuilder);\n+\n+        topicRequest.process(\n+            () -> new TopicCommandTransformer(topicStoreName, dispatcher),\n+            topicStoreName\n+        );\n+\n+        return builder.build(kafkaProperties);\n+    }\n+\n+    private static class TopicCommandTransformer implements Processor<String, TopicCommand> {\n+        private final String topicStoreName;\n+        private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+        private KeyValueStore<String, Topic> store;\n+\n+        public TopicCommandTransformer(\n+                String topicStoreName,\n+                ForeachAction<? super String, ? super Integer> dispatcher\n+        ) {\n+            this.topicStoreName = topicStoreName;\n+            this.dispatcher = dispatcher;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\"unchecked\")\n+        public void init(ProcessorContext context) {\n+            store = (KeyValueStore<String, Topic>) context.getStateStore(topicStoreName);\n+        }\n+\n+        @Override\n+        public void process(String key, TopicCommand value) {\n+            String uuid = value.getUuid();\n+            TopicCommand.Type type = value.getType();\n+            Integer result = null;\n+            switch (type) {\n+                case CREATE:\n+                    Topic previous = store.putIfAbsent(key, value.getTopic());\n+                    if (previous != null) {\n+                        result = KafkaStreamsTopicStore.toIndex(TopicStore.EntityExistsException.class);", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MzI4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464383288", "bodyText": "That's why I used putIfAbsent", "author": "alesj", "createdAt": "2020-08-03T12:32:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjI0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4OTg5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464389893", "bodyText": "Ah, Mondays!", "author": "tombentley", "createdAt": "2020-08-03T12:46:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjI0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjgxMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464332813", "bodyText": "Is this used? Can it not be private?", "author": "tombentley", "createdAt": "2020-08-03T10:37:12Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/WaitForResultService.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * We first register CompletableFuture,\n+ * which will be completed once Streams transformer handles CRUD command.\n+ */\n+public class WaitForResultService implements AsyncBiFunctionService.WithSerdes<String, String, Integer> {\n+    public static final String NAME = \"WaitForResultService\";\n+\n+    private final long timeoutMillis;\n+    private final Map<String, ResultCF> waitingResults = new ConcurrentHashMap<>();\n+    private final ScheduledExecutorService executorService;\n+\n+    public WaitForResultService(long timeoutMillis, ForeachActionDispatcher<String, Integer> dispatcher) {\n+        this.timeoutMillis = timeoutMillis;\n+        dispatcher.register(this::topicUpdated);\n+        executorService = new ScheduledThreadPoolExecutor(1);\n+        executorService.scheduleAtFixedRate(this::checkStaleResults, timeoutMillis / 2, timeoutMillis / 2, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private void checkStaleResults() {\n+        long now = System.currentTimeMillis();\n+        Iterator<Map.Entry<String, ResultCF>> iterator = waitingResults.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            ResultCF rcf = iterator.next().getValue();\n+            if (now - rcf.ts > timeoutMillis) {\n+                rcf.complete(KafkaStreamsTopicStore.toIndex(TopicStore.InvalidStateException.class));\n+                iterator.remove();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Notification (from transformer)\n+     */\n+    private void topicUpdated(String uuid, Integer i) {\n+        CompletableFuture<Integer> cf = waitingResults.remove(uuid);\n+        if (cf != null) {\n+            cf.complete(i);\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    public Serde<String> keySerde() {\n+        return Serdes.String();\n+    }\n+\n+    @Override\n+    public Serde<String> reqSerde() {\n+        return Serdes.String();\n+    }\n+\n+    @Override\n+    public Serde<Integer> resSerde() {\n+        return Serdes.Integer();\n+    }\n+\n+    @Override\n+    public CompletionStage<Integer> apply(String name, String uuid) {\n+        ResultCF cf = new ResultCF();\n+        waitingResults.put(uuid, cf);\n+        return cf;\n+    }\n+\n+    private static class ResultCF extends CompletableFuture<Integer> {\n+        final long ts;", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMzI5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464333293", "bodyText": "Yeah, here we probably need to assert not only that it threw but that the state was not (or perhaps was) updated.", "author": "tombentley", "createdAt": "2020-08-03T10:38:25Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4NDkyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464384923", "bodyText": "How would this look in this vert.x code?", "author": "alesj", "createdAt": "2020-08-03T12:36:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMzI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzNDIzMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464334232", "bodyText": "Why are we changing this?", "author": "tombentley", "createdAt": "2020-08-03T10:40:42Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/ZkTopicStoreTest.java", "diffHunk": "@@ -6,38 +6,28 @@\n \n import io.strimzi.operator.topic.zk.Zk;\n import io.strimzi.test.EmbeddedZooKeeper;\n-import io.vertx.core.Promise;\n import io.vertx.core.Vertx;\n import io.vertx.junit5.Checkpoint;\n-import io.vertx.junit5.VertxExtension;\n import io.vertx.junit5.VertxTestContext;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Disabled;\n-import org.junit.jupiter.api.Test;\n-import org.junit.jupiter.api.extension.ExtendWith;\n \n import java.io.IOException;\n-import java.util.Collections;\n \n-import static org.hamcrest.CoreMatchers.instanceOf;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.hamcrest.CoreMatchers.nullValue;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-\n-@Disabled\n-@ExtendWith(VertxExtension.class)\n-public class ZkTopicStoreTest {\n-\n-    private EmbeddedZooKeeper zkServer;\n+public class ZkTopicStoreTest extends TopicStoreTestBase {", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4NDM1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464384353", "bodyText": "To not duplicate the tests -- I extracted them to TopicStoreTestBase class.", "author": "alesj", "createdAt": "2020-08-03T12:35:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzNDIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464338647", "bodyText": "We should probably validate the topic at this point. We don't really want to rely on autocreation because :\n\nThat will eventually be removed.\nIt's quite easy to end up with a misconfigured topic that way (e.g. if default RF=1).\n\nIt's a little tricky because the TO needs to work in clusters with only 1 broker, but in clusters with 3 or more we'd want RF=3 and min.insync.replicas=2. You should be able to use the Kafka Admin client something like this:\n\nDescribe the cluster.\nCheck whether the topic exists:\nIf the topic does not exist try to create it with RF=min(3, clusterSize) and minISR=RF-1. If created OK we're good. If not (e.g. authorization) then log an error telling the user to create it and exit.\nIf the topic already exists check whether RF=min(3, clusterSize) && minISR=RF-1. If it does we're good. If not log a warning that the durability of the topic is not sufficient for production use and proceed.", "author": "tombentley", "createdAt": "2020-08-03T10:50:55Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4ODg3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464388870", "bodyText": "What do you mean under \"describe the cluster\"?\nI guess AdminClient has some API for this?", "author": "alesj", "createdAt": "2020-08-03T12:44:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM5MDg3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464390871", "bodyText": "Yes, describeCluster(), describeTopics() and describeConfigs() are all probably relevant to doing this.", "author": "tombentley", "createdAt": "2020-08-03T12:48:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQwMzE2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464403168", "bodyText": "If not (e.g. authorization) then log an error telling the user to create it and exit.\n\n@tombentley out of curiosity, doesn't the TO uses a super-user for doing that anyway?", "author": "ppatierno", "createdAt": "2020-08-03T13:11:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ0ODkzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464448939", "bodyText": "@tombentley how many partitions?\nAnd how do I set minISR when creating a new topic?", "author": "alesj", "createdAt": "2020-08-03T14:27:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1MjA1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464452053", "bodyText": "@ppatierno is does when it's deployed by the CO, but there are users who run it stand alone, with Kafka clusters not managed by the CO, so we can't assume we have authz.", "author": "tombentley", "createdAt": "2020-08-03T14:32:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1MzkwMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464453901", "bodyText": "@alesj min.insync.replicas is a topic config, so NewTopic(...).configs(topicConfigs) will do it (yeah, the fact there there's not a constructor taking the configs makes it a little less discoverable).", "author": "tombentley", "createdAt": "2020-08-03T14:34:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1NDYwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464454603", "bodyText": "tbh I am not sure how many people are using TO in this way but I have no clue otherwise I would remove the support for it.", "author": "ppatierno", "createdAt": "2020-08-03T14:35:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1OTI3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464459274", "bodyText": "BTH @ppatierno supporting that use case adds very little additional complexity. Removing it would only serve to have fewer users, not simplify anything significantly.", "author": "tombentley", "createdAt": "2020-08-03T14:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ2NTU3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464465575", "bodyText": "I know, it's just about a warning :-)", "author": "ppatierno", "createdAt": "2020-08-03T14:53:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM0MDA3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464340074", "bodyText": "As discussed elsewhere, we don't really need the distributed aspect of this. If there some other implementation which would avoid the need for the GRPC layer?", "author": "tombentley", "createdAt": "2020-08-03T10:54:10Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+\n+            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+            closeables.add(serviceImpl);\n+\n+            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+            streams = new KafkaStreams(topology, kafkaProperties);\n+            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+            closeables.add(streams);\n+            streams.start();\n+\n+            String appServer = config.get(Config.APPLICATION_SERVER);\n+            String[] hostPort = appServer.split(\":\");\n+            log.info(\"Application server gRPC: '{}'\", appServer);\n+            HostInfo hostInfo = new HostInfo(hostPort[0], Integer.parseInt(hostPort[1]));\n+\n+            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n+\n+            DistributedReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(", "originalCommit": "3f76fa46b74fdd815a7b4c631704e639821047be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4NzczMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464387732", "bodyText": "If it's only ever gonna be single instance, then we're good with Streams already provide -- some sort of in-memory KVStore.\nOtoh, I would somehow leave my current distributed code -- so it doesn't go to waste,\nor if someone needs more then one instance.\nHow do you suggest to do this?", "author": "alesj", "createdAt": "2020-08-03T12:42:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM0MDA3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1ODcwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464858704", "bodyText": "should we check that it's really just one partition and not more than that?", "author": "ppatierno", "createdAt": "2020-08-04T07:38:02Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();", "originalCommit": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwOTQ0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464909445", "bodyText": "If it's more than one partition, which replicas size do we take then? Max, avg, min, ... ?", "author": "alesj", "createdAt": "2020-08-04T09:06:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1ODcwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMzM5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464913396", "bodyText": "Kafka doesn't really have a first-class concept of RF even though it's routinely talked about. Different partitions of the same topic can be replicated to different numbers of brokers. And indeed, that might happen (transiently) even in a well-managed cluster, e.g. increasing RF, but doing it in a batched way. For our purposes here using the min, or just using partition 0 would make sense, but I guess using the min might be preferable since it detects the case of inhomogeneous topic.", "author": "tombentley", "createdAt": "2020-08-04T09:13:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1ODcwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464859633", "bodyText": "the entire logic of this code is kind of blocking because of the usage of the get calls on the different KafkaFuture.\nI was wondering if we need a more async way (so maybe some Vert.x executeBlocking calls) to do that but @tombentley knows better than me if it fits well in the overall TO logic calling this piece of code.", "author": "ppatierno", "createdAt": "2020-08-04T07:39:52Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "originalCommit": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMDI1NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464910255", "bodyText": "This is configuration validation, and the topic needs to exist before Kafka Streams kick-in.", "author": "alesj", "createdAt": "2020-08-04T09:08:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTU0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464911541", "bodyText": "Yeah, I know but it doesn't mean that the code cannot be async. You can chain async calls in order to do configuration first and then starting Kafka Streams. But as I said I don't know better the calling code and the overall TO architecture, so @tombentley can answer to that. Maybe I am wrong :-)", "author": "ppatierno", "createdAt": "2020-08-04T09:10:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxNzUzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464917533", "bodyText": "You should be able to use KafkaFuture.whenComplete() in a similar way to vertx's Future.compose(). This is what KafkaAvailability does IIRC. It's also possible to convert KafkaFuture to vertx Future (see io.strimzi.operator.common.Util#kafkaFutureToVertxFuture, and note that it completes the Future on the vertx event loop thread).", "author": "tombentley", "createdAt": "2020-08-04T09:20:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkzMTMwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464931303", "bodyText": "Do we really want/need to drag Vert.x into this configuration?\nAfais, it's 2-3 get() calls, which shouldn't take long.\nIt would just make this configuration even more complex -- I'm already over the class import limit. :-)", "author": "alesj", "createdAt": "2020-08-04T09:44:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkzNjIzMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464936232", "bodyText": "This is how our operators are developed ... the code is async everywhere using Vert.x. We wrapped the Kubernetes REST API calls in the same way and of course any kind of call to Kafka as Admin client. I don't think you can make any assumption on the time needed for the get() calls.", "author": "ppatierno", "createdAt": "2020-08-04T09:53:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAxMTU3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465011576", "bodyText": "OK, let me try to impl this config::start in an async way then.", "author": "alesj", "createdAt": "2020-08-04T12:25:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}], "type": "inlineReview"}, {"oid": "2f2aa454a32bd0191be099a625a7b704dd1e8c39", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2f2aa454a32bd0191be099a625a7b704dd1e8c39", "message": "Fix admin lookup.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-08-04T08:51:19Z", "type": "forcePushed"}, {"oid": "e700a5677bdc37106242f802061c9db70d744205", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e700a5677bdc37106242f802061c9db70d744205", "message": "Make distribution optional, handle admin stuff in async.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-08-04T15:40:48Z", "type": "forcePushed"}, {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/72eba48b82d18923e67ad20d175fd61e42ab87c9", "message": "Make distribution optional, handle admin stuff in async.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-08-04T15:41:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2ODMzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465168337", "bodyText": "I think the message should be more actionable: What do they need to do to make this warning do away? And there's no harm is mentioning the topic name explicitly.", "author": "tombentley", "createdAt": "2020-08-04T16:14:31Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -4,246 +4,163 @@\n  */\n package io.strimzi.operator.topic;\n \n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n-import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n-import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.LocalService;\n-import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n-import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n-import io.apicurio.registry.streams.distore.FilterPredicate;\n-import io.apicurio.registry.streams.distore.KeyValueSerde;\n-import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n-import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n-import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n-import io.apicurio.registry.streams.utils.Lifecycle;\n import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n-import io.apicurio.registry.utils.ConcurrentUtil;\n import io.apicurio.registry.utils.kafka.AsyncProducer;\n import io.apicurio.registry.utils.kafka.ProducerActions;\n-import io.grpc.Server;\n-import io.grpc.ServerBuilder;\n-import io.grpc.ServerInterceptors;\n-import io.grpc.Status;\n import org.apache.kafka.clients.admin.Admin;\n-import org.apache.kafka.clients.admin.DescribeClusterResult;\n-import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.admin.NewTopic;\n-import org.apache.kafka.clients.admin.TopicDescription;\n+import org.apache.kafka.common.KafkaFuture;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.streams.KafkaStreams;\n import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.errors.InvalidStateStoreException;\n-import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.IOException;\n-import java.io.UncheckedIOException;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n-import java.util.Map;\n import java.util.Properties;\n import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n \n import static java.lang.Integer.parseInt;\n \n /**\n  * Configuration required for KafkaStreamsTopicStore\n  */\n-@SuppressWarnings(\"checkstyle:ClassDataAbstractionCoupling\")\n public class KafkaStreamsConfiguration {\n     private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n \n     private final List<AutoCloseable> closeables = new ArrayList<>();\n \n     /* test */ KafkaStreams streams;\n-    private TopicStore topicStore;\n-\n-    public void start(Config config, Properties kafkaProperties) {\n-        try {\n-            String storeTopic = config.get(Config.STORE_TOPIC);\n-            String storeName = config.get(Config.STORE_NAME);\n-\n-            // check if entry topic has the right configuration\n-            Admin admin = Admin.create(kafkaProperties);\n-            DescribeClusterResult clusterResult = admin.describeCluster();\n-            int clusterSize = clusterResult.nodes().get().size();\n-            Set<String> topics = admin.listTopics().names().get();\n-            if (topics.contains(storeTopic)) {\n-                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n-                int rf = topicDescription.partitions().get(0).replicas().size();\n-                ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n-                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(storeTopicConfigResource));\n-                int minISR = parseInt(configsResult.values().get(storeTopicConfigResource).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n-                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n-                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n-                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n-                }\n-            } else {\n-                int rf = Math.min(3, clusterSize);\n-                int minISR = rf - 1;\n-                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n-                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n-                admin.createTopics(Collections.singleton(newTopic)).all().get();\n-            }\n-\n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n-            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n-            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n-            closeables.add(serviceImpl);\n-\n-            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n-            streams = new KafkaStreams(topology, kafkaProperties);\n-            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n-            closeables.add(streams);\n-            streams.start();\n-\n-            String appServer = config.get(Config.APPLICATION_SERVER);\n-            String[] hostPort = appServer.split(\":\");\n-            log.info(\"Application server gRPC: '{}'\", appServer);\n-            HostInfo hostInfo = new HostInfo(hostPort[0], parseInt(hostPort[1]));\n-\n-            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n-\n-            DistributedReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(\n-                    streams,\n-                    hostInfo,\n-                    storeName,\n-                    Serdes.String(),\n-                    new TopicSerde(),\n-                    new DefaultGrpcChannelProvider(),\n-                    true,\n-                    filter\n-            );\n-            closeables.add(store);\n-\n-            ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n-                    kafkaProperties,\n-                    Serdes.String().serializer(),\n-                    new TopicCommandSerde()\n-            );\n-            closeables.add(producer);\n-\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localService =\n-                    new LocalService<>(WaitForResultService.NAME, serviceImpl);\n-            AsyncBiFunctionService<String, String, Integer> service = new DistributedAsyncBiFunctionService<>(\n-                    streams, hostInfo, storeName, localService, new DefaultGrpcChannelProvider()\n-            );\n-            closeables.add(service);\n-\n-            // gRPC\n-\n-            KeyValueStoreGrpc.KeyValueStoreImplBase kvGrpc = streamsKeyValueStoreGrpcImpl(streams, storeName, filter);\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase fnGrpc = streamsAsyncBiFunctionServiceGrpcImpl(localService);\n-            Lifecycle server = streamsGrpcServer(hostInfo, kvGrpc, fnGrpc);\n-            server.start();\n-            AutoCloseable serverCloseable = server::stop;\n-            closeables.add(serverCloseable);\n-\n-            topicStore = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n-        } catch (Exception e) {\n-            stop(); // stop what we already started for any exception\n-            throw new IllegalStateException(e);\n-        }\n-    }\n-\n-    private KeyValueStoreGrpc.KeyValueStoreImplBase streamsKeyValueStoreGrpcImpl(\n-            KafkaStreams streams,\n-            String storeName,\n-            FilterPredicate<String, Topic> filterPredicate\n-    ) {\n-        return new KeyValueStoreGrpcImplLocalDispatcher(\n-                streams,\n-                KeyValueSerde\n-                        .newRegistry()\n-                        .register(\n-                                storeName,\n-                                Serdes.String(), new TopicSerde()\n-                        ),\n-                filterPredicate\n-        );\n-    }\n-\n-    private AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl(\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localWaitForResultService\n-    ) {\n-        return new AsyncBiFunctionServiceGrpcLocalDispatcher(Collections.singletonList(localWaitForResultService));\n-    }\n-\n-    private Lifecycle streamsGrpcServer(\n-            HostInfo localHost,\n-            KeyValueStoreGrpc.KeyValueStoreImplBase streamsStoreGrpcImpl,\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl\n-    ) {\n-        UnknownStatusDescriptionInterceptor unknownStatusDescriptionInterceptor =\n-                new UnknownStatusDescriptionInterceptor(\n-                        Map.of(\n-                                IllegalArgumentException.class, Status.INVALID_ARGUMENT,\n-                                IllegalStateException.class, Status.FAILED_PRECONDITION,\n-                                InvalidStateStoreException.class, Status.FAILED_PRECONDITION,\n-                                Throwable.class, Status.INTERNAL\n-                        )\n-                );\n-\n-        Server server = ServerBuilder\n-                .forPort(localHost.port())\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsStoreGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsAsyncBiFunctionServiceGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .build();\n-\n-        return new Lifecycle() {\n-            @Override\n-            public void start() {\n-                try {\n-                    server.start();\n-                } catch (IOException e) {\n-                    throw new UncheckedIOException(e);\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                                        c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);", "originalCommit": "72eba48b82d18923e67ad20d175fd61e42ab87c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5MTc2Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465191766", "bodyText": "What would make it go away?\n\"Replication factor should be at least 3 and {} one less.\" ?", "author": "alesj", "createdAt": "2020-08-04T16:52:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2ODMzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTUzNTk1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465535952", "bodyText": "Increase the replication factor of topic {} to at least 3 and configure the min.in.sync.replicas to {}", "author": "tombentley", "createdAt": "2020-08-05T07:43:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2ODMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2OTc0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465169747", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Simple local / in-memory store configuration.\n          \n          \n            \n             * Simple local / in-memory store configuration which is sufficient when the TO runs in a single pod.", "author": "tombentley", "createdAt": "2020-08-04T16:16:35Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.lang.reflect.InvocationHandler;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Proxy;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * Simple local / in-memory store configuration.", "originalCommit": "72eba48b82d18923e67ad20d175fd61e42ab87c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465177201", "bodyText": "I think we're going to need something more than just a tool which the user has to run in order to migrate. I think we need the TO to do the migration automatically on start up:\nif !exists(topic) {\n    if exists(`/strimzi` znode) {\n        migrate using the logic you have here. \n        if migrated ok {\n            delete(`/strimzi` znode)\n        } else {\n            delete(topic)\n            exit process\n        }\n    }\n}\n\nWe're also going to need some integration test for this.", "author": "tombentley", "createdAt": "2020-08-04T16:28:24Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Zk2KafkaStreams.java", "diffHunk": "@@ -18,24 +18,23 @@ public static void main(String[] args) {\n         // TODO\n     }\n \n-    public static void move(Zk zk, Config config, Properties kafkaProperties) {\n+    public static void move(Zk zk, Config config, Properties kafkaProperties) throws Exception {\n         String topicsPath = config.get(Config.TOPICS_PATH);\n         TopicStore zkTopicStore = new ZkTopicStore(zk, topicsPath);\n \n         KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n-        try {\n-            configuration.start(config, kafkaProperties);\n-            TopicStore ksTopicStore = configuration.getTopicStore();\n-\n-            zk.children(topicsPath, result -> result.map(list -> {\n-                list.forEach(topicName -> {\n-                    Future<Topic> ft = zkTopicStore.read(new TopicName(topicName));\n-                    ft.onSuccess(ksTopicStore::create);\n-                });\n-                return null;\n-            }));\n-        } finally {\n-            configuration.stop();\n-        }\n+        configuration.start(config, kafkaProperties)", "originalCommit": "72eba48b82d18923e67ad20d175fd61e42ab87c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5MjQ4NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465192485", "bodyText": "This is just a PoC, something to discuss, WIP.", "author": "alesj", "createdAt": "2020-08-04T16:53:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM0Mjg4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r471342886", "bodyText": "Any progress on this?", "author": "tombentley", "createdAt": "2020-08-17T09:05:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNDQ2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r472104468", "bodyText": "Not yet.\nNeed to think of a way to test this ...", "author": "alesj", "createdAt": "2020-08-18T11:23:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ=="}], "type": "inlineReview"}, {"oid": "801044c896811377f01330ef27317411eef16464", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/801044c896811377f01330ef27317411eef16464", "message": "Make distribution optional, handle admin stuff in async.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-08-04T16:44:27Z", "type": "forcePushed"}, {"oid": "80038bbc6c994a94562bb4e41eda74202be9a467", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/80038bbc6c994a94562bb4e41eda74202be9a467", "message": "Use 1.3.0.Final Registry (extracted) jar.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-08-31T12:27:16Z", "type": "forcePushed"}, {"oid": "d480f7431d1ebe85a2c07e878cc1336ba009f766", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d480f7431d1ebe85a2c07e878cc1336ba009f766", "message": "Initial design document.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-09-02T13:48:52Z", "type": "forcePushed"}, {"oid": "3d622b3caecc6295cf632aab08d7c272bad759bd", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3d622b3caecc6295cf632aab08d7c272bad759bd", "message": "Initial design document.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-09-02T14:18:42Z", "type": "forcePushed"}, {"oid": "78e8670b4f99d69289d95abdfe0f8ccd028303bf", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/78e8670b4f99d69289d95abdfe0f8ccd028303bf", "message": "Fix the vertx hang (tnx julienv).\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-09-17T14:23:41Z", "type": "forcePushed"}, {"oid": "7bd739d78d71b88e550522d12d3bb1cf4123aff6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7bd739d78d71b88e550522d12d3bb1cf4123aff6", "message": "Use embedded Kafka cluster.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-09-24T12:16:44Z", "type": "forcePushed"}, {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "message": "Move assume into the test method, not initialization.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-09-24T12:50:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NDc2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494454768", "bodyText": "Can we document somewhere, either on the methods or the class that configure() must be called before getStore() and getLookupService() can return non-null?\nAlso I find the name a bit confusing. It's not really the configuration of a store, it's more of a factory for stores and lookups.", "author": "tombentley", "createdAt": "2020-09-24T16:30:38Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * SPI for the store configuration.\n+ * * in-memory / local\n+ * * distributed\n+ */\n+interface StoreConfiguration {", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg1NDU1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494854552", "bodyText": "OK, StoreAndServiceFactory it is.\nAnd it's now just one method, create which returns StoreContext (a tuple with store and service).", "author": "alesj", "createdAt": "2020-09-25T09:10:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NDc2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NjY3Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494456677", "bodyText": "You're assuming the appServer actually contains a (single) :. Best to validate.", "author": "tombentley", "createdAt": "2020-09-24T16:33:52Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/DistributedStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.utils.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.utils.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.diservice.LocalService;\n+import io.apicurio.registry.utils.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.utils.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.utils.streams.distore.FilterPredicate;\n+import io.apicurio.registry.utils.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.utils.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.utils.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.utils.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.utils.streams.ext.Lifecycle;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Add configuration for distributed store (via gRPC).\n+ * Required when we're running more than one instance/node of topic operator.\n+ */\n+class DistributedStoreConfiguration implements StoreConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(DistributedStoreConfiguration.class);\n+\n+    private ReadOnlyKeyValueStore<String, Topic> store;\n+    private BiFunction<String, String, CompletionStage<Integer>> lookupService;\n+\n+    @Override\n+    public ReadOnlyKeyValueStore<String, Topic> getStore() {\n+        return store;\n+    }\n+\n+    @Override\n+    public BiFunction<String, String, CompletionStage<Integer>> getLookupService() {\n+        return lookupService;\n+    }\n+\n+    public void configure(\n+            Config config,\n+            Properties kafkaProperties,\n+            KafkaStreams streams,\n+            AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl,\n+            List<AutoCloseable> closeables\n+    ) {\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        String appServer = config.get(Config.APPLICATION_SERVER);\n+        String[] hostPort = appServer.split(\":\");", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NzY1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494457651", "bodyText": "We should document this method. The passing in of closeables which the method adds to it a little non-intuitive and what are the String parameters to the lookupService? From the doc for AsyncBiFunctionService I learn that they're keys and requests, but this doesn't really answer the question \"what are they\"?", "author": "tombentley", "createdAt": "2020-09-24T16:35:23Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * SPI for the store configuration.\n+ * * in-memory / local\n+ * * distributed\n+ */\n+interface StoreConfiguration {\n+    void configure(", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2MTM0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494861340", "bodyText": "Docs added.", "author": "alesj", "createdAt": "2020-09-25T09:22:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NzY1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MjcwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494462709", "bodyText": "This class does not represent configuration (as in, a bunch of values, properties for configuring something). Rather its start() configures Kafka streams and returns a CompletionState of a topic store which happens to use Kafka streams, but it's clearly a component with a lifecycle (it has a stop). So wouldn't something like KafkaStreamTopicStoreService be a better name?", "author": "tombentley", "createdAt": "2020-09-24T16:43:39Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2MTE4MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494861180", "bodyText": "OK, renamed.", "author": "alesj", "createdAt": "2020-09-25T09:22:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MjcwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzUxMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494463513", "bodyText": "It would help readability if you factored the sequence of actions which you compose into separate methods with sensible names.", "author": "tombentley", "createdAt": "2020-09-24T16:44:54Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2NDg1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494864852", "bodyText": "Done.", "author": "alesj", "createdAt": "2020-09-25T09:28:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzUxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzY3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494463678", "bodyText": "Another method here", "author": "tombentley", "createdAt": "2020-09-24T16:45:07Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })\n+                .thenCompose(v -> {\n+                    long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+                    ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+                    WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+                    closeables.add(serviceImpl);\n+\n+                    boolean[] done = new boolean[1];\n+                    CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+                    KafkaStreams.StateListener listener = (newState, oldState) -> {\n+                        if (newState == KafkaStreams.State.RUNNING && !done[0]) {\n+                            done[0] = true; // no need for dup complete\n+                            cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+                        }\n+                        if (newState == KafkaStreams.State.ERROR) {\n+                            cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+                        }\n+                    };\n+\n+                    Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+                    streams = new KafkaStreams(topology, kafkaProperties);\n+                    streams.setStateListener(listener);\n+                    streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+                    closeables.add(streams);\n+                    streams.start();\n+\n+                    return cf;\n+                })", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzgyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494463820", "bodyText": "And another method here.", "author": "tombentley", "createdAt": "2020-09-24T16:45:21Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })\n+                .thenCompose(v -> {\n+                    long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+                    ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+                    WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+                    closeables.add(serviceImpl);\n+\n+                    boolean[] done = new boolean[1];\n+                    CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+                    KafkaStreams.StateListener listener = (newState, oldState) -> {\n+                        if (newState == KafkaStreams.State.RUNNING && !done[0]) {\n+                            done[0] = true; // no need for dup complete\n+                            cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+                        }\n+                        if (newState == KafkaStreams.State.ERROR) {\n+                            cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+                        }\n+                    };\n+\n+                    Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+                    streams = new KafkaStreams(topology, kafkaProperties);\n+                    streams.setStateListener(listener);\n+                    streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+                    closeables.add(streams);\n+                    streams.start();\n+\n+                    return cf;\n+                })\n+                .thenApply(serviceImpl -> {\n+                    ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                        kafkaProperties,\n+                        Serdes.String().serializer(),\n+                        new TopicCommandSerde()\n+                    );\n+                    closeables.add(producer);\n+\n+                    StoreConfiguration storeConfiguration;\n+                    if (config.get(Config.DISTRIBUTED_STORE)) {\n+                        storeConfiguration = new DistributedStoreConfiguration();\n+                    } else {\n+                        storeConfiguration = new LocalStoreConfiguration();\n+                    }\n+                    storeConfiguration.configure(config, kafkaProperties, streams, serviceImpl, closeables);\n+                    ReadOnlyKeyValueStore<String, Topic> store = storeConfiguration.getStore();\n+                    BiFunction<String, String, CompletionStage<Integer>> service = storeConfiguration.getLookupService();\n+\n+                    this.store = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n+                    return this.store;", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2NjY4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494466684", "bodyText": "Does it need to be public? If so you should document the contract with the thing that calls it with integers.", "author": "tombentley", "createdAt": "2020-09-24T16:50:00Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2OTExNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494469116", "bodyText": "I think a little javadoc explaining what stale results are and why they need to be checked would be valuable.", "author": "tombentley", "createdAt": "2020-09-24T16:53:56Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/WaitForResultService.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * We first register CompletableFuture,\n+ * which will be completed once Streams transformer handles CRUD command.\n+ */\n+public class WaitForResultService implements AsyncBiFunctionService.WithSerdes<String, String, Integer> {\n+    public static final String NAME = \"WaitForResultService\";\n+\n+    private final long timeoutMillis;\n+    private final Map<String, ResultCF> waitingResults = new ConcurrentHashMap<>();\n+    private final ScheduledExecutorService executorService;\n+\n+    public WaitForResultService(long timeoutMillis, ForeachActionDispatcher<String, Integer> dispatcher) {\n+        this.timeoutMillis = timeoutMillis;\n+        dispatcher.register(this::topicUpdated);\n+        executorService = new ScheduledThreadPoolExecutor(1);\n+        executorService.scheduleAtFixedRate(this::checkStaleResults, timeoutMillis / 2, timeoutMillis / 2, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private void checkStaleResults() {", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2OTY1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494969650", "bodyText": "Can you add this doc?", "author": "tombentley", "createdAt": "2020-09-25T12:59:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2OTExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDE4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470184", "bodyText": "This should fail the test, right?", "author": "tombentley", "createdAt": "2020-09-24T16:55:40Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDI0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470246", "bodyText": "This should fail the test, right?", "author": "tombentley", "createdAt": "2020-09-24T16:55:45Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+        String tn2 = \"Topic2\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t2 = new Topic.Builder(tn2, 1).build();\n+        zkTS.create(t2).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2NDc4MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494864781", "bodyText": "Ah, a debugging leftover ... will fix it ...", "author": "alesj", "createdAt": "2020-09-25T09:28:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDI0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDgyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470828", "bodyText": "You should check that the Future returned by read() succeeded before you go using the result.", "author": "tombentley", "createdAt": "2020-09-24T16:56:42Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+        String tn2 = \"Topic2\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t2 = new Topic.Builder(tn2, 1).build();\n+        zkTS.create(t2).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+\n+        Properties kafkaProperties = new Properties();\n+        kafkaProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, config.get(Config.KAFKA_BOOTSTRAP_SERVERS));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_ID_CONFIG, config.get(Config.APPLICATION_ID));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, config.get(Config.APPLICATION_SERVER));\n+\n+        ConcurrentUtil.result(Zk2KafkaStreams.upgrade(zk, config, kafkaProperties, true));\n+\n+        KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n+        try {\n+            TopicStore kTS = ConcurrentUtil.result(configuration.start(config, kafkaProperties));\n+            Topic topic1 = kTS.read(new TopicName(tn1)).result();", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDg5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470890", "bodyText": "Same comment.", "author": "tombentley", "createdAt": "2020-09-24T16:56:49Z", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+        String tn2 = \"Topic2\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t2 = new Topic.Builder(tn2, 1).build();\n+        zkTS.create(t2).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+\n+        Properties kafkaProperties = new Properties();\n+        kafkaProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, config.get(Config.KAFKA_BOOTSTRAP_SERVERS));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_ID_CONFIG, config.get(Config.APPLICATION_ID));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, config.get(Config.APPLICATION_SERVER));\n+\n+        ConcurrentUtil.result(Zk2KafkaStreams.upgrade(zk, config, kafkaProperties, true));\n+\n+        KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n+        try {\n+            TopicStore kTS = ConcurrentUtil.result(configuration.start(config, kafkaProperties));\n+            Topic topic1 = kTS.read(new TopicName(tn1)).result();\n+            Assertions.assertNotNull(topic1);\n+            Topic topic2 = kTS.read(new TopicName(tn2)).result();", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MjA4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494472086", "bodyText": "It wasn't clear to me that this listener is always run on the same thread (and note that it's implicitly initialized to {false} on this thread). It would be safer to just use AtomicBoolean I think.", "author": "tombentley", "createdAt": "2020-09-24T16:58:45Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })\n+                .thenCompose(v -> {\n+                    long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+                    ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+                    WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+                    closeables.add(serviceImpl);\n+\n+                    boolean[] done = new boolean[1];\n+                    CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+                    KafkaStreams.StateListener listener = (newState, oldState) -> {\n+                        if (newState == KafkaStreams.State.RUNNING && !done[0]) {\n+                            done[0] = true; // no need for dup complete", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3NjEyNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494476125", "bodyText": "AFAICS this is plain old double checked locking. I don't see how it can be thread safe (if I'm wrong can you explain). But I rather suspect that it's not worth trying to optimize here and just making it synchronized invoke() would give adequate performance and be correct from a thread safety pov.", "author": "tombentley", "createdAt": "2020-09-24T17:05:29Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.lang.reflect.InvocationHandler;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Proxy;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * Simple local / in-memory store configuration which is sufficient when the TO runs in a single pod\n+ */\n+class LocalStoreConfiguration implements StoreConfiguration {\n+    private ReadOnlyKeyValueStore<String, Topic> store;\n+    private BiFunction<String, String, CompletionStage<Integer>> lookupService;\n+\n+    @Override\n+    public ReadOnlyKeyValueStore<String, Topic> getStore() {\n+        return store;\n+    }\n+\n+    @Override\n+    public BiFunction<String, String, CompletionStage<Integer>> getLookupService() {\n+        return lookupService;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void configure(\n+            Config config,\n+            Properties kafkaProperties,\n+            KafkaStreams streams,\n+            AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl,\n+            List<AutoCloseable> closeables\n+    ) {\n+        String storeName = config.get(Config.STORE_NAME);\n+        // we need to lazily create the store as streams might not be ready yet\n+        store = (ReadOnlyKeyValueStore<String, Topic>) Proxy.newProxyInstance(\n+                getClass().getClassLoader(),\n+                new Class[]{ReadOnlyKeyValueStore.class},\n+                new InvocationHandler() {\n+                    private ReadOnlyKeyValueStore<String, Topic> store;\n+\n+                    @Override\n+                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n+                        if (store == null) {\n+                            synchronized (this) {\n+                                if (store == null) {\n+                                    store = streams.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));", "originalCommit": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg3MDM2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494870368", "bodyText": "This is classic double check to handle concurrency, and you don't need full synchronized on the method.\nBut yeah, no need to optimize this, will just use plain sync on the method.", "author": "alesj", "createdAt": "2020-09-25T09:38:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3NjEyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg3ODg1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494878854", "bodyText": "AFAIU (and I don't claim to be an expert) double checked locking doesn't work because although the reference to store is handled correctly it's still possible that the state of the allocated object get published unsafely. I don't what streams.store() does, but assuming it's not synchronized I think you have to assume that it returns a new object and so the construction overall is not safe. This is why I think just making the whole thing synchronized makes sense here, because it safes us from having to reason about whether DCL is really safe here.", "author": "tombentley", "createdAt": "2020-09-25T09:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3NjEyNQ=="}], "type": "inlineReview"}, {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2ed45236ececba48537b7cb72c09df6f09cbe0a4", "message": "Move assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-09-25T11:28:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2NjM5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494966397", "bodyText": "It's probably a better idea to give the enum members an explict id rather than rely on the declaration order remaining unchanged.", "author": "tombentley", "createdAt": "2020-09-25T12:52:58Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());", "originalCommit": "2ed45236ececba48537b7cb72c09df6f09cbe0a4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494966981", "bodyText": "I can't remember if I asked this before, but maybe it's a good idea to add an explicit version property to the JSON so the schema could be easily evolved?", "author": "tombentley", "createdAt": "2020-09-25T12:54:07Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());", "originalCommit": "2ed45236ececba48537b7cb72c09df6f09cbe0a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMzAxOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498713019", "bodyText": "Do we have version elsewhere?", "author": "alesj", "createdAt": "2020-10-02T09:25:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMzM5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498713394", "bodyText": "So you would be able to properly (de)serialize things based on the version?\nIs this really necessary? Or we could just adjust the code to understand any potential changes in the future.", "author": "alesj", "createdAt": "2020-10-02T09:26:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg3NDMyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498874320", "bodyText": "So that deserialization is unambiguous, indeed. Kafka does this religiously everywhere data gets persisted. I know we don't expect this to change and the argument can be made that no version => version 0, but I'd still prefer it.", "author": "tombentley", "createdAt": "2020-10-02T14:57:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494968768", "bodyText": "Does this base64 encode the json bytes? Why not just use the JSON representation?", "author": "tombentley", "createdAt": "2020-09-25T12:57:20Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "originalCommit": "2ed45236ececba48537b7cb72c09df6f09cbe0a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMTM1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498711352", "bodyText": "This is json, as bytes. Using existing/previous to-json code.", "author": "alesj", "createdAt": "2020-10-02T09:22:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMjI1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498712258", "bodyText": "From the jdoc:\n/**\n * Returns the UTF-8 encoded JSON to reflect the given Topic.\n * This is what is stored in the znodes owned by the {@link ZkTopicStore}.\n */\npublic static byte[] toJson(Topic topic) {", "author": "alesj", "createdAt": "2020-10-02T09:23:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg3MTQ0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498871445", "bodyText": "So why not just embed the JSON representation, rather than the byte serialization of it?", "author": "tombentley", "createdAt": "2020-10-02T14:52:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1OTA2MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499159060", "bodyText": "So to put JsonObject directly ... OK, can do that.", "author": "alesj", "createdAt": "2020-10-03T15:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDU1NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499160555", "bodyText": "Thanks!", "author": "tombentley", "createdAt": "2020-10-03T16:12:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTE4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499165187", "bodyText": "This is committed now ...", "author": "alesj", "createdAt": "2020-10-03T17:13:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}], "type": "inlineReview"}, {"oid": "7a5d65fe10f71eba11a9db9713eaa1ec4b723771", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7a5d65fe10f71eba11a9db9713eaa1ec4b723771", "message": "Add TopicCommand id, so we don't rely on ordinal.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-10-02T09:40:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTEyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499441120", "bodyText": "shouldn't you move this into the switch as a default case?", "author": "samuel-hawker", "createdAt": "2020-10-05T08:53:46Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+        }\n+        throw new IllegalStateException(\"Invalid index: \" + index);", "originalCommit": "86d41167a9352c997c6dc7629801994ce6fd4255", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUwOTk5Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499509992", "bodyText": "Moved ...", "author": "alesj", "createdAt": "2020-10-05T10:52:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTEyMA=="}], "type": "inlineReview"}, {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3a6636b73e3aea816dcc03fdf83ffa91508c0df3", "message": "Move invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-03T11:14:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwNjQxMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516606413", "bodyText": "what about this comment?", "author": "ppatierno", "createdAt": "2020-11-03T11:43:52Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Session.java", "diffHunk": "@@ -172,7 +186,26 @@ public void start(Promise<Void> start) {\n                 LOGGER.debug(\"Using ZooKeeper {}\", zk);\n \n                 String topicsPath = config.get(Config.TOPICS_PATH);\n-                ZkTopicStore topicStore = new ZkTopicStore(zk, topicsPath);\n+                TopicStore topicStore;\n+                if (config.get(Config.USE_ZOOKEEPER_TOPIC_STORE)) {\n+                    topicStore = new ZkTopicStore(zk, topicsPath);\n+                } else {\n+                    // TODO -- better async handling?", "originalCommit": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzODA2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516838068", "bodyText": "I thought TopicOperator could somehow use async result of TopicStore creation, but I guess that's not the case, hence forcing the result with ConcurrentUtil::result.\nIf anyone has some better async ideas ...", "author": "alesj", "createdAt": "2020-11-03T17:29:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwNjQxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwODI3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516608270", "bodyText": "genuine question ... what's the rationale behind this value?", "author": "ppatierno", "createdAt": "2020-11-03T11:47:19Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));", "originalCommit": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzOTE2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516839162", "bodyText": "Nothing more then what the javadoc says.\nI saw this being used in most real examples + I remember we used this setting in our previous prod code as well.\nNever really thought too much since the first usage.\n@tombentley any thoughts on this?", "author": "alesj", "createdAt": "2020-11-03T17:30:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwODI3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg2MjU3Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516862572", "bodyText": "The default is 1GiB, which would be more than we needed, since it's a compacted topic anyway. Whether 64MiB is the best default, who knows?", "author": "tombentley", "createdAt": "2020-11-03T18:09:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwODI3MA=="}], "type": "inlineReview"}, {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f4fcd5861a9ea12a286f414ee231f0b2e9f6aced", "message": "Fix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-03T19:39:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517138559", "bodyText": "I am fine with using the __ prefix for an internal topic (see the store topic) but why using this kind of notation for something that isn't a topic, so the store itself? Isn't it enough just \"topics-store\" or something like that? While the topic is something visible to the user (the TO will create a KafkaTopic resource named __strimzi_store_topic), the store is not. @tombentley @alesj any thoughts? My confusion comes from using \"store\" and \"topic\" too many times where the order they are used matters.", "author": "ppatierno", "createdAt": "2020-11-04T07:21:40Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "originalCommit": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIwNzM3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517207376", "bodyText": "Streams create (log) topics based on the store name.\nThis way you can immediately tell this is an internal topic.\n(although I guess they already have \"log\" in the name, which tells you it's a Streams log topic ...)", "author": "alesj", "createdAt": "2020-11-04T09:31:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxOTExNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517219116", "bodyText": "Right, I forgot the log topic ... can you double-check the final name so that we can figure out a better name for the store?", "author": "ppatierno", "createdAt": "2020-11-04T09:49:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI2NDU3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517264575", "bodyText": "<application.id>-<store.name>-changelog", "author": "alesj", "createdAt": "2020-11-04T11:04:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI2NzUwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517267504", "bodyText": "so it's going to be strimzi-topic-store-__strimzi_topic_store-changelog ... I really think it's a crazy one!! :-D\nThe __ doesn't make sense anymore because it's not at the beginning of the topic name so it doesn't highlight it as an internal topic, then again strimzi, topic and store used too much :-) Isn't it possible to override the changelog topic name?", "author": "ppatierno", "createdAt": "2020-11-04T11:09:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI3MDE5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517270190", "bodyText": "Isn't it possible to override the changelog topic name?\n\nNo idea ...", "author": "alesj", "createdAt": "2020-11-04T11:14:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMzMzEwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517333108", "bodyText": "So after some investigation, it turned out that it's not possible to override the name completely. The internal Kafka Streams topics have always -changelog or -repartition suffixes for topics related to store and repartitioning.\nSo we can just \"play\" with <application.id> and <store.name> as for the format you pasted.\nFirst, if we want the final topic starting with __ then we need the application id starting with __.\nMaybe something like:\napplication.id = \"__strimzi-topic-operator-kstreams\"\nstore.name = \"topic-store\"\nso changelog topic will be __strimzi-topic-operator-kstreams-topic-store-changelog\nTbh I run out of ideas :-)", "author": "ppatierno", "createdAt": "2020-11-04T13:14:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM1MDI3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517350271", "bodyText": "OK, I'll put this values into Config ...", "author": "alesj", "createdAt": "2020-11-04T13:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517144781", "bodyText": "So I was thinking about this scenario ...\nInitially, the cluster is made by 3 brokers so we are going to create the store topic with rf = 3 and minISR=2 (as per createNewStoreTopic method).\nThen imagine that we scale up to 9 as far as I understood from this code, the store topic will remain with same configuration. I am not sure that it will be for production use anymore, with a cluster of 9 but a topic with rf = 3 and minISR=2. Shouldn't we increase the replication factor accordingly? @tombentley thoughts?", "author": "ppatierno", "createdAt": "2020-11-04T07:36:56Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreService.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * A service to configure and start/stop KafkaStreamsTopicStore.\n+ */\n+public class KafkaStreamsTopicStoreService {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStoreService.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        return validateExistingStoreTopic(storeTopic, admin, c);\n+                    } else {\n+                        return createNewStoreTopic(storeTopic, admin, c);\n+                    }\n+                })\n+                .thenCompose(v -> createKafkaStreams(config, kafkaProperties, storeTopic, storeName))\n+                .thenApply(serviceImpl -> createKafkaTopicStore(config, kafkaProperties, storeTopic, serviceImpl))\n+                .whenComplete((v, t) -> {\n+                    if (t != null) {\n+                        stop();\n+                    }\n+                });\n+    }\n+\n+    private TopicStore createKafkaTopicStore(Config config, Properties kafkaProperties, String storeTopic, AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl) {\n+        ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                kafkaProperties,\n+            Serdes.String().serializer(),\n+            new TopicCommandSerde()\n+        );\n+        closeables.add(producer);\n+\n+        StoreAndServiceFactory factory;\n+        if (config.get(Config.DISTRIBUTED_STORE)) {\n+            factory = new DistributedStoreAndServiceFactory();\n+        } else {\n+            factory = new LocalStoreAndServiceFactory();\n+        }\n+        StoreAndServiceFactory.StoreContext sc = factory.create(config, kafkaProperties, streams, serviceImpl, closeables);\n+\n+        this.store = new KafkaStreamsTopicStore(sc.getStore(), storeTopic, producer, sc.getService());\n+        return this.store;\n+    }\n+\n+    private CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> createKafkaStreams(Config config, Properties kafkaProperties, String storeTopic, String storeName) {\n+        long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+        ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+        WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+        closeables.add(serviceImpl);\n+\n+        AtomicBoolean done = new AtomicBoolean(false); // no need for dup complete\n+        CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+        KafkaStreams.StateListener listener = (newState, oldState) -> {\n+            if (newState == KafkaStreams.State.RUNNING && !done.getAndSet(true)) {\n+                cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+            }\n+            if (newState == KafkaStreams.State.ERROR) {\n+                cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+            }\n+        };\n+\n+        Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+        streams = new KafkaStreams(topology, kafkaProperties);\n+        streams.setStateListener(listener);\n+        streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+        closeables.add(streams);\n+        streams.start();\n+\n+        return cf;\n+    }\n+\n+    private CompletionStage<Void> createNewStoreTopic(String storeTopic, Admin admin, Context c) {\n+        int rf = Math.min(3, c.clusterSize);\n+        int minISR = Math.max(rf - 1, 1);\n+        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+    }\n+\n+    private CompletionStage<Void> validateExistingStoreTopic(String storeTopic, Admin admin, Context c) {\n+        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+            .thenApply(c3 -> {\n+                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {", "originalCommit": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxMTc1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517211752", "bodyText": "This is what @tombentley suggested ... dunno if you really need (rf == cluster size) with big clusters ...", "author": "alesj", "createdAt": "2020-11-04T09:38:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxNTg1NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517215855", "bodyText": "Maybe not exactly cluster size but something more close to it", "author": "ppatierno", "createdAt": "2020-11-04T09:44:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIzMjk5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517232999", "bodyText": "I'll let @tombentley work out the rf value algorithm", "author": "alesj", "createdAt": "2020-11-04T10:11:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517149982", "bodyText": "genuine question because I don't know Apicurio ... what's the value of using this \"apicurio\" producer instead of a plain Kafka producer? Aren't we going just to send a message in the store topic and then the Kafka Streams application takes care of it storing information in the topic store?", "author": "ppatierno", "createdAt": "2020-11-04T07:48:40Z", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+            default:\n+                throw new IllegalStateException(\"Invalid index: \" + index);\n+        }\n+    }\n+\n+    public static Integer toIndex(Class<? extends Throwable> ct) {\n+        if (ct.equals(TopicStore.EntityExistsException.class)) {\n+            return 1;\n+        }\n+        if (ct.equals(TopicStore.NoSuchEntityExistsException.class)) {\n+            return 2;\n+        }\n+        if (ct.equals(TopicStore.InvalidStateException.class)) {\n+            return 3;\n+        }\n+        throw new IllegalStateException(\"Unexpected value: \" + ct);\n+    }\n+\n+    @Override\n+    public Future<Topic> read(TopicName name) {\n+        try {\n+            Topic topic = topicStore.get(name.toString());\n+            return Future.succeededFuture(topic);\n+        } catch (Throwable t) {\n+            return Future.failedFuture(t);\n+        }\n+    }\n+\n+    private Future<Void> handleTopicCommand(TopicCommand cmd) {\n+        String key = cmd.getKey();\n+        CompletionStage<Throwable> result = resultService.apply(key, cmd.getUuid())\n+                .thenApply(KafkaStreamsTopicStore::toThrowable);\n+        // Kafka Streams can re-balance in-between these two calls ...\n+        producer.apply(new ProducerRecord<>(storeTopic, key, cmd))", "originalCommit": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxNTQ1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517215450", "bodyText": "It's just a bit more resilient then plain Kafka producer -- see the code.\nIt also transparently handles fatal errors in callback as well.\nAnd it uses CS as a callback API, which makes it easier to use -- when you have a CS chain.\n(here we actually don't take full advantage of this)", "author": "alesj", "createdAt": "2020-11-04T09:44:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIyMDM2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517220361", "bodyText": "My concern is that we are adding an apicurio dependency in the topic operator just for this. Or apicurio stuff is used even somewhere else?", "author": "ppatierno", "createdAt": "2020-11-04T09:51:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIzMjkxOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517232919", "bodyText": "Yes, in a few classes.\nMostly in order to easily abstract away local vs distributed storage + async function.\nAnd some utils - concurrency and serde.", "author": "alesj", "createdAt": "2020-11-04T10:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg=="}], "type": "inlineReview"}, {"oid": "ddffc8b5fb2b2ca8f42cc27e3be49b9fc845b993", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ddffc8b5fb2b2ca8f42cc27e3be49b9fc845b993", "message": "Stop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-04T11:13:00Z", "type": "forcePushed"}, {"oid": "8da885278b10f9e07123fb76396c4ce44217c6ca", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8da885278b10f9e07123fb76396c4ce44217c6ca", "message": "Stop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-04T11:13:33Z", "type": "forcePushed"}, {"oid": "8dc7590a0a4f71e8f0b0109d3d0d60cb01cdb781", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8dc7590a0a4f71e8f0b0109d3d0d60cb01cdb781", "message": "Stop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-04T13:42:14Z", "type": "forcePushed"}, {"oid": "8b4dc78c2dc0237f9b8a28ac6ffbcf401a5a78c4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8b4dc78c2dc0237f9b8a28ac6ffbcf401a5a78c4", "message": "Stop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-04T15:01:18Z", "type": "forcePushed"}, {"oid": "afd8539d75fda71912e70028c78416b3296a8fdb", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/afd8539d75fda71912e70028c78416b3296a8fdb", "message": "Stop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-09T13:30:44Z", "type": "forcePushed"}, {"oid": "4f1ba623514317f774596623939d9d8b3fc292d0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/4f1ba623514317f774596623939d9d8b3fc292d0", "message": "Stop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-11T11:57:37Z", "type": "forcePushed"}, {"oid": "db814eeb3375ca017f5bc709e03dedba71c07b33", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/db814eeb3375ca017f5bc709e03dedba71c07b33", "message": "Close admin on complete, wait for the Vertx context to finish.\nStop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-11T16:45:34Z", "type": "forcePushed"}, {"oid": "16d6747a6cfa565c23168cb94bbe9d9c3a40f314", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/16d6747a6cfa565c23168cb94bbe9d9c3a40f314", "message": "Close admin on complete, wait for the Vertx context to finish.\nStop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-11T16:46:17Z", "type": "forcePushed"}, {"oid": "b5d0229962a7b02dabe8fc31f905e150b0e13b1f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b5d0229962a7b02dabe8fc31f905e150b0e13b1f", "message": "Close admin on complete, wait for the Vertx context to finish.\nStop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-11T16:46:56Z", "type": "forcePushed"}, {"oid": "d39fa14801d2cd7bedf95bbf02c876978f5abb28", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d39fa14801d2cd7bedf95bbf02c876978f5abb28", "message": "Close admin on complete, wait for the Vertx context to finish.\nStop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-12T09:56:52Z", "type": "forcePushed"}, {"oid": "b4876a22497e1ed190195b4f0e42769f8f22f45a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b4876a22497e1ed190195b4f0e42769f8f22f45a", "message": "More logging to KSTS, do not delete internal topics ...\nClose admin on complete, wait for the Vertx context to finish.\nStop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-13T12:03:32Z", "type": "forcePushed"}, {"oid": "e66abc7890460869434b765893e75379ba07df45", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e66abc7890460869434b765893e75379ba07df45", "message": "More logging to KSTS, do not delete internal topics ...\nClose admin on complete, wait for the Vertx context to finish.\nStop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>\n\nCheck for existing topics.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-11-23T10:49:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532019849", "bodyText": "You did IMHO not changed anything in User Operator. So this should not be here.", "author": "scholzj", "createdAt": "2020-11-28T10:12:35Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "originalCommit": "38be9898fef957959a1060e93ea024e932459c98", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNjEzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532236138", "bodyText": "Yeah ... but if I didn't prolong the delay here as well, it didn't go through ...", "author": "alesj", "createdAt": "2020-11-29T16:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzE5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237194", "bodyText": "The tests are passing completely fine outside of your PR. So there is no need to change this unless you changed something in the User Operator.", "author": "scholzj", "createdAt": "2020-11-29T17:02:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzc0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237748", "bodyText": "This will definitely go out, just wanted to get a clean run of ST locally and CI ...", "author": "alesj", "createdAt": "2020-11-29T17:06:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzODU0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532238541", "bodyText": "Two of the testsets timed out - which could be also very well related to these changes since you probably increased the time needed to run the tests by more then one hour. So keep in mind that this might not always just help.", "author": "scholzj", "createdAt": "2020-11-29T17:13:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532022060", "bodyText": "Same as above ... should this really be here? Does the TO now with your changes really need 2 minutes to start? I do not see to be changed for the regular just for the STs which suggests it should not be here.", "author": "scholzj", "createdAt": "2020-11-28T10:15:13Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n                     .endUserOperator()\n                     .editTopicOperator()\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "originalCommit": "38be9898fef957959a1060e93ea024e932459c98", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNjQwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532236400", "bodyText": "No idea what's going on, but I saw in the log that KafkaStreamsTopicStoreService took ~1min to start (from \"Starting\" log to \"Started\").\nIt does a simple Kafka AdminClient check + starts Kafka Streams (topology),\nwhich I really don't see why it should take so long ...", "author": "alesj", "createdAt": "2020-11-29T16:56:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzI2Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237267", "bodyText": "If this is expected from the new Topic Operator, then it certainly needs to be increased for all other places as well. Not just for running the system tests.", "author": "scholzj", "createdAt": "2020-11-29T17:03:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzYzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237637", "bodyText": "It's not expected, it should be fast -- not as fast as ZKTopicStore, which didn't have anything to pre-start,\nbut few AdminClient calls + starting up Kafka Streams shouldn't take 1min.", "author": "alesj", "createdAt": "2020-11-29T17:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA=="}], "type": "inlineReview"}, {"oid": "cd6a674237bc328f379e3412b44de5bd50e17776", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/cd6a674237bc328f379e3412b44de5bd50e17776", "message": "Change roller test to start with 3 brokers and then scale-up.\nIgnore the delete-CA test for now -- TODO discuss proper fix.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-12-14T13:37:14Z", "type": "forcePushed"}, {"oid": "64383c0dd2eac45d358c382ca6f82030225cc8b7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/64383c0dd2eac45d358c382ca6f82030225cc8b7", "message": "Change roller test to start with 3 brokers and then scale-up.\nIgnore the delete-CA test for now -- TODO discuss proper fix.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-12-16T10:55:23Z", "type": "forcePushed"}, {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6f586b15027e8873b6f8c56d01c6e6e1f4699b2d", "message": "Fix TO startup check - with probe.\nMore log to Zk2KafkaStreams upgrade.\nUse sync ZK.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2020-12-23T12:12:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTM1MTkzNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551351934", "bodyText": "what's this? :-)", "author": "ppatierno", "createdAt": "2021-01-04T14:34:20Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1382,7 +1382,10 @@ void testConsumerOffsetFiles() {\n         String result = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n                 \"/bin/bash\", \"-c\", commandToGetFiles).out();\n \n-        assertThat(\"Folder kafka-log0 has data in files\", result.equals(\"\"));\n+        // TODO / FIXME\n+        //assertThat(\"Folder kafka-log0 has data in files:\\n\" + result, result.equals(\"\"));", "originalCommit": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0MTMwNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551841306", "bodyText": "This was empty before, as ZKTS didn't use Kafka.\nWith KSTS, this is already populated -- I guess it can be removed, unless someone has a better idea?", "author": "alesj", "createdAt": "2021-01-05T10:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTM1MTkzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551573616", "bodyText": "What are the plans for fixing this? Also, any GitHub issues to link to here for this?", "author": "scholzj", "createdAt": "2021-01-04T21:16:55Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "originalCommit": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0MzIxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551843217", "bodyText": "@tombentley any idea what can be done?", "author": "alesj", "createdAt": "2021-01-05T10:25:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0NzcyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551847720", "bodyText": "Did I disable it? Anyway, I've no recollection of doing this.", "author": "tombentley", "createdAt": "2021-01-05T10:34:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1MjY0NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551852644", "bodyText": "@tombentley no, I disabled it ... but this is what you had to say about it :-)\n\n    @Disabled // fix-it ...\n    /*\n     * I'm guessing that this test passed before because we never had a consumer in this test,\n     * so there was no __consumer_offsets for it to fail on.\n     * The test topic which is used has min.isr == replicas,\n     * so KafkaAvailability would always have ignored that because it's unrollable otherwise.\n     * Now there's a partition with a minisr set to 1 but with >1 replicas,\n     * so KafkaAvailability cannot ignore it.\n     * And because the key was changed it means that the other brokers don't trust the new broker 0 cert\n     * (because it's signed with a key they don't yet trust). So they can't connect to do follower fetches.\n     * So we've kind of deadlocked.\n     */", "author": "alesj", "createdAt": "2021-01-05T10:43:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2Mjk1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551862954", "bodyText": "Thanks, I remember now. The test isn't so realistic, having a topic with minisr== replicas. Having 3 replicas and min.isr=2 would be more realistic. So I think we should change the test. WDYT @Frawless @scholzj ?", "author": "tombentley", "createdAt": "2021-01-05T11:03:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2MzE3Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551863172", "bodyText": "@alesj I think the explanation makes sense. But the // fix-it ... comment alone is not the right solution. We should either fix it in this PR (whatever we decide is the right fix - maybe we just remove it, maybe we want to add some additional logging to make the situation more readable for users etc.). Or we should have a Github issue for fixing this later and have it linked here so that we get back to it.", "author": "scholzj", "createdAt": "2021-01-05T11:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2NzE2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551867162", "bodyText": "@tombentley I think the problem is that having the test more realistic also means it does not pass. When the CA secret is deleted, the operator just generates a new one and rolls the pods. But the cluster has to get partitioned during that. The old (not-yet-rolled) pods and the new (already-rolled) pods will each use distinct set of SSL keys and will form two separate partitions which will not link. So with some traffic going on, this always has to fail because you cannot keep the replicas in-sync. So I think this test will be deleted sooner or later. But we should probably think about if we need to somehow improve the logging in these situations etc. Because it could be confusing for the user.", "author": "scholzj", "createdAt": "2021-01-05T11:12:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjUyNzI0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r552527243", "bodyText": "#4187", "author": "alesj", "createdAt": "2021-01-06T11:45:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3NDAyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551574028", "bodyText": "Can you keep the formatting we had before here? It was intentional.", "author": "scholzj", "createdAt": "2021-01-04T21:17:52Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java", "diffHunk": "@@ -72,19 +72,27 @@ void testKafkaRollsWhenTopicIsUnderReplicated() {\n         String topicName = KafkaTopicUtils.generateRandomNameOfTopic();\n         timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n \n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 4)\n-            .editSpec()\n+        // We need to start with 3 replicas / brokers,\n+        // so that KafkaStreamsTopicStore topic gets set/distributed on this first 3 [0, 1, 2],\n+        // since this topic has replication-factor 3 and minISR 2.\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+                .editSpec()\n                 .editKafka()\n-                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .addToConfig(\"auto.create.topics.enable\", \"false\")\n                 .endKafka()\n-            .endSpec()\n-            .done();\n+                .endSpec()\n+                .done();", "originalCommit": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0MzY3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551843671", "bodyText": "Ah, OK.", "author": "alesj", "createdAt": "2021-01-05T10:26:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3NDAyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU4MjI0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551582242", "bodyText": "Are these the names of the topics? Shouldn't they have the __ prefix? Also, would it make sense to start with the same name? E.g. strimzi-topic-operator-store instead of strimzi-store-topic when the other topic is named strimzi-topic-operator...?", "author": "scholzj", "createdAt": "2021-01-04T21:35:28Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java", "diffHunk": "@@ -671,6 +671,8 @@ void setupEnvironment() throws Exception {\n         list.add(CruiseControlUtils.CRUISE_CONTROL_METRICS_TOPIC);\n         list.add(CruiseControlUtils.CRUISE_CONTROL_MODEL_TRAINING_SAMPLES_TOPIC);\n         list.add(CruiseControlUtils.CRUISE_CONTROL_PARTITION_METRICS_SAMPLES_TOPIC);\n+        list.add(\"strimzi-store-topic\");\n+        list.add(\"strimzi-topic-operator-kstreams-topic-store-changelog\");", "originalCommit": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0ODY5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551848690", "bodyText": "This was already a long discussion ... but sure, we can always change it?\n@ppatierno @tombentley wdyt? ^", "author": "alesj", "createdAt": "2021-01-05T10:36:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU4MjI0Mg=="}], "type": "inlineReview"}, {"oid": "50f799f26677d96e6b478bd6474d095a8b8d2181", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/50f799f26677d96e6b478bd6474d095a8b8d2181", "message": "Check existing topic in KSTS.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-11T17:16:51Z", "type": "forcePushed"}, {"oid": "d68aa5ac549eaf5d3d65b5fd5861873246c44f0d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d68aa5ac549eaf5d3d65b5fd5861873246c44f0d", "message": "Fix npe (if race).\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-15T09:07:10Z", "type": "forcePushed"}, {"oid": "fc42f0decc7629c5852d4d4b9abb885aeab50307", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fc42f0decc7629c5852d4d4b9abb885aeab50307", "message": "More logging to KSTS, do not delete internal topics ...\nClose admin on complete, wait for the Vertx context to finish.\nStop KSTS service in the test.\nFix spotbugs errors.\nMove invalid index into default.\nAdd version to topic command schema.\nSerialize topic as json node directly.\nAdd TopicCommand id, so we don't rely on ordinal.\nMove assume into the test method, not initialization.\nApply feeback wrt class rename, etc.\nUse embedded Kafka cluster.\nFix the vertx hang (tnx julienv).\nUse CS to get exception on the get result.\nFix upgrade async code, add test for the upgrade/migration.\nMove KafkaStreams running check into state listener.\nAdd upgrade config option.\nApply design doc feedback.\nUpdate topic-store.md, initial review and proposed changes. I may have changed the meaning of some sentences due to the lack of understanding.\nI skipped the last part - could not decipher the meaning.\nInitial design document.\nUse 1.3.0.Final Registry (extracted) jar.\nUnwrap invocation exception to get the real cause.\nAdmin msg #2.\nMore info/docs, better warn msg.\nMake distribution optional, handle admin stuff in async.\nCheck all partitions, take min replicas size as rf\nFix admin lookup.\nApply feedback.\nInitial Kafka Streams TopicStore.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>\n\nCheck for existing topics.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T08:57:02Z", "type": "commit"}, {"oid": "b1b76a81c148dba2ef658d1f3295a04e3902abdc", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b1b76a81c148dba2ef658d1f3295a04e3902abdc", "message": "Prolong topic-op, user-op delay (temp!), tweak tests a bit, add more logging to KafkaStreams impls.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T08:57:02Z", "type": "commit"}, {"oid": "36d7012bd40f7740c743f4f489930fc62bd5d2a5", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/36d7012bd40f7740c743f4f489930fc62bd5d2a5", "message": "Undo user-op delay.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T08:57:02Z", "type": "commit"}, {"oid": "c040be8912b258f31eb83fdc490ab04d8fce5f06", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c040be8912b258f31eb83fdc490ab04d8fce5f06", "message": "Remove readiness delay, set liveness to 2min.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T08:57:02Z", "type": "commit"}, {"oid": "bef2aa33f4271f9b732e269c4ca886f45755ba12", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bef2aa33f4271f9b732e269c4ca886f45755ba12", "message": "Change roller test to start with 3 brokers and then scale-up.\nIgnore the delete-CA test for now -- TODO discuss proper fix.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T08:57:02Z", "type": "commit"}, {"oid": "31edd91541b2c7739e6ae6c68ed9be165aada794", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/31edd91541b2c7739e6ae6c68ed9be165aada794", "message": "Fix TO startup check - with probe.\nMore log to Zk2KafkaStreams upgrade.\nUse sync ZK.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T08:57:02Z", "type": "commit"}, {"oid": "a5bc463887fca18b217e3302843d62c30bba8b34", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a5bc463887fca18b217e3302843d62c30bba8b34", "message": "Check existing topic in KSTS.\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T08:57:02Z", "type": "commit"}, {"oid": "28b734df433a07743a99c07e98c8594ca5387daf", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/28b734df433a07743a99c07e98c8594ca5387daf", "message": "Fix npe (if race).\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T09:08:15Z", "type": "commit"}, {"oid": "28b734df433a07743a99c07e98c8594ca5387daf", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/28b734df433a07743a99c07e98c8594ca5387daf", "message": "Fix npe (if race).\n\nSigned-off-by: Ales Justin <ales.justin@gmail.com>", "committedDate": "2021-01-18T09:08:15Z", "type": "forcePushed"}]}