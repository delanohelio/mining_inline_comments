{"pr_number": 3469, "pr_title": "Dynamcially changeable logging levels in Kafka", "pr_createdAt": "2020-08-06T14:10:45Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1ODkzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r466658938", "bodyText": "I'm not sure I understand the naming and the Javadoc here. If you add something to the map, it is not updated yet so the parameter name is wrong? I'm also not sure why a separate method is needed here. It looks like any call for this method can be done in single line.", "author": "scholzj", "createdAt": "2020-08-06T20:10:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -111,10 +112,11 @@ private boolean isEntryReadOnly(ConfigEntry entry) {\n     }\n \n     /**\n-     * @return A map which can be used for dynamic configuration of kafka broker\n+     * Adds an entry to a map which can be used for dynamic configuration of kafka broker\n+     * @param updatedConfig map to add an entry\n      */\n-    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n-        return Collections.singletonMap(Util.getBrokersConfig(brokerId), diff);\n+    public void addConfigDiff(Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig) {", "originalCommit": "88e9fdb021e0332369ca3b84a2b8a87329b90792", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b7e36673afedf918dcdb9dd5db0df189ce352b4e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b7e36673afedf918dcdb9dd5db0df189ce352b4e", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T08:49:44Z", "type": "forcePushed"}, {"oid": "703cf1db7e2ab15d31ea5bd8a5f40bec90c572e0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/703cf1db7e2ab15d31ea5bd8a5f40bec90c572e0", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T09:07:18Z", "type": "forcePushed"}, {"oid": "97dfcf4427fa1032d0ea0f0a456a603f17c42585", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/97dfcf4427fa1032d0ea0f0a456a603f17c42585", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T10:09:37Z", "type": "forcePushed"}, {"oid": "5dde48c57ebd145860e333bcd33b20c40869e1d8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5dde48c57ebd145860e333bcd33b20c40869e1d8", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T10:33:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r467919163", "bodyText": "Can you please move these tests before the @BeforeAll (after the bridge logging test)?", "author": "im-konge", "createdAt": "2020-08-10T13:53:13Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -472,4 +472,212 @@ protected void tearDownEnvironmentAfterAll() {\n     protected void assertNoCoErrorsLogged(long sinceSeconds) {\n         LOGGER.info(\"Skipping assertion if CO has some unexpected errors\");\n     }\n+\n+    @Test\n+    void testDynamicallySetKafkaLoggingLevels() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging ilOff = new InlineLogging();\n+        ilOff.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"INFO\"));\n+\n+        LOGGER.info(\"Changing rootLogger level to DEBUG with inline logging\");\n+        InlineLogging ilDebug = new InlineLogging();\n+        ilDebug.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"DEBUG\"));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(ilDebug);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                .contains(\"root=DEBUG\"));\n+\n+        LOGGER.info(\"Setting external logging INFO\");\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-configmap\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+\n+        ExternalLogging elKafka = new ExternalLoggingBuilder()\n+                .withName(\"external-configmap\")\n+                .build();\n+\n+        LOGGER.info(\"Setting log level of kafka INFO\");\n+        // change to external logging\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(elKafka);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                    \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                    .contains(\"root=INFO\"));\n+\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLogger() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.logger.paprika\", \"INFO\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"paprika=INFO\"));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLoggerValue() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"PAPRIKA\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetKafkaExternalLogging() {\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        ExternalLogging el = new ExternalLogging();\n+        el.setName(\"external-cm\");\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1)\n+                .editOrNewSpec()\n+                    .editKafka()\n+                        .withExternalLogging(el)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=ERROR\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=ERROR\"));\n+\n+        // log4j.appender.CONSOLE.layout.ConversionPattern is changed and thus we need RU\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=DEBUG\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=DEBUG\"));\n+    }", "originalCommit": "5dde48c57ebd145860e333bcd33b20c40869e1d8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3OTA3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468179078", "bodyText": "Do the STs have BeforeAll methods at the end of the class?", "author": "scholzj", "createdAt": "2020-08-10T20:52:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODQ1ODg2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468458863", "bodyText": "They should have \ud83d\ude04 at least for me it's more readable. I think that some STs are little bit mixed, but I'm gonna fix this in some future PRs :)", "author": "im-konge", "createdAt": "2020-08-11T09:46:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODQ1OTIwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468459200", "bodyText": "Or do you think that BeforeAll should be on the beginning of the class?", "author": "im-konge", "createdAt": "2020-08-11T09:47:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}], "type": "inlineReview"}, {"oid": "e19cef5d56736102267da6bdea22231921ce6499", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e19cef5d56736102267da6bdea22231921ce6499", "message": "comment\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-12T13:26:54Z", "type": "forcePushed"}, {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c7022f109100a8ee2e2b96a3574380f89c20eeee", "message": "simpification\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-17T07:56:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwNzQzMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471307430", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (entry.getKey().startsWith(\"log4j.appender\")) {\n          \n          \n            \n                        if (entry.getKey().startsWith(\"log4j.appender.\")) {", "author": "tombentley", "createdAt": "2020-08-17T08:00:22Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3471,6 +3474,18 @@ String getInternalServiceHostname(String serviceName)    {\n \n     }\n \n+    private String getLoggingAppenders(String loggingConfiguration) {\n+        OrderedProperties ops = new OrderedProperties();\n+        ops.addStringPairs(loggingConfiguration);\n+        StringBuilder result = new StringBuilder();\n+        for (Map.Entry<String, String> entry: ops.asMap().entrySet()) {\n+            if (entry.getKey().startsWith(\"log4j.appender\")) {", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwODI5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471308297", "bodyText": "Factor out patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true) into a constant.", "author": "tombentley", "createdAt": "2020-08-17T08:02:02Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -159,8 +159,8 @@ private static boolean isIgnorableProperty(String key) {\n \n         fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n \n-        JsonNode source = patchMapper().valueToTree(currentMap);\n-        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwODc3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471308776", "bodyText": "And use the constant here.", "author": "tombentley", "createdAt": "2020-08-17T08:03:04Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMTUxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471311515", "bodyText": "Any reason why you can't change this in the desired map like you do for the ones starting log4j.logger?", "author": "tombentley", "createdAt": "2020-08-17T08:08:47Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMjgyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471312823", "bodyText": "More efficient to use indexOf(\",\") + trim(). Both split and replaceAll will have to use Pattern under the hood.", "author": "tombentley", "createdAt": "2020-08-17T08:11:17Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                        String level = parseLogLevelFromAppenderCouple(desiredMap.get(pathValueWithoutSlash));\n+                        if (!level.equals(currentMap.get(\"root\"))) {\n+                            updateOrAddRoot(level, updatedCE);\n+                        }\n+                    } else {\n+                        // entry is not in the current, it is added\n+                        updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String lev) {\n+        String[] arr = lev.split(\",\");\n+        String level = arr[0].replaceAll(\"\\\\s\", \"\");\n+        return level;", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMzYyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471313623", "bodyText": "What do you think I'm going to tell you about these two methods?", "author": "tombentley", "createdAt": "2020-08-17T08:12:42Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                        String level = parseLogLevelFromAppenderCouple(desiredMap.get(pathValueWithoutSlash));\n+                        if (!level.equals(currentMap.get(\"root\"))) {\n+                            updateOrAddRoot(level, updatedCE);\n+                        }\n+                    } else {\n+                        // entry is not in the current, it is added\n+                        updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String lev) {\n+        String[] arr = lev.split(\",\");\n+        String level = arr[0].replaceAll(\"\\\\s\", \"\");\n+        return level;\n+    }\n+\n+    private static void updateOrAddRoot(String level, Collection<AlterConfigOp> updatedCE) {\n+        level = parseLogLevelFromAppenderCouple(level);\n+        if (isValidLoggerLevel(level)) {\n+            updatedCE.add(new AlterConfigOp(new ConfigEntry(\"root\", level), AlterConfigOp.OpType.SET));\n+            log.trace(\"{} not set in current or has deprecated value. Setting to {}\", \"root\", level);\n+        } else {\n+            log.warn(\"Level {} is not valid logging level\", level);\n+        }\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n+            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n+            if (isValidLoggerLevel(level)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n+            } else {\n+                log.warn(\"Level {} is not valid logging level\", level);\n+            }\n+        }\n+    }", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMzk1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471313959", "bodyText": "It doesn't return a Future.", "author": "tombentley", "createdAt": "2020-08-17T08:13:18Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -433,22 +444,44 @@ private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, Int\n      * @return a Future which completes with the config of the given broker.\n      */\n     protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n-        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        ConfigResource resource = Util.getBrokersConfig(brokerId);\n         return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n             30, TimeUnit.SECONDS,\n             error -> new ForceableProblem(\"Error getting broker config\", error)\n         );\n     }\n \n-    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)\n+    /**\n+     * Returns a Future which completes with the logging of the given broker.", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzEwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471317103", "bodyText": "TBH, I'm not completely convinced it's worth adding the dynamic logging changes to the KafkaRoller. KafkaRoller's job is to safely orchestrate the restart or reconfiguration of some or all brokers in the cluster. Changing a broker's logging doesn't seem to me to be a very risky thing to do. It's not going to need any special pre- or post-conditions before/after happening, for example. I can see that from a certain PoV it makes sense to do it here, but the question is whether adding extra complexity to KafkaRoller to do it here really makes sense, compared with some other phase of the reconciliation which updates the logging after brokers have been restarted/reconfigured.", "author": "tombentley", "createdAt": "2020-08-17T08:18:47Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -271,13 +274,15 @@ public String toString() {\n         private final boolean needsRestart;\n         private final boolean needsReconfig;\n         private final KafkaBrokerConfigurationDiff diff;\n+        private final KafkaBrokerLoggingConfigurationDiff logDiff;\n         private final Admin adminClient;\n \n-        public RestartPlan(Admin adminClient, boolean needsRestart, boolean needsReconfig, KafkaBrokerConfigurationDiff diff) {\n+        public RestartPlan(Admin adminClient, boolean needsRestart, boolean needsReconfig, KafkaBrokerConfigurationDiff diff, KafkaBrokerLoggingConfigurationDiff logDiff) {", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471317847", "bodyText": "Do we need this? It seems the annotation is added, but never used? If we do need it then I think a little Javadoc about how it's used wouldn't go amiss.", "author": "tombentley", "createdAt": "2020-08-17T08:20:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -110,6 +110,7 @@\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM1NjYxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471356614", "bodyText": "It is used in KAO", "author": "sknot-rh", "createdAt": "2020-08-17T09:31:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2Mzg5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471363894", "bodyText": "Yeah, it's added there, (https://github.com/strimzi/strimzi-kafka-operator/pull/3469/files#diff-f19c7c3bdf294affc86939228666be84R2458), but never read. Why is it needed if it's never read? Is it simply so that we can tell that the logging was changed? If so then add a comment explaining how that works.", "author": "tombentley", "createdAt": "2020-08-17T09:44:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2Njk5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471366994", "bodyText": "It changes the annotation an thus kafka pods are rolled (appenders are not changeable dynamically). I will add a doc.", "author": "sknot-rh", "createdAt": "2020-08-17T09:50:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2OTUyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471369528", "bodyText": "Thanks, I know it makes sense to us, here and now, but I for one will forget this within about 5 minutes and stuff like this makes it harder for people who are not familiar with it to understand what the purpose of this is. Can you also comment the other hash which is used in the same way?", "author": "tombentley", "createdAt": "2020-08-17T09:54:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472486889", "bodyText": "What is the ANNO_STRIMZI_LOGGING_HASH used for now? Is it still used somewhere? I saw it is not used anymore on line 2458 in Kafka assembly operator.", "author": "scholzj", "createdAt": "2020-08-18T20:53:51Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";", "originalCommit": "ef83c883c47bf680286850675c6bca21245ecdc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc0NTA4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472745088", "bodyText": "It is still used for rolling updates of ZK pods when ZK logging is changed.", "author": "sknot-rh", "createdAt": "2020-08-19T06:13:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc0OTg2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472749863", "bodyText": "Ok, makes sense. Thanks.", "author": "scholzj", "createdAt": "2020-08-19T06:20:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjUxMzI1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472513258", "bodyText": "Should these be in Annotations.java?", "author": "scholzj", "createdAt": "2020-08-18T21:49:57Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "originalCommit": "ef83c883c47bf680286850675c6bca21245ecdc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyODAwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472928000", "bodyText": "Can be static?", "author": "tombentley", "createdAt": "2020-08-19T10:30:31Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3471,6 +3474,18 @@ String getInternalServiceHostname(String serviceName)    {\n \n     }\n \n+    private String getLoggingAppenders(String loggingConfiguration) {", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzY3NzkxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r473677914", "bodyText": "In the other PR it is under Utils class so it will be refactored later.", "author": "sknot-rh", "createdAt": "2020-08-20T07:10:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyODAwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyOTM3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472929374", "bodyText": "I think we probably don't want to force the rest of Fabric 8's patch code to use ordered maps when computing diffs. So we should probably instantiate our own ObjectMapper here since it's the only place where we do case about order (right?)", "author": "tombentley", "createdAt": "2020-08-19T10:33:04Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private static final ObjectMapper MAPPER = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMDQ2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472930462", "bodyText": "Why did use use a List for VALID_LOGGER_LEVELS if you're only needing contains()? A HashSet will be more efficient since it likely needs only 1 hash() and 1 or maybe 2 equals().", "author": "tombentley", "createdAt": "2020-08-19T10:34:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private static final ObjectMapper MAPPER = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = MAPPER.valueToTree(currentMap);\n+        JsonNode target = MAPPER.valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    // entry is not in the current, it is added\n+                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String level) {\n+        int index = level.indexOf(\",\");\n+        if (index > 0) {\n+            return level.substring(0, index).trim();\n+        } else {\n+            return level.trim();\n+        }\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n+            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n+            if (isValidLoggerLevel(level)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n+            } else {\n+                log.warn(\"Level {} is not valid logging level\", level);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * All loggers can be set dynamically. If the logger is not set in desire, set it to ERROR. Loggers with already set to ERROR should be skipped.\n+     * ERROR is set as inactive because log4j does not support OFF logger value.\n+     * We want to skip \"root\" logger as well to avoid duplicated key in alterConfigOps collection.\n+     * @param alterConfigOps collection of AlterConfigOp\n+     * @param pathValueWithoutSlash name of \"removed\" logger\n+     * @param entry entry to be removed (set to ERROR)\n+     */\n+    private static void removeProperty(Collection<AlterConfigOp> alterConfigOps, String pathValueWithoutSlash, ConfigEntry entry) {\n+        if (!pathValueWithoutSlash.contains(\"log4j.appender\") && !pathValueWithoutSlash.equals(\"root\") && !\"ERROR\".equals(entry.value())) {\n+            alterConfigOps.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, \"ERROR\"), AlterConfigOp.OpType.SET));\n+            log.trace(\"{} not set in desired, setting to ERROR\", entry.name());\n+        }\n+    }\n+\n+    /**\n+     * @return whether the current config and the desired config are identical (thus, no update is necessary).\n+     */\n+    @Override\n+    public boolean isEmpty() {\n+        return  diff.size() == 0;\n+    }\n+\n+    private static boolean isValidLoggerLevel(String level) {\n+        return VALID_LOGGER_LEVELS.contains(level);", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMjE3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472932174", "bodyText": "You just wrote a Util method for new ConfigResource(ConfigResource.Type.BROKER_LOGGER, so use it!", "author": "tombentley", "createdAt": "2020-08-19T10:38:03Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -420,35 +427,61 @@ private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, Int\n                     needsRestart = true;\n                 }\n             }\n+            if (loggingDiff.getDiffSize() > 0) {\n+                log.info(\"{}: Pod {} logging needs to be reconfigured.\", reconciliation, podId);\n+                needsReconfig = true;\n+            }\n         } else {\n             log.info(\"{}: Pod {} needs to be restarted. Reason: {}\", reconciliation, podId, reasonToRestartPod);\n         }\n-        return new RestartPlan(adminClient, needsRestart, needsReconfig, diff);\n+        return new RestartPlan(adminClient, needsRestart, needsReconfig, diff, loggingDiff);\n     }\n \n     /**\n-     * Returns a Future which completes with the config of the given broker.\n+     * Returns a config of the given broker.\n      * @param ac The admin client\n      * @param brokerId The id of the broker.\n      * @return a Future which completes with the config of the given broker.\n      */\n     protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n-        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        ConfigResource resource = Util.getBrokersConfig(brokerId);\n         return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n             30, TimeUnit.SECONDS,\n             error -> new ForceableProblem(\"Error getting broker config\", error)\n         );\n     }\n \n-    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)\n+    /**\n+     * Returns logging of the given broker.\n+     * @param ac The admin client\n+     * @param brokerId The id of the broker.\n+     * @return a Future which completes with the logging of the given broker.\n+     */\n+    protected Config brokerLogging(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n+        ConfigResource resource = Util.getBrokersLogging(brokerId);\n+        return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n+                30, TimeUnit.SECONDS,\n+            error -> new ForceableProblem(\"Error getting broker logging\", error)\n+        );\n+    }\n+\n+    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff, KafkaBrokerLoggingConfigurationDiff logDiff)\n             throws ForceableProblem, InterruptedException {\n-        Map<ConfigResource, Collection<AlterConfigOp>> configDiff = configurationDiff.getConfigDiff();\n-        log.debug(\"{}: Altering broker {} with {}\", reconciliation, podId, configDiff);\n-        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configDiff);\n+        Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig = new HashMap<>(2);\n+        updatedConfig.put(Util.getBrokersConfig(podId), configurationDiff.getConfigDiff());\n+        updatedConfig.put(Util.getBrokersLogging(podId), logDiff.getLoggingDiff());\n+\n+        log.info(\"{}: Altering broker {} with {}\", reconciliation, podId, updatedConfig);\n+\n+        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(updatedConfig);\n         KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(podId)));\n+        KafkaFuture<Void> brokerLoggingConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER_LOGGER, Integer.toString(podId)));", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMzMzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472933337", "bodyText": "Is it even worth putting a two line file in a resource like that? Just use a String literal.\nIf you must use a file, the name desired-kafka-broker-logging.conf is a bit weird. It's a properties file, so call it with .properties suffix at least.", "author": "tombentley", "createdAt": "2020-08-19T10:40:20Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaBrokerLoggingConfigurationDiffTest {\n+\n+    private final int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker-logging.conf\")) {", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNDE2NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472934164", "bodyText": "Same comment.", "author": "tombentley", "createdAt": "2020-08-19T10:41:49Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaBrokerLoggingConfigurationDiffTest {\n+\n+    private final int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker-logging.conf\")) {\n+            StringBuilder desiredConfigString = new StringBuilder(TestUtils.readResource(is));\n+\n+            for (ConfigEntry ce : additional) {\n+                desiredConfigString.append(\"\\n\").append(ce.name()).append(\"=\").append(ce.value());\n+            }\n+\n+            return desiredConfigString.toString();\n+        } catch (IOException e) {\n+            fail(e);\n+        }\n+        return \"\";\n+    }\n+\n+    private Config getCurrentConfiguration(List<ConfigEntry> additional) {\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker-logging.conf\")) {", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzOTQxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472939414", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n          \n          \n            \n                 * Annotations for rolling a cluster whenever the logging (or it's part) has changed.\n          \n          \n            \n                   By changing the annotation we force a restart since the pod will be out of date compared to the statefulset.", "author": "tombentley", "createdAt": "2020-08-19T10:51:53Z", "path": "operator-common/src/main/java/io/strimzi/operator/common/Annotations.java", "diffHunk": "@@ -18,6 +18,12 @@\n \n     public static final String STRIMZI_DOMAIN = \"strimzi.io/\";\n     public static final String STRIMZI_LOGGING_ANNOTATION = STRIMZI_DOMAIN + \"logging\";\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fdffb2d5504562a8f01ee94786bf2880744ed62a", "message": "dyn log changes in kafka\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:33:01Z", "type": "commit"}, {"oid": "141ae67259d20f20245d6017ee7c24163063df6e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/141ae67259d20f20245d6017ee7c24163063df6e", "message": "checkstyle\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:37:46Z", "type": "commit"}, {"oid": "0f4230eb5270eea8c920bd61f7688885ad9eb342", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0f4230eb5270eea8c920bd61f7688885ad9eb342", "message": "rolling\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:37:46Z", "type": "commit"}, {"oid": "3144a85554461710a0bf49802184c806695ddb81", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3144a85554461710a0bf49802184c806695ddb81", "message": "fixes\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:39:58Z", "type": "commit"}, {"oid": "eadc54662e112d812d703aa260ea3549c57732a9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/eadc54662e112d812d703aa260ea3549c57732a9", "message": "commit\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:39:58Z", "type": "commit"}, {"oid": "10f99e3bfc8c3172d7550090dc1327435d82f840", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/10f99e3bfc8c3172d7550090dc1327435d82f840", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:39:58Z", "type": "commit"}, {"oid": "f8d99bd3ba20db97d82d178cddd4549377e46cf1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f8d99bd3ba20db97d82d178cddd4549377e46cf1", "message": "comment\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:41:52Z", "type": "commit"}, {"oid": "5d0a413c910a3dd080c5bb8fdc292e3d634546dd", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5d0a413c910a3dd080c5bb8fdc292e3d634546dd", "message": "simpification\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:41:52Z", "type": "commit"}, {"oid": "36737e1c06ea5f96fe918ccc2a78626ab1818a4c", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/36737e1c06ea5f96fe918ccc2a78626ab1818a4c", "message": "review comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:41:52Z", "type": "commit"}, {"oid": "ceeddb22adb88d42c146f3d4742a6dccca29d4d4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ceeddb22adb88d42c146f3d4742a6dccca29d4d4", "message": "Jakub's rc\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:43:27Z", "type": "commit"}, {"oid": "de057864d5b91a9bef3e3da7f7cae629c964ccaa", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/de057864d5b91a9bef3e3da7f7cae629c964ccaa", "message": "Tom's comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:43:27Z", "type": "commit"}, {"oid": "bccfb86bf20212ed311c5d511850dd93b5143f48", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bccfb86bf20212ed311c5d511850dd93b5143f48", "message": "rebase to master\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T08:01:13Z", "type": "commit"}, {"oid": "bccfb86bf20212ed311c5d511850dd93b5143f48", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bccfb86bf20212ed311c5d511850dd93b5143f48", "message": "rebase to master\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T08:01:13Z", "type": "forcePushed"}]}