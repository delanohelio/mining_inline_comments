{"pr_number": 4137, "pr_title": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests", "pr_createdAt": "2020-12-21T13:11:28Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137", "timeline": [{"oid": "f1d7acd15f7150c1423afa305007a7fe55de59d9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f1d7acd15f7150c1423afa305007a7fe55de59d9", "message": "[MO] - [system tests] -> 3rd step (templates, re-worked resources, first tests suites works\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-01-27T11:06:22Z", "type": "forcePushed"}, {"oid": "bed3d52e656c5df34051a8f3d3bffef993cd2f29", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bed3d52e656c5df34051a8f3d3bffef993cd2f29", "message": "rebasing shit...\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-02-03T21:34:47Z", "type": "forcePushed"}, {"oid": "cd417e9ac68d880ee2a589764fa46797cce9311e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/cd417e9ac68d880ee2a589764fa46797cce9311e", "message": "[MO] - rebase, tracing full fix, security almost\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-02-18T00:26:46Z", "type": "forcePushed"}, {"oid": "9ba9cff68ff77915e4629a687b662887fb67c025", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9ba9cff68ff77915e4629a687b662887fb67c025", "message": "rebase again\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-02-18T00:52:00Z", "type": "forcePushed"}, {"oid": "e98dcdb2d3c547b6062ffef65089a49448aadda0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e98dcdb2d3c547b6062ffef65089a49448aadda0", "message": "mo - updating\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-02-24T12:21:46Z", "type": "forcePushed"}, {"oid": "a62894d518835dce8c120d04a3977db542cd0fb9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a62894d518835dce8c120d04a3977db542cd0fb9", "message": "more fixes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-04T18:16:47Z", "type": "forcePushed"}, {"oid": "452f622d76fc83d675c3996bb283bca88b02372a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/452f622d76fc83d675c3996bb283bca88b02372a", "message": "rebased\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-12T21:09:05Z", "type": "forcePushed"}, {"oid": "40f306f3801944f92682fdbc85ce98b0034108c3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/40f306f3801944f92682fdbc85ce98b0034108c3", "message": "rebased\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-12T21:35:13Z", "type": "forcePushed"}, {"oid": "630cef113cbe78004550d57f5fe0074f956dbb16", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/630cef113cbe78004550d57f5fe0074f956dbb16", "message": "Fixes for failures\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-13T15:50:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjA1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593802052", "bodyText": "Maybe this is already fixed in master?", "author": "Frawless", "createdAt": "2021-03-13T21:14:47Z", "path": "api-conversion/src/test/java/io/strimzi/kafka/api/conversion/cli/CrdUpgradeCommandIT.java", "diffHunk": "@@ -74,7 +75,7 @@ public void testCrdUpgradeWithConvertedResources() {\n                             .endEphemeralStorage()\n                             .withNewJmxPrometheusExporterMetricsConfig()\n                                 .withNewValueFrom()\n-                                    .withNewConfigMapKeyRef(\"zoo-metrics\", \"metrics-cm\", false)\n+                                    .withConfigMapKeyRef(new ConfigMapKeySelectorBuilder().withName(\"zoo-metrics\").withKey(\"metrics-cm\").withOptional(false).build())", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwMTk3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593901979", "bodyText": "Hmmm, It's not working... :D I don't have the method withNew...", "author": "see-quick", "createdAt": "2021-03-14T13:36:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjA1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjA3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593802074", "bodyText": "Leftover", "author": "Frawless", "createdAt": "2021-03-13T21:15:03Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/Environment.java", "diffHunk": "@@ -128,7 +128,7 @@\n     private static final String STRIMZI_LOG_LEVEL_DEFAULT = \"DEBUG\";\n     private static final String STRIMZI_COMPONENTS_LOG_LEVEL_DEFAULT = \"INFO\";\n     static final String KUBERNETES_DOMAIN_DEFAULT = \".nip.io\";\n-    public static final String COMPONENTS_IMAGE_PULL_POLICY_ENV_DEFAULT = Constants.IF_NOT_PRESENT_IMAGE_PULL_POLICY;\n+    public static final String COMPONENTS_IMAGE_PULL_POLICY_ENV_DEFAULT = Constants.ALWAYS_IMAGE_PULL_POLICY;", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjU5Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593802592", "bodyText": "Is this log needed? If yes, it should tell somethng more reasonable than this", "author": "Frawless", "createdAt": "2021-03-13T21:18:47Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/interfaces/TestSeparator.java", "diffHunk": "@@ -22,15 +22,19 @@\n \n     @BeforeEach\n     default void beforeEachTest(ExtensionContext testContext) {\n-        TimeMeasuringSystem.getInstance().setTestName(testContext.getRequiredTestClass().getName(), testContext.getRequiredTestMethod().getName());\n-        TimeMeasuringSystem.getInstance().startOperation(Operation.TEST_EXECUTION);\n+        LOGGER.info(\"HELLO FROM:\\nCLASS:{}\\nMETHOD:{}\", testContext.getRequiredTestClass().getName(), testContext.getRequiredTestMethod().getName());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzg5ODAxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593898011", "bodyText": "No, it's not thanks for catching.", "author": "see-quick", "createdAt": "2021-03-14T13:08:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjU5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjcyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593802723", "bodyText": "Better log?", "author": "Frawless", "createdAt": "2021-03-13T21:20:02Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/BasicExternalKafkaClient.java", "diffHunk": "@@ -109,13 +109,15 @@ public int sendMessagesTls(long timeoutMs) {\n         IntPredicate msgCntPredicate = x -> x == messageCount;\n \n         this.caCertName = this.caCertName == null ?\n-            KafkaResource.getKafkaExternalListenerCaCertName(namespaceName, clusterName, listenerName) :\n+            KafkaUtils.getKafkaExternalListenerCaCertName(namespaceName, clusterName, listenerName) :\n             this.caCertName;\n \n         LOGGER.info(\"Going to use the following CA certificate: {}\", caCertName);\n \n         ProducerProperties properties = this.producerProperties;\n \n+        LOGGER.info(\"==============This bootstrap {}\", getBootstrapServerFromStatus());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjc3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593802778", "bodyText": "Don't we pass the whole bootstrap address?", "author": "Frawless", "createdAt": "2021-03-13T21:20:40Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java", "diffHunk": "@@ -254,6 +254,9 @@ public String getListenerName() {\n     public String getSecretPrefix() {\n         return secretPrefix;\n     }\n+    public String getClusterName() {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMjk0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593802940", "bodyText": "Well I guess we won't create anything manually during ST.", "author": "Frawless", "createdAt": "2021-03-13T21:22:38Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzA4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803089", "bodyText": "Same as above. Everything should be done automatically.", "author": "Frawless", "createdAt": "2021-03-13T21:24:32Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                    continue;\n+                }\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+                assertTrue(waitResourceCondition(resource, type::isReady),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+                // sync because without this block more threads can access updated variable and re-write before we invoke\n+                // type.refreshResource()...\n+                synchronized (this) {\n+                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+                    type.refreshResource(resource, updated);\n+                }\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzMyNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803326", "bodyText": "Wouldn't be better to use assertThat with some reasonable message for users?", "author": "Frawless", "createdAt": "2021-03-13T21:26:31Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                    continue;\n+                }\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+                assertTrue(waitResourceCondition(resource, type::isReady),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+                // sync because without this block more threads can access updated variable and re-write before we invoke\n+                // type.refreshResource()...\n+                synchronized (this) {\n+                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+                    type.refreshResource(resource, updated);\n+                }\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzM2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803361", "bodyText": "Reasonable log", "author": "Frawless", "createdAt": "2021-03-13T21:26:46Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                    continue;\n+                }\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+                assertTrue(waitResourceCondition(resource, type::isReady),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+                // sync because without this block more threads can access updated variable and re-write before we invoke\n+                // type.refreshResource()...\n+                synchronized (this) {\n+                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+                    type.refreshResource(resource, updated);\n+                }\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);\n+            } catch (InterruptedException e) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is not {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzM5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803399", "bodyText": "Maybe add resource type and name?", "author": "Frawless", "createdAt": "2021-03-13T21:27:00Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                    continue;\n+                }\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+                assertTrue(waitResourceCondition(resource, type::isReady),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+                // sync because without this block more threads can access updated variable and re-write before we invoke\n+                // type.refreshResource()...\n+                synchronized (this) {\n+                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+                    type.refreshResource(resource, updated);\n+                }\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);\n+            } catch (InterruptedException e) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is not {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                Thread.currentThread().interrupt();\n+                return false;\n+            }\n+        }\n+        T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+        boolean pass = condition.test(res);\n+        if (!pass) {\n+            LOGGER.info(\"Resource failed condition check: {}\", resourceToString(res));", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzU1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803551", "bodyText": "Could we add more info about the resource into log? And I think it should have ERROR or at least WARN log level.", "author": "Frawless", "createdAt": "2021-03-13T21:27:53Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                    continue;\n+                }\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+                assertTrue(waitResourceCondition(resource, type::isReady),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+                // sync because without this block more threads can access updated variable and re-write before we invoke\n+                // type.refreshResource()...\n+                synchronized (this) {\n+                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+                    type.refreshResource(resource, updated);\n+                }\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);\n+            } catch (InterruptedException e) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is not {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                Thread.currentThread().interrupt();\n+                return false;\n+            }\n+        }\n+        T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+        boolean pass = condition.test(res);\n+        if (!pass) {\n+            LOGGER.info(\"Resource failed condition check: {}\", resourceToString(res));\n+        }\n+        return pass;\n     }\n \n-    private static void waitForDeletion(KafkaBridge kafkaBridge) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaBridge {}\", kafkaBridge.getMetadata().getName());\n+    private static final ObjectMapper MAPPER = new ObjectMapper(new YAMLFactory());\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public static <T extends HasMetadata> String resourceToString(T resource) {\n+        if (resource == null) {\n+            return \"null\";\n+        }\n+        try {\n+            return MAPPER.writeValueAsString(resource);\n+        } catch (JsonProcessingException e) {\n+            LOGGER.info(\"Failed converting resource to YAML: {}\", e.getMessage());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzYxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803611", "bodyText": "Maybe better log message?", "author": "Frawless", "createdAt": "2021-03-13T21:28:31Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                    continue;\n+                }\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+                assertTrue(waitResourceCondition(resource, type::isReady),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+                // sync because without this block more threads can access updated variable and re-write before we invoke\n+                // type.refreshResource()...\n+                synchronized (this) {\n+                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+                    type.refreshResource(resource, updated);\n+                }\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);\n+            } catch (InterruptedException e) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is not {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                Thread.currentThread().interrupt();\n+                return false;\n+            }\n+        }\n+        T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+        boolean pass = condition.test(res);\n+        if (!pass) {\n+            LOGGER.info(\"Resource failed condition check: {}\", resourceToString(res));\n+        }\n+        return pass;\n     }\n \n-    private static void waitForDeletion(KafkaBridge kafkaBridge) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaBridge {}\", kafkaBridge.getMetadata().getName());\n+    private static final ObjectMapper MAPPER = new ObjectMapper(new YAMLFactory());\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public static <T extends HasMetadata> String resourceToString(T resource) {\n+        if (resource == null) {\n+            return \"null\";\n+        }\n+        try {\n+            return MAPPER.writeValueAsString(resource);\n+        } catch (JsonProcessingException e) {\n+            LOGGER.info(\"Failed converting resource to YAML: {}\", e.getMessage());\n+            return \"unknown\";\n+        }\n     }\n \n-    private static void waitForDeletion(Deployment deployment) {\n-        LOGGER.info(\"Waiting when all the pods are terminated for Deployment {}\", deployment.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(deployment.getMetadata().getName());\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(deployment.getMetadata().getName()))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n+        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n+        T resource = namedResource.get();\n+        editor.accept(resource);\n+        namedResource.replace(resource);\n     }\n \n-    public static void deleteClassResources() {\n-        LOGGER.info(\"-----CLEARING CLASS RESOURCES-----\");\n-        while (!classResources.empty()) {\n-            classResources.pop().run();\n+    public void deleteResources(ExtensionContext testContext) throws Exception {\n+        LOGGER.info(String.join(\"\", Collections.nCopies(76, \"#\")));\n+        LOGGER.info(\"Going to clear all resources for {}\", testContext.getDisplayName());\n+        LOGGER.info(String.join(\"\", Collections.nCopies(76, \"#\")));\n+        if (!STORED_RESOURCES.containsKey(testContext.getDisplayName()) || STORED_RESOURCES.get(testContext.getDisplayName()).isEmpty()) {\n+            LOGGER.info(\"Nothing to delete\");", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzY2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803668", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T21:29:07Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +107,169 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new HelmResource(),\n+        new OlmResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                continue;\n+            }\n \n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                if (type == null) {\n+                    LOGGER.warn(\"Can't find resource in list, please create it manually\");\n+                    continue;\n+                }\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+                assertTrue(waitResourceCondition(resource, type::isReady),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+                // sync because without this block more threads can access updated variable and re-write before we invoke\n+                // type.refreshResource()...\n+                synchronized (this) {\n+                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+                    type.refreshResource(resource, updated);\n+                }\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);\n+            } catch (InterruptedException e) {\n+                // TODO create toString for conditions and print there Readyu or NotReady\n+                LOGGER.info(\"Resource {} in namespace {} is not {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                Thread.currentThread().interrupt();\n+                return false;\n+            }\n+        }\n+        T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+        boolean pass = condition.test(res);\n+        if (!pass) {\n+            LOGGER.info(\"Resource failed condition check: {}\", resourceToString(res));\n+        }\n+        return pass;\n     }\n \n-    private static void waitForDeletion(KafkaBridge kafkaBridge) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaBridge {}\", kafkaBridge.getMetadata().getName());\n+    private static final ObjectMapper MAPPER = new ObjectMapper(new YAMLFactory());\n \n-        DeploymentUtils.waitForDeploymentDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public static <T extends HasMetadata> String resourceToString(T resource) {\n+        if (resource == null) {\n+            return \"null\";\n+        }\n+        try {\n+            return MAPPER.writeValueAsString(resource);\n+        } catch (JsonProcessingException e) {\n+            LOGGER.info(\"Failed converting resource to YAML: {}\", e.getMessage());\n+            return \"unknown\";\n+        }\n     }\n \n-    private static void waitForDeletion(Deployment deployment) {\n-        LOGGER.info(\"Waiting when all the pods are terminated for Deployment {}\", deployment.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(deployment.getMetadata().getName());\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(deployment.getMetadata().getName()))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n+        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n+        T resource = namedResource.get();\n+        editor.accept(resource);\n+        namedResource.replace(resource);\n     }\n \n-    public static void deleteClassResources() {\n-        LOGGER.info(\"-----CLEARING CLASS RESOURCES-----\");\n-        while (!classResources.empty()) {\n-            classResources.pop().run();\n+    public void deleteResources(ExtensionContext testContext) throws Exception {\n+        LOGGER.info(String.join(\"\", Collections.nCopies(76, \"#\")));\n+        LOGGER.info(\"Going to clear all resources for {}\", testContext.getDisplayName());\n+        LOGGER.info(String.join(\"\", Collections.nCopies(76, \"#\")));\n+        if (!STORED_RESOURCES.containsKey(testContext.getDisplayName()) || STORED_RESOURCES.get(testContext.getDisplayName()).isEmpty()) {\n+            LOGGER.info(\"Nothing to delete\");\n         }\n-        classResources.clear();\n-        LOGGER.info(\"-----CLASS RESOURCES CLEARED-----\");\n-    }\n-\n-    public static void deleteMethodResources() {\n-        LOGGER.info(\"-----CLEARING METHOD RESOURCES-----\");\n-        while (!methodResources.empty()) {\n-            methodResources.pop().run();\n+        while (STORED_RESOURCES.containsKey(testContext.getDisplayName()) && !STORED_RESOURCES.get(testContext.getDisplayName()).isEmpty()) {\n+            STORED_RESOURCES.get(testContext.getDisplayName()).pop().run();\n         }\n-        methodResources.clear();\n-        pointerResources = classResources;\n-        LOGGER.info(\"-----METHOD RESOURCES CLEARED-----\");\n+        LOGGER.info(String.join(\"\", Collections.nCopies(76, \"#\")));\n+        LOGGER.info(\"\");", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzczOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803738", "bodyText": "Fix javadoc", "author": "Frawless", "createdAt": "2021-03-13T21:29:40Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -440,28 +325,29 @@ public static void deleteMethodResources() {\n      * @return returns CR", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwMzg2NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593803865", "bodyText": "Maybe you could add javadoc for those as well?", "author": "Frawless", "createdAt": "2021-03-13T21:30:59Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceType.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources;\n+\n+import io.fabric8.kubernetes.api.model.HasMetadata;\n+\n+public interface ResourceType<T extends HasMetadata> {\n+    String getKind();\n+\n+    T get(String namespace, String name);\n+\n+    void create(T resource);\n+\n+    void delete(T resource) throws Exception;", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDMwNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593804307", "bodyText": "What if namespace won't be set for a resource?", "author": "Frawless", "createdAt": "2021-03-13T21:35:43Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java", "diffHunk": "@@ -5,124 +5,51 @@\n package io.strimzi.systemtest.resources.crd;\n \n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaBridgeList;\n import io.strimzi.api.kafka.model.KafkaBridge;\n-import io.strimzi.api.kafka.model.KafkaBridgeBuilder;\n-import io.strimzi.api.kafka.model.KafkaBridgeResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.enums.CustomResourceStatus;\n+import io.strimzi.systemtest.resources.ResourceType;\n import io.strimzi.systemtest.resources.ResourceManager;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n+public class KafkaBridgeResource implements ResourceType<KafkaBridge> {\n \n-public class KafkaBridgeResource {\n+    public KafkaBridgeResource() { }\n \n-    public static final String PATH_TO_KAFKA_BRIDGE_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/bridge/kafka-bridge.yaml\";\n-    public static final String PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/metrics/kafka-bridge-metrics.yaml\";\n-\n-    public static MixedOperation<KafkaBridge, KafkaBridgeList, Resource<KafkaBridge>> kafkaBridgeClient() {\n-        return Crds.kafkaBridgeV1Beta2Operation(ResourceManager.kubeClient().getClient());\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String bootstrap, int kafkaBridgeReplicas) {\n-        return kafkaBridge(name, name, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridgeWithCors(name, name, bootstrap, kafkaBridgeReplicas, allowedCorsOrigin, allowedCorsMethods);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridge(name, clusterName, bootstrap, kafkaBridgeReplicas)\n-            .editSpec()\n-                .editHttp()\n-                    .withNewCors()\n-                        .withAllowedOrigins(allowedCorsOrigin)\n-                        .withAllowedMethods(allowedCorsMethods != null ? allowedCorsMethods : \"GET,POST,PUT,DELETE,OPTIONS,PATCH\")\n-                    .endCors()\n-                .endHttp()\n-            .endSpec();\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap) {\n-        return kafkaBridgeWithMetrics(name, clusterName, bootstrap, 1);\n+    @Override\n+    public String getKind() {\n+        return KafkaBridge.RESOURCE_KIND;\n     }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG);\n-\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+    @Override\n+    public KafkaBridge get(String namespace, String name) {\n+        return kafkaBridgeClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    private static KafkaBridgeBuilder defaultKafkaBridge(KafkaBridge kafkaBridge, String name, String kafkaClusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        return new KafkaBridgeBuilder(kafkaBridge)\n-            .withNewMetadata()\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .withClusterName(kafkaClusterName)\n-            .endMetadata()\n-            .editSpec()\n-                .withBootstrapServers(bootstrap)\n-                .withReplicas(kafkaBridgeReplicas)\n-                .withNewInlineLogging()\n-                    .addToLoggers(\"bridge.root.logger\", Environment.STRIMZI_COMPONENTS_LOG_LEVEL)\n-                .endInlineLogging()\n-            .endSpec();\n+    @Override\n+    public void create(KafkaBridge resource) {\n+        kafkaBridgeClient().inNamespace(resource.getMetadata().getNamespace()).createOrReplace(resource);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwMDE5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593900190", "bodyText": "It must be, otherwise, it will be null and then the client will create a resource in null namespace which obviously does not exist.", "author": "see-quick", "createdAt": "2021-03-14T13:24:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDMwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDQwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593804403", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                void refreshResource(T existing, T newResource);\n          \n          \n            \n                void replaceResource(T existing, T newResource);", "author": "Frawless", "createdAt": "2021-03-13T21:36:54Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceType.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources;\n+\n+import io.fabric8.kubernetes.api.model.HasMetadata;\n+\n+public interface ResourceType<T extends HasMetadata> {\n+    String getKind();\n+\n+    T get(String namespace, String name);\n+\n+    void create(T resource);\n+\n+    void delete(T resource) throws Exception;\n+\n+    /**\n+     * Check if this resource is marked as ready or not.\n+     *\n+     * @return true if ready.\n+     */\n+    boolean isReady(T resource);\n+\n+    /**\n+     * Update the resource with the latest state on the Kubernetes API.\n+     */\n+    void refreshResource(T existing, T newResource);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDU4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593804582", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T21:37:58Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java", "diffHunk": "@@ -4,241 +4,78 @@\n  */\n package io.strimzi.systemtest.resources.crd;\n \n-import io.fabric8.kubernetes.api.model.ContainerBuilder;\n-import io.fabric8.kubernetes.api.model.PodSpec;\n-import io.fabric8.kubernetes.api.model.PodSpecBuilder;\n-import io.fabric8.kubernetes.api.model.Quantity;\n-import io.fabric8.kubernetes.api.model.ResourceRequirementsBuilder;\n-import io.fabric8.kubernetes.api.model.Secret;\n import io.fabric8.kubernetes.api.model.apps.Deployment;\n-import io.fabric8.kubernetes.api.model.apps.DeploymentBuilder;\n-import io.strimzi.api.kafka.model.KafkaUser;\n-import io.strimzi.api.kafka.model.KafkaUserScramSha512ClientAuthentication;\n-import io.strimzi.api.kafka.model.KafkaUserTlsClientAuthentication;\n-import io.strimzi.operator.common.model.Labels;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n import io.strimzi.systemtest.resources.ResourceManager;\n-import org.apache.kafka.clients.consumer.ConsumerConfig;\n-import org.apache.kafka.clients.producer.ProducerConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.utils.ClientUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.nio.charset.Charset;\n-import java.util.Base64;\n-import java.util.Collections;\n-import java.util.Map;\n-\n-import static io.strimzi.test.TestUtils.toYamlString;\n-\n-public class KafkaClientsResource {\n+public class KafkaClientsResource implements ResourceType<Deployment> {\n \n     private static final Logger LOGGER = LogManager.getLogger(KafkaClientsResource.class);\n \n-    public static DeploymentBuilder deployKafkaClients(String kafkaClusterName) {\n-        return deployKafkaClients(false, kafkaClusterName, null);\n-    }\n+    public KafkaClientsResource() {}\n \n-    public static DeploymentBuilder deployKafkaClients(boolean tlsListener, String kafkaClientsName, KafkaUser... kafkaUsers) {\n-        return deployKafkaClients(tlsListener, kafkaClientsName, true,  null, null, kafkaUsers);\n+    @Override\n+    public String getKind() {\n+        return \"Deployment\";\n     }\n-\n-    public static DeploymentBuilder deployKafkaClients(boolean tlsListener, String kafkaClientsName, String listenerName,\n-                                                        KafkaUser... kafkaUsers) {\n-        return deployKafkaClients(tlsListener, kafkaClientsName, true, listenerName, null, kafkaUsers);\n+    @Override\n+    public Deployment get(String namespace, String name) {\n+        String deploymentName = ResourceManager.kubeClient().namespace(namespace).getDeploymentNameByPrefix(name);\n+        return deploymentName != null ?  ResourceManager.kubeClient().getDeployment(deploymentName) : null;\n+//        Deployment[] deployment = new Deployment[1];\n+\n+//        switch (phase) {\n+//            case CREATE_PHASE:\n+//                TestUtils.waitFor(\" until deployment is present\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+//                    () -> {\n+//                        deployment[0] = ResourceManager.kubeClient().namespace(namespace).getDeployment(ResourceManager.kubeClient().getDeploymentNameByPrefix(name));\n+//                        return deployment[0] != null;\n+//                    });\n+//                return ResourceManager.kubeClient().namespace(namespace).getDeployment(ResourceManager.kubeClient().getDeploymentNameByPrefix(name));\n+//            case DELETE_PHASE:\n+//                TestUtils.waitFor(\" until deployment is null\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+//                    () -> {\n+//                        try {\n+//                            deployment[0] = ResourceManager.kubeClient().namespace(namespace).getDeployment(ResourceManager.kubeClient().getDeploymentNameByPrefix(name));\n+//                            return deployment[0] == null;\n+//                        } catch (IndexOutOfBoundsException | NullPointerException exception) {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDY2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593804668", "bodyText": "Don't we have some constant for that?", "author": "Frawless", "createdAt": "2021-03-13T21:38:49Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java", "diffHunk": "@@ -4,241 +4,78 @@\n  */\n package io.strimzi.systemtest.resources.crd;\n \n-import io.fabric8.kubernetes.api.model.ContainerBuilder;\n-import io.fabric8.kubernetes.api.model.PodSpec;\n-import io.fabric8.kubernetes.api.model.PodSpecBuilder;\n-import io.fabric8.kubernetes.api.model.Quantity;\n-import io.fabric8.kubernetes.api.model.ResourceRequirementsBuilder;\n-import io.fabric8.kubernetes.api.model.Secret;\n import io.fabric8.kubernetes.api.model.apps.Deployment;\n-import io.fabric8.kubernetes.api.model.apps.DeploymentBuilder;\n-import io.strimzi.api.kafka.model.KafkaUser;\n-import io.strimzi.api.kafka.model.KafkaUserScramSha512ClientAuthentication;\n-import io.strimzi.api.kafka.model.KafkaUserTlsClientAuthentication;\n-import io.strimzi.operator.common.model.Labels;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n import io.strimzi.systemtest.resources.ResourceManager;\n-import org.apache.kafka.clients.consumer.ConsumerConfig;\n-import org.apache.kafka.clients.producer.ProducerConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.utils.ClientUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.nio.charset.Charset;\n-import java.util.Base64;\n-import java.util.Collections;\n-import java.util.Map;\n-\n-import static io.strimzi.test.TestUtils.toYamlString;\n-\n-public class KafkaClientsResource {\n+public class KafkaClientsResource implements ResourceType<Deployment> {\n \n     private static final Logger LOGGER = LogManager.getLogger(KafkaClientsResource.class);\n \n-    public static DeploymentBuilder deployKafkaClients(String kafkaClusterName) {\n-        return deployKafkaClients(false, kafkaClusterName, null);\n-    }\n+    public KafkaClientsResource() {}\n \n-    public static DeploymentBuilder deployKafkaClients(boolean tlsListener, String kafkaClientsName, KafkaUser... kafkaUsers) {\n-        return deployKafkaClients(tlsListener, kafkaClientsName, true,  null, null, kafkaUsers);\n+    @Override\n+    public String getKind() {\n+        return \"Deployment\";", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDY5MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593804691", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T21:39:16Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectResource.java", "diffHunk": "@@ -4,133 +4,56 @@\n  */\n package io.strimzi.systemtest.resources.crd;\n \n-import io.fabric8.kubernetes.api.model.ConfigMap;\n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaConnectList;\n-import io.strimzi.api.kafka.model.CertSecretSourceBuilder;\n import io.strimzi.api.kafka.model.KafkaConnect;\n-import io.strimzi.api.kafka.model.KafkaConnectBuilder;\n-import io.strimzi.api.kafka.model.KafkaConnectResources;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaConnectUtils;\n import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.test.k8s.KubeClusterResource;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+public class KafkaConnectResource implements ResourceType<KafkaConnect> {\n \n-public class KafkaConnectResource {\n-    public static final String PATH_TO_KAFKA_CONNECT_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/connect/kafka-connect.yaml\";\n+    public KafkaConnectResource() { }\n \n-    public static MixedOperation<KafkaConnect, KafkaConnectList, Resource<KafkaConnect>> kafkaConnectClient() {\n-        return Crds.kafkaConnectV1Beta2Operation(ResourceManager.kubeClient().getClient());\n+    @Override\n+    public String getKind() {\n+        return KafkaConnect.RESOURCE_KIND;\n     }\n-\n-    public static KafkaConnectBuilder kafkaConnect(String name, int kafkaConnectReplicas) {\n-        return kafkaConnect(name, name, kafkaConnectReplicas);\n+    @Override\n+    public KafkaConnect get(String namespace, String name) {\n+        return kafkaConnectClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    public static KafkaConnectBuilder kafkaConnect(String name, String clusterName, int kafkaConnectReplicas) {\n-        KafkaConnect kafkaConnect = getKafkaConnectFromYaml(PATH_TO_KAFKA_CONNECT_CONFIG);\n-        return defaultKafkaConnect(kafkaConnect, name, clusterName, kafkaConnectReplicas);\n+    @Override\n+    public void create(KafkaConnect resource) {\n+        // TODO: same as KafkaBridge", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDc5NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593804795", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T21:40:09Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectS2IResource.java", "diffHunk": "@@ -5,101 +5,55 @@\n package io.strimzi.systemtest.resources.crd;\n \n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaConnectS2IList;\n-import io.strimzi.api.kafka.model.CertSecretSourceBuilder;\n import io.strimzi.api.kafka.model.KafkaConnectS2I;\n-import io.strimzi.api.kafka.model.KafkaConnectS2IBuilder;\n-import io.strimzi.api.kafka.model.KafkaConnectS2IResources;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.enums.CustomResourceStatus;\n+import io.strimzi.systemtest.resources.ResourceType;\n import io.strimzi.systemtest.resources.ResourceManager;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n-\n // Deprecation is suppressed because of KafkaConnectS2I\n @SuppressWarnings(\"deprecation\")\n-public class KafkaConnectS2IResource {\n+public class KafkaConnectS2IResource implements ResourceType<KafkaConnectS2I> {\n \n-    public static MixedOperation<KafkaConnectS2I, KafkaConnectS2IList, Resource<KafkaConnectS2I>> kafkaConnectS2IClient() {\n-        return Crds.kafkaConnectS2iV1Beta2Operation(ResourceManager.kubeClient().getClient());\n-    }\n+    public KafkaConnectS2IResource() { }\n \n-    public static KafkaConnectS2IBuilder kafkaConnectS2I(String name, String clusterName, int kafkaConnectS2IReplicas) {\n-        KafkaConnectS2I kafkaConnectS2I = getKafkaConnectS2IFromYaml(Constants.PATH_TO_KAFKA_CONNECT_S2I_CONFIG);\n-        return defaultKafkaConnectS2I(kafkaConnectS2I, name, clusterName, kafkaConnectS2IReplicas);\n+    @Override\n+    public String getKind() {\n+        return KafkaConnectS2I.RESOURCE_KIND;\n     }\n-\n-    public static KafkaConnectS2IBuilder defaultKafkaConnectS2I(KafkaConnectS2I kafkaConnectS2I, String name, String kafkaClusterName, int kafkaConnectReplicas) {\n-        return new KafkaConnectS2IBuilder(kafkaConnectS2I)\n-            .withNewMetadata()\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .withClusterName(kafkaClusterName)\n-            .endMetadata()\n-            .editSpec()\n-                .withVersion(Environment.ST_KAFKA_VERSION)\n-                .withBootstrapServers(KafkaResources.tlsBootstrapAddress(kafkaClusterName))\n-                .withReplicas(kafkaConnectReplicas)\n-                // Try it without TLS\n-                .withNewTls()\n-                    .withTrustedCertificates(new CertSecretSourceBuilder().withNewSecretName(kafkaClusterName + \"-cluster-ca-cert\").withCertificate(\"ca.crt\").build())\n-                .endTls()\n-                .withInsecureSourceRepository(true)\n-                .withNewInlineLogging()\n-                    .addToLoggers(\"connect.root.logger.level\", Environment.STRIMZI_COMPONENTS_LOG_LEVEL)\n-                .endInlineLogging()\n-            .endSpec();\n+    @Override\n+    public KafkaConnectS2I get(String namespace, String name) {\n+        return kafkaConnectS2IClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    public static KafkaConnectS2I createAndWaitForReadiness(KafkaConnectS2I kafkaConnectS2I) {\n-        KubernetesResource.allowNetworkPolicySettingsForResource(kafkaConnectS2I, KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-\n-        TestUtils.waitFor(\"KafkaConnect creation\", Constants.POLL_INTERVAL_FOR_RESOURCE_CREATION, CR_CREATION_TIMEOUT,\n-            () -> {\n-                try {\n-                    kafkaConnectS2IClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaConnectS2I);\n-                    return true;\n-                } catch (KubernetesClientException e) {\n-                    if (e.getMessage().contains(\"object is being deleted\")) {\n-                        return false;\n-                    } else {\n-                        throw e;\n-                    }\n-                }\n-            }\n-        );\n-        return waitFor(deleteLater(kafkaConnectS2I));\n+    @Override\n+    public void create(KafkaConnectS2I resource) {\n+        // TODO: same as KafkaBridge and KafkaConnect", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNDkwMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593804901", "bodyText": "Why we have replace and refresh methods? Isn't it basically the same?", "author": "Frawless", "createdAt": "2021-03-13T21:41:15Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectS2IResource.java", "diffHunk": "@@ -5,101 +5,55 @@\n package io.strimzi.systemtest.resources.crd;\n \n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaConnectS2IList;\n-import io.strimzi.api.kafka.model.CertSecretSourceBuilder;\n import io.strimzi.api.kafka.model.KafkaConnectS2I;\n-import io.strimzi.api.kafka.model.KafkaConnectS2IBuilder;\n-import io.strimzi.api.kafka.model.KafkaConnectS2IResources;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.enums.CustomResourceStatus;\n+import io.strimzi.systemtest.resources.ResourceType;\n import io.strimzi.systemtest.resources.ResourceManager;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n-\n // Deprecation is suppressed because of KafkaConnectS2I\n @SuppressWarnings(\"deprecation\")\n-public class KafkaConnectS2IResource {\n+public class KafkaConnectS2IResource implements ResourceType<KafkaConnectS2I> {\n \n-    public static MixedOperation<KafkaConnectS2I, KafkaConnectS2IList, Resource<KafkaConnectS2I>> kafkaConnectS2IClient() {\n-        return Crds.kafkaConnectS2iV1Beta2Operation(ResourceManager.kubeClient().getClient());\n-    }\n+    public KafkaConnectS2IResource() { }\n \n-    public static KafkaConnectS2IBuilder kafkaConnectS2I(String name, String clusterName, int kafkaConnectS2IReplicas) {\n-        KafkaConnectS2I kafkaConnectS2I = getKafkaConnectS2IFromYaml(Constants.PATH_TO_KAFKA_CONNECT_S2I_CONFIG);\n-        return defaultKafkaConnectS2I(kafkaConnectS2I, name, clusterName, kafkaConnectS2IReplicas);\n+    @Override\n+    public String getKind() {\n+        return KafkaConnectS2I.RESOURCE_KIND;\n     }\n-\n-    public static KafkaConnectS2IBuilder defaultKafkaConnectS2I(KafkaConnectS2I kafkaConnectS2I, String name, String kafkaClusterName, int kafkaConnectReplicas) {\n-        return new KafkaConnectS2IBuilder(kafkaConnectS2I)\n-            .withNewMetadata()\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .withClusterName(kafkaClusterName)\n-            .endMetadata()\n-            .editSpec()\n-                .withVersion(Environment.ST_KAFKA_VERSION)\n-                .withBootstrapServers(KafkaResources.tlsBootstrapAddress(kafkaClusterName))\n-                .withReplicas(kafkaConnectReplicas)\n-                // Try it without TLS\n-                .withNewTls()\n-                    .withTrustedCertificates(new CertSecretSourceBuilder().withNewSecretName(kafkaClusterName + \"-cluster-ca-cert\").withCertificate(\"ca.crt\").build())\n-                .endTls()\n-                .withInsecureSourceRepository(true)\n-                .withNewInlineLogging()\n-                    .addToLoggers(\"connect.root.logger.level\", Environment.STRIMZI_COMPONENTS_LOG_LEVEL)\n-                .endInlineLogging()\n-            .endSpec();\n+    @Override\n+    public KafkaConnectS2I get(String namespace, String name) {\n+        return kafkaConnectS2IClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    public static KafkaConnectS2I createAndWaitForReadiness(KafkaConnectS2I kafkaConnectS2I) {\n-        KubernetesResource.allowNetworkPolicySettingsForResource(kafkaConnectS2I, KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-\n-        TestUtils.waitFor(\"KafkaConnect creation\", Constants.POLL_INTERVAL_FOR_RESOURCE_CREATION, CR_CREATION_TIMEOUT,\n-            () -> {\n-                try {\n-                    kafkaConnectS2IClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaConnectS2I);\n-                    return true;\n-                } catch (KubernetesClientException e) {\n-                    if (e.getMessage().contains(\"object is being deleted\")) {\n-                        return false;\n-                    } else {\n-                        throw e;\n-                    }\n-                }\n-            }\n-        );\n-        return waitFor(deleteLater(kafkaConnectS2I));\n+    @Override\n+    public void create(KafkaConnectS2I resource) {\n+        // TODO: same as KafkaBridge and KafkaConnect\n+        kafkaConnectS2IClient().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).createOrReplace(resource);\n     }\n-\n-    public static KafkaConnectS2I kafkaConnectS2IWithoutWait(KafkaConnectS2I kafkaConnectS2I) {\n-        kafkaConnectS2IClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaConnectS2I);\n-        return kafkaConnectS2I;\n-    }\n-\n-    public static void deleteKafkaConnectS2IWithoutWait(String resourceName) {\n-        kafkaConnectS2IClient().inNamespace(ResourceManager.kubeClient().getNamespace()).withName(resourceName).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n+    @Override\n+    public void delete(KafkaConnectS2I resource) throws Exception {\n+        kafkaConnectS2IClient().inNamespace(resource.getMetadata().getNamespace()).withName(\n+            resource.getMetadata().getName()).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n     }\n \n-    private static KafkaConnectS2I getKafkaConnectS2IFromYaml(String yamlPath) {\n-        return TestUtils.configFromYaml(yamlPath, KafkaConnectS2I.class);\n+    @Override\n+    public boolean isReady(KafkaConnectS2I resource) {\n+        return ResourceManager.waitForResourceStatus(kafkaConnectS2IClient(), resource, CustomResourceStatus.Ready);\n     }\n-\n-    private static KafkaConnectS2I waitFor(KafkaConnectS2I kafkaConnectS2I) {\n-        return ResourceManager.waitForResourceStatus(kafkaConnectS2IClient(), kafkaConnectS2I, Ready);\n+    @Override\n+    public void refreshResource(KafkaConnectS2I existing, KafkaConnectS2I newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+        existing.setStatus(newResource.getStatus());\n     }\n \n-    private static KafkaConnectS2I deleteLater(KafkaConnectS2I kafkaConnectS2I) {\n-        return ResourceManager.deleteLater(kafkaConnectS2IClient(), kafkaConnectS2I);\n+    public static MixedOperation<KafkaConnectS2I, KafkaConnectS2IList, Resource<KafkaConnectS2I>> kafkaConnectS2IClient() {\n+        return Crds.kafkaConnectS2iV1Beta2Operation(ResourceManager.kubeClient().getClient());\n     }\n \n     public static void replaceConnectS2IResource(String resourceName, Consumer<KafkaConnectS2I> editor) {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTA1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805054", "bodyText": "It's probably in other resources as well, but the status is set as well?", "author": "Frawless", "createdAt": "2021-03-13T21:43:24Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaUserResource.java", "diffHunk": "@@ -19,67 +19,43 @@\n \n import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n \n-public class KafkaUserResource {\n+public class KafkaUserResource implements ResourceType<KafkaUser> {\n     private static final Logger LOGGER = LogManager.getLogger(KafkaUserResource.class);\n \n-    public static MixedOperation<KafkaUser, KafkaUserList, Resource<KafkaUser>> kafkaUserClient() {\n-        return Crds.kafkaUserV1Beta2Operation(ResourceManager.kubeClient().getClient());\n-    }\n+    public KafkaUserResource() {}\n \n-    public static KafkaUserBuilder tlsUser(String clusterName, String name) {\n-        return defaultUser(clusterName, name)\n-            .withNewSpec()\n-                .withNewKafkaUserTlsClientAuthentication()\n-                .endKafkaUserTlsClientAuthentication()\n-            .endSpec();\n+    @Override\n+    public String getKind() {\n+        return KafkaUser.RESOURCE_KIND;\n     }\n-\n-    public static KafkaUserBuilder scramShaUser(String clusterName, String name) {\n-        return defaultUser(clusterName, name)\n-            .withNewSpec()\n-                .withNewKafkaUserScramSha512ClientAuthentication()\n-                .endKafkaUserScramSha512ClientAuthentication()\n-            .endSpec();\n+    @Override\n+    public KafkaUser get(String namespace, String name) {\n+        return kafkaUserClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    public static KafkaUserBuilder defaultUser(String clusterName, String name) {\n-        return new KafkaUserBuilder()\n-            .withNewMetadata()\n-                .withClusterName(clusterName)\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .addToLabels(Labels.STRIMZI_CLUSTER_LABEL, clusterName)\n-            .endMetadata();\n+    @Override\n+    public void create(KafkaUser resource) {\n+        kafkaUserClient().inNamespace(resource.getMetadata().getNamespace()).createOrReplace(resource);\n     }\n \n-    public static KafkaUser createAndWaitForReadiness(KafkaUser user) {\n-        kafkaUserClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(user);\n-        LOGGER.info(\"Created KafkaUser {}\", user.getMetadata().getName());\n-        return waitFor(deleteLater(user));\n+    @Override\n+    public void delete(KafkaUser resource) throws Exception {\n+        kafkaUserClient().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n     }\n \n-    public static KafkaUser kafkaUserWithoutWait(KafkaUser user) {\n-        kafkaUserClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(user);\n-        return user;\n+    @Override\n+    public boolean isReady(KafkaUser resource) {\n+        return ResourceManager.waitForResourceStatus(kafkaUserClient(), resource, Ready);\n     }\n \n-    private static KafkaUser waitFor(KafkaUser kafkaUser) {\n-        return ResourceManager.waitForResourceStatus(kafkaUserClient(), kafkaUser, Ready);\n+    @Override\n+    public void refreshResource(KafkaUser existing, KafkaUser newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+        existing.setStatus(newResource.getStatus());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTIwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805209", "bodyText": "Does it allow to restart Job in case of client failure? If yes, it should not be changed", "author": "Frawless", "createdAt": "2021-03-13T21:45:14Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/kafkaclients/KafkaBasicExampleClients.java", "diffHunk": "@@ -198,10 +196,12 @@ public JobBuilder defaultProducerStrimzi() {\n                 .withBackoffLimit(0)\n                 .withNewTemplate()\n                     .withNewMetadata()\n-                    .withLabels(producerLabels)\n+                        .withName(producerName)\n+                        .withNamespace(ResourceManager.kubeClient().getNamespace())\n+                        .withLabels(producerLabels)\n                     .endMetadata()\n                     .withNewSpec()\n-                        .withRestartPolicy(\"Never\")\n+                        .withRestartPolicy(\"OnFailure\")", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTMzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805336", "bodyText": "indent, and in the following resoures as well", "author": "Frawless", "createdAt": "2021-03-13T21:46:03Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.rbac.ClusterRoleBinding;\n+import io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder;\n+import io.fabric8.kubernetes.api.model.rbac.SubjectBuilder;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.test.TestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class ClusterRoleBindingResource implements ResourceType<ClusterRoleBinding> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(ClusterRoleBindingResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"ClusterRoleBinding\";\n+    }\n+    @Override\n+    public ClusterRoleBinding get(String namespace, String name) {\n+        return ResourceManager.kubeClient().namespace(namespace).getClusterRoleBinding(name);\n+    }\n+    @Override\n+    public void create(ClusterRoleBinding resource) {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).createOrReplaceClusterRoleBinding(resource);\n+    }\n+    @Override\n+    public void delete(ClusterRoleBinding resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteClusterRoleBinding(resource);\n+    }\n+    @Override\n+    public boolean isReady(ClusterRoleBinding resource) {\n+        return resource != null;\n+    }\n+    @Override\n+    public void refreshResource(ClusterRoleBinding existing, ClusterRoleBinding newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+    }\n+\n+    public static List<ClusterRoleBinding> clusterRoleBindingsForAllNamespaces(String namespace) {\n+        LOGGER.info(\"Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects\");\n+        return clusterRoleBindingsForAllNamespaces(namespace, \"strimzi-cluster-operator\");\n+    }\n+\n+    public static List<ClusterRoleBinding> clusterRoleBindingsForAllNamespaces(String namespace, String coName) {\n+        LOGGER.info(\"Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects\");\n+\n+        List<ClusterRoleBinding> kCRBList = new ArrayList<>();\n+\n+        kCRBList.add(\n+            new ClusterRoleBindingBuilder()\n+                .withNewMetadata()\n+                .withName(coName + \"-namespaced\")\n+                .endMetadata()\n+                .withNewRoleRef()\n+                .withApiGroup(\"rbac.authorization.k8s.io\")\n+                .withKind(\"ClusterRole\")\n+                .withName(\"strimzi-cluster-operator-namespaced\")\n+                .endRoleRef()\n+                .withSubjects(new SubjectBuilder()\n+                    .withKind(\"ServiceAccount\")\n+                    .withName(\"strimzi-cluster-operator\")\n+                    .withNamespace(namespace)\n+                    .build()\n+                )\n+                .build()", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTM1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805356", "bodyText": "Better log message?", "author": "Frawless", "createdAt": "2021-03-13T21:46:19Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.rbac.ClusterRoleBinding;\n+import io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder;\n+import io.fabric8.kubernetes.api.model.rbac.SubjectBuilder;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.test.TestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class ClusterRoleBindingResource implements ResourceType<ClusterRoleBinding> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(ClusterRoleBindingResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"ClusterRoleBinding\";\n+    }\n+    @Override\n+    public ClusterRoleBinding get(String namespace, String name) {\n+        return ResourceManager.kubeClient().namespace(namespace).getClusterRoleBinding(name);\n+    }\n+    @Override\n+    public void create(ClusterRoleBinding resource) {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).createOrReplaceClusterRoleBinding(resource);\n+    }\n+    @Override\n+    public void delete(ClusterRoleBinding resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteClusterRoleBinding(resource);\n+    }\n+    @Override\n+    public boolean isReady(ClusterRoleBinding resource) {\n+        return resource != null;\n+    }\n+    @Override\n+    public void refreshResource(ClusterRoleBinding existing, ClusterRoleBinding newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+    }\n+\n+    public static List<ClusterRoleBinding> clusterRoleBindingsForAllNamespaces(String namespace) {\n+        LOGGER.info(\"Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects\");\n+        return clusterRoleBindingsForAllNamespaces(namespace, \"strimzi-cluster-operator\");\n+    }\n+\n+    public static List<ClusterRoleBinding> clusterRoleBindingsForAllNamespaces(String namespace, String coName) {\n+        LOGGER.info(\"Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects\");\n+\n+        List<ClusterRoleBinding> kCRBList = new ArrayList<>();\n+\n+        kCRBList.add(\n+            new ClusterRoleBindingBuilder()\n+                .withNewMetadata()\n+                .withName(coName + \"-namespaced\")\n+                .endMetadata()\n+                .withNewRoleRef()\n+                .withApiGroup(\"rbac.authorization.k8s.io\")\n+                .withKind(\"ClusterRole\")\n+                .withName(\"strimzi-cluster-operator-namespaced\")\n+                .endRoleRef()\n+                .withSubjects(new SubjectBuilder()\n+                    .withKind(\"ServiceAccount\")\n+                    .withName(\"strimzi-cluster-operator\")\n+                    .withNamespace(namespace)\n+                    .build()\n+                )\n+                .build()\n+        );\n+\n+        kCRBList.add(\n+            new ClusterRoleBindingBuilder()\n+                .withNewMetadata()\n+                .withName(coName + \"-entity-operator\")\n+                .endMetadata()\n+                .withNewRoleRef()\n+                .withApiGroup(\"rbac.authorization.k8s.io\")\n+                .withKind(\"ClusterRole\")\n+                .withName(\"strimzi-entity-operator\")\n+                .endRoleRef()\n+                .withSubjects(new SubjectBuilder()\n+                    .withKind(\"ServiceAccount\")\n+                    .withName(\"strimzi-cluster-operator\")\n+                    .withNamespace(namespace)\n+                    .build()\n+                )\n+                .build()\n+        );\n+\n+        kCRBList.add(\n+            new ClusterRoleBindingBuilder()\n+                .withNewMetadata()\n+                .withName(coName + \"-topic-operator\")\n+                .endMetadata()\n+                .withNewRoleRef()\n+                .withApiGroup(\"rbac.authorization.k8s.io\")\n+                .withKind(\"ClusterRole\")\n+                .withName(\"strimzi-topic-operator\")\n+                .endRoleRef()\n+                .withSubjects(new SubjectBuilder()\n+                    .withKind(\"ServiceAccount\")\n+                    .withName(\"strimzi-cluster-operator\")\n+                    .withNamespace(namespace)\n+                    .build()\n+                )\n+                .build()\n+        );\n+        return kCRBList;\n+    }\n+\n+    public static ClusterRoleBinding clusterRoleBinding(ExtensionContext extensionContext, String yamlPath, String namespace) {\n+        LOGGER.info(\"Creating ClusterRoleBinding from {} in namespace {}\", yamlPath, namespace);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTcxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805714", "bodyText": "Maybe those methods shouldn't be called in templates like in KafkaConnectTemplate ? It can case unexpected failures even if user is trying to deploy KafkaCLients before connect in the same call for resource creation.", "author": "Frawless", "createdAt": "2021-03-13T21:49:26Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.HasMetadata;\n+import io.fabric8.kubernetes.api.model.LabelSelector;\n+import io.fabric8.kubernetes.api.model.LabelSelectorBuilder;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicy;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder;\n+import io.fabric8.kubernetes.client.CustomResource;\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaExporterResources;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.Spec;\n+import io.strimzi.api.kafka.model.status.Status;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.enums.DefaultNetworkPolicy;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import java.util.List;\n+\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public class NetworkPolicyResource implements ResourceType<NetworkPolicy> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(NetworkPolicyResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"NetworkPolicy\";\n+    }\n+    @Override\n+    public NetworkPolicy get(String namespace, String name) {\n+        return ResourceManager.kubeClient().namespace(namespace).getNetworkPolicy(name);\n+    }\n+    @Override\n+    public void create(NetworkPolicy resource) {\n+        LOGGER.info(\"Namespace.... \" + resource.getMetadata().getNamespace());\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).createNetworkPolicy(resource);\n+    }\n+    @Override\n+    public void delete(NetworkPolicy resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteNetworkPolicy(resource.getMetadata().getName());\n+    }\n+    @Override\n+    public boolean isReady(NetworkPolicy resource) {\n+        return resource != null;\n+    }\n+    @Override\n+    public void refreshResource(NetworkPolicy existing, NetworkPolicy newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+    }\n+\n+    public static NetworkPolicyBuilder networkPolicyBuilder(String name) {\n+        return networkPolicyBuilder(name, null)\n+            .withNewSpec()\n+                .withNewPodSelector()\n+                .endPodSelector()\n+                .withPolicyTypes(\"Ingress\")\n+            .endSpec();\n+    }\n+\n+    public static NetworkPolicyBuilder networkPolicyBuilder(String name, LabelSelector labelSelector) {\n+        return new NetworkPolicyBuilder()\n+            .withNewApiVersion(\"networking.k8s.io/v1\")\n+                .withNewKind(\"NetworkPolicy\")\n+                    .withNewMetadata()\n+                        .withName(name + \"-allow\")\n+                    .endMetadata()\n+                    .withNewSpec()\n+                        .addNewIngress()\n+                            .addNewFrom()\n+                                .withPodSelector(labelSelector)\n+                            .endFrom()\n+                        .endIngress()\n+                        .withPolicyTypes(\"Ingress\")\n+                    .endSpec();\n+    }\n+\n+    /**\n+     * Method for allowing network policies for ClusterOperator\n+     */\n+    public static void allowNetworkPolicySettingsForClusterOperator(ExtensionContext extensionContext) {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTk4Njk5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r595986990", "bodyText": "Sure, I agree but this is a huge change that will impact all the tests. I will create an issue for that", "author": "see-quick", "createdAt": "2021-03-17T12:53:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTc4MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805781", "bodyText": "Indent", "author": "Frawless", "createdAt": "2021-03-13T21:49:51Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.HasMetadata;\n+import io.fabric8.kubernetes.api.model.LabelSelector;\n+import io.fabric8.kubernetes.api.model.LabelSelectorBuilder;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicy;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder;\n+import io.fabric8.kubernetes.client.CustomResource;\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaExporterResources;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.Spec;\n+import io.strimzi.api.kafka.model.status.Status;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.enums.DefaultNetworkPolicy;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import java.util.List;\n+\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public class NetworkPolicyResource implements ResourceType<NetworkPolicy> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(NetworkPolicyResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"NetworkPolicy\";\n+    }\n+    @Override\n+    public NetworkPolicy get(String namespace, String name) {\n+        return ResourceManager.kubeClient().namespace(namespace).getNetworkPolicy(name);\n+    }\n+    @Override\n+    public void create(NetworkPolicy resource) {\n+        LOGGER.info(\"Namespace.... \" + resource.getMetadata().getNamespace());\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).createNetworkPolicy(resource);\n+    }\n+    @Override\n+    public void delete(NetworkPolicy resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteNetworkPolicy(resource.getMetadata().getName());\n+    }\n+    @Override\n+    public boolean isReady(NetworkPolicy resource) {\n+        return resource != null;\n+    }\n+    @Override\n+    public void refreshResource(NetworkPolicy existing, NetworkPolicy newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+    }\n+\n+    public static NetworkPolicyBuilder networkPolicyBuilder(String name) {\n+        return networkPolicyBuilder(name, null)\n+            .withNewSpec()\n+                .withNewPodSelector()\n+                .endPodSelector()\n+                .withPolicyTypes(\"Ingress\")\n+            .endSpec();\n+    }\n+\n+    public static NetworkPolicyBuilder networkPolicyBuilder(String name, LabelSelector labelSelector) {\n+        return new NetworkPolicyBuilder()\n+            .withNewApiVersion(\"networking.k8s.io/v1\")\n+                .withNewKind(\"NetworkPolicy\")\n+                    .withNewMetadata()\n+                        .withName(name + \"-allow\")\n+                    .endMetadata()\n+                    .withNewSpec()\n+                        .addNewIngress()\n+                            .addNewFrom()\n+                                .withPodSelector(labelSelector)\n+                            .endFrom()\n+                        .endIngress()\n+                        .withPolicyTypes(\"Ingress\")\n+                    .endSpec();\n+    }\n+\n+    /**\n+     * Method for allowing network policies for ClusterOperator\n+     */\n+    public static void allowNetworkPolicySettingsForClusterOperator(ExtensionContext extensionContext) {\n+        String clusterOperatorKind = \"cluster-operator\";\n+        LabelSelector labelSelector = new LabelSelectorBuilder()\n+            .addToMatchLabels(Constants.KAFKA_CLIENTS_LABEL_KEY, Constants.KAFKA_CLIENTS_LABEL_VALUE)\n+            .build();\n+\n+        LOGGER.info(\"Apply NetworkPolicy access to {} from pods with LabelSelector {}\", clusterOperatorKind, labelSelector);\n+\n+        NetworkPolicy networkPolicy = networkPolicyBuilder(clusterOperatorKind, labelSelector)\n+            .editSpec()\n+                .editFirstIngress()\n+                    .addNewPort()\n+                        .withNewPort(Constants.CLUSTER_OPERATOR_METRICS_PORT)\n+                        .withNewProtocol(\"TCP\")\n+                    .endPort()\n+                .endIngress()\n+                .withNewPodSelector()\n+                    .addToMatchLabels(\"strimzi.io/kind\", clusterOperatorKind)\n+                .endPodSelector()\n+            .endSpec()\n+            .build();\n+\n+        LOGGER.debug(\"Going to apply the following NetworkPolicy: {}\", networkPolicy.toString());\n+        ResourceManager.getInstance().createResource(extensionContext, networkPolicy);\n+        LOGGER.info(\"Network policy for LabelSelector {} successfully applied\", labelSelector);\n+    }\n+\n+    public static void allowNetworkPolicySettingsForEntityOperator(ExtensionContext extensionContext, String clusterName) {\n+        LabelSelector labelSelector = new LabelSelectorBuilder()\n+                .addToMatchLabels(Constants.KAFKA_CLIENTS_LABEL_KEY, Constants.KAFKA_CLIENTS_LABEL_VALUE)\n+                .build();\n+\n+        String eoDeploymentName = KafkaResources.entityOperatorDeploymentName(clusterName);\n+\n+        LOGGER.info(\"Apply NetworkPolicy access to {} from pods with LabelSelector {}\", eoDeploymentName, labelSelector);\n+\n+        NetworkPolicy networkPolicy = networkPolicyBuilder(eoDeploymentName, labelSelector)\n+            .editSpec()\n+                .editFirstIngress()\n+                    .addNewPort()\n+                        .withNewPort(Constants.TOPIC_OPERATOR_METRICS_PORT)\n+                        .withNewProtocol(\"TCP\")\n+                    .endPort()\n+                    .addNewPort()\n+                        .withNewPort(Constants.USER_OPERATOR_METRICS_PORT)\n+                        .withNewProtocol(\"TCP\")\n+                    .endPort()\n+                .endIngress()\n+                .withNewPodSelector()\n+                    .addToMatchLabels(\"strimzi.io/cluster\", clusterName)\n+                    .addToMatchLabels(\"strimzi.io/kind\", Kafka.RESOURCE_KIND)\n+                    .addToMatchLabels(\"strimzi.io/name\", eoDeploymentName)\n+                .endPodSelector()\n+            .endSpec()\n+            .build();\n+\n+        LOGGER.debug(\"Going to apply the following NetworkPolicy: {}\", networkPolicy.toString());\n+        ResourceManager.getInstance().createResource(extensionContext, networkPolicy);\n+        LOGGER.info(\"Network policy for LabelSelector {} successfully applied\", labelSelector);\n+    }\n+\n+    public static void allowNetworkPolicySettingsForKafkaExporter(ExtensionContext extensionContext, String clusterName) {\n+        String kafkaExporterDeploymentName = KafkaExporterResources.deploymentName(clusterName);\n+        LabelSelector labelSelector = new LabelSelectorBuilder()\n+                .addToMatchLabels(Constants.KAFKA_CLIENTS_LABEL_KEY, Constants.KAFKA_CLIENTS_LABEL_VALUE)\n+                .build();\n+\n+        LOGGER.info(\"Apply NetworkPolicy access to {} from pods with LabelSelector {}\", kafkaExporterDeploymentName, labelSelector);\n+\n+        NetworkPolicy networkPolicy = networkPolicyBuilder(kafkaExporterDeploymentName, labelSelector)\n+            .editSpec()\n+                .editFirstIngress()\n+                    .addNewPort()\n+                        .withNewPort(Constants.COMPONENTS_METRICS_PORT)\n+                        .withNewProtocol(\"TCP\")\n+                    .endPort()\n+                .endIngress()\n+                .withNewPodSelector()\n+                    .addToMatchLabels(\"strimzi.io/cluster\", clusterName)\n+                    .addToMatchLabels(\"strimzi.io/kind\", Kafka.RESOURCE_KIND)\n+                    .addToMatchLabels(\"strimzi.io/name\", kafkaExporterDeploymentName)\n+                .endPodSelector()\n+            .endSpec()\n+            .build();\n+\n+        LOGGER.debug(\"Going to apply the following NetworkPolicy: {}\", networkPolicy.toString());\n+        ResourceManager.getInstance().createResource(extensionContext, networkPolicy);\n+        LOGGER.info(\"Network policy for LabelSelector {} successfully applied\", labelSelector);\n+    }\n+\n+    /**\n+     * Method for allowing network policies for Connect or ConnectS2I\n+     * @param resource mean Connect or ConnectS2I resource\n+     * @param deploymentName name of resource deployment - for setting strimzi.io/name\n+     */\n+    public static void allowNetworkPolicySettingsForResource(ExtensionContext extensionContext, HasMetadata resource, String deploymentName) {\n+        LabelSelector labelSelector = new LabelSelectorBuilder()\n+            .addToMatchLabels(Constants.KAFKA_CLIENTS_LABEL_KEY, Constants.KAFKA_CLIENTS_LABEL_VALUE)\n+            .build();\n+\n+        if (kubeClient().listPods(labelSelector).size() == 0) {\n+            throw new RuntimeException(\"You did not create the Kafka Client instance(pod) before using the \" + resource.getKind());\n+        }\n+\n+        LOGGER.info(\"Apply NetworkPolicy access to {} from pods with LabelSelector {}\", deploymentName, labelSelector);\n+\n+        NetworkPolicy networkPolicy = new NetworkPolicyBuilder()\n+            .withNewApiVersion(\"networking.k8s.io/v1\")\n+            .withNewKind(\"NetworkPolicy\")\n+            .withNewMetadata()\n+                .withName(resource.getMetadata().getName() + \"-allow\")\n+                .withNamespace(resource.getMetadata().getNamespace())\n+            .endMetadata()\n+            .withNewSpec()\n+                .addNewIngress()\n+                    .addNewFrom()\n+                        .withPodSelector(labelSelector)\n+                    .endFrom()\n+                    .addNewPort()\n+                        .withNewPort(8083)\n+                        .withNewProtocol(\"TCP\")\n+                    .endPort()\n+                    .addNewPort()\n+                        .withNewPort(9404)\n+                        .withNewProtocol(\"TCP\")\n+                    .endPort()\n+                    .addNewPort()\n+                        .withNewPort(8080)\n+                        .withNewProtocol(\"TCP\")\n+                    .endPort()\n+                .endIngress()\n+                .withNewPodSelector()\n+                    .addToMatchLabels(\"strimzi.io/cluster\", resource.getMetadata().getName())\n+                    .addToMatchLabels(\"strimzi.io/kind\", resource.getKind())\n+                    .addToMatchLabels(\"strimzi.io/name\", deploymentName)\n+                .endPodSelector()\n+                .withPolicyTypes(\"Ingress\")\n+            .endSpec()\n+            .build();\n+\n+        LOGGER.debug(\"Going to apply the following NetworkPolicy: {}\", networkPolicy.toString());\n+        ResourceManager.getInstance().createResource(extensionContext, networkPolicy);\n+        LOGGER.info(\"Network policy for LabelSelector {} successfully applied\", labelSelector);\n+    }\n+\n+    public static void applyDefaultNetworkPolicySettings(ExtensionContext extensionContext, List<String> namespaces) {\n+        for (String namespace : namespaces) {\n+            if (Environment.DEFAULT_TO_DENY_NETWORK_POLICIES) {\n+                applyDefaultNetworkPolicy(extensionContext, namespace, DefaultNetworkPolicy.DEFAULT_TO_DENY);\n+            } else {\n+                applyDefaultNetworkPolicy(extensionContext, namespace, DefaultNetworkPolicy.DEFAULT_TO_ALLOW);\n+            }\n+            LOGGER.info(\"NetworkPolicy successfully set to: {} for namespace: {}\", Environment.DEFAULT_TO_DENY_NETWORK_POLICIES, namespace);\n+        }\n+    }\n+\n+    public static <T extends CustomResource<? extends Spec, ? extends Status>> void deployNetworkPolicyForResource(ExtensionContext extensionContext, T resource, String deploymentName) {\n+        if (Environment.DEFAULT_TO_DENY_NETWORK_POLICIES) {\n+            allowNetworkPolicySettingsForResource(extensionContext, resource, deploymentName);\n+        }\n+    }\n+\n+    public static NetworkPolicy applyDefaultNetworkPolicy(ExtensionContext extensionContext, String namespace, DefaultNetworkPolicy policy) {\n+        NetworkPolicy networkPolicy = new NetworkPolicyBuilder()\n+            .withNewApiVersion(\"networking.k8s.io/v1\")\n+            .withNewKind(\"NetworkPolicy\")\n+            .withNewMetadata()\n+            .withName(\"global-network-policy\")\n+            .withNamespace(namespace)\n+            .endMetadata()\n+            .withNewSpec()\n+            .withNewPodSelector()\n+            .endPodSelector()\n+            .withPolicyTypes(\"Ingress\")\n+            .endSpec()\n+            .build();\n+\n+        if (policy.equals(DefaultNetworkPolicy.DEFAULT_TO_ALLOW)) {\n+            networkPolicy = new NetworkPolicyBuilder(networkPolicy)\n+                .editSpec()\n+                .addNewIngress()\n+                .endIngress()\n+                .endSpec()\n+                .build();\n+        }", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTg3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805873", "bodyText": "typo", "author": "Frawless", "createdAt": "2021-03-13T21:50:41Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/operator/BundleResource.java", "diffHunk": "@@ -4,39 +4,92 @@\n  */\n package io.strimzi.systemtest.resources.operator;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n import io.fabric8.kubernetes.api.model.EnvVar;\n import io.fabric8.kubernetes.api.model.apps.Deployment;\n import io.fabric8.kubernetes.api.model.apps.DeploymentBuilder;\n import io.strimzi.systemtest.Constants;\n import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n+import io.strimzi.systemtest.enums.DeploymentTypes;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.resources.kubernetes.DeploymentResource;\n import io.strimzi.systemtest.utils.StUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.test.TestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n \n import java.util.List;\n \n-public class BundleResource {\n+public class BundleResource implements ResourceType<Deployment> {\n+    private static final Logger LOGGER = LogManager.getLogger(BundleResource.class);\n+\n     public static final String PATH_TO_CO_CONFIG = TestUtils.USER_PATH + \"/../packaging/install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml\";\n \n+    @Override\n+    public String getKind() {\n+        return \"Deployment\";\n+    }\n+    @Override\n+    public Deployment get(String namespace, String name) {\n+        String deploymentName = ResourceManager.kubeClient().namespace(namespace).getDeploymentNameByPrefix(name);\n+        return deploymentName != null ? ResourceManager.kubeClient().getDeployment(deploymentName) : null;\n+    }\n+    @Override\n+    public void create(Deployment resource) {\n+        ResourceManager.kubeClient().createOrReplaceDeployment(resource);\n+    }\n+    @Override\n+    public void delete(Deployment resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteDeployment(resource.getMetadata().getName());\n+    }\n+\n+    @Override\n+    @SuppressFBWarnings(value = \"RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE\")\n+    public boolean isReady(Deployment resource) {\n+        LOGGER.info(\"==========================================================\");\n+        LOGGER.info(\"Resource -_>\" + resource);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTkzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593805938", "bodyText": "For OLM the deployment name of Strimzi is different. You should check if it's working even wth your change.", "author": "Frawless", "createdAt": "2021-03-13T21:51:40Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/operator/BundleResource.java", "diffHunk": "@@ -4,39 +4,92 @@\n  */\n package io.strimzi.systemtest.resources.operator;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n import io.fabric8.kubernetes.api.model.EnvVar;\n import io.fabric8.kubernetes.api.model.apps.Deployment;\n import io.fabric8.kubernetes.api.model.apps.DeploymentBuilder;\n import io.strimzi.systemtest.Constants;\n import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n+import io.strimzi.systemtest.enums.DeploymentTypes;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.resources.kubernetes.DeploymentResource;\n import io.strimzi.systemtest.utils.StUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.test.TestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n \n import java.util.List;\n \n-public class BundleResource {\n+public class BundleResource implements ResourceType<Deployment> {\n+    private static final Logger LOGGER = LogManager.getLogger(BundleResource.class);\n+\n     public static final String PATH_TO_CO_CONFIG = TestUtils.USER_PATH + \"/../packaging/install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml\";\n \n+    @Override\n+    public String getKind() {\n+        return \"Deployment\";\n+    }\n+    @Override\n+    public Deployment get(String namespace, String name) {\n+        String deploymentName = ResourceManager.kubeClient().namespace(namespace).getDeploymentNameByPrefix(name);\n+        return deploymentName != null ? ResourceManager.kubeClient().getDeployment(deploymentName) : null;\n+    }\n+    @Override\n+    public void create(Deployment resource) {\n+        ResourceManager.kubeClient().createOrReplaceDeployment(resource);\n+    }\n+    @Override\n+    public void delete(Deployment resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteDeployment(resource.getMetadata().getName());\n+    }\n+\n+    @Override\n+    @SuppressFBWarnings(value = \"RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE\")\n+    public boolean isReady(Deployment resource) {\n+        LOGGER.info(\"==========================================================\");\n+        LOGGER.info(\"Resource -_>\" + resource);\n+\n+        return resource != null\n+            && resource.getMetadata() != null\n+            && resource.getMetadata().getName() != null\n+            && resource.getStatus() != null\n+            && DeploymentUtils.waitForDeploymentAndPodsReady(resource.getMetadata().getName(), resource.getSpec().getReplicas());\n+    }\n+    @Override\n+    public void refreshResource(Deployment existing, Deployment newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+        existing.setStatus(newResource.getStatus());\n+    }\n     public static DeploymentBuilder clusterOperator(String namespace, long operationTimeout) {\n-        return defaultClusterOperator(namespace, operationTimeout, Constants.RECONCILIATION_INTERVAL);\n+        return defaultClusterOperator(Constants.STRIMZI_DEPLOYMENT_NAME, namespace, namespace, operationTimeout, Constants.RECONCILIATION_INTERVAL);\n+    }\n+\n+    public static DeploymentBuilder clusterOperator(String namespace, String namespaceEnv, long reconciliationInterval) {\n+        return defaultClusterOperator(Constants.STRIMZI_DEPLOYMENT_NAME, namespace, namespaceEnv, Constants.CO_OPERATION_TIMEOUT_DEFAULT, reconciliationInterval);\n     }\n \n     public static DeploymentBuilder clusterOperator(String namespace, long operationTimeout, long reconciliationInterval) {\n-        return defaultClusterOperator(namespace, operationTimeout, reconciliationInterval);\n+        return defaultClusterOperator(Constants.STRIMZI_DEPLOYMENT_NAME, namespace, namespace, operationTimeout, reconciliationInterval);\n+    }\n+\n+    public static DeploymentBuilder clusterOperator(String name, String namespace, long operationTimeout, long reconciliationInterval) {\n+        return defaultClusterOperator(name, namespace, namespace, operationTimeout, reconciliationInterval);\n     }\n \n     public static DeploymentBuilder clusterOperator(String namespace) {\n-        return defaultClusterOperator(namespace, Constants.CO_OPERATION_TIMEOUT_DEFAULT, Constants.RECONCILIATION_INTERVAL);\n+        return defaultClusterOperator(Constants.STRIMZI_DEPLOYMENT_NAME, namespace, namespace, Constants.CO_OPERATION_TIMEOUT_DEFAULT, Constants.RECONCILIATION_INTERVAL);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTk5MjcxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r595992715", "bodyText": "Yes, I see but this change only corresponded to the Bundle format, not the OLM.", "author": "see-quick", "createdAt": "2021-03-17T13:01:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNTkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNjM5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593806390", "bodyText": "Why it returns deployment? CO is deployed via helm client in that case.", "author": "Frawless", "createdAt": "2021-03-13T21:56:48Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/operator/HelmResource.java", "diffHunk": "@@ -35,15 +40,48 @@\n     public static final String LIMITS_MEMORY = \"512Mi\";\n     public static final String LIMITS_CPU = \"1000m\";\n \n-    public static void clusterOperator() {\n-        clusterOperator(Constants.CO_OPERATION_TIMEOUT_DEFAULT);\n+    @Override\n+    public String getKind() {\n+        return \"Deployment\";\n+    }\n+    @Override\n+    public Deployment get(String namespace, String name) {\n+        String deploymentName = ResourceManager.kubeClient().namespace(namespace).getDeploymentNameByPrefix(name);\n+        return deploymentName != null ?  ResourceManager.kubeClient().getDeployment(deploymentName) : null;\n+    }\n+    @Override\n+    public void create(Deployment resource) {\n+        ResourceManager.kubeClient().createOrReplaceDeployment(resource);\n+    }\n+    @Override\n+    public void delete(Deployment resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteDeployment(resource.getMetadata().getName());\n+    }\n+    @Override\n+    public boolean isReady(Deployment resource) {\n+        return resource != null\n+            && resource.getMetadata() != null\n+            && resource.getMetadata().getName() != null\n+            && resource.getStatus() != null\n+            && DeploymentUtils.waitForDeploymentAndPodsReady(resource.getMetadata().getName(), resource.getSpec().getReplicas());\n+    }\n+\n+    @Override\n+    public void refreshResource(Deployment existing, Deployment newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+        existing.setStatus(newResource.getStatus());\n+    }\n+\n+    public static Deployment clusterOperator() {\n+        return clusterOperator(Constants.CO_OPERATION_TIMEOUT_DEFAULT);\n     }\n \n-    public static void clusterOperator(long operationTimeout) {\n-        clusterOperator(operationTimeout, Constants.RECONCILIATION_INTERVAL);\n+    public static Deployment clusterOperator(long operationTimeout) {\n+        return clusterOperator(operationTimeout, Constants.RECONCILIATION_INTERVAL);\n     }\n \n-    public static void clusterOperator(long operationTimeout, long reconciliationInterval) {\n+    public static Deployment clusterOperator(long operationTimeout, long reconciliationInterval) {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNjgwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593806809", "bodyText": "OlmResource is actually a subscription. It would be nice to explain why it implements Deployment type", "author": "Frawless", "createdAt": "2021-03-13T22:00:28Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/operator/OlmResource.java", "diffHunk": "@@ -33,31 +37,63 @@\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n \n-public class OlmResource {\n+public class OlmResource implements ResourceType<Deployment> {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNjg1NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593806855", "bodyText": "For proper deletion, you should delete specific subscription.", "author": "Frawless", "createdAt": "2021-03-13T22:00:51Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/operator/OlmResource.java", "diffHunk": "@@ -89,12 +125,19 @@ public static void clusterOperator(String namespace, long operationTimeout, long\n         String deploymentName = ResourceManager.kubeClient().getDeploymentNameByPrefix(Environment.OLM_OPERATOR_DEPLOYMENT_NAME);\n         ResourceManager.setCoDeploymentName(deploymentName);\n \n-\n-        ResourceManager.getPointerResources().push(() -> deleteOlm(deploymentName, namespace, csvName));\n+        // TODO: how??", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwMjY1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593902659", "bodyText": "Yes, I will do it.", "author": "see-quick", "createdAt": "2021-03-14T13:40:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNjg1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNjkxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593806915", "bodyText": "Maybe we could have constant for /../packaging/examples for all template classes.", "author": "Frawless", "createdAt": "2021-03-13T22:01:55Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.templates.crd;\n+\n+import io.fabric8.kubernetes.client.KubernetesClientException;\n+import io.fabric8.kubernetes.client.dsl.MixedOperation;\n+import io.fabric8.kubernetes.client.dsl.Resource;\n+import io.strimzi.api.kafka.Crds;\n+import io.strimzi.api.kafka.KafkaBridgeList;\n+import io.strimzi.api.kafka.model.KafkaBridge;\n+import io.strimzi.api.kafka.model.KafkaBridgeBuilder;\n+import io.strimzi.api.kafka.model.KafkaBridgeResources;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.kubernetes.NetworkPolicyResource;\n+import io.strimzi.test.TestUtils;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n+\n+public class KafkaBridgeTemplates {\n+\n+    public static final String PATH_TO_KAFKA_BRIDGE_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/bridge/kafka-bridge.yaml\";", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNjk3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593806978", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T22:02:49Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.templates.crd;\n+\n+import io.fabric8.kubernetes.client.KubernetesClientException;\n+import io.fabric8.kubernetes.client.dsl.MixedOperation;\n+import io.fabric8.kubernetes.client.dsl.Resource;\n+import io.strimzi.api.kafka.Crds;\n+import io.strimzi.api.kafka.KafkaBridgeList;\n+import io.strimzi.api.kafka.model.KafkaBridge;\n+import io.strimzi.api.kafka.model.KafkaBridgeBuilder;\n+import io.strimzi.api.kafka.model.KafkaBridgeResources;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.kubernetes.NetworkPolicyResource;\n+import io.strimzi.test.TestUtils;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n+\n+public class KafkaBridgeTemplates {\n+\n+    public static final String PATH_TO_KAFKA_BRIDGE_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/bridge/kafka-bridge.yaml\";\n+\n+    private KafkaBridgeTemplates() {}\n+\n+    public static MixedOperation<KafkaBridge, KafkaBridgeList, Resource<KafkaBridge>> kafkaBridgeClient() {\n+        return Crds.kafkaBridgeOperation(ResourceManager.kubeClient().getClient());\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridge(String name, String bootstrap, int kafkaBridgeReplicas) {\n+        return kafkaBridge(name, name, bootstrap, kafkaBridgeReplicas);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridge(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n+        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n+        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String bootstrap, int kafkaBridgeReplicas,\n+                                                          String allowedCorsOrigin, String allowedCorsMethods) {\n+        return kafkaBridgeWithCors(name, name, bootstrap, kafkaBridgeReplicas, allowedCorsOrigin, allowedCorsMethods);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String clusterName, String bootstrap,\n+                                                  int kafkaBridgeReplicas, String allowedCorsOrigin,\n+                                                  String allowedCorsMethods) {\n+        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n+\n+        KafkaBridgeBuilder kafkaBridgeBuilder = defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+\n+        kafkaBridgeBuilder\n+            .editSpec()\n+                .editHttp()\n+                    .withNewCors()\n+                        .withAllowedOrigins(allowedCorsOrigin)\n+                        .withAllowedMethods(allowedCorsMethods != null ? allowedCorsMethods : \"GET,POST,PUT,DELETE,OPTIONS,PATCH\")\n+                    .endCors()\n+                .endHttp()\n+            .endSpec();\n+\n+        return kafkaBridgeBuilder;\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap) throws Exception {\n+        return kafkaBridgeWithMetrics(name, clusterName, bootstrap, 1);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) throws Exception {\n+        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n+\n+        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas)\n+            .editSpec()\n+                .withEnableMetrics(true)\n+            .endSpec();\n+    }\n+\n+    private static KafkaBridgeBuilder defaultKafkaBridge(KafkaBridge kafkaBridge, String name, String kafkaClusterName, String bootstrap, int kafkaBridgeReplicas) {\n+        return new KafkaBridgeBuilder(kafkaBridge)\n+            .withNewMetadata()\n+                .withName(name)\n+                .withNamespace(ResourceManager.kubeClient().getNamespace())\n+                .withClusterName(kafkaClusterName)\n+            .endMetadata()\n+            .editSpec()\n+                .withBootstrapServers(bootstrap)\n+                .withReplicas(kafkaBridgeReplicas)\n+                .withNewInlineLogging()\n+                    .addToLoggers(\"bridge.root.logger\", \"DEBUG\")\n+                .endInlineLogging()\n+            .endSpec();\n+    }\n+\n+    public static KafkaBridge createAndWaitForReadiness(ExtensionContext extensionContext, KafkaBridge kafkaBridge) {\n+        NetworkPolicyResource.deployNetworkPolicyForResource(extensionContext, kafkaBridge, KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n+\n+        TestUtils.waitFor(\"KafkaBridge creation\", Constants.POLL_INTERVAL_FOR_RESOURCE_CREATION, CR_CREATION_TIMEOUT,\n+            () -> {\n+                try {\n+                    kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaBridge);\n+                    return true;\n+                } catch (KubernetesClientException e) {\n+                    if (e.getMessage().contains(\"object is being deleted\")) {\n+                        return false;\n+                    } else {\n+                        throw e;\n+                    }\n+                }\n+            }\n+        );\n+        return kafkaBridge;\n+    }\n+\n+//    private static DoneableKafkaBridge deployKafkaBridge(KafkaBridge kafkaBridge) {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwNjk4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593806987", "bodyText": "Isn't this implemented in RM?", "author": "Frawless", "createdAt": "2021-03-13T22:03:03Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.templates.crd;\n+\n+import io.fabric8.kubernetes.client.KubernetesClientException;\n+import io.fabric8.kubernetes.client.dsl.MixedOperation;\n+import io.fabric8.kubernetes.client.dsl.Resource;\n+import io.strimzi.api.kafka.Crds;\n+import io.strimzi.api.kafka.KafkaBridgeList;\n+import io.strimzi.api.kafka.model.KafkaBridge;\n+import io.strimzi.api.kafka.model.KafkaBridgeBuilder;\n+import io.strimzi.api.kafka.model.KafkaBridgeResources;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.kubernetes.NetworkPolicyResource;\n+import io.strimzi.test.TestUtils;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n+\n+public class KafkaBridgeTemplates {\n+\n+    public static final String PATH_TO_KAFKA_BRIDGE_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/bridge/kafka-bridge.yaml\";\n+\n+    private KafkaBridgeTemplates() {}\n+\n+    public static MixedOperation<KafkaBridge, KafkaBridgeList, Resource<KafkaBridge>> kafkaBridgeClient() {\n+        return Crds.kafkaBridgeOperation(ResourceManager.kubeClient().getClient());\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridge(String name, String bootstrap, int kafkaBridgeReplicas) {\n+        return kafkaBridge(name, name, bootstrap, kafkaBridgeReplicas);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridge(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n+        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n+        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String bootstrap, int kafkaBridgeReplicas,\n+                                                          String allowedCorsOrigin, String allowedCorsMethods) {\n+        return kafkaBridgeWithCors(name, name, bootstrap, kafkaBridgeReplicas, allowedCorsOrigin, allowedCorsMethods);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String clusterName, String bootstrap,\n+                                                  int kafkaBridgeReplicas, String allowedCorsOrigin,\n+                                                  String allowedCorsMethods) {\n+        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n+\n+        KafkaBridgeBuilder kafkaBridgeBuilder = defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+\n+        kafkaBridgeBuilder\n+            .editSpec()\n+                .editHttp()\n+                    .withNewCors()\n+                        .withAllowedOrigins(allowedCorsOrigin)\n+                        .withAllowedMethods(allowedCorsMethods != null ? allowedCorsMethods : \"GET,POST,PUT,DELETE,OPTIONS,PATCH\")\n+                    .endCors()\n+                .endHttp()\n+            .endSpec();\n+\n+        return kafkaBridgeBuilder;\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap) throws Exception {\n+        return kafkaBridgeWithMetrics(name, clusterName, bootstrap, 1);\n+    }\n+\n+    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) throws Exception {\n+        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n+\n+        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas)\n+            .editSpec()\n+                .withEnableMetrics(true)\n+            .endSpec();\n+    }\n+\n+    private static KafkaBridgeBuilder defaultKafkaBridge(KafkaBridge kafkaBridge, String name, String kafkaClusterName, String bootstrap, int kafkaBridgeReplicas) {\n+        return new KafkaBridgeBuilder(kafkaBridge)\n+            .withNewMetadata()\n+                .withName(name)\n+                .withNamespace(ResourceManager.kubeClient().getNamespace())\n+                .withClusterName(kafkaClusterName)\n+            .endMetadata()\n+            .editSpec()\n+                .withBootstrapServers(bootstrap)\n+                .withReplicas(kafkaBridgeReplicas)\n+                .withNewInlineLogging()\n+                    .addToLoggers(\"bridge.root.logger\", \"DEBUG\")\n+                .endInlineLogging()\n+            .endSpec();\n+    }\n+\n+    public static KafkaBridge createAndWaitForReadiness(ExtensionContext extensionContext, KafkaBridge kafkaBridge) {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwODY5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593808697", "bodyText": "deployment-type could be constant as well", "author": "Frawless", "createdAt": "2021-03-13T22:21:18Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.templates.crd;\n+\n+import io.fabric8.kubernetes.api.model.ContainerBuilder;\n+import io.fabric8.kubernetes.api.model.PodSpec;\n+import io.fabric8.kubernetes.api.model.PodSpecBuilder;\n+import io.fabric8.kubernetes.api.model.Quantity;\n+import io.fabric8.kubernetes.api.model.ResourceRequirementsBuilder;\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.DeploymentBuilder;\n+import io.strimzi.api.kafka.model.KafkaUser;\n+import io.strimzi.api.kafka.model.KafkaUserScramSha512ClientAuthentication;\n+import io.strimzi.api.kafka.model.KafkaUserTlsClientAuthentication;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.enums.DeploymentTypes;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaClientsResource;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.nio.charset.Charset;\n+import java.util.Base64;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.test.TestUtils.toYamlString;\n+\n+public class KafkaClientsTemplates {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(KafkaClientsResource.class);\n+\n+    private KafkaClientsTemplates() { }\n+\n+    public static DeploymentBuilder kafkaClients(String kafkaClusterName) {\n+        return kafkaClients(false, kafkaClusterName, null);\n+    }\n+\n+    public static DeploymentBuilder kafkaClients(boolean tlsListener, String kafkaClientsName, KafkaUser... kafkaUsers) {\n+        return kafkaClients(tlsListener, kafkaClientsName, true,  null, null, kafkaUsers);\n+    }\n+\n+    public static DeploymentBuilder kafkaClients(boolean tlsListener, String kafkaClientsName, String listenerName,\n+                                          KafkaUser... kafkaUsers) {\n+        return kafkaClients(tlsListener, kafkaClientsName, true, listenerName, null, kafkaUsers);\n+    }\n+\n+    public static DeploymentBuilder kafkaClients(boolean tlsListener, String kafkaClientsName, boolean hostnameVerification,\n+                                          KafkaUser... kafkaUsers) {\n+        return kafkaClients(tlsListener, kafkaClientsName, hostnameVerification, null, null, kafkaUsers);\n+    }\n+\n+    public static DeploymentBuilder kafkaClients(boolean tlsListener, String kafkaClientsName, boolean hostnameVerification,\n+                                          String listenerName, String secretPrefix, KafkaUser... kafkaUsers) {\n+        Map<String, String> label = new HashMap<>();\n+\n+        label.put(Constants.KAFKA_CLIENTS_LABEL_KEY, Constants.KAFKA_CLIENTS_LABEL_VALUE);\n+        label.put(\"deployment-type\", DeploymentTypes.KafkaClients.name());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwOTQ4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593809486", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static KafkaUserBuilder userWithQuota(KafkaUser user, Integer prodRate, Integer consRate, Integer requestPerc) {\n          \n          \n            \n                public static KafkaUserBuilder userWithQuotas(KafkaUser user, Integer prodRate, Integer consRate, Integer requestPerc) {", "author": "Frawless", "createdAt": "2021-03-13T22:29:32Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.templates.crd;\n+\n+import io.fabric8.kubernetes.client.dsl.MixedOperation;\n+import io.fabric8.kubernetes.client.dsl.Resource;\n+import io.strimzi.api.kafka.Crds;\n+import io.strimzi.api.kafka.KafkaList;\n+import io.strimzi.api.kafka.KafkaUserList;\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaUser;\n+import io.strimzi.api.kafka.model.KafkaUserBuilder;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+\n+public class KafkaUserTemplates {\n+\n+    private KafkaUserTemplates() {}\n+\n+    public static MixedOperation<KafkaUser, KafkaUserList, Resource<KafkaUser>> kafkaUserClient() {\n+        return Crds.kafkaUserOperation(ResourceManager.kubeClient().getClient());\n+    }\n+\n+    public static MixedOperation<Kafka, KafkaList, Resource<Kafka>> kafkaClient() {\n+        return Crds.kafkaOperation(ResourceManager.kubeClient().getClient());\n+    }\n+\n+    public static KafkaUserBuilder tlsUser(String clusterName, String name) {\n+        return defaultUser(clusterName, name)\n+            .withNewSpec()\n+                .withNewKafkaUserTlsClientAuthentication()\n+                .endKafkaUserTlsClientAuthentication()\n+            .endSpec();\n+    }\n+\n+    public static KafkaUserBuilder scramShaUser(String clusterName, String name) {\n+        return defaultUser(clusterName, name)\n+            .withNewSpec()\n+                .withNewKafkaUserScramSha512ClientAuthentication()\n+                .endKafkaUserScramSha512ClientAuthentication()\n+            .endSpec();\n+    }\n+\n+    public static KafkaUserBuilder defaultUser(String clusterName, String name) {\n+        return new KafkaUserBuilder()\n+            .withNewMetadata()\n+                .withClusterName(clusterName)\n+                .withName(name)\n+                .withNamespace(ResourceManager.kubeClient().getNamespace())\n+                .addToLabels(Labels.STRIMZI_CLUSTER_LABEL, clusterName)\n+            .endMetadata();\n+    }\n+\n+    public static KafkaUser kafkaUserWithoutWait(KafkaUser user) {\n+        kafkaUserClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(user);\n+        return user;\n+    }\n+\n+    public static KafkaUserBuilder userWithQuota(KafkaUser user, Integer prodRate, Integer consRate, Integer requestPerc) {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwOTU5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593809598", "bodyText": "I think Michal opened PR which changes this little bit, but overall, we could log complete status of the job in some reasonable format.", "author": "Frawless", "createdAt": "2021-03-13T22:30:38Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java", "diffHunk": "@@ -57,7 +63,10 @@ public static void waitTillContinuousClientsFinish(String producerName, String c\n     public static void waitForClientSuccess(String jobName, String namespace, int messageCount) {\n         LOGGER.info(\"Waiting for producer/consumer:{} to finished\", jobName);\n         TestUtils.waitFor(\"job finished\", Constants.GLOBAL_POLL_INTERVAL, timeoutForClientFinishJob(messageCount),\n-            () -> kubeClient().namespace(namespace).getJobStatus(jobName));\n+            () -> {\n+                LOGGER.info(\"Job {} in namespace {}, has status {}\", jobName, namespace, kubeClient().namespace(namespace).getJobStatus(jobName));\n+                return kubeClient().namespace(namespace).getJobStatus(jobName);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwOTY0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593809646", "bodyText": "Better log message?", "author": "Frawless", "createdAt": "2021-03-13T22:31:50Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java", "diffHunk": "@@ -79,6 +88,46 @@ private static long timeoutForClientFinishJob(int messagesCount) {\n         return (long) messagesCount * 1000 + Duration.ofMinutes(2).toMillis();\n     }\n \n+    public static Deployment waitUntilClientsArePresent(Deployment resource) {\n+        Deployment[] deployment = new Deployment[1];\n+        deployment[0] = resource;\n+\n+        TestUtils.waitFor(\" for resource: \" + resource + \" to be present\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT, () -> {\n+            deployment[0] = ResourceManager.kubeClient().getDeployment(ResourceManager.kubeClient().getDeploymentBySubstring(resource.getMetadata().getName()));\n+            LOGGER.info(\"Resource is present: {}\", deployment[0] != null);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwOTczNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593809734", "bodyText": "I don't think we need to rerun the client at this point. Guess we should remove the waitFor after verification of mm2 tests without it.", "author": "Frawless", "createdAt": "2021-03-13T22:32:50Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java", "diffHunk": "@@ -79,6 +88,46 @@ private static long timeoutForClientFinishJob(int messagesCount) {\n         return (long) messagesCount * 1000 + Duration.ofMinutes(2).toMillis();\n     }\n \n+    public static Deployment waitUntilClientsArePresent(Deployment resource) {\n+        Deployment[] deployment = new Deployment[1];\n+        deployment[0] = resource;\n+\n+        TestUtils.waitFor(\" for resource: \" + resource + \" to be present\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT, () -> {\n+            deployment[0] = ResourceManager.kubeClient().getDeployment(ResourceManager.kubeClient().getDeploymentBySubstring(resource.getMetadata().getName()));\n+            LOGGER.info(\"Resource is present: {}\", deployment[0] != null);\n+            return deployment[0] != null;\n+        });\n+\n+        return deployment[0];\n+    }\n+\n+    public static void waitUntilProducerAndConsumerSuccessfullySendAndReceiveMessages(ExtensionContext extensionContext,", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjAxODM4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r596018388", "bodyText": "Removed fixed", "author": "see-quick", "createdAt": "2021-03-17T13:30:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwOTczNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwOTg2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593809861", "bodyText": "Better log message", "author": "Frawless", "createdAt": "2021-03-13T22:34:56Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -92,7 +102,9 @@ public static void waitUntilKafkaStatusConditionContainsMessage(String clusterNa\n             Constants.GLOBAL_POLL_INTERVAL, timeout, () -> {\n                 List<Condition> conditions = KafkaResource.kafkaClient().inNamespace(namespace).withName(clusterName).get().getStatus().getConditions();\n                 for (Condition condition : conditions) {\n-                    if (condition.getMessage().matches(message)) {\n+                    String conditionMessage = condition.getMessage();\n+                    LOGGER.debug(conditionMessage);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgwOTg3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593809876", "bodyText": "Why it's here? You cant reuse it from RM?", "author": "Frawless", "createdAt": "2021-03-13T22:35:08Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -59,17 +65,21 @@\n \n     private KafkaUtils() {}\n \n-    public static void waitForKafkaReady(String clusterName) {\n-        waitForKafkaStatus(clusterName, Ready);\n+    public static MixedOperation<Kafka, KafkaList, Resource<Kafka>> kafkaClient() {\n+        return Crds.kafkaOperation(ResourceManager.kubeClient().getClient());\n+    }", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDIyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810221", "bodyText": "Don't miss it", "author": "Frawless", "createdAt": "2021-03-13T22:38:12Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/specific/KeycloakUtils.java", "diffHunk": "@@ -29,7 +28,8 @@ private KeycloakUtils() {}\n \n     public static void deployKeycloak(String namespace) {\n         LOGGER.info(\"Prepare Keycloak in namespace: {}\", namespace);\n-        ResourceManager.getPointerResources().push(() -> deleteKeycloak(namespace));\n+        // TODO: how???", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwNjI3Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593906277", "bodyText": "Yes, fixed.", "author": "see-quick", "createdAt": "2021-03-14T14:07:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDIyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDQ1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810454", "bodyText": "Maybe remove test from names?", "author": "Frawless", "createdAt": "2021-03-13T22:40:40Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -77,13 +78,19 @@\n         Crds.registerCustomKinds();\n     }\n \n+    protected final ResourceManager resourceManager = ResourceManager.getInstance();\n     protected KubeClusterResource cluster;\n     protected static TimeMeasuringSystem timeMeasuringSystem = TimeMeasuringSystem.getInstance();\n     private static final Logger LOGGER = LogManager.getLogger(AbstractST.class);\n+    private final Object lock = new Object();\n+    private final Object lockForTimeMeasuringSystem = new Object();\n+\n+    // maps for local variables {thread safe}\n+    protected static Map<String, String> mapTestWithClusterNames = new HashMap<>();\n+    protected static Map<String, String> mapTestWithTestTopics = new HashMap<>();\n+    protected static Map<String, String> mapTestWithTestUsers = new HashMap<>();\n+    protected static Map<String, String> mapTestWithKafkaClientNames = new HashMap<>();", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDUyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810524", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T22:41:54Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +704,108 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+//        AssertionError assertionError = null;", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDU4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810589", "bodyText": "This is actually useful I think, but maybe it should be put into debug lvl?", "author": "Frawless", "createdAt": "2021-03-13T22:42:46Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +704,108 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+//        AssertionError assertionError = null;\n+//        // this is because more threads can access 'testDuration' variable and modify it...\n+//        synchronized (lockForTimeMeasuringSystem) {\n+//            try {\n+//                long testDuration = TimeMeasuringSystem.getInstance().getDurationInSeconds(\n+//                    testContext.getRequiredTestClass().getName(),\n+//                    testContext.getRequiredTestMethod().getName(),\n+//                    Operation.TEST_EXECUTION.name());\n+//                assertNoCoErrorsLogged(testDuration);\n+//            } catch (AssertionError e) {\n+//                LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n+//                assertionError = new AssertionError(e);\n+//            }\n+//        }\n+//\n+//        if (assertionError != null) {\n+//            throw assertionError;\n+//        }\n+        if (!Environment.SKIP_TEARDOWN) {\n+            ResourceManager.getInstance().deleteResources(testContext);\n+        }\n+    }\n \n-    @BeforeEach\n-    void createTestResources(ExtensionContext testContext) {\n-        if (testContext.getTestMethod().isPresent()) {\n-            testName = testContext.getTestMethod().get().getName();\n+    protected void afterAllMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            teardownEnvForOperator();\n+            ResourceManager.getInstance().deleteResources(testContext);\n         }\n-        ResourceManager.setMethodResources();\n+    }\n \n-        // This is needed to distinguish created Kafka cluster in ResourceManager and don't delete cluster which are still in use by parallel test cases\n-        if (previousClusterName == null) {\n-            LOGGER.info(\"Executing the first test case, using {} as a cluster name\", clusterName);\n-            previousClusterName = clusterName;\n-        } else {\n-            clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-            kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n-            LOGGER.info(\"Current test case is not the first one, generated new cluster name - {}\", clusterName);\n+    /**\n+     * BeforeEachMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeEachMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeEachMayOverride(ExtensionContext extensionContext) {\n+        // this is because we need to have different clusterName and kafkaClientsName in each test case without\n+        // synchronization it can produce `data-race`\n+        String testName = null;\n+\n+        synchronized (lock) {\n+            if (extensionContext.getTestMethod().isPresent()) {\n+                testName = extensionContext.getTestMethod().get().getName();\n+            }\n+\n+            LOGGER.info(\"Not first test we are gonna generate cluster name\");\n+            String clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n+\n+            mapTestWithClusterNames.put(testName, clusterName);\n+            mapTestWithTestTopics.put(testName, KafkaTopicUtils.generateRandomNameOfTopic());\n+            mapTestWithTestUsers.put(testName, KafkaUserUtils.generateRandomNameOfKafkaUser());\n+            mapTestWithKafkaClientNames.put(testName, clusterName + \"-\" + Constants.KAFKA_CLIENTS);\n+\n+            LOGGER.info(\"CLUSTER_NAMES_MAP: \\n{}\", mapTestWithClusterNames);\n+            LOGGER.info(\"USERS_NAME_MAP: \\n{}\", mapTestWithTestUsers);\n+            LOGGER.info(\"TOPIC_NAMES_MAP: \\n{}\", mapTestWithTestTopics);\n+            LOGGER.info(\"============THIS IS CLIENTS MAP:\\n{}\", mapTestWithKafkaClientNames);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDY1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810650", "bodyText": "I will not put the note into each test, but you should basically use this map instead of building the name as you do it here in tests.", "author": "Frawless", "createdAt": "2021-03-13T22:43:30Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +704,108 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+//        AssertionError assertionError = null;\n+//        // this is because more threads can access 'testDuration' variable and modify it...\n+//        synchronized (lockForTimeMeasuringSystem) {\n+//            try {\n+//                long testDuration = TimeMeasuringSystem.getInstance().getDurationInSeconds(\n+//                    testContext.getRequiredTestClass().getName(),\n+//                    testContext.getRequiredTestMethod().getName(),\n+//                    Operation.TEST_EXECUTION.name());\n+//                assertNoCoErrorsLogged(testDuration);\n+//            } catch (AssertionError e) {\n+//                LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n+//                assertionError = new AssertionError(e);\n+//            }\n+//        }\n+//\n+//        if (assertionError != null) {\n+//            throw assertionError;\n+//        }\n+        if (!Environment.SKIP_TEARDOWN) {\n+            ResourceManager.getInstance().deleteResources(testContext);\n+        }\n+    }\n \n-    @BeforeEach\n-    void createTestResources(ExtensionContext testContext) {\n-        if (testContext.getTestMethod().isPresent()) {\n-            testName = testContext.getTestMethod().get().getName();\n+    protected void afterAllMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            teardownEnvForOperator();\n+            ResourceManager.getInstance().deleteResources(testContext);\n         }\n-        ResourceManager.setMethodResources();\n+    }\n \n-        // This is needed to distinguish created Kafka cluster in ResourceManager and don't delete cluster which are still in use by parallel test cases\n-        if (previousClusterName == null) {\n-            LOGGER.info(\"Executing the first test case, using {} as a cluster name\", clusterName);\n-            previousClusterName = clusterName;\n-        } else {\n-            clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-            kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n-            LOGGER.info(\"Current test case is not the first one, generated new cluster name - {}\", clusterName);\n+    /**\n+     * BeforeEachMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeEachMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeEachMayOverride(ExtensionContext extensionContext) {\n+        // this is because we need to have different clusterName and kafkaClientsName in each test case without\n+        // synchronization it can produce `data-race`\n+        String testName = null;\n+\n+        synchronized (lock) {\n+            if (extensionContext.getTestMethod().isPresent()) {\n+                testName = extensionContext.getTestMethod().get().getName();\n+            }\n+\n+            LOGGER.info(\"Not first test we are gonna generate cluster name\");\n+            String clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n+\n+            mapTestWithClusterNames.put(testName, clusterName);\n+            mapTestWithTestTopics.put(testName, KafkaTopicUtils.generateRandomNameOfTopic());\n+            mapTestWithTestUsers.put(testName, KafkaUserUtils.generateRandomNameOfKafkaUser());\n+            mapTestWithKafkaClientNames.put(testName, clusterName + \"-\" + Constants.KAFKA_CLIENTS);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDcxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810714", "bodyText": "Is this only for debug purposes? I don't see any useful value.", "author": "Frawless", "createdAt": "2021-03-13T22:44:25Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +704,108 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+//        AssertionError assertionError = null;\n+//        // this is because more threads can access 'testDuration' variable and modify it...\n+//        synchronized (lockForTimeMeasuringSystem) {\n+//            try {\n+//                long testDuration = TimeMeasuringSystem.getInstance().getDurationInSeconds(\n+//                    testContext.getRequiredTestClass().getName(),\n+//                    testContext.getRequiredTestMethod().getName(),\n+//                    Operation.TEST_EXECUTION.name());\n+//                assertNoCoErrorsLogged(testDuration);\n+//            } catch (AssertionError e) {\n+//                LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n+//                assertionError = new AssertionError(e);\n+//            }\n+//        }\n+//\n+//        if (assertionError != null) {\n+//            throw assertionError;\n+//        }\n+        if (!Environment.SKIP_TEARDOWN) {\n+            ResourceManager.getInstance().deleteResources(testContext);\n+        }\n+    }\n \n-    @BeforeEach\n-    void createTestResources(ExtensionContext testContext) {\n-        if (testContext.getTestMethod().isPresent()) {\n-            testName = testContext.getTestMethod().get().getName();\n+    protected void afterAllMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            teardownEnvForOperator();\n+            ResourceManager.getInstance().deleteResources(testContext);\n         }\n-        ResourceManager.setMethodResources();\n+    }\n \n-        // This is needed to distinguish created Kafka cluster in ResourceManager and don't delete cluster which are still in use by parallel test cases\n-        if (previousClusterName == null) {\n-            LOGGER.info(\"Executing the first test case, using {} as a cluster name\", clusterName);\n-            previousClusterName = clusterName;\n-        } else {\n-            clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-            kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n-            LOGGER.info(\"Current test case is not the first one, generated new cluster name - {}\", clusterName);\n+    /**\n+     * BeforeEachMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeEachMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeEachMayOverride(ExtensionContext extensionContext) {\n+        // this is because we need to have different clusterName and kafkaClientsName in each test case without\n+        // synchronization it can produce `data-race`\n+        String testName = null;\n+\n+        synchronized (lock) {\n+            if (extensionContext.getTestMethod().isPresent()) {\n+                testName = extensionContext.getTestMethod().get().getName();\n+            }\n+\n+            LOGGER.info(\"Not first test we are gonna generate cluster name\");\n+            String clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n+\n+            mapTestWithClusterNames.put(testName, clusterName);\n+            mapTestWithTestTopics.put(testName, KafkaTopicUtils.generateRandomNameOfTopic());\n+            mapTestWithTestUsers.put(testName, KafkaUserUtils.generateRandomNameOfKafkaUser());\n+            mapTestWithKafkaClientNames.put(testName, clusterName + \"-\" + Constants.KAFKA_CLIENTS);\n+\n+            LOGGER.info(\"CLUSTER_NAMES_MAP: \\n{}\", mapTestWithClusterNames);\n+            LOGGER.info(\"USERS_NAME_MAP: \\n{}\", mapTestWithTestUsers);\n+            LOGGER.info(\"TOPIC_NAMES_MAP: \\n{}\", mapTestWithTestTopics);\n+            LOGGER.info(\"============THIS IS CLIENTS MAP:\\n{}\", mapTestWithKafkaClientNames);\n         }\n     }\n \n-    @BeforeAll\n-    void setTestClassName(ExtensionContext testContext) {\n+    /**\n+     * BeforeAllMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeAllMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeAllMayOverride(ExtensionContext extensionContext) {\n         cluster = KubeClusterResource.getInstance();\n-        if (testContext.getTestClass().isPresent()) {\n-            testClass = testContext.getTestClass().get().getName();\n+        String testClass = null;\n+\n+        if (extensionContext.getTestClass().isPresent()) {\n+            testClass = extensionContext.getTestClass().get().getName();\n         }\n-        // Name for the first test case, other test cases will need different name\n-        previousClusterName = null;\n-        clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-        kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n     }\n \n-    @AfterEach\n-    void teardownEnvironmentMethod(ExtensionContext testContext) throws Exception {\n-        TimeMeasuringSystem.getInstance().stopOperation(Operation.TEST_EXECUTION);\n-        AssertionError assertionError = null;\n-        try {\n-            long testDuration = timeMeasuringSystem.getDurationInSeconds(testContext.getRequiredTestClass().getName(), testContext.getRequiredTestMethod().getName(), Operation.TEST_EXECUTION.name());\n-            assertNoCoErrorsLogged(testDuration);\n-        } catch (AssertionError e) {\n-            LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n-            assertionError = new AssertionError(e);\n-        }\n+    @BeforeEach\n+    void setUpTestCase(ExtensionContext testContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE EACH] has been called\", this.getClass().getName());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDczNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810736", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T22:44:39Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +704,108 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+//        AssertionError assertionError = null;\n+//        // this is because more threads can access 'testDuration' variable and modify it...\n+//        synchronized (lockForTimeMeasuringSystem) {\n+//            try {\n+//                long testDuration = TimeMeasuringSystem.getInstance().getDurationInSeconds(\n+//                    testContext.getRequiredTestClass().getName(),\n+//                    testContext.getRequiredTestMethod().getName(),\n+//                    Operation.TEST_EXECUTION.name());\n+//                assertNoCoErrorsLogged(testDuration);\n+//            } catch (AssertionError e) {\n+//                LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n+//                assertionError = new AssertionError(e);\n+//            }\n+//        }\n+//\n+//        if (assertionError != null) {\n+//            throw assertionError;\n+//        }\n+        if (!Environment.SKIP_TEARDOWN) {\n+            ResourceManager.getInstance().deleteResources(testContext);\n+        }\n+    }\n \n-    @BeforeEach\n-    void createTestResources(ExtensionContext testContext) {\n-        if (testContext.getTestMethod().isPresent()) {\n-            testName = testContext.getTestMethod().get().getName();\n+    protected void afterAllMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            teardownEnvForOperator();\n+            ResourceManager.getInstance().deleteResources(testContext);\n         }\n-        ResourceManager.setMethodResources();\n+    }\n \n-        // This is needed to distinguish created Kafka cluster in ResourceManager and don't delete cluster which are still in use by parallel test cases\n-        if (previousClusterName == null) {\n-            LOGGER.info(\"Executing the first test case, using {} as a cluster name\", clusterName);\n-            previousClusterName = clusterName;\n-        } else {\n-            clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-            kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n-            LOGGER.info(\"Current test case is not the first one, generated new cluster name - {}\", clusterName);\n+    /**\n+     * BeforeEachMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeEachMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeEachMayOverride(ExtensionContext extensionContext) {\n+        // this is because we need to have different clusterName and kafkaClientsName in each test case without\n+        // synchronization it can produce `data-race`\n+        String testName = null;\n+\n+        synchronized (lock) {\n+            if (extensionContext.getTestMethod().isPresent()) {\n+                testName = extensionContext.getTestMethod().get().getName();\n+            }\n+\n+            LOGGER.info(\"Not first test we are gonna generate cluster name\");\n+            String clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n+\n+            mapTestWithClusterNames.put(testName, clusterName);\n+            mapTestWithTestTopics.put(testName, KafkaTopicUtils.generateRandomNameOfTopic());\n+            mapTestWithTestUsers.put(testName, KafkaUserUtils.generateRandomNameOfKafkaUser());\n+            mapTestWithKafkaClientNames.put(testName, clusterName + \"-\" + Constants.KAFKA_CLIENTS);\n+\n+            LOGGER.info(\"CLUSTER_NAMES_MAP: \\n{}\", mapTestWithClusterNames);\n+            LOGGER.info(\"USERS_NAME_MAP: \\n{}\", mapTestWithTestUsers);\n+            LOGGER.info(\"TOPIC_NAMES_MAP: \\n{}\", mapTestWithTestTopics);\n+            LOGGER.info(\"============THIS IS CLIENTS MAP:\\n{}\", mapTestWithKafkaClientNames);\n         }\n     }\n \n-    @BeforeAll\n-    void setTestClassName(ExtensionContext testContext) {\n+    /**\n+     * BeforeAllMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeAllMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeAllMayOverride(ExtensionContext extensionContext) {\n         cluster = KubeClusterResource.getInstance();\n-        if (testContext.getTestClass().isPresent()) {\n-            testClass = testContext.getTestClass().get().getName();\n+        String testClass = null;\n+\n+        if (extensionContext.getTestClass().isPresent()) {\n+            testClass = extensionContext.getTestClass().get().getName();\n         }\n-        // Name for the first test case, other test cases will need different name\n-        previousClusterName = null;\n-        clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-        kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n     }\n \n-    @AfterEach\n-    void teardownEnvironmentMethod(ExtensionContext testContext) throws Exception {\n-        TimeMeasuringSystem.getInstance().stopOperation(Operation.TEST_EXECUTION);\n-        AssertionError assertionError = null;\n-        try {\n-            long testDuration = timeMeasuringSystem.getDurationInSeconds(testContext.getRequiredTestClass().getName(), testContext.getRequiredTestMethod().getName(), Operation.TEST_EXECUTION.name());\n-            assertNoCoErrorsLogged(testDuration);\n-        } catch (AssertionError e) {\n-            LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n-            assertionError = new AssertionError(e);\n-        }\n+    @BeforeEach\n+    void setUpTestCase(ExtensionContext testContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE EACH] has been called\", this.getClass().getName());\n+        beforeEachMayOverride(testContext);\n+    }\n \n-        if (!Environment.SKIP_TEARDOWN) {\n-            tearDownEnvironmentAfterEach();\n-        }\n+    @BeforeAll\n+    void setUpTestSuite(ExtensionContext testContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE ALL] has been called\", this.getClass().getName());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDc0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810741", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T22:44:45Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +704,108 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+//        AssertionError assertionError = null;\n+//        // this is because more threads can access 'testDuration' variable and modify it...\n+//        synchronized (lockForTimeMeasuringSystem) {\n+//            try {\n+//                long testDuration = TimeMeasuringSystem.getInstance().getDurationInSeconds(\n+//                    testContext.getRequiredTestClass().getName(),\n+//                    testContext.getRequiredTestMethod().getName(),\n+//                    Operation.TEST_EXECUTION.name());\n+//                assertNoCoErrorsLogged(testDuration);\n+//            } catch (AssertionError e) {\n+//                LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n+//                assertionError = new AssertionError(e);\n+//            }\n+//        }\n+//\n+//        if (assertionError != null) {\n+//            throw assertionError;\n+//        }\n+        if (!Environment.SKIP_TEARDOWN) {\n+            ResourceManager.getInstance().deleteResources(testContext);\n+        }\n+    }\n \n-    @BeforeEach\n-    void createTestResources(ExtensionContext testContext) {\n-        if (testContext.getTestMethod().isPresent()) {\n-            testName = testContext.getTestMethod().get().getName();\n+    protected void afterAllMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            teardownEnvForOperator();\n+            ResourceManager.getInstance().deleteResources(testContext);\n         }\n-        ResourceManager.setMethodResources();\n+    }\n \n-        // This is needed to distinguish created Kafka cluster in ResourceManager and don't delete cluster which are still in use by parallel test cases\n-        if (previousClusterName == null) {\n-            LOGGER.info(\"Executing the first test case, using {} as a cluster name\", clusterName);\n-            previousClusterName = clusterName;\n-        } else {\n-            clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-            kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n-            LOGGER.info(\"Current test case is not the first one, generated new cluster name - {}\", clusterName);\n+    /**\n+     * BeforeEachMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeEachMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeEachMayOverride(ExtensionContext extensionContext) {\n+        // this is because we need to have different clusterName and kafkaClientsName in each test case without\n+        // synchronization it can produce `data-race`\n+        String testName = null;\n+\n+        synchronized (lock) {\n+            if (extensionContext.getTestMethod().isPresent()) {\n+                testName = extensionContext.getTestMethod().get().getName();\n+            }\n+\n+            LOGGER.info(\"Not first test we are gonna generate cluster name\");\n+            String clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n+\n+            mapTestWithClusterNames.put(testName, clusterName);\n+            mapTestWithTestTopics.put(testName, KafkaTopicUtils.generateRandomNameOfTopic());\n+            mapTestWithTestUsers.put(testName, KafkaUserUtils.generateRandomNameOfKafkaUser());\n+            mapTestWithKafkaClientNames.put(testName, clusterName + \"-\" + Constants.KAFKA_CLIENTS);\n+\n+            LOGGER.info(\"CLUSTER_NAMES_MAP: \\n{}\", mapTestWithClusterNames);\n+            LOGGER.info(\"USERS_NAME_MAP: \\n{}\", mapTestWithTestUsers);\n+            LOGGER.info(\"TOPIC_NAMES_MAP: \\n{}\", mapTestWithTestTopics);\n+            LOGGER.info(\"============THIS IS CLIENTS MAP:\\n{}\", mapTestWithKafkaClientNames);\n         }\n     }\n \n-    @BeforeAll\n-    void setTestClassName(ExtensionContext testContext) {\n+    /**\n+     * BeforeAllMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeAllMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeAllMayOverride(ExtensionContext extensionContext) {\n         cluster = KubeClusterResource.getInstance();\n-        if (testContext.getTestClass().isPresent()) {\n-            testClass = testContext.getTestClass().get().getName();\n+        String testClass = null;\n+\n+        if (extensionContext.getTestClass().isPresent()) {\n+            testClass = extensionContext.getTestClass().get().getName();\n         }\n-        // Name for the first test case, other test cases will need different name\n-        previousClusterName = null;\n-        clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-        kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n     }\n \n-    @AfterEach\n-    void teardownEnvironmentMethod(ExtensionContext testContext) throws Exception {\n-        TimeMeasuringSystem.getInstance().stopOperation(Operation.TEST_EXECUTION);\n-        AssertionError assertionError = null;\n-        try {\n-            long testDuration = timeMeasuringSystem.getDurationInSeconds(testContext.getRequiredTestClass().getName(), testContext.getRequiredTestMethod().getName(), Operation.TEST_EXECUTION.name());\n-            assertNoCoErrorsLogged(testDuration);\n-        } catch (AssertionError e) {\n-            LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n-            assertionError = new AssertionError(e);\n-        }\n+    @BeforeEach\n+    void setUpTestCase(ExtensionContext testContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE EACH] has been called\", this.getClass().getName());\n+        beforeEachMayOverride(testContext);\n+    }\n \n-        if (!Environment.SKIP_TEARDOWN) {\n-            tearDownEnvironmentAfterEach();\n-        }\n+    @BeforeAll\n+    void setUpTestSuite(ExtensionContext testContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE ALL] has been called\", this.getClass().getName());\n+        beforeAllMayOverride(testContext);\n+    }\n \n-        if (assertionError != null) {\n-            throw assertionError;\n-        }\n+    @AfterEach\n+    void tearDownTestCase(ExtensionContext testContext) throws Exception {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [AFTER EACH] has been called\", this.getClass().getName());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDc0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810746", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T22:44:54Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +704,108 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+//        AssertionError assertionError = null;\n+//        // this is because more threads can access 'testDuration' variable and modify it...\n+//        synchronized (lockForTimeMeasuringSystem) {\n+//            try {\n+//                long testDuration = TimeMeasuringSystem.getInstance().getDurationInSeconds(\n+//                    testContext.getRequiredTestClass().getName(),\n+//                    testContext.getRequiredTestMethod().getName(),\n+//                    Operation.TEST_EXECUTION.name());\n+//                assertNoCoErrorsLogged(testDuration);\n+//            } catch (AssertionError e) {\n+//                LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n+//                assertionError = new AssertionError(e);\n+//            }\n+//        }\n+//\n+//        if (assertionError != null) {\n+//            throw assertionError;\n+//        }\n+        if (!Environment.SKIP_TEARDOWN) {\n+            ResourceManager.getInstance().deleteResources(testContext);\n+        }\n+    }\n \n-    @BeforeEach\n-    void createTestResources(ExtensionContext testContext) {\n-        if (testContext.getTestMethod().isPresent()) {\n-            testName = testContext.getTestMethod().get().getName();\n+    protected void afterAllMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            teardownEnvForOperator();\n+            ResourceManager.getInstance().deleteResources(testContext);\n         }\n-        ResourceManager.setMethodResources();\n+    }\n \n-        // This is needed to distinguish created Kafka cluster in ResourceManager and don't delete cluster which are still in use by parallel test cases\n-        if (previousClusterName == null) {\n-            LOGGER.info(\"Executing the first test case, using {} as a cluster name\", clusterName);\n-            previousClusterName = clusterName;\n-        } else {\n-            clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-            kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n-            LOGGER.info(\"Current test case is not the first one, generated new cluster name - {}\", clusterName);\n+    /**\n+     * BeforeEachMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeEachMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeEachMayOverride(ExtensionContext extensionContext) {\n+        // this is because we need to have different clusterName and kafkaClientsName in each test case without\n+        // synchronization it can produce `data-race`\n+        String testName = null;\n+\n+        synchronized (lock) {\n+            if (extensionContext.getTestMethod().isPresent()) {\n+                testName = extensionContext.getTestMethod().get().getName();\n+            }\n+\n+            LOGGER.info(\"Not first test we are gonna generate cluster name\");\n+            String clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n+\n+            mapTestWithClusterNames.put(testName, clusterName);\n+            mapTestWithTestTopics.put(testName, KafkaTopicUtils.generateRandomNameOfTopic());\n+            mapTestWithTestUsers.put(testName, KafkaUserUtils.generateRandomNameOfKafkaUser());\n+            mapTestWithKafkaClientNames.put(testName, clusterName + \"-\" + Constants.KAFKA_CLIENTS);\n+\n+            LOGGER.info(\"CLUSTER_NAMES_MAP: \\n{}\", mapTestWithClusterNames);\n+            LOGGER.info(\"USERS_NAME_MAP: \\n{}\", mapTestWithTestUsers);\n+            LOGGER.info(\"TOPIC_NAMES_MAP: \\n{}\", mapTestWithTestTopics);\n+            LOGGER.info(\"============THIS IS CLIENTS MAP:\\n{}\", mapTestWithKafkaClientNames);\n         }\n     }\n \n-    @BeforeAll\n-    void setTestClassName(ExtensionContext testContext) {\n+    /**\n+     * BeforeAllMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeAllMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeAllMayOverride(ExtensionContext extensionContext) {\n         cluster = KubeClusterResource.getInstance();\n-        if (testContext.getTestClass().isPresent()) {\n-            testClass = testContext.getTestClass().get().getName();\n+        String testClass = null;\n+\n+        if (extensionContext.getTestClass().isPresent()) {\n+            testClass = extensionContext.getTestClass().get().getName();\n         }\n-        // Name for the first test case, other test cases will need different name\n-        previousClusterName = null;\n-        clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-        kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n     }\n \n-    @AfterEach\n-    void teardownEnvironmentMethod(ExtensionContext testContext) throws Exception {\n-        TimeMeasuringSystem.getInstance().stopOperation(Operation.TEST_EXECUTION);\n-        AssertionError assertionError = null;\n-        try {\n-            long testDuration = timeMeasuringSystem.getDurationInSeconds(testContext.getRequiredTestClass().getName(), testContext.getRequiredTestMethod().getName(), Operation.TEST_EXECUTION.name());\n-            assertNoCoErrorsLogged(testDuration);\n-        } catch (AssertionError e) {\n-            LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n-            assertionError = new AssertionError(e);\n-        }\n+    @BeforeEach\n+    void setUpTestCase(ExtensionContext testContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE EACH] has been called\", this.getClass().getName());\n+        beforeEachMayOverride(testContext);\n+    }\n \n-        if (!Environment.SKIP_TEARDOWN) {\n-            tearDownEnvironmentAfterEach();\n-        }\n+    @BeforeAll\n+    void setUpTestSuite(ExtensionContext testContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE ALL] has been called\", this.getClass().getName());\n+        beforeAllMayOverride(testContext);\n+    }\n \n-        if (assertionError != null) {\n-            throw assertionError;\n-        }\n+    @AfterEach\n+    void tearDownTestCase(ExtensionContext testContext) throws Exception {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [AFTER EACH] has been called\", this.getClass().getName());\n+        afterEachMayOverride(testContext);\n     }\n \n     @AfterAll\n-    void teardownEnvironmentClass() {\n-        if (!Environment.SKIP_TEARDOWN) {\n-            tearDownEnvironmentAfterAll();\n-            teardownEnvForOperator();\n-        }\n+    void tearDownTestSuite(ExtensionContext testContext) throws Exception {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [AFTER ALL] has been called\", this.getClass().getName());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMDgxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593810815", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T22:45:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/bridge/HttpBridgeAbstractST.java", "diffHunk": "@@ -29,17 +32,15 @@\n     protected WebClient client;\n     protected static KafkaBridgeExampleClients kafkaBridgeClientJob;\n \n-    void deployClusterOperator(String namespace) {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(namespace);\n-    }\n-\n     @BeforeAll\n     void createBridgeClient() {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE ALL] has been called\", this.getClass().getName());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTAyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811027", "bodyText": "Log", "author": "Frawless", "createdAt": "2021-03-13T22:47:22Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/bridge/HttpBridgeKafkaExternalListenersST.java", "diffHunk": "@@ -121,55 +133,65 @@ private void testWeirdUsername(String weirdUserName, KafkaListenerAuthentication\n                         .endGenericKafkaListener()\n                     .endListeners()\n                 .endKafka()\n-                .endSpec()\n-            .build());\n+            .endSpec().build());\n+\n+        kafkaBridgeClientJob = kafkaBridgeClientJob.toBuilder()\n+            .withBootstrapAddress(KafkaBridgeResources.serviceName(clusterName))\n+            .withProducerName(clusterName + \"-\" + producerName)\n+            .withConsumerName(clusterName + \"-\" + consumerName)\n+            .withTopicName(topicName)\n+            .build();\n \n         // Create topic\n-        KafkaTopicResource.createAndWaitForReadiness(KafkaTopicResource.topic(clusterName, TOPIC_NAME).build());\n+        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n \n         // Create user\n         if (auth.getType().equals(Constants.TLS_LISTENER_DEFAULT_NAME)) {\n-            KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.tlsUser(clusterName, weirdUserName).build());\n-            KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.tlsUser(clusterName, aliceUser).build());\n+            resourceManager.createResource(extensionContext, KafkaUserTemplates.tlsUser(clusterName, weirdUserName).build());\n+            resourceManager.createResource(extensionContext, KafkaUserTemplates.tlsUser(clusterName, aliceUser).build());\n         } else {\n-            KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.scramShaUser(clusterName, weirdUserName).build());\n-            KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.scramShaUser(clusterName, aliceUser).build());\n+            resourceManager.createResource(extensionContext, KafkaUserTemplates.scramShaUser(clusterName, weirdUserName).build());\n+            resourceManager.createResource(extensionContext, KafkaUserTemplates.scramShaUser(clusterName, aliceUser).build());\n         }\n \n-        KafkaClientsResource.createAndWaitForReadiness(KafkaClientsResource.deployKafkaClients(true, kafkaClientsName).build());\n+        String kafkaClientsName = mapTestWithKafkaClientNames.get(extensionContext.getDisplayName());\n+\n+        resourceManager.createResource(extensionContext, KafkaClientsTemplates.kafkaClients(true, kafkaClientsName).build());\n \n         // Deploy http bridge\n-        KafkaBridgeResource.createAndWaitForReadiness(KafkaBridgeResource.kafkaBridge(clusterName, KafkaResources.tlsBootstrapAddress(clusterName), 1)\n-            .withNewSpecLike(spec)\n-                .withBootstrapServers(KafkaResources.tlsBootstrapAddress(clusterName))\n-                .withNewHttp(Constants.HTTP_BRIDGE_DEFAULT_PORT)\n+        resourceManager.createResource(extensionContext, KafkaBridgeTemplates.kafkaBridge(clusterName, KafkaResources.tlsBootstrapAddress(clusterName), 1)\n+                .withNewSpecLike(spec)\n+                    .withBootstrapServers(KafkaResources.tlsBootstrapAddress(clusterName))\n+                    .withNewHttp(Constants.HTTP_BRIDGE_DEFAULT_PORT)\n                 .withNewConsumer()\n                     .addToConfig(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n                 .endConsumer()\n             .endSpec()\n             .build());\n \n         Service service = KafkaBridgeUtils.createBridgeNodePortService(clusterName, NAMESPACE, BRIDGE_EXTERNAL_SERVICE);\n-        KubernetesResource.createServiceResource(service, NAMESPACE);\n+        ServiceResource.createServiceResource(extensionContext, service, NAMESPACE);\n \n-        kafkaBridgeClientJob.createAndWaitForReadiness(kafkaBridgeClientJob.consumerStrimziBridge().build());\n+        resourceManager.createResource(extensionContext, kafkaBridgeClientJob.consumerStrimziBridge().build());\n \n         BasicExternalKafkaClient basicExternalKafkaClient = new BasicExternalKafkaClient.Builder()\n             .withClusterName(clusterName)\n             .withNamespaceName(NAMESPACE)\n-            .withTopicName(TOPIC_NAME)\n+            .withTopicName(topicName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withKafkaUsername(weirdUserName)\n             .withSecurityProtocol(securityProtocol)\n             .withListenerName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n             .build();\n \n         assertThat(basicExternalKafkaClient.sendMessagesTls(), is(MESSAGE_COUNT));\n-        ClientUtils.waitForClientSuccess(consumerName, NAMESPACE, MESSAGE_COUNT);\n+        ClientUtils.waitForClientSuccess(clusterName + \"-\" + consumerName, NAMESPACE, MESSAGE_COUNT);\n     }\n \n     @BeforeAll\n-    void createClassResources() throws Exception {\n-        deployClusterOperator(NAMESPACE);\n+    void createClassResources(ExtensionContext extensionContext) {\n+        LOGGER.info(\"===============================================================\");\n+        LOGGER.info(\"{} - [BEFORE ALL] has been called\", this.getClass().getName());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTEwNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811105", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T22:47:53Z", "path": "test/src/main/java/io/strimzi/test/timemeasuring/TimeMeasuringSystem.java", "diffHunk": "@@ -45,18 +46,13 @@\n      *      - operation 1\n      * where operation run time data are stored in MeasureRecord and keys provide names of classes and methods.\n      */\n-    private Map<String, Map<String, Map<String, MeasureRecord>>> measuringMap;\n-    private String testClass;\n-    private String testName;\n-    private String operationID;\n-\n-    private TimeMeasuringSystem() {\n-        measuringMap = new LinkedHashMap<>();\n-    }\n+    private static Map<String, Map<String, Map<String, MeasureRecord>>> measuringMap;\n \n     public static synchronized TimeMeasuringSystem getInstance() {\n         if (instance == null) {\n+            LOGGER.info(\"====================Hello from singleton\");", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTE0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811140", "bodyText": "Leftover? Or make it more useful and probably move to debug", "author": "Frawless", "createdAt": "2021-03-13T22:48:17Z", "path": "test/src/main/java/io/strimzi/test/timemeasuring/TimeMeasuringSystem.java", "diffHunk": "@@ -71,34 +67,43 @@ private String createOperationsID(Operation operation) {\n         return id;\n     }\n \n-    private void addRecord(String operationID, MeasureRecord record) {\n+    private void addRecord(String operationID, MeasureRecord record, String testClass, String testName) {\n+        LOGGER.info(\"============ ADDING RECORD ========\");\n         if (measuringMap.get(testClass) == null) {\n-            LinkedHashMap<String, Map<String, MeasureRecord>> newData = new LinkedHashMap<>();\n-            LinkedHashMap<String, MeasureRecord> newRecord = new LinkedHashMap<>();\n+            // new test suite\n+            LOGGER.info(\"Hello testClass {} is new....\", testClass);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTE1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811153", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T22:48:22Z", "path": "test/src/main/java/io/strimzi/test/timemeasuring/TimeMeasuringSystem.java", "diffHunk": "@@ -71,34 +67,43 @@ private String createOperationsID(Operation operation) {\n         return id;\n     }\n \n-    private void addRecord(String operationID, MeasureRecord record) {\n+    private void addRecord(String operationID, MeasureRecord record, String testClass, String testName) {\n+        LOGGER.info(\"============ ADDING RECORD ========\");\n         if (measuringMap.get(testClass) == null) {\n-            LinkedHashMap<String, Map<String, MeasureRecord>> newData = new LinkedHashMap<>();\n-            LinkedHashMap<String, MeasureRecord> newRecord = new LinkedHashMap<>();\n+            // new test suite\n+            LOGGER.info(\"Hello testClass {} is new....\", testClass);\n+            Map<String, Map<String, MeasureRecord>> newData = new LinkedHashMap<>();\n+            Map<String, MeasureRecord> newRecord = new LinkedHashMap<>();\n             newData.put(testName, newRecord);\n             newRecord.put(operationID, record);\n             measuringMap.put(testClass, newData);\n         } else if (measuringMap.get(testClass).get(testName) == null) {\n-            LinkedHashMap<String, MeasureRecord> newRecord = new LinkedHashMap<>();\n+            LOGGER.info(\"Hello testMethod:{} is new but we know testClass:{}....\", testName, testClass);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTE3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811170", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T22:48:33Z", "path": "test/src/main/java/io/strimzi/test/timemeasuring/TimeMeasuringSystem.java", "diffHunk": "@@ -109,21 +114,19 @@ private void setEndTime(String id) {\n             id = createOperationsID(Operation.CO_DELETION);\n         }\n         try {\n-            measuringMap.get(testClass).get(testName).get(id).setEndTime(System.currentTimeMillis());\n+            LOGGER.info(testClass);\n+            LOGGER.info(testName);\n+            LOGGER.info(measuringMap.toString());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTI4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811284", "bodyText": "This is to improve the default timeouts of the client?", "author": "Frawless", "createdAt": "2021-03-13T22:49:36Z", "path": "test/src/main/java/io/strimzi/test/k8s/cluster/Kubernetes.java", "diffHunk": "@@ -49,7 +54,16 @@ public KubeCmdClient defaultCmdClient() {\n \n     @Override\n     public KubeClient defaultClient() {\n-        return new KubeClient(new DefaultKubernetesClient(CONFIG), \"default\");\n+        OkHttpClient httpClient = HttpClientUtils.createHttpClient(CONFIG);\n+\n+        httpClient = httpClient.newBuilder()\n+            .protocols(Collections.singletonList(Protocol.HTTP_1_1))\n+            .connectTimeout(Duration.ofSeconds(60))\n+            .writeTimeout(Duration.ofSeconds(60))\n+            .readTimeout(Duration.ofSeconds(60))\n+            .build();", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwNzI0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593907241", "bodyText": "Yes.", "author": "see-quick", "createdAt": "2021-03-14T14:14:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTUwMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811501", "bodyText": "indent", "author": "Frawless", "createdAt": "2021-03-13T22:52:33Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/bridge/HttpBridgeScramShaST.java", "diffHunk": "@@ -89,13 +97,13 @@ void testReceiveSimpleMessageTlsScramSha() {\n     }\n \n     @BeforeAll\n-    void setup() throws Exception {\n-        deployClusterOperator(NAMESPACE);\n+    void setup(ExtensionContext extensionContext) {\n+        installClusterOperator(extensionContext, NAMESPACE);\n         LOGGER.info(\"Deploy Kafka and KafkaBridge before tests\");\n \n         // Deploy kafka\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(httpBridgeScramShaClusterName, 1, 1)\n-            .editSpec()\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(httpBridgeScramShaClusterName, 1, 1)\n+                .editSpec()", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTk1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593811953", "bodyText": "Maybe we could somehow unify the way how to generate the names for components like connect or bridge. In all tests I mean.", "author": "Frawless", "createdAt": "2021-03-13T22:57:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectBuilderST.java", "diffHunk": "@@ -278,8 +283,10 @@ void testPushIntoImageStream() {\n         assertTrue(kafkaConnect.getStatus().getConnectorPlugins().stream().anyMatch(connectorPlugin -> connectorPlugin.getConnectorClass().contains(ECHO_SINK_CLASS_NAME)));\n     }\n \n-    @Test\n-    void testUpdateConnectWithAnotherPlugin() {\n+    @ParallelTest\n+    void testUpdateConnectWithAnotherPlugin(ExtensionContext extensionContext) {\n+        String connectClusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName()) + \"-connect\";", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwNzc5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593907793", "bodyText": "We can create a ticket for that :D", "author": "see-quick", "createdAt": "2021-03-14T14:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMTk1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMjAyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593812024", "bodyText": "Maybe better log message?", "author": "Frawless", "createdAt": "2021-03-13T22:58:00Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectBuilderST.java", "diffHunk": "@@ -320,62 +324,60 @@ void testUpdateConnectWithAnotherPlugin() {\n                     .addToLoggers(\"connect.root.logger.level\", \"INFO\")\n                 .endInlineLogging()\n             .endSpec()\n-            .build(), true);\n+            .build());\n \n         Map<String, Object> echoSinkConfig = new HashMap<>();\n         echoSinkConfig.put(\"topics\", topicName);\n         echoSinkConfig.put(\"level\", \"INFO\");\n \n         LOGGER.info(\"Creating EchoSink connector\");\n-        KafkaConnectorResource.createAndWaitForReadiness(KafkaConnectorResource.kafkaConnector(echoConnector, clusterName)\n+        resourceManager.createResource(extensionContext, KafkaConnectorTemplates.kafkaConnector(echoConnector, connectClusterName)\n             .editOrNewSpec()\n                 .withClassName(ECHO_SINK_CLASS_NAME)\n                 .withConfig(echoSinkConfig)\n             .endSpec()\n             .build());\n \n-        String deploymentName = KafkaConnectResources.deploymentName(clusterName);\n+        String deploymentName = KafkaConnectResources.deploymentName(connectClusterName);\n         Map<String, String> connectSnapshot = DeploymentUtils.depSnapshot(deploymentName);\n \n         LOGGER.info(\"Checking that KafkaConnect API contains EchoSink connector and not Camel-Telegram Connector class name\");\n-        String plugins = cmdKubeClient().execInPod(kafkaClientsPodName, \"curl\", \"-X\", \"GET\", \"http://\" + KafkaConnectResources.serviceName(clusterName) + \":8083/connector-plugins\").out();\n+        String plugins = cmdKubeClient().execInPod(kafkaClientsPodName, \"curl\", \"-X\", \"GET\", \"http://\" + KafkaConnectResources.serviceName(connectClusterName) + \":8083/connector-plugins\").out();\n \n         assertFalse(plugins.contains(CAMEL_CONNECTOR_CLASS_NAME));\n         assertTrue(plugins.contains(ECHO_SINK_CLASS_NAME));\n \n         LOGGER.info(\"Adding one more connector to the KafkaConnect\");\n-        KafkaConnectResource.replaceKafkaConnectResource(clusterName, kafkaConnect -> {\n+        KafkaConnectResource.replaceKafkaConnectResource(connectClusterName, kafkaConnect -> {\n             kafkaConnect.getSpec().getBuild().getPlugins().add(secondPlugin);\n         });\n \n         DeploymentUtils.waitTillDepHasRolled(deploymentName, 1, connectSnapshot);\n \n         Map<String, Object> camelHttpConfig = new HashMap<>();\n-        camelHttpConfig.put(\"camel.sink.path.httpUri\", \"http://\" + KafkaConnectResources.serviceName(clusterName) + \":8083\");\n+        camelHttpConfig.put(\"camel.sink.path.httpUri\", \"http://\" + KafkaConnectResources.serviceName(connectClusterName) + \":8083\");\n         camelHttpConfig.put(\"topics\", topicName);\n \n         LOGGER.info(\"Creating Camel-HTTP-Sink connector\");\n-        KafkaConnectorResource.createAndWaitForReadiness(KafkaConnectorResource.kafkaConnector(camelConnector, clusterName)\n+        resourceManager.createResource(extensionContext, KafkaConnectorTemplates.kafkaConnector(camelConnector, connectClusterName)\n             .editOrNewSpec()\n                 .withClassName(CAMEL_CONNECTOR_CLASS_NAME)\n                 .withConfig(camelHttpConfig)\n             .endSpec()\n             .build());\n \n-        KafkaConnect kafkaConnect = KafkaConnectResource.kafkaConnectClient().inNamespace(NAMESPACE).withName(clusterName).get();\n+        KafkaConnect kafkaConnect = KafkaConnectResource.kafkaConnectClient().inNamespace(NAMESPACE).withName(connectClusterName).get();\n \n-        LOGGER.info(\"Checking if both Connectors were created and Connect contains both plugins\");\n-        assertThat(KafkaConnectorResource.kafkaConnectorClient().inNamespace(NAMESPACE).list().getItems().size(), is(2));\n+        LOGGER.info(\"Checking if Connect contains both plugins\");", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMjI3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593812275", "bodyText": "Better message?", "author": "Frawless", "createdAt": "2021-03-13T23:00:23Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectS2IST.java", "diffHunk": "@@ -575,7 +598,9 @@ void testChangeConnectS2IConfig() {\n \n         String deploymentConfigName = KafkaConnectS2IResources.deploymentName(connectS2IClusterName);\n \n-        List<String> connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnectS2I.RESOURCE_KIND);\n+        List<Pod> connectPods = kubeClient().listKafkaConnectS2IPods(connectS2IClusterName);\n+\n+        LOGGER.debug(\"=== {}\", connectPods);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzEzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813139", "bodyText": "Can we remove ordering now?", "author": "Frawless", "createdAt": "2021-03-13T23:09:14Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/cruisecontrol/CruiseControlConfigurationST.java", "diffHunk": "@@ -67,7 +68,7 @@\n     private static final String CRUISE_CONTROL_POD_PREFIX = CRUISE_CONTROL_CONFIGURATION_CLUSTER_NAME + \"-cruise-control-\";\n \n     @Order(1)", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwODA0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593908043", "bodyText": "I am not 100% sure about that but I have created an issue.", "author": "see-quick", "createdAt": "2021-03-14T14:19:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzEzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzE3Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813172", "bodyText": "debug leftover?", "author": "Frawless", "createdAt": "2021-03-13T23:09:42Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/cruisecontrol/CruiseControlIsolatedST.java", "diffHunk": "@@ -55,20 +58,29 @@\n     private static final String CRUISE_CONTROL_MODEL_TRAINING_SAMPLES_TOPIC = \"strimzi.cruisecontrol.modeltrainingsamples\"; // partitions 32 , rf - 2\n     private static final String CRUISE_CONTROL_PARTITION_METRICS_SAMPLES_TOPIC = \"strimzi.cruisecontrol.partitionmetricsamples\"; // partitions 32 , rf - 2\n \n-    @Test\n-    void testAutoCreationOfCruiseControlTopics() {\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaWithCruiseControl(clusterName, 3, 3)\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n+    void testAutoCreationOfCruiseControlTopics(ExtensionContext extensionContext) {\n+        String clusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName());\n+\n+        LOGGER.info(\"testAutoCreationOfCruiseControlTopics kafka cruise control....with cluster {}\", clusterName);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzMzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813338", "bodyText": "debug leftover?", "author": "Frawless", "createdAt": "2021-03-13T23:11:51Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/cruisecontrol/CruiseControlIsolatedST.java", "diffHunk": "@@ -183,14 +205,18 @@ void testCruiseControlReplicaMovementStrategy() {\n         assertThat(ccConfFileContent, containsString(newReplicaMovementStrategies));\n     }\n \n-    @Test\n-    void testHostAliases() {\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n+    void testHostAliases(ExtensionContext extensionContext) {\n+        String clusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName());\n+\n         HostAlias hostAlias = new HostAliasBuilder()\n             .withIp(aliasIp)\n             .withHostnames(aliasHostname)\n             .build();\n \n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaWithCruiseControl(clusterName, 3, 3)\n+        LOGGER.info(\"======= testHostAliases{}\", clusterName);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzcwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813703", "bodyText": "hehe, leftover?", "author": "Frawless", "createdAt": "2021-03-13T23:16:28Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/specific/SpecificST.java", "diffHunk": "@@ -127,34 +134,38 @@ void testRackAware() {\n             .withDelayMs(0)\n             .build();\n \n-        kafkaBasicClientJob.createAndWaitForReadiness(kafkaBasicClientJob.producerStrimzi().build());\n-        kafkaBasicClientJob.createAndWaitForReadiness(kafkaBasicClientJob.consumerStrimzi().build());\n+        resourceManager.createResource(extensionContext, kafkaBasicClientJob.producerStrimzi().build());\n+        resourceManager.createResource(extensionContext, kafkaBasicClientJob.consumerStrimzi().build());\n     }\n \n-    @Test\n+    @IsolatedTest(\"Modification of shared Cluster Operator configuration\")\n     @Tag(CONNECT)\n     @Tag(REGRESSION)\n     @Tag(INTERNAL_CLIENTS_USED)\n-    void testRackAwareConnectWrongDeployment() {\n+    void testRackAwareConnectWrongDeployment(ExtensionContext extensionContext) {\n         assumeFalse(Environment.isNamespaceRbacScope());\n \n+        String clusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName());\n+        String kafkaClientsName = mapTestWithKafkaClientNames.get(extensionContext.getDisplayName());\n         Map<String, String> label = Collections.singletonMap(\"my-label\", \"value\");\n         Map<String, String> anno = Collections.singletonMap(\"my-annotation\", \"value\");\n \n         // We need to update CO configuration to set OPERATION_TIMEOUT to shorter value, because we expect timeout in that test\n         Map<String, String> coSnapshot = DeploymentUtils.depSnapshot(ResourceManager.getCoDeploymentName());\n         // We have to install CO in class stack, otherwise it will be deleted at the end of test case and all following tests will fail\n-        ResourceManager.setClassResources();\n-        BundleResource.createAndWaitForReadiness(BundleResource.clusterOperator(NAMESPACE, CO_OPERATION_TIMEOUT_SHORT).build());\n+\n+        // TODO: how to solve this SHIT???", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzc0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813748", "bodyText": "Solved or?", "author": "Frawless", "createdAt": "2021-03-13T23:16:43Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/specific/SpecificST.java", "diffHunk": "@@ -213,9 +226,10 @@ void testRackAwareConnectWrongDeployment() {\n         KafkaConnectUtils.sendReceiveMessagesThroughConnect(kcPods.get(0), TOPIC_NAME, kafkaClientsPodName, NAMESPACE, clusterName);\n \n         // Revert changes for CO deployment\n-        ResourceManager.setClassResources();\n-        BundleResource.createAndWaitForReadiness(BundleResource.clusterOperator(NAMESPACE).build());\n-        ResourceManager.setMethodResources();\n+        // TODO: how to solve this...???\n+//        ResourceManager.setClassResources();\n+        resourceManager.createResource(sharedExtensionContext, BundleResource.clusterOperator(NAMESPACE).build());\n+//        ResourceManager.setMethodResources();", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkwOTU0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593909543", "bodyText": "It's solved by sharedExtensionContext, which ensures that the resource manager will delete resource if and only if @afterall", "author": "see-quick", "createdAt": "2021-03-14T14:30:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzc0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzc2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813763", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T23:16:56Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/specific/SpecificST.java", "diffHunk": "@@ -226,38 +240,44 @@ void testRackAwareConnectWrongDeployment() {\n         assertThat(actualAnno, is(anno));\n     }\n \n-    @Test\n+    @IsolatedTest(\"Modification of shared Cluster Operator configuration\")\n     @Tag(CONNECT)\n     @Tag(REGRESSION)\n     @Tag(INTERNAL_CLIENTS_USED)\n-    public void testRackAwareConnectCorrectDeployment() {\n+    public void testRackAwareConnectCorrectDeployment(ExtensionContext extensionContext) {\n         assumeFalse(Environment.isNamespaceRbacScope());\n+\n+        String kafkaClientsName = mapTestWithKafkaClientNames.get(extensionContext.getDisplayName());\n+        String clusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName());\n+\n         // We need to update CO configuration to set OPERATION_TIMEOUT to shorter value, because we expect timeout in that test\n         Map<String, String> coSnapshot = DeploymentUtils.depSnapshot(ResourceManager.getCoDeploymentName());\n         // We have to install CO in class stack, otherwise it will be deleted at the end of test case and all following tests will fail\n-        ResourceManager.setClassResources();\n-        BundleResource.createAndWaitForReadiness(BundleResource.clusterOperator(NAMESPACE, CO_OPERATION_TIMEOUT_SHORT).build());\n+        // TODO: how to solve this...???\n+//        ResourceManager.setClassResources();\n+        resourceManager.createResource(sharedExtensionContext, BundleResource.clusterOperator(NAMESPACE, CO_OPERATION_TIMEOUT_SHORT).build());\n         // Now we set pointer stack to method again\n-        ResourceManager.setMethodResources();\n+//        ResourceManager.setMethodResources();", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzc3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813771", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T23:17:09Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/specific/SpecificST.java", "diffHunk": "@@ -283,26 +308,28 @@ public void testRackAwareConnectCorrectDeployment() {\n             assertThat(connectPodNodeSelectorRequirement.getKey(), is(rackKey));\n             assertThat(connectPodNodeSelectorRequirement.getOperator(), is(\"Exists\"));\n \n-            String topicName = \"rw-\" + KafkaTopicUtils.generateRandomNameOfTopic();\n-            KafkaTopicResource.createAndWaitForReadiness(KafkaTopicResource.topic(clusterName, topicName).build());\n+            topicName = \"rw-\" + KafkaTopicUtils.generateRandomNameOfTopic();\n+            resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n             KafkaConnectUtils.sendReceiveMessagesThroughConnect(connectPodName, topicName, kafkaClientsPodName, NAMESPACE, clusterName);\n         }\n \n         // Revert changes for CO deployment\n-        ResourceManager.setClassResources();\n-        BundleResource.createAndWaitForReadiness(BundleResource.clusterOperator(NAMESPACE).build());\n-        ResourceManager.setMethodResources();\n+        // TODO: how???\n+//        ResourceManager.setClassResources();\n+        resourceManager.createResource(sharedExtensionContext, BundleResource.clusterOperator(NAMESPACE).build());\n+//        ResourceManager.setMethodResources();", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxMzk2Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593813967", "bodyText": "Why you removed it?", "author": "Frawless", "createdAt": "2021-03-13T23:18:46Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/listeners/BackwardsCompatibleListenersST.java", "diffHunk": "@@ -1,436 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.kafka.listeners;\n-\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n-import io.fabric8.kubernetes.client.dsl.MixedOperation;\n-import io.fabric8.kubernetes.client.dsl.Resource;\n-import io.strimzi.api.kafka.Crds;\n-import io.strimzi.api.kafka.KafkaList;\n-import io.strimzi.api.kafka.model.Kafka;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.KafkaUser;\n-import io.strimzi.api.kafka.model.listener.KafkaListenerAuthenticationScramSha512;\n-import io.strimzi.api.kafka.model.listener.KafkaListenerAuthenticationTls;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.api.kafka.model.listener.arraylistener.ArrayOrObjectKafkaListeners;\n-import io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListenerBuilder;\n-import io.strimzi.api.kafka.model.listener.arraylistener.KafkaListenerType;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.annotations.OpenShiftOnly;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.kafkaclients.internalClients.InternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.ResourceOperation;\n-import io.strimzi.systemtest.resources.crd.KafkaClientsResource;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaTopicUtils;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.ServiceUtils;\n-import io.strimzi.test.TestUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.Map;\n-\n-import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.INTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.Constants.TLS_LISTENER_DEFAULT_NAME;\n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n-import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n-import static java.util.Arrays.asList;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-\n-@Tag(REGRESSION)\n-public class BackwardsCompatibleListenersST extends AbstractST {", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxNDE5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593814198", "bodyText": "leftover?", "author": "Frawless", "createdAt": "2021-03-13T23:21:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/operators/RecoveryST.java", "diffHunk": "@@ -42,33 +45,34 @@\n class RecoveryST extends AbstractST {\n \n     static final String NAMESPACE = \"recovery-cluster-test\";\n-    static String clusterName;\n-    static String kafkaClientsName;\n+    static String sharedClusterName;\n \n     private static final Logger LOGGER = LogManager.getLogger(RecoveryST.class);\n \n-    @Test\n-    void testRecoveryFromEntityOperatorDeletion() {\n-        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+    @IsolatedTest(\"We need for each test case its own Cluster Operator\")\n+    void testRecoveryFromEntityOperatorDeletion(ExtensionContext extensionContext) {\n+        String operationId = timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY, extensionContext.getRequiredTestClass().getName(), extensionContext.getDisplayName());\n+\n         // kafka cluster already deployed\n-        LOGGER.info(\"Running testRecoveryFromEntityOperatorDeletion with cluster {}\", clusterName);\n-        String entityOperatorDeploymentName = KafkaResources.entityOperatorDeploymentName(clusterName);\n+        LOGGER.info(\"Running testRecoveryFromEntityOperatorDeletion with cluster {}\", sharedClusterName);\n+        String entityOperatorDeploymentName = KafkaResources.entityOperatorDeploymentName(sharedClusterName);\n         String entityOperatorDeploymentUid = kubeClient().getDeploymentUid(entityOperatorDeploymentName);\n         kubeClient().deleteDeployment(entityOperatorDeploymentName);\n         PodUtils.waitForPodsWithPrefixDeletion(entityOperatorDeploymentName);\n         LOGGER.info(\"Waiting for recovery {}\", entityOperatorDeploymentName);\n         DeploymentUtils.waitForDeploymentRecovery(entityOperatorDeploymentName, entityOperatorDeploymentUid);\n         DeploymentUtils.waitForDeploymentAndPodsReady(entityOperatorDeploymentName, 1);\n \n-        timeMeasuringSystem.stopOperation(timeMeasuringSystem.getOperationID());\n+        timeMeasuringSystem.stopOperation(operationId, extensionContext.getRequiredTestClass().getName(), extensionContext.getDisplayName());\n     }\n \n-    @Test\n-    void testRecoveryFromKafkaStatefulSetDeletion() {\n-        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+    @IsolatedTest(\"We need for each test case its own Cluster Operator\")\n+    void testRecoveryFromKafkaStatefulSetDeletion(ExtensionContext extensionContext) {\n+        String operationId = timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY, extensionContext.getRequiredTestClass().getName(), extensionContext.getDisplayName());\n+\n         // kafka cluster already deployed\n-        LOGGER.info(\"Running deleteKafkaStatefulSet with cluster {}\", clusterName);\n-        String kafkaStatefulSetName = KafkaResources.kafkaStatefulSetName(clusterName);\n+        LOGGER.info(\"Running deleteKafkaStatefulSet with cluster {}\", sharedClusterName);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxNDI0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593814246", "bodyText": "is it really useful?", "author": "Frawless", "createdAt": "2021-03-13T23:22:08Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/operators/RecoveryST.java", "diffHunk": "@@ -79,15 +83,16 @@ void testRecoveryFromKafkaStatefulSetDeletion() {\n         StatefulSetUtils.waitForStatefulSetRecovery(kafkaStatefulSetName, kafkaStatefulSetUid);\n         StatefulSetUtils.waitForAllStatefulSetPodsReady(kafkaStatefulSetName, 3);\n \n-        timeMeasuringSystem.stopOperation(timeMeasuringSystem.getOperationID());\n+        timeMeasuringSystem.stopOperation(operationId, extensionContext.getRequiredTestClass().getName(), extensionContext.getDisplayName());\n     }\n \n-    @Test\n-    void testRecoveryFromZookeeperStatefulSetDeletion() {\n-        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+    @IsolatedTest(\"We need for each test case its own Cluster Operator\")\n+    void testRecoveryFromZookeeperStatefulSetDeletion(ExtensionContext extensionContext) {\n+        String operationId = timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY, extensionContext.getRequiredTestClass().getName(), extensionContext.getDisplayName());\n+\n         // kafka cluster already deployed\n-        LOGGER.info(\"Running deleteZookeeperStatefulSet with cluster {}\", clusterName);\n-        String zookeeperStatefulSetName = KafkaResources.zookeeperStatefulSetName(clusterName);\n+        LOGGER.info(\"Running deleteZookeeperStatefulSet with cluster {}\", sharedClusterName);", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxNDQ5Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593814492", "bodyText": "Solved?", "author": "Frawless", "createdAt": "2021-03-13T23:25:35Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/listeners/MultipleListenersST.java", "diffHunk": "@@ -150,102 +167,105 @@ private void runListenersTest(List<GenericKafkaListener> listeners) {\n             .endSpec()\n             .build());\n \n-        String kafkaUsername = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUser kafkaUserInstance = KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.tlsUser(clusterName, kafkaUsername).build());\n-\n-        for (GenericKafkaListener listener : listeners) {\n-\n-            String topicName = KafkaTopicUtils.generateRandomNameOfTopic();\n-            KafkaTopicResource.createAndWaitForReadiness(KafkaTopicResource.topic(clusterName, topicName).build());\n-\n-            boolean isTlsEnabled = listener.isTls();\n-\n-            if (listener.getType() != KafkaListenerType.INTERNAL) {\n-                if (isTlsEnabled) {\n-                    BasicExternalKafkaClient externalTlsKafkaClient = new BasicExternalKafkaClient.Builder()\n-                        .withTopicName(topicName)\n-                        .withNamespaceName(NAMESPACE)\n-                        .withClusterName(clusterName)\n-                        .withMessageCount(MESSAGE_COUNT)\n-                        .withKafkaUsername(kafkaUsername)\n-                        .withListenerName(listener.getName())\n-                        .withSecurityProtocol(SecurityProtocol.SSL)\n-                        .withListenerName(listener.getName())\n-                        .build();\n-\n-                    LOGGER.info(\"Verifying {} listener\", Constants.TLS_LISTENER_DEFAULT_NAME);\n-\n-                    // verify phase\n-                    externalTlsKafkaClient.verifyProducedAndConsumedMessages(\n-                        externalTlsKafkaClient.sendMessagesTls(),\n-                        externalTlsKafkaClient.receiveMessagesTls()\n-                    );\n-                } else {\n-                    BasicExternalKafkaClient externalPlainKafkaClient = new BasicExternalKafkaClient.Builder()\n-                        .withTopicName(topicName)\n-                        .withNamespaceName(NAMESPACE)\n-                        .withClusterName(clusterName)\n-                        .withMessageCount(MESSAGE_COUNT)\n-                        .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-                        .withListenerName(listener.getName())\n-                        .build();\n-\n-                    LOGGER.info(\"Verifying {} listener\", Constants.PLAIN_LISTENER_DEFAULT_NAME);\n-\n-                    // verify phase\n-                    externalPlainKafkaClient.verifyProducedAndConsumedMessages(\n-                        externalPlainKafkaClient.sendMessagesPlain(),\n-                        externalPlainKafkaClient.receiveMessagesPlain()\n-                    );\n-                }\n-            } else {\n-                // using internal clients\n-                if (isTlsEnabled) {\n-                    KafkaClientsResource.createAndWaitForReadiness(KafkaClientsResource.deployKafkaClients(true, kafkaClientsName + \"-tls\",\n-                        listener.getName(), kafkaUserInstance).build());\n-\n-                    final String kafkaClientsTlsPodName =\n-                        ResourceManager.kubeClient().listPodsByPrefixInName(kafkaClientsName + \"-tls\").get(0).getMetadata().getName();\n-\n-                    InternalKafkaClient internalTlsKafkaClient = new InternalKafkaClient.Builder()\n-                        .withUsingPodName(kafkaClientsTlsPodName)\n-                        .withListenerName(listener.getName())\n-                        .withTopicName(topicName)\n-                        .withNamespaceName(NAMESPACE)\n-                        .withClusterName(clusterName)\n-                        .withKafkaUsername(kafkaUsername)\n-                        .withMessageCount(MESSAGE_COUNT)\n-                        .build();\n-\n-                    LOGGER.info(\"Checking produced and consumed messages to pod:{}\", kafkaClientsTlsPodName);\n-\n-                    // verify phase\n-                    internalTlsKafkaClient.checkProducedAndConsumedMessages(\n-                        internalTlsKafkaClient.sendMessagesTls(),\n-                        internalTlsKafkaClient.receiveMessagesTls()\n-                    );\n+        // only on thread can access to verification phase (here is a lot of variables which can be modified in run-time (data-race))\n+        // TODO: move local variables to the test methods and make it more parallel (currently only kafka clusters will spin up in parallel)", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxNTA4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593815082", "bodyText": "I think it's not a good way to use the test name in the switch. Would be better some annotation/tag ?", "author": "Frawless", "createdAt": "2021-03-13T23:31:09Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/security/SecurityST.java", "diffHunk": "@@ -168,24 +177,87 @@ private static void verifyCerts(String certificate, String component) {\n         assertThat(certificate, containsString(OPENSSL_RETURN_CODE));\n     }\n \n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n+    @Tag(INTERNAL_CLIENTS_USED)\n+    @Tag(ROLLING_UPDATE)\n+    void testAutoRenewClusterCaCertsTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoRenewSomeCaCertsTriggeredByAnno(\n+                extensionContext,\n+                /* ZK node need new certs */\n+                true,\n+                /* brokers need new certs */\n+                true,\n+                /* eo needs new cert */\n+                true);\n+    }\n+\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n+    @Tag(INTERNAL_CLIENTS_USED)\n+    @Tag(ROLLING_UPDATE)\n+    void testAutoRenewClientsCaCertsTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoRenewSomeCaCertsTriggeredByAnno(\n+            extensionContext,\n+                /* no communication between clients and zk, so no need to roll */\n+                false,\n+                /* brokers need to trust client certs with new cert */\n+                true,\n+                /* eo needs to generate new client certs */\n+                true);\n+    }\n+\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n+    @Tag(ACCEPTANCE)\n+    @Tag(INTERNAL_CLIENTS_USED)\n+    @Tag(ROLLING_UPDATE)\n+    void testAutoRenewAllCaCertsTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoRenewSomeCaCertsTriggeredByAnno(\n+            extensionContext,\n+                true,\n+                true,\n+                true);\n+    }\n+\n+    @SuppressWarnings({\"checkstyle:MethodLength\", \"checkstyle:NPathComplexity\"})\n     void autoRenewSomeCaCertsTriggeredByAnno(\n-            final List<String> secretsToAnnotate,\n+            ExtensionContext extensionContext,\n             boolean zkShouldRoll,\n             boolean kafkaShouldRoll,\n             boolean eoShouldRoll) {\n \n-        createKafkaCluster();\n+        createKafkaCluster(extensionContext);\n+\n+        String kafkaClientsName = mapTestWithKafkaClientNames.get(extensionContext.getDisplayName());\n+        String clusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName());\n+        String topicName = mapTestWithTestTopics.get(extensionContext.getDisplayName());\n+        String userName = mapTestWithTestUsers.get(extensionContext.getDisplayName());\n+        List<String> secrets = null;\n+\n+        // to make it parallel we need decision maker...\n+        switch (extensionContext.getDisplayName()) {\n+            case \"testAutoRenewClusterCaCertsTriggeredByAnno\":\n+                secrets = Arrays.asList(clusterCaCertificateSecretName(clusterName));\n+                break;\n+            case \"testAutoRenewClientsCaCertsTriggeredByAnno\":\n+                secrets = Arrays.asList(clientsCaCertificateSecretName(clusterName));\n+                break;\n+            case \"testAutoRenewAllCaCertsTriggeredByAnno\":\n+                secrets = Arrays.asList(clusterCaCertificateSecretName(clusterName),\n+                    clientsCaCertificateSecretName(clusterName));\n+                break;", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxNTA5Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593815092", "bodyText": "Same as above", "author": "Frawless", "createdAt": "2021-03-13T23:31:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/security/SecurityST.java", "diffHunk": "@@ -286,62 +362,79 @@ void autoRenewSomeCaCertsTriggeredByAnno(\n         }\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(INTERNAL_CLIENTS_USED)\n     @Tag(ROLLING_UPDATE)\n-    void testAutoRenewClusterCaCertsTriggeredByAnno() {\n-        autoRenewSomeCaCertsTriggeredByAnno(asList(\n-                clusterCaCertificateSecretName(clusterName)),\n-                /* ZK node need new certs */\n+    void testAutoReplaceClusterCaKeysTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoReplaceSomeKeysTriggeredByAnno(\n+            extensionContext,\n                 true,\n-                /* brokers need new certs */\n                 true,\n-                /* eo needs new cert */\n                 true);\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(INTERNAL_CLIENTS_USED)\n     @Tag(ROLLING_UPDATE)\n-    void testAutoRenewClientsCaCertsTriggeredByAnno() {\n-        autoRenewSomeCaCertsTriggeredByAnno(asList(\n-                clientsCaCertificateSecretName(clusterName)),\n-                /* no communication between clients and zk, so no need to roll */\n+    void testAutoReplaceClientsCaKeysTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoReplaceSomeKeysTriggeredByAnno(\n+            extensionContext,\n                 false,\n-                /* brokers need to trust client certs with new cert */\n                 true,\n-                /* eo needs to generate new client certs */\n                 true);\n     }\n \n-    @Test\n-    @Tag(ACCEPTANCE)\n+    @ParallelTest\n     @Tag(INTERNAL_CLIENTS_USED)\n     @Tag(ROLLING_UPDATE)\n-    void testAutoRenewAllCaCertsTriggeredByAnno() {\n-        autoRenewSomeCaCertsTriggeredByAnno(asList(\n-                clusterCaCertificateSecretName(clusterName),\n-                clientsCaCertificateSecretName(clusterName)),\n+    void testAutoReplaceAllCaKeysTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoReplaceSomeKeysTriggeredByAnno(\n+            extensionContext,\n                 true,\n                 true,\n                 true);\n     }\n \n-    void autoReplaceSomeKeysTriggeredByAnno(final List<String> secrets,\n+    @SuppressWarnings({\"checkstyle:MethodLength\", \"checkstyle:NPathComplexity\"})\n+    void autoReplaceSomeKeysTriggeredByAnno(ExtensionContext extensionContext,\n                                             boolean zkShouldRoll,\n                                             boolean kafkaShouldRoll,\n                                             boolean eoShouldRoll) {\n-        createKafkaCluster();\n \n-        KafkaUser user = KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.tlsUser(clusterName, KafkaUserUtils.generateRandomNameOfKafkaUser()).build());\n+\n+        String clusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName());\n+        String kafkaClientsName = mapTestWithKafkaClientNames.get(extensionContext.getDisplayName());\n+        List<String> secrets = null;\n+\n+        // to make it parallel we need decision maker...\n+        switch (extensionContext.getDisplayName()) {\n+            case \"testAutoReplaceClusterCaKeysTriggeredByAnno\":\n+                secrets = Arrays.asList(clusterCaKeySecretName(clusterName));\n+                break;\n+            case \"testAutoReplaceClientsCaKeysTriggeredByAnno\":\n+                secrets = Arrays.asList(clientsCaKeySecretName(clusterName));\n+                break;\n+            case \"testAutoReplaceAllCaKeysTriggeredByAnno\":\n+                secrets = Arrays.asList(clusterCaKeySecretName(clusterName),\n+                    clientsCaKeySecretName(clusterName));\n+                break;", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgxNTA5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r593815096", "bodyText": "better log message", "author": "Frawless", "createdAt": "2021-03-13T23:31:36Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/security/SecurityST.java", "diffHunk": "@@ -286,62 +362,79 @@ void autoRenewSomeCaCertsTriggeredByAnno(\n         }\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(INTERNAL_CLIENTS_USED)\n     @Tag(ROLLING_UPDATE)\n-    void testAutoRenewClusterCaCertsTriggeredByAnno() {\n-        autoRenewSomeCaCertsTriggeredByAnno(asList(\n-                clusterCaCertificateSecretName(clusterName)),\n-                /* ZK node need new certs */\n+    void testAutoReplaceClusterCaKeysTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoReplaceSomeKeysTriggeredByAnno(\n+            extensionContext,\n                 true,\n-                /* brokers need new certs */\n                 true,\n-                /* eo needs new cert */\n                 true);\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(INTERNAL_CLIENTS_USED)\n     @Tag(ROLLING_UPDATE)\n-    void testAutoRenewClientsCaCertsTriggeredByAnno() {\n-        autoRenewSomeCaCertsTriggeredByAnno(asList(\n-                clientsCaCertificateSecretName(clusterName)),\n-                /* no communication between clients and zk, so no need to roll */\n+    void testAutoReplaceClientsCaKeysTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoReplaceSomeKeysTriggeredByAnno(\n+            extensionContext,\n                 false,\n-                /* brokers need to trust client certs with new cert */\n                 true,\n-                /* eo needs to generate new client certs */\n                 true);\n     }\n \n-    @Test\n-    @Tag(ACCEPTANCE)\n+    @ParallelTest\n     @Tag(INTERNAL_CLIENTS_USED)\n     @Tag(ROLLING_UPDATE)\n-    void testAutoRenewAllCaCertsTriggeredByAnno() {\n-        autoRenewSomeCaCertsTriggeredByAnno(asList(\n-                clusterCaCertificateSecretName(clusterName),\n-                clientsCaCertificateSecretName(clusterName)),\n+    void testAutoReplaceAllCaKeysTriggeredByAnno(ExtensionContext extensionContext) {\n+        autoReplaceSomeKeysTriggeredByAnno(\n+            extensionContext,\n                 true,\n                 true,\n                 true);\n     }\n \n-    void autoReplaceSomeKeysTriggeredByAnno(final List<String> secrets,\n+    @SuppressWarnings({\"checkstyle:MethodLength\", \"checkstyle:NPathComplexity\"})\n+    void autoReplaceSomeKeysTriggeredByAnno(ExtensionContext extensionContext,\n                                             boolean zkShouldRoll,\n                                             boolean kafkaShouldRoll,\n                                             boolean eoShouldRoll) {\n-        createKafkaCluster();\n \n-        KafkaUser user = KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.tlsUser(clusterName, KafkaUserUtils.generateRandomNameOfKafkaUser()).build());\n+\n+        String clusterName = mapTestWithClusterNames.get(extensionContext.getDisplayName());\n+        String kafkaClientsName = mapTestWithKafkaClientNames.get(extensionContext.getDisplayName());\n+        List<String> secrets = null;\n+\n+        // to make it parallel we need decision maker...\n+        switch (extensionContext.getDisplayName()) {\n+            case \"testAutoReplaceClusterCaKeysTriggeredByAnno\":\n+                secrets = Arrays.asList(clusterCaKeySecretName(clusterName));\n+                break;\n+            case \"testAutoReplaceClientsCaKeysTriggeredByAnno\":\n+                secrets = Arrays.asList(clientsCaKeySecretName(clusterName));\n+                break;\n+            case \"testAutoReplaceAllCaKeysTriggeredByAnno\":\n+                secrets = Arrays.asList(clusterCaKeySecretName(clusterName),\n+                    clientsCaKeySecretName(clusterName));\n+                break;\n+        }\n+\n+        LOGGER.info(\"-----{} has {} secrets\", extensionContext.getDisplayName(), secrets.toString());", "originalCommit": "84d92ffdf285ed46502e4dcb6d082122946a992d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "91bc3059ec735ae0167905fdb505866c856ad8fa", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/91bc3059ec735ae0167905fdb505866c856ad8fa", "message": "last rebase..?\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-17T21:23:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0NDkyNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597244925", "bodyText": "Does this return immediately? The call to waitForResourceStatus suggests it is not. And that means it should be named accordingly I think!", "author": "scholzj", "createdAt": "2021-03-18T21:10:54Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java", "diffHunk": "@@ -5,124 +5,51 @@\n package io.strimzi.systemtest.resources.crd;\n \n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaBridgeList;\n import io.strimzi.api.kafka.model.KafkaBridge;\n-import io.strimzi.api.kafka.model.KafkaBridgeBuilder;\n-import io.strimzi.api.kafka.model.KafkaBridgeResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.enums.CustomResourceStatus;\n+import io.strimzi.systemtest.resources.ResourceType;\n import io.strimzi.systemtest.resources.ResourceManager;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n+public class KafkaBridgeResource implements ResourceType<KafkaBridge> {\n \n-public class KafkaBridgeResource {\n+    public KafkaBridgeResource() { }\n \n-    public static final String PATH_TO_KAFKA_BRIDGE_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/bridge/kafka-bridge.yaml\";\n-    public static final String PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/metrics/kafka-bridge-metrics.yaml\";\n-\n-    public static MixedOperation<KafkaBridge, KafkaBridgeList, Resource<KafkaBridge>> kafkaBridgeClient() {\n-        return Crds.kafkaBridgeOperation(ResourceManager.kubeClient().getClient());\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String bootstrap, int kafkaBridgeReplicas) {\n-        return kafkaBridge(name, name, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridgeWithCors(name, name, bootstrap, kafkaBridgeReplicas, allowedCorsOrigin, allowedCorsMethods);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridge(name, clusterName, bootstrap, kafkaBridgeReplicas)\n-            .editSpec()\n-                .editHttp()\n-                    .withNewCors()\n-                        .withAllowedOrigins(allowedCorsOrigin)\n-                        .withAllowedMethods(allowedCorsMethods != null ? allowedCorsMethods : \"GET,POST,PUT,DELETE,OPTIONS,PATCH\")\n-                    .endCors()\n-                .endHttp()\n-            .endSpec();\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap) {\n-        return kafkaBridgeWithMetrics(name, clusterName, bootstrap, 1);\n+    @Override\n+    public String getKind() {\n+        return KafkaBridge.RESOURCE_KIND;\n     }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG);\n-\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+    @Override\n+    public KafkaBridge get(String namespace, String name) {\n+        return kafkaBridgeClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    private static KafkaBridgeBuilder defaultKafkaBridge(KafkaBridge kafkaBridge, String name, String kafkaClusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        return new KafkaBridgeBuilder(kafkaBridge)\n-            .withNewMetadata()\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .withClusterName(kafkaClusterName)\n-            .endMetadata()\n-            .editSpec()\n-                .withBootstrapServers(bootstrap)\n-                .withReplicas(kafkaBridgeReplicas)\n-                .withNewInlineLogging()\n-                    .addToLoggers(\"bridge.root.logger\", Environment.STRIMZI_COMPONENTS_LOG_LEVEL)\n-                .endInlineLogging()\n-            .endSpec();\n+    @Override\n+    public void create(KafkaBridge resource) {\n+        kafkaBridgeClient().inNamespace(resource.getMetadata().getNamespace()).createOrReplace(resource);\n     }\n-\n-    public static KafkaBridge createAndWaitForReadiness(KafkaBridge kafkaBridge) {\n-        KubernetesResource.deployNetworkPolicyForResource(kafkaBridge, KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-\n-        TestUtils.waitFor(\"KafkaBridge creation\", Constants.POLL_INTERVAL_FOR_RESOURCE_CREATION, CR_CREATION_TIMEOUT,\n-            () -> {\n-                try {\n-                    kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaBridge);\n-                    return true;\n-                } catch (KubernetesClientException e) {\n-                    if (e.getMessage().contains(\"object is being deleted\")) {\n-                        return false;\n-                    } else {\n-                        throw e;\n-                    }\n-                }\n-            }\n-        );\n-        return waitFor(deleteLater(kafkaBridge));\n+    @Override\n+    public void delete(KafkaBridge resource) throws Exception {\n+        kafkaBridgeClient().inNamespace(resource.getMetadata().getNamespace()).withName(\n+            resource.getMetadata().getName()).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n     }\n-\n-    public static KafkaBridge kafkaBridgeWithoutWait(KafkaBridge kafkaBridge) {\n-        kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaBridge);\n-        return kafkaBridge;\n+    @Override\n+    public boolean isReady(KafkaBridge resource) {", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI2ODcwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597268703", "bodyText": "I have changed it to -> waitForReadiness", "author": "see-quick", "createdAt": "2021-03-18T21:55:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0NDkyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0NTM5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597245398", "bodyText": "Same as other classes - from the name it should be clear whether it returns immediately or wait ... but the code here suggests it waits.", "author": "scholzj", "createdAt": "2021-03-18T21:11:50Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java", "diffHunk": "@@ -4,241 +4,53 @@\n  */\n package io.strimzi.systemtest.resources.crd;\n \n-import io.fabric8.kubernetes.api.model.ContainerBuilder;\n-import io.fabric8.kubernetes.api.model.PodSpec;\n-import io.fabric8.kubernetes.api.model.PodSpecBuilder;\n-import io.fabric8.kubernetes.api.model.Quantity;\n-import io.fabric8.kubernetes.api.model.ResourceRequirementsBuilder;\n-import io.fabric8.kubernetes.api.model.Secret;\n import io.fabric8.kubernetes.api.model.apps.Deployment;\n-import io.fabric8.kubernetes.api.model.apps.DeploymentBuilder;\n-import io.strimzi.api.kafka.model.KafkaUser;\n-import io.strimzi.api.kafka.model.KafkaUserScramSha512ClientAuthentication;\n-import io.strimzi.api.kafka.model.KafkaUserTlsClientAuthentication;\n-import io.strimzi.operator.common.model.Labels;\n import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n import io.strimzi.systemtest.resources.ResourceManager;\n-import org.apache.kafka.clients.consumer.ConsumerConfig;\n-import org.apache.kafka.clients.producer.ProducerConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.utils.ClientUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.nio.charset.Charset;\n-import java.util.Base64;\n-import java.util.Collections;\n-import java.util.Map;\n-\n-import static io.strimzi.test.TestUtils.toYamlString;\n-\n-public class KafkaClientsResource {\n+public class KafkaClientsResource implements ResourceType<Deployment> {\n \n     private static final Logger LOGGER = LogManager.getLogger(KafkaClientsResource.class);\n \n-    public static DeploymentBuilder deployKafkaClients(String kafkaClusterName) {\n-        return deployKafkaClients(false, kafkaClusterName, null);\n-    }\n+    public KafkaClientsResource() {}\n \n-    public static DeploymentBuilder deployKafkaClients(boolean tlsListener, String kafkaClientsName, KafkaUser... kafkaUsers) {\n-        return deployKafkaClients(tlsListener, kafkaClientsName, true,  null, null, kafkaUsers);\n+    @Override\n+    public String getKind() {\n+        return Constants.DEPLOYMENT;\n     }\n-\n-    public static DeploymentBuilder deployKafkaClients(boolean tlsListener, String kafkaClientsName, String listenerName,\n-                                                        KafkaUser... kafkaUsers) {\n-        return deployKafkaClients(tlsListener, kafkaClientsName, true, listenerName, null, kafkaUsers);\n+    @Override\n+    public Deployment get(String namespace, String name) {\n+        String deploymentName = ResourceManager.kubeClient().namespace(namespace).getDeploymentNameByPrefix(name);\n+        return deploymentName != null ?  ResourceManager.kubeClient().getDeployment(deploymentName) : null;\n     }\n \n-    public static DeploymentBuilder deployKafkaClients(boolean tlsListener, String kafkaClientsName, boolean hostnameVerification,\n-                                                        KafkaUser... kafkaUsers) {\n-        return deployKafkaClients(tlsListener, kafkaClientsName, hostnameVerification, null, null, kafkaUsers);\n+    @Override\n+    public void create(Deployment resource) {\n+        ResourceManager.kubeClient().createOrReplaceDeployment(resource);\n     }\n-\n-    public static DeploymentBuilder deployKafkaClients(boolean tlsListener, String kafkaClientsName, boolean hostnameVerification,\n-                                                        String listenerName, String secretPrefix, KafkaUser... kafkaUsers) {\n-        Map<String, String> label = Collections.singletonMap(Constants.KAFKA_CLIENTS_LABEL_KEY, Constants.KAFKA_CLIENTS_LABEL_VALUE);\n-        return new DeploymentBuilder()\n-            .withNewMetadata()\n-                .withName(kafkaClientsName)\n-                .withLabels(label)\n-            .endMetadata()\n-            .withNewSpec()\n-                .withNewSelector()\n-                    .addToMatchLabels(\"app\", kafkaClientsName)\n-                    .addToMatchLabels(label)\n-                .endSelector()\n-                .withReplicas(1)\n-                .withNewTemplate()\n-                    .withNewMetadata()\n-                        .addToLabels(\"app\", kafkaClientsName)\n-                        .addToLabels(label)\n-                    .endMetadata()\n-                    .withSpec(createClientSpec(tlsListener, kafkaClientsName, hostnameVerification, listenerName, secretPrefix, kafkaUsers))\n-                .endTemplate()\n-            .endSpec();\n+    @Override\n+    public void delete(Deployment resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteDeployment(resource.getMetadata().getName());\n     }\n \n-    private static PodSpec createClientSpec(boolean tlsListener, String kafkaClientsName, boolean hostnameVerification,\n-                                            String listenerName, String secretPrefix, KafkaUser... kafkaUsers) {\n-        PodSpecBuilder podSpecBuilder = new PodSpecBuilder();\n-        ContainerBuilder containerBuilder = new ContainerBuilder()\n-            .withName(kafkaClientsName)\n-            .withImage(Environment.TEST_CLIENT_IMAGE)\n-            .withCommand(\"sleep\")\n-            .withArgs(\"infinity\")\n-            .withImagePullPolicy(Environment.COMPONENTS_IMAGE_PULL_POLICY);\n-\n-        String producerConfiguration = ProducerConfig.ACKS_CONFIG + \"=all\\n\";\n-        String consumerConfiguration = ConsumerConfig.AUTO_OFFSET_RESET_CONFIG + \"=earliest\\n\";\n-\n-        if (kafkaUsers == null) {\n-            containerBuilder.addNewEnv().withName(\"PRODUCER_CONFIGURATION\").withValue(producerConfiguration).endEnv();\n-            containerBuilder.addNewEnv().withName(\"CONSUMER_CONFIGURATION\").withValue(consumerConfiguration).endEnv();\n-\n-        } else {\n-            for (KafkaUser kafkaUser : kafkaUsers) {\n-                String kafkaUserName = secretPrefix == null ? kafkaUser.getMetadata().getName() : secretPrefix + kafkaUser.getMetadata().getName();\n-                boolean tlsUser = kafkaUser.getSpec() != null && kafkaUser.getSpec().getAuthentication() instanceof KafkaUserTlsClientAuthentication;\n-                boolean scramShaUser = kafkaUser.getSpec() != null && kafkaUser.getSpec().getAuthentication() instanceof KafkaUserScramSha512ClientAuthentication;\n-\n-                containerBuilder.addNewEnv().withName(\"PRODUCER_CONFIGURATION\").withValue(producerConfiguration).endEnv();\n-                containerBuilder.addNewEnv().withName(\"CONSUMER_CONFIGURATION\").withValue(consumerConfiguration).endEnv();\n-\n-                String envVariablesSuffix = String.format(\"_%s\", kafkaUserName.replace(\"-\", \"_\"));\n-                containerBuilder.addNewEnv().withName(\"KAFKA_USER\" + envVariablesSuffix).withValue(kafkaUserName).endEnv();\n-\n-                if (tlsListener) {\n-                    if (scramShaUser) {\n-                        producerConfiguration += \"security.protocol=SASL_SSL\\n\";\n-                        producerConfiguration += saslConfigs(kafkaUser, secretPrefix);\n-                        consumerConfiguration += \"security.protocol=SASL_SSL\\n\";\n-                        consumerConfiguration += saslConfigs(kafkaUser, secretPrefix);\n-                    } else {\n-                        producerConfiguration += \"security.protocol=SSL\\n\";\n-                        consumerConfiguration += \"security.protocol=SSL\\n\";\n-                    }\n-                    producerConfiguration +=\n-                        SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG + \"=/tmp/\" + kafkaUserName + \"-truststore.p12\\n\" +\n-                            SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG + \"=pkcs12\\n\";\n-                    consumerConfiguration +=\n-                        SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG + \"=/tmp/\" + kafkaUserName + \"-truststore.p12\\n\" +\n-                            SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG + \"=pkcs12\\n\";\n-                } else {\n-                    if (scramShaUser) {\n-                        producerConfiguration += \"security.protocol=SASL_PLAINTEXT\\n\";\n-                        producerConfiguration += saslConfigs(kafkaUser, secretPrefix);\n-                        consumerConfiguration += \"security.protocol=SASL_PLAINTEXT\\n\";\n-                        consumerConfiguration += saslConfigs(kafkaUser, secretPrefix);\n-                    } else {\n-                        producerConfiguration += \"security.protocol=PLAINTEXT\\n\";\n-                        consumerConfiguration += \"security.protocol=PLAINTEXT\\n\";\n-                    }\n-                }\n-\n-                if (tlsUser) {\n-                    producerConfiguration +=\n-                        SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG + \"=/tmp/\" + kafkaUserName + \"-keystore.p12\\n\" +\n-                            SslConfigs.SSL_KEYSTORE_TYPE_CONFIG + \"=pkcs12\\n\";\n-                    consumerConfiguration +=\n-                        SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG + \"=/tmp/\" + kafkaUserName + \"-keystore.p12\\n\" +\n-                            SslConfigs.SSL_KEYSTORE_TYPE_CONFIG + \"=pkcs12\\n\";\n-\n-                    containerBuilder.addNewEnv().withName(\"PRODUCER_TLS\" + envVariablesSuffix).withValue(\"TRUE\").endEnv()\n-                        .addNewEnv().withName(\"CONSUMER_TLS\" + envVariablesSuffix).withValue(\"TRUE\").endEnv();\n+    @Override\n+    public boolean isReady(Deployment resource) {", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0NTc5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597245796", "bodyText": "Same as the other comments. I will not mention it anymore, but it should be fixed everywhere.", "author": "scholzj", "createdAt": "2021-03-18T21:12:38Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectResource.java", "diffHunk": "@@ -4,133 +4,55 @@\n  */\n package io.strimzi.systemtest.resources.crd;\n \n-import io.fabric8.kubernetes.api.model.ConfigMap;\n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaConnectList;\n-import io.strimzi.api.kafka.model.CertSecretSourceBuilder;\n import io.strimzi.api.kafka.model.KafkaConnect;\n-import io.strimzi.api.kafka.model.KafkaConnectBuilder;\n-import io.strimzi.api.kafka.model.KafkaConnectResources;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaConnectUtils;\n import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.test.k8s.KubeClusterResource;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+public class KafkaConnectResource implements ResourceType<KafkaConnect> {\n \n-public class KafkaConnectResource {\n-    public static final String PATH_TO_KAFKA_CONNECT_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/connect/kafka-connect.yaml\";\n+    public KafkaConnectResource() { }\n \n-    public static MixedOperation<KafkaConnect, KafkaConnectList, Resource<KafkaConnect>> kafkaConnectClient() {\n-        return Crds.kafkaConnectOperation(ResourceManager.kubeClient().getClient());\n+    @Override\n+    public String getKind() {\n+        return KafkaConnect.RESOURCE_KIND;\n     }\n-\n-    public static KafkaConnectBuilder kafkaConnect(String name, int kafkaConnectReplicas) {\n-        return kafkaConnect(name, name, kafkaConnectReplicas);\n+    @Override\n+    public KafkaConnect get(String namespace, String name) {\n+        return kafkaConnectClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    public static KafkaConnectBuilder kafkaConnect(String name, String clusterName, int kafkaConnectReplicas) {\n-        KafkaConnect kafkaConnect = getKafkaConnectFromYaml(PATH_TO_KAFKA_CONNECT_CONFIG);\n-        return defaultKafkaConnect(kafkaConnect, name, clusterName, kafkaConnectReplicas);\n+    @Override\n+    public void create(KafkaConnect resource) {\n+        kafkaConnectClient().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).createOrReplace(resource);\n     }\n-\n-    public static KafkaConnectBuilder kafkaConnectWithMetrics(String name, int kafkaConnectReplicas) {\n-        return kafkaConnectWithMetrics(name, name, kafkaConnectReplicas);\n+    @Override\n+    public void delete(KafkaConnect resource) throws Exception {\n+        kafkaConnectClient().inNamespace(resource.getMetadata().getNamespace()).withName(\n+            resource.getMetadata().getName()).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n     }\n-\n-    public static KafkaConnectBuilder kafkaConnectWithMetrics(String name, String clusterName, int kafkaConnectReplicas) {\n-        KafkaConnect kafkaConnect = getKafkaConnectFromYaml(Constants.PATH_TO_KAFKA_CONNECT_METRICS_CONFIG);\n-\n-        ConfigMap metricsCm = TestUtils.configMapFromYaml(Constants.PATH_TO_KAFKA_CONNECT_METRICS_CONFIG, \"connect-metrics\");\n-        KubeClusterResource.kubeClient().getClient().configMaps().inNamespace(kubeClient().getNamespace()).createOrReplace(metricsCm);\n-\n-        return defaultKafkaConnect(kafkaConnect, name, clusterName, kafkaConnectReplicas);\n-    }\n-\n-    private static KafkaConnectBuilder defaultKafkaConnect(KafkaConnect kafkaConnect, String name, String kafkaClusterName, int kafkaConnectReplicas) {\n-        return new KafkaConnectBuilder(kafkaConnect)\n-            .withNewMetadata()\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .withClusterName(kafkaClusterName)\n-            .endMetadata()\n-            .editOrNewSpec()\n-                .withVersion(Environment.ST_KAFKA_VERSION)\n-                .withBootstrapServers(KafkaResources.tlsBootstrapAddress(kafkaClusterName))\n-                .withReplicas(kafkaConnectReplicas)\n-                .withNewTls()\n-                    .withTrustedCertificates(new CertSecretSourceBuilder().withNewSecretName(kafkaClusterName + \"-cluster-ca-cert\").withCertificate(\"ca.crt\").build())\n-                .endTls()\n-                .addToConfig(\"group.id\", KafkaConnectResources.deploymentName(kafkaClusterName))\n-                .addToConfig(\"offset.storage.topic\", KafkaConnectResources.configStorageTopicOffsets(kafkaClusterName))\n-                .addToConfig(\"config.storage.topic\", KafkaConnectResources.metricsAndLogConfigMapName(kafkaClusterName))\n-                .addToConfig(\"status.storage.topic\", KafkaConnectResources.configStorageTopicStatus(kafkaClusterName))\n-                .withNewInlineLogging()\n-                    .addToLoggers(\"connect.root.logger.level\", Environment.STRIMZI_COMPONENTS_LOG_LEVEL)\n-                .endInlineLogging()\n-            .endSpec();\n+    @Override\n+    public boolean isReady(KafkaConnect resource) {", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0ODY3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597248676", "bodyText": "How does this work? I can see from the JavaDoc that it should replace a resource in Kubernetes. But I do not see it done here. It seems to only update one other resource without the Kube API being involved.\nI also did not find any use of this method.", "author": "scholzj", "createdAt": "2021-03-18T21:17:43Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java", "diffHunk": "@@ -5,124 +5,51 @@\n package io.strimzi.systemtest.resources.crd;\n \n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaBridgeList;\n import io.strimzi.api.kafka.model.KafkaBridge;\n-import io.strimzi.api.kafka.model.KafkaBridgeBuilder;\n-import io.strimzi.api.kafka.model.KafkaBridgeResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.enums.CustomResourceStatus;\n+import io.strimzi.systemtest.resources.ResourceType;\n import io.strimzi.systemtest.resources.ResourceManager;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n+public class KafkaBridgeResource implements ResourceType<KafkaBridge> {\n \n-public class KafkaBridgeResource {\n+    public KafkaBridgeResource() { }\n \n-    public static final String PATH_TO_KAFKA_BRIDGE_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/bridge/kafka-bridge.yaml\";\n-    public static final String PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/metrics/kafka-bridge-metrics.yaml\";\n-\n-    public static MixedOperation<KafkaBridge, KafkaBridgeList, Resource<KafkaBridge>> kafkaBridgeClient() {\n-        return Crds.kafkaBridgeOperation(ResourceManager.kubeClient().getClient());\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String bootstrap, int kafkaBridgeReplicas) {\n-        return kafkaBridge(name, name, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridgeWithCors(name, name, bootstrap, kafkaBridgeReplicas, allowedCorsOrigin, allowedCorsMethods);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridge(name, clusterName, bootstrap, kafkaBridgeReplicas)\n-            .editSpec()\n-                .editHttp()\n-                    .withNewCors()\n-                        .withAllowedOrigins(allowedCorsOrigin)\n-                        .withAllowedMethods(allowedCorsMethods != null ? allowedCorsMethods : \"GET,POST,PUT,DELETE,OPTIONS,PATCH\")\n-                    .endCors()\n-                .endHttp()\n-            .endSpec();\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap) {\n-        return kafkaBridgeWithMetrics(name, clusterName, bootstrap, 1);\n+    @Override\n+    public String getKind() {\n+        return KafkaBridge.RESOURCE_KIND;\n     }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG);\n-\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+    @Override\n+    public KafkaBridge get(String namespace, String name) {\n+        return kafkaBridgeClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    private static KafkaBridgeBuilder defaultKafkaBridge(KafkaBridge kafkaBridge, String name, String kafkaClusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        return new KafkaBridgeBuilder(kafkaBridge)\n-            .withNewMetadata()\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .withClusterName(kafkaClusterName)\n-            .endMetadata()\n-            .editSpec()\n-                .withBootstrapServers(bootstrap)\n-                .withReplicas(kafkaBridgeReplicas)\n-                .withNewInlineLogging()\n-                    .addToLoggers(\"bridge.root.logger\", Environment.STRIMZI_COMPONENTS_LOG_LEVEL)\n-                .endInlineLogging()\n-            .endSpec();\n+    @Override\n+    public void create(KafkaBridge resource) {\n+        kafkaBridgeClient().inNamespace(resource.getMetadata().getNamespace()).createOrReplace(resource);\n     }\n-\n-    public static KafkaBridge createAndWaitForReadiness(KafkaBridge kafkaBridge) {\n-        KubernetesResource.deployNetworkPolicyForResource(kafkaBridge, KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-\n-        TestUtils.waitFor(\"KafkaBridge creation\", Constants.POLL_INTERVAL_FOR_RESOURCE_CREATION, CR_CREATION_TIMEOUT,\n-            () -> {\n-                try {\n-                    kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaBridge);\n-                    return true;\n-                } catch (KubernetesClientException e) {\n-                    if (e.getMessage().contains(\"object is being deleted\")) {\n-                        return false;\n-                    } else {\n-                        throw e;\n-                    }\n-                }\n-            }\n-        );\n-        return waitFor(deleteLater(kafkaBridge));\n+    @Override\n+    public void delete(KafkaBridge resource) throws Exception {\n+        kafkaBridgeClient().inNamespace(resource.getMetadata().getNamespace()).withName(\n+            resource.getMetadata().getName()).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n     }\n-\n-    public static KafkaBridge kafkaBridgeWithoutWait(KafkaBridge kafkaBridge) {\n-        kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaBridge);\n-        return kafkaBridge;\n+    @Override\n+    public boolean isReady(KafkaBridge resource) {\n+        return ResourceManager.waitForResourceStatus(kafkaBridgeClient(), resource, CustomResourceStatus.Ready);\n     }\n-\n-    public static void deleteKafkaBridgeWithoutWait(String resourceName) {\n-        kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).withName(resourceName).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n+    @Override\n+    public void replaceResource(KafkaBridge existing, KafkaBridge newResource) {", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI3NDM1Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597274357", "bodyText": "synchronized (this) {\n                    T updated = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n                    type.replaceResource(resource, updated);\n                }\n\nhere :)", "author": "see-quick", "createdAt": "2021-03-18T22:06:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0ODY3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI4MjY2NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597282665", "bodyText": "Link to the file would be helpful to understand what the whole code is doing.", "author": "scholzj", "createdAt": "2021-03-18T22:24:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0ODY3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NDU2MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597254560", "bodyText": "Does this really run with the System Tests? Should this really be ParallelTest?", "author": "scholzj", "createdAt": "2021-03-18T21:28:43Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/utils/KafkaVersionUtilsTest.java", "diffHunk": "@@ -4,15 +4,17 @@\n  */\n package io.strimzi.systemtest.utils;\n \n-import org.junit.jupiter.api.Test;\n+import io.strimzi.systemtest.annotations.ParallelSuite;\n+import io.strimzi.systemtest.annotations.ParallelTest;\n \n import java.util.List;\n \n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n+@ParallelSuite\n public class KafkaVersionUtilsTest {\n \n-    @Test", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI3MTkzMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597271931", "bodyText": "Yes and yes it's ok that this test is parallel (by default all tests are serial)", "author": "see-quick", "createdAt": "2021-03-18T22:01:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NDU2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzIxNjUyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597216523", "bodyText": "Just a reminder: After #4596 will be merged, revert these changes.", "author": "im-konge", "createdAt": "2021-03-18T20:23:21Z", "path": "api-conversion/src/test/java/io/strimzi/kafka/api/conversion/cli/CrdUpgradeCommandIT.java", "diffHunk": "@@ -74,7 +75,7 @@ public void testCrdUpgradeWithConvertedResources() {\n                             .endEphemeralStorage()\n                             .withNewJmxPrometheusExporterMetricsConfig()\n                                 .withNewValueFrom()\n-                                    .withNewConfigMapKeyRef(\"zoo-metrics\", \"metrics-cm\", false)\n+                                    .withConfigMapKeyRef(new ConfigMapKeySelectorBuilder().withName(\"zoo-metrics\").withKey(\"metrics-cm\").withOptional(false).build())", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzMwNjY0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597306643", "bodyText": "Maybe I will be quicker :P", "author": "see-quick", "createdAt": "2021-03-18T23:23:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzIxNjUyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzIxODQyOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597218429", "bodyText": "Still leftover \ud83d\ude04", "author": "im-konge", "createdAt": "2021-03-18T20:26:30Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/Environment.java", "diffHunk": "@@ -128,7 +128,7 @@\n     private static final String STRIMZI_LOG_LEVEL_DEFAULT = \"DEBUG\";\n     private static final String STRIMZI_COMPONENTS_LOG_LEVEL_DEFAULT = \"INFO\";\n     static final String KUBERNETES_DOMAIN_DEFAULT = \".nip.io\";\n-    public static final String COMPONENTS_IMAGE_PULL_POLICY_ENV_DEFAULT = Constants.IF_NOT_PRESENT_IMAGE_PULL_POLICY;\n+    public static final String COMPONENTS_IMAGE_PULL_POLICY_ENV_DEFAULT = Constants.ALWAYS_IMAGE_PULL_POLICY;", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzIzMjI1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597232259", "bodyText": "Will we ever replace status? I guess that's only thing that we cannot set.", "author": "im-konge", "createdAt": "2021-03-18T20:49:23Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java", "diffHunk": "@@ -5,124 +5,51 @@\n package io.strimzi.systemtest.resources.crd;\n \n import io.fabric8.kubernetes.api.model.DeletionPropagation;\n-import io.fabric8.kubernetes.client.KubernetesClientException;\n import io.fabric8.kubernetes.client.dsl.MixedOperation;\n import io.fabric8.kubernetes.client.dsl.Resource;\n import io.strimzi.api.kafka.Crds;\n import io.strimzi.api.kafka.KafkaBridgeList;\n import io.strimzi.api.kafka.model.KafkaBridge;\n-import io.strimzi.api.kafka.model.KafkaBridgeBuilder;\n-import io.strimzi.api.kafka.model.KafkaBridgeResources;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.KubernetesResource;\n-import io.strimzi.test.TestUtils;\n+import io.strimzi.systemtest.enums.CustomResourceStatus;\n+import io.strimzi.systemtest.resources.ResourceType;\n import io.strimzi.systemtest.resources.ResourceManager;\n \n import java.util.function.Consumer;\n \n-import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n+public class KafkaBridgeResource implements ResourceType<KafkaBridge> {\n \n-public class KafkaBridgeResource {\n+    public KafkaBridgeResource() { }\n \n-    public static final String PATH_TO_KAFKA_BRIDGE_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/bridge/kafka-bridge.yaml\";\n-    public static final String PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG = TestUtils.USER_PATH + \"/../packaging/examples/metrics/kafka-bridge-metrics.yaml\";\n-\n-    public static MixedOperation<KafkaBridge, KafkaBridgeList, Resource<KafkaBridge>> kafkaBridgeClient() {\n-        return Crds.kafkaBridgeOperation(ResourceManager.kubeClient().getClient());\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String bootstrap, int kafkaBridgeReplicas) {\n-        return kafkaBridge(name, name, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridge(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_CONFIG);\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridgeWithCors(name, name, bootstrap, kafkaBridgeReplicas, allowedCorsOrigin, allowedCorsMethods);\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithCors(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas, String allowedCorsOrigin, String allowedCorsMethods) {\n-        return kafkaBridge(name, clusterName, bootstrap, kafkaBridgeReplicas)\n-            .editSpec()\n-                .editHttp()\n-                    .withNewCors()\n-                        .withAllowedOrigins(allowedCorsOrigin)\n-                        .withAllowedMethods(allowedCorsMethods != null ? allowedCorsMethods : \"GET,POST,PUT,DELETE,OPTIONS,PATCH\")\n-                    .endCors()\n-                .endHttp()\n-            .endSpec();\n-    }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap) {\n-        return kafkaBridgeWithMetrics(name, clusterName, bootstrap, 1);\n+    @Override\n+    public String getKind() {\n+        return KafkaBridge.RESOURCE_KIND;\n     }\n-\n-    public static KafkaBridgeBuilder kafkaBridgeWithMetrics(String name, String clusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        KafkaBridge kafkaBridge = getKafkaBridgeFromYaml(PATH_TO_KAFKA_BRIDGE_METRICS_CONFIG);\n-\n-        return defaultKafkaBridge(kafkaBridge, name, clusterName, bootstrap, kafkaBridgeReplicas);\n+    @Override\n+    public KafkaBridge get(String namespace, String name) {\n+        return kafkaBridgeClient().inNamespace(namespace).withName(name).get();\n     }\n-\n-    private static KafkaBridgeBuilder defaultKafkaBridge(KafkaBridge kafkaBridge, String name, String kafkaClusterName, String bootstrap, int kafkaBridgeReplicas) {\n-        return new KafkaBridgeBuilder(kafkaBridge)\n-            .withNewMetadata()\n-                .withName(name)\n-                .withNamespace(ResourceManager.kubeClient().getNamespace())\n-                .withClusterName(kafkaClusterName)\n-            .endMetadata()\n-            .editSpec()\n-                .withBootstrapServers(bootstrap)\n-                .withReplicas(kafkaBridgeReplicas)\n-                .withNewInlineLogging()\n-                    .addToLoggers(\"bridge.root.logger\", Environment.STRIMZI_COMPONENTS_LOG_LEVEL)\n-                .endInlineLogging()\n-            .endSpec();\n+    @Override\n+    public void create(KafkaBridge resource) {\n+        kafkaBridgeClient().inNamespace(resource.getMetadata().getNamespace()).createOrReplace(resource);\n     }\n-\n-    public static KafkaBridge createAndWaitForReadiness(KafkaBridge kafkaBridge) {\n-        KubernetesResource.deployNetworkPolicyForResource(kafkaBridge, KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-\n-        TestUtils.waitFor(\"KafkaBridge creation\", Constants.POLL_INTERVAL_FOR_RESOURCE_CREATION, CR_CREATION_TIMEOUT,\n-            () -> {\n-                try {\n-                    kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaBridge);\n-                    return true;\n-                } catch (KubernetesClientException e) {\n-                    if (e.getMessage().contains(\"object is being deleted\")) {\n-                        return false;\n-                    } else {\n-                        throw e;\n-                    }\n-                }\n-            }\n-        );\n-        return waitFor(deleteLater(kafkaBridge));\n+    @Override\n+    public void delete(KafkaBridge resource) throws Exception {\n+        kafkaBridgeClient().inNamespace(resource.getMetadata().getNamespace()).withName(\n+            resource.getMetadata().getName()).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n     }\n-\n-    public static KafkaBridge kafkaBridgeWithoutWait(KafkaBridge kafkaBridge) {\n-        kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).createOrReplace(kafkaBridge);\n-        return kafkaBridge;\n+    @Override\n+    public boolean isReady(KafkaBridge resource) {\n+        return ResourceManager.waitForResourceStatus(kafkaBridgeClient(), resource, CustomResourceStatus.Ready);\n     }\n-\n-    public static void deleteKafkaBridgeWithoutWait(String resourceName) {\n-        kafkaBridgeClient().inNamespace(ResourceManager.kubeClient().getNamespace()).withName(resourceName).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();\n+    @Override\n+    public void replaceResource(KafkaBridge existing, KafkaBridge newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+        existing.setStatus(newResource.getStatus());", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI5NTc2MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597295760", "bodyText": "This method is no longer used... :D So it does not matter.  (I will removed it)", "author": "see-quick", "createdAt": "2021-03-18T22:55:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzIzMjI1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0MjQwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597242403", "bodyText": "Should be this in some ClusterRoleBindingTemplate like other resources?", "author": "im-konge", "createdAt": "2021-03-18T21:06:30Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.rbac.ClusterRoleBinding;\n+import io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder;\n+import io.fabric8.kubernetes.api.model.rbac.SubjectBuilder;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.test.TestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class ClusterRoleBindingResource implements ResourceType<ClusterRoleBinding> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(ClusterRoleBindingResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"ClusterRoleBinding\";\n+    }\n+    @Override\n+    public ClusterRoleBinding get(String namespace, String name) {\n+        return ResourceManager.kubeClient().namespace(namespace).getClusterRoleBinding(name);\n+    }\n+    @Override\n+    public void create(ClusterRoleBinding resource) {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).createOrReplaceClusterRoleBinding(resource);\n+    }\n+    @Override\n+    public void delete(ClusterRoleBinding resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteClusterRoleBinding(resource);\n+    }\n+    @Override\n+    public boolean isReady(ClusterRoleBinding resource) {\n+        return resource != null;\n+    }\n+    @Override\n+    public void replaceResource(ClusterRoleBinding existing, ClusterRoleBinding newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+    }\n+\n+    public static List<ClusterRoleBinding> clusterRoleBindingsForAllNamespaces(String namespace) {\n+        LOGGER.info(\"Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects\");\n+        return clusterRoleBindingsForAllNamespaces(namespace, \"strimzi-cluster-operator\");\n+    }\n+\n+    public static List<ClusterRoleBinding> clusterRoleBindingsForAllNamespaces(String namespace, String coName) {", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0MzM1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597243356", "bodyText": "(same for other methods in this class, which are not in the original resource - clusterRoleBinding etc.)", "author": "im-konge", "createdAt": "2021-03-18T21:08:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0MjQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0NDA1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597244054", "bodyText": "Maybe put these into some NetworkPolicyUtils or something? Just to have resources clear?", "author": "im-konge", "createdAt": "2021-03-18T21:09:23Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.HasMetadata;\n+import io.fabric8.kubernetes.api.model.LabelSelector;\n+import io.fabric8.kubernetes.api.model.LabelSelectorBuilder;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicy;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder;\n+import io.fabric8.kubernetes.client.CustomResource;\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaExporterResources;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.Spec;\n+import io.strimzi.api.kafka.model.status.Status;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.enums.DefaultNetworkPolicy;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import java.util.List;\n+\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public class NetworkPolicyResource implements ResourceType<NetworkPolicy> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(NetworkPolicyResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"NetworkPolicy\";\n+    }\n+    @Override\n+    public NetworkPolicy get(String namespace, String name) {\n+        return ResourceManager.kubeClient().namespace(namespace).getNetworkPolicy(name);\n+    }\n+    @Override\n+    public void create(NetworkPolicy resource) {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).createNetworkPolicy(resource);\n+    }\n+    @Override\n+    public void delete(NetworkPolicy resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteNetworkPolicy(resource.getMetadata().getName());\n+    }\n+    @Override\n+    public boolean isReady(NetworkPolicy resource) {\n+        return resource != null;\n+    }\n+    @Override\n+    public void replaceResource(NetworkPolicy existing, NetworkPolicy newResource) {\n+        existing.setMetadata(newResource.getMetadata());\n+        existing.setSpec(newResource.getSpec());\n+    }\n+\n+    public static NetworkPolicyBuilder networkPolicyBuilder(String name) {\n+        return networkPolicyBuilder(name, null)\n+            .withNewSpec()\n+                .withNewPodSelector()\n+                .endPodSelector()\n+                .withPolicyTypes(\"Ingress\")\n+            .endSpec();\n+    }\n+\n+    public static NetworkPolicyBuilder networkPolicyBuilder(String name, LabelSelector labelSelector) {\n+        return new NetworkPolicyBuilder()\n+            .withNewApiVersion(\"networking.k8s.io/v1\")\n+                .withNewKind(\"NetworkPolicy\")\n+                    .withNewMetadata()\n+                        .withName(name + \"-allow\")\n+                        .withNamespace(kubeClient().getNamespace())\n+                    .endMetadata()\n+                    .withNewSpec()\n+                        .addNewIngress()\n+                            .addNewFrom()\n+                                .withPodSelector(labelSelector)\n+                            .endFrom()\n+                        .endIngress()\n+                        .withPolicyTypes(\"Ingress\")\n+                    .endSpec();\n+    }\n+\n+    /**\n+     * Method for allowing network policies for ClusterOperator\n+     */\n+    public static void allowNetworkPolicySettingsForClusterOperator(ExtensionContext extensionContext) {", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzMwMzkzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597303939", "bodyText": "Hmmmm, I think everything which using ResourceManager.getInstance().createResource(extensionContext, networkPolicy); should be inside Resource. I agree with moving some methods, which just returns resource object.", "author": "see-quick", "createdAt": "2021-03-18T23:16:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0NDA1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0ODQ1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597248452", "bodyText": "Is this here needed and will we keep it? If yes, can we do something like\nLOGGER.debug(String.join(\"\", Collections.nCopies(76, \"=\")));\n\nto follow some pattern?", "author": "im-konge", "createdAt": "2021-03-18T21:17:24Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -704,63 +707,90 @@ protected void testDockerImagesForKafkaCluster(String clusterName, String namesp\n \n         LOGGER.info(\"Docker images verified\");\n     }\n+    protected void afterEachMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            ResourceManager.getInstance().deleteResources(testContext);\n+        }\n+    }\n \n-    @BeforeEach\n-    void createTestResources(ExtensionContext testContext) {\n-        if (testContext.getTestMethod().isPresent()) {\n-            testName = testContext.getTestMethod().get().getName();\n+    protected void afterAllMayOverride(ExtensionContext testContext) throws Exception {\n+        if (!Environment.SKIP_TEARDOWN) {\n+            teardownEnvForOperator();\n+            ResourceManager.getInstance().deleteResources(testContext);\n         }\n-        ResourceManager.setMethodResources();\n+    }\n \n-        // This is needed to distinguish created Kafka cluster in ResourceManager and don't delete cluster which are still in use by parallel test cases\n-        if (previousClusterName == null) {\n-            LOGGER.info(\"Executing the first test case, using {} as a cluster name\", clusterName);\n-            previousClusterName = clusterName;\n-        } else {\n-            clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-            kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n-            LOGGER.info(\"Current test case is not the first one, generated new cluster name - {}\", clusterName);\n+    /**\n+     * BeforeEachMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeEachMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeEachMayOverride(ExtensionContext extensionContext) {\n+        // this is because we need to have different clusterName and kafkaClientsName in each test case without\n+        // synchronization it can produce `data-race`\n+        String testName = null;\n+\n+        synchronized (lock) {\n+            if (extensionContext.getTestMethod().isPresent()) {\n+                testName = extensionContext.getTestMethod().get().getName();\n+            }\n+\n+            LOGGER.info(\"Not first test we are gonna generate cluster name\");\n+            String clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n+\n+            mapWithClusterNames.put(testName, clusterName);\n+            mapWithTestTopics.put(testName, KafkaTopicUtils.generateRandomNameOfTopic());\n+            mapWithTestUsers.put(testName, KafkaUserUtils.generateRandomNameOfKafkaUser());\n+            mapWithKafkaClientNames.put(testName, clusterName + \"-\" + Constants.KAFKA_CLIENTS);\n+\n+            LOGGER.debug(\"CLUSTER_NAMES_MAP: \\n{}\", mapWithClusterNames);\n+            LOGGER.debug(\"USERS_NAME_MAP: \\n{}\", mapWithTestUsers);\n+            LOGGER.debug(\"TOPIC_NAMES_MAP: \\n{}\", mapWithTestTopics);\n+            LOGGER.debug(\"============THIS IS CLIENTS MAP:\\n{}\", mapWithKafkaClientNames);\n         }\n     }\n \n-    @BeforeAll\n-    void setTestClassName(ExtensionContext testContext) {\n+    /**\n+     * BeforeAllMayOverride, is a method, which gives you option to override @BeforeAll in sub-classes and\n+     * ensure that this is also executed if you call it with super.beforeAllMayOverride(). You can also skip it and\n+     * you your implementation in sub-class as you want.\n+     * @param extensionContext\n+     */\n+    protected void beforeAllMayOverride(ExtensionContext extensionContext) {\n         cluster = KubeClusterResource.getInstance();\n-        if (testContext.getTestClass().isPresent()) {\n-            testClass = testContext.getTestClass().get().getName();\n+        String testClass = null;\n+\n+        if (extensionContext.getTestClass().isPresent()) {\n+            testClass = extensionContext.getTestClass().get().getName();\n         }\n-        // Name for the first test case, other test cases will need different name\n-        previousClusterName = null;\n-        clusterName = CLUSTER_NAME_PREFIX + new Random().nextInt(Integer.MAX_VALUE);\n-        kafkaClientsName = clusterName + \"-\" + Constants.KAFKA_CLIENTS;\n     }\n \n-    @AfterEach\n-    void teardownEnvironmentMethod(ExtensionContext testContext) throws Exception {\n-        TimeMeasuringSystem.getInstance().stopOperation(Operation.TEST_EXECUTION);\n-        AssertionError assertionError = null;\n-        try {\n-            long testDuration = timeMeasuringSystem.getDurationInSeconds(testContext.getRequiredTestClass().getName(), testContext.getRequiredTestMethod().getName(), Operation.TEST_EXECUTION.name());\n-            assertNoCoErrorsLogged(testDuration);\n-        } catch (AssertionError e) {\n-            LOGGER.error(\"Cluster Operator contains unexpected errors!\");\n-            assertionError = new AssertionError(e);\n-        }\n+    @BeforeEach\n+    void setUpTestCase(ExtensionContext testContext) {\n+        LOGGER.debug(\"===============================================================\");\n+        LOGGER.debug(\"{} - [BEFORE EACH] has been called\", this.getClass().getName());", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI0OTQ4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597249487", "bodyText": ".build() on new line please", "author": "im-konge", "createdAt": "2021-03-18T21:19:16Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/bridge/HttpBridgeTlsST.java", "diffHunk": "@@ -106,13 +113,16 @@ void createClassResources() throws Exception {\n                         .endGenericKafkaListener()\n                     .endListeners()\n                 .endKafka()\n-            .endSpec()\n-            .build());\n+            .endSpec().build());", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NDk4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597254982", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            StUtils.checkEnvVarInPod(kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName(), usedVariable),\n          \n          \n            \n                            StUtils.checkEnvVarInPod(kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName(), usedVariable),", "author": "im-konge", "createdAt": "2021-03-18T21:29:22Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -593,11 +628,12 @@ void testCustomAndUpdatedValues() {\n \n         LOGGER.info(\"Check if actual env variable {} has different value than {}\", usedVariable, \"test.value\");\n         assertThat(\n-                StUtils.checkEnvVarInPod(kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName(), usedVariable),\n+                StUtils.checkEnvVarInPod(kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName(), usedVariable),", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NTI2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597255269", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:29:52Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -469,67 +503,69 @@ void testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication() {\n             .endSpec()\n             .build());\n \n-        KafkaUser kafkaUser = KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.scramShaUser(clusterName, USER_NAME).build());\n-\n-        KafkaTopicResource.createAndWaitForReadiness(KafkaTopicResource.topic(clusterName, CONNECT_TOPIC_NAME).build());\n+        KafkaUser kafkaUser = KafkaUserTemplates.scramShaUser(clusterName, userName).build();\n \n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 1)\n-                .editSpec()\n-                    .addToConfig(\"key.converter.schemas.enable\", false)\n-                    .addToConfig(\"value.converter.schemas.enable\", false)\n-                    .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                    .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                    .withNewTls()\n-                        .addNewTrustedCertificate()\n-                            .withSecretName(clusterName + \"-cluster-ca-cert\")\n-                            .withCertificate(\"ca.crt\")\n-                        .endTrustedCertificate()\n-                    .endTls()\n-                    .withBootstrapServers(clusterName + \"-kafka-bootstrap:9093\")\n-                    .withNewKafkaClientAuthenticationScramSha512()\n-                        .withUsername(USER_NAME)\n-                        .withNewPasswordSecret()\n-                            .withSecretName(USER_NAME)\n-                            .withPassword(\"password\")\n-                        .endPasswordSecret()\n-                    .endKafkaClientAuthenticationScramSha512()\n-                .endSpec()\n-                .build());\n+        resourceManager.createResource(extensionContext, kafkaUser);\n+        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 1)\n+            .editSpec()\n+                .addToConfig(\"key.converter.schemas.enable\", false)\n+                .addToConfig(\"value.converter.schemas.enable\", false)\n+                .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+                .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+                .withNewTls()\n+                    .addNewTrustedCertificate()\n+                        .withSecretName(clusterName + \"-cluster-ca-cert\")\n+                        .withCertificate(\"ca.crt\")\n+                    .endTrustedCertificate()\n+                .endTls()\n+                .withBootstrapServers(clusterName + \"-kafka-bootstrap:9093\")\n+                .withNewKafkaClientAuthenticationScramSha512()\n+                    .withUsername(userName)\n+                    .withNewPasswordSecret()\n+                        .withSecretName(userName)\n+                        .withPassword(\"password\")\n+                    .endPasswordSecret()\n+                .endKafkaClientAuthenticationScramSha512()\n+            .endSpec()\n+            .build());\n \n-        String kafkaConnectPodName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NTUxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597255518", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:30:22Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -359,85 +383,90 @@ void testKafkaConnectScaleUpScaleDown() {\n         KafkaConnectResource.replaceKafkaConnectResource(clusterName, c -> c.getSpec().setReplicas(scaleTo));\n \n         DeploymentUtils.waitForDeploymentAndPodsReady(deploymentName, scaleTo);\n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n         assertThat(connectPods.size(), is(scaleTo));\n \n         LOGGER.info(\"Scaling down to {}\", initialReplicas);\n         KafkaConnectResource.replaceKafkaConnectResource(clusterName, c -> c.getSpec().setReplicas(initialReplicas));\n \n         DeploymentUtils.waitForDeploymentAndPodsReady(deploymentName, initialReplicas);\n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n         assertThat(connectPods.size(), is(initialReplicas));\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(INTERNAL_CLIENTS_USED)\n-    void testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication() {\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .addNewGenericKafkaListener()\n-                                .withName(Constants.TLS_LISTENER_DEFAULT_NAME)\n-                                .withPort(9093)\n-                                .withType(KafkaListenerType.INTERNAL)\n-                                .withTls(true)\n-                                .withAuth(new KafkaListenerAuthenticationTls())\n-                            .endGenericKafkaListener()\n-                        .endListeners()\n-                    .endKafka()\n-                .endSpec()\n-                .build());\n+    void testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        String userName = mapWithTestUsers.get(extensionContext.getDisplayName());\n+        String topicName = mapWithTestTopics.get(extensionContext.getDisplayName());\n+        String kafkaClientsName = mapWithKafkaClientNames.get(extensionContext.getDisplayName());\n \n-        KafkaUser kafkaUser = KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.tlsUser(clusterName, USER_NAME).build());\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(Constants.TLS_LISTENER_DEFAULT_NAME)\n+                            .withPort(9093)\n+                            .withType(KafkaListenerType.INTERNAL)\n+                            .withTls(true)\n+                            .withAuth(new KafkaListenerAuthenticationTls())\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                .endKafka()\n+            .endSpec()\n+            .build());\n \n-        KafkaTopicResource.createAndWaitForReadiness(KafkaTopicResource.topic(clusterName, CONNECT_TOPIC_NAME).build());\n+        KafkaUser kafkaUser = KafkaUserTemplates.tlsUser(clusterName, userName).build();\n \n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 1)\n-                .editSpec()\n-                    .addToConfig(\"key.converter.schemas.enable\", false)\n-                    .addToConfig(\"value.converter.schemas.enable\", false)\n-                    .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                    .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                    .withNewTls()\n-                        .addNewTrustedCertificate()\n-                            .withSecretName(clusterName + \"-cluster-ca-cert\")\n-                            .withCertificate(\"ca.crt\")\n-                        .endTrustedCertificate()\n+        resourceManager.createResource(extensionContext, kafkaUser);\n+        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 1)\n+            .editSpec()\n+                .addToConfig(\"key.converter.schemas.enable\", false)\n+                .addToConfig(\"value.converter.schemas.enable\", false)\n+                .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+                .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+                .withNewTls()\n+                .addNewTrustedCertificate()\n+                    .withSecretName(clusterName + \"-cluster-ca-cert\")\n+                    .withCertificate(\"ca.crt\")\n+                .endTrustedCertificate()\n                     .endTls()\n                     .withBootstrapServers(clusterName + \"-kafka-bootstrap:9093\")\n                     .withNewKafkaClientAuthenticationTls()\n                         .withNewCertificateAndKey()\n-                            .withSecretName(USER_NAME)\n+                            .withSecretName(userName)\n                             .withCertificate(\"user.crt\")\n                             .withKey(\"user.key\")\n                         .endCertificateAndKey()\n                     .endKafkaClientAuthenticationTls()\n                 .endSpec()\n                 .build());\n \n-        String kafkaConnectPodName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NTg2Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597255867", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:31:03Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -340,17 +360,21 @@ void testJvmAndResources() {\n                 \"-Xmx200m\", \"-Xms200m\", \"-XX:+UseG1GC\");\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(SCALABILITY)\n-    void testKafkaConnectScaleUpScaleDown() {\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3).build());\n+    void testKafkaConnectScaleUpScaleDown(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3).build());\n+\n         LOGGER.info(\"Running kafkaConnectScaleUP {} in namespace\", NAMESPACE);\n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 1).build());\n+\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 1).build());\n \n         String deploymentName = KafkaConnectResources.deploymentName(clusterName);\n \n         // kafka cluster Connect already deployed\n-        List<String> connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NTk4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597255987", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:31:16Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -359,85 +383,90 @@ void testKafkaConnectScaleUpScaleDown() {\n         KafkaConnectResource.replaceKafkaConnectResource(clusterName, c -> c.getSpec().setReplicas(scaleTo));\n \n         DeploymentUtils.waitForDeploymentAndPodsReady(deploymentName, scaleTo);\n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NjA4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597256084", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:31:30Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -359,85 +383,90 @@ void testKafkaConnectScaleUpScaleDown() {\n         KafkaConnectResource.replaceKafkaConnectResource(clusterName, c -> c.getSpec().setReplicas(scaleTo));\n \n         DeploymentUtils.waitForDeploymentAndPodsReady(deploymentName, scaleTo);\n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n         assertThat(connectPods.size(), is(scaleTo));\n \n         LOGGER.info(\"Scaling down to {}\", initialReplicas);\n         KafkaConnectResource.replaceKafkaConnectResource(clusterName, c -> c.getSpec().setReplicas(initialReplicas));\n \n         DeploymentUtils.waitForDeploymentAndPodsReady(deploymentName, initialReplicas);\n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NjMwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597256308", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:31:56Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -189,109 +198,118 @@ void testKafkaConnectWithFileSinkPlugin() {\n         KafkaConnectUtils.waitForMessagesInKafkaConnectFileSink(kafkaConnectPodName, Constants.DEFAULT_SINK_FILE_PATH, \"99\");\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(INTERNAL_CLIENTS_USED)\n-    void testKafkaConnectWithPlainAndScramShaAuthentication() {\n-        // Use a Kafka with plain listener disabled\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .addNewGenericKafkaListener()\n-                                .withName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n-                                .withPort(9092)\n-                                .withType(KafkaListenerType.INTERNAL)\n-                                .withTls(false)\n-                                .withAuth(new KafkaListenerAuthenticationScramSha512())\n-                            .endGenericKafkaListener()\n-                        .endListeners()\n-                    .endKafka()\n-                .endSpec()\n-                .build());\n+    void testKafkaConnectWithPlainAndScramShaAuthentication(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        String userName = mapWithTestUsers.get(extensionContext.getDisplayName());\n+        String topicName = mapWithTestTopics.get(extensionContext.getDisplayName());\n+        String kafkaClientsName = mapWithKafkaClientNames.get(extensionContext.getDisplayName());\n \n-        KafkaUser kafkaUser = KafkaUserResource.createAndWaitForReadiness(KafkaUserResource.scramShaUser(clusterName, USER_NAME).build());\n+        // Use a Kafka with plain listener disabled\n \n-        KafkaTopicResource.createAndWaitForReadiness(KafkaTopicResource.topic(clusterName, CONNECT_TOPIC_NAME).build());\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n+                            .withPort(9092)\n+                            .withType(KafkaListenerType.INTERNAL)\n+                            .withTls(false)\n+                            .withAuth(new KafkaListenerAuthenticationScramSha512())\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                .endKafka()\n+            .endSpec()\n+            .build());\n \n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 1)\n-                .withNewSpec()\n-                    .withBootstrapServers(KafkaResources.plainBootstrapAddress(clusterName))\n-                    .withNewKafkaClientAuthenticationScramSha512()\n-                        .withNewUsername(USER_NAME)\n-                        .withPasswordSecret(new PasswordSecretSourceBuilder()\n-                            .withSecretName(USER_NAME)\n-                            .withPassword(\"password\")\n-                            .build())\n-                    .endKafkaClientAuthenticationScramSha512()\n-                    .addToConfig(\"key.converter.schemas.enable\", false)\n-                    .addToConfig(\"value.converter.schemas.enable\", false)\n-                    .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                    .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                    .withVersion(Environment.ST_KAFKA_VERSION)\n-                    .withReplicas(1)\n-                .endSpec()\n-                .build());\n+        KafkaUser kafkaUser =  KafkaUserTemplates.scramShaUser(clusterName, userName).build();\n+\n+        resourceManager.createResource(extensionContext, KafkaUserTemplates.scramShaUser(clusterName, userName).build());\n+        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 1)\n+            .withNewSpec()\n+                .withBootstrapServers(KafkaResources.plainBootstrapAddress(clusterName))\n+                .withNewKafkaClientAuthenticationScramSha512()\n+                    .withNewUsername(userName)\n+                    .withPasswordSecret(new PasswordSecretSourceBuilder()\n+                        .withSecretName(userName)\n+                        .withPassword(\"password\")\n+                        .build())\n+                .endKafkaClientAuthenticationScramSha512()\n+                .addToConfig(\"key.converter.schemas.enable\", false)\n+                .addToConfig(\"value.converter.schemas.enable\", false)\n+                .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+                .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+                .withVersion(Environment.ST_KAFKA_VERSION)\n+                .withReplicas(1)\n+            .endSpec()\n+            .build());\n \n-        String kafkaConnectPodName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NjQ0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597256448", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:32:11Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -166,15 +175,15 @@ void testKafkaConnectWithFileSinkPlugin() {\n             .endSpec()\n             .build());\n \n-        String kafkaConnectPodName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String kafkaConnectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NjU2Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597256567", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String connectImageName = PodUtils.getFirstContainerImageNameFromPod(kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").\n          \n          \n            \n                    String connectImageName = PodUtils.getFirstContainerImageNameFromPod(kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).", "author": "im-konge", "createdAt": "2021-03-18T21:32:29Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -125,19 +132,19 @@ void testDeployUndeploy() {\n         assertThat(kafkaPodJson, hasJsonPath(StUtils.globalVariableJsonPathBuilder(0, \"KAFKA_CONNECT_BOOTSTRAP_SERVERS\"),\n                 hasItem(KafkaResources.tlsBootstrapAddress(clusterName))));\n         assertThat(StUtils.getPropertiesFromJson(0, kafkaPodJson, \"KAFKA_CONNECT_CONFIGURATION\"), is(exceptedConfig));\n-        testDockerImagesForKafkaConnect();\n+        testDockerImagesForKafkaConnect(clusterName);\n \n         verifyLabelsOnPods(clusterName, \"connect\", null, \"KafkaConnect\");\n         verifyLabelsForService(clusterName, \"connect-api\", \"KafkaConnect\");\n         verifyLabelsForConfigMaps(clusterName, null, \"\");\n         verifyLabelsForServiceAccounts(clusterName, null);\n     }\n \n-    private void testDockerImagesForKafkaConnect() {\n+    private void testDockerImagesForKafkaConnect(String clusterName) {\n         LOGGER.info(\"Verifying docker image names\");\n         Map<String, String> imgFromDeplConf = getImagesFromConfig();\n         //Verifying docker image for kafka connect\n-        String connectImageName = PodUtils.getFirstContainerImageNameFromPod(kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).\n+        String connectImageName = PodUtils.getFirstContainerImageNameFromPod(kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1Njk4MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597256981", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String connectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String connectPodName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:33:17Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -656,18 +695,18 @@ void testKafkaConnectorWithConnectAndConnectS2IWithSameName() {\n \n         KafkaConnectS2IUtils.waitForConnectS2INotReady(clusterName);\n \n-        KafkaConnectorResource.createAndWaitForReadiness(KafkaConnectorResource.kafkaConnector(clusterName)\n+        resourceManager.createResource(extensionContext, KafkaConnectorTemplates.kafkaConnector(clusterName)\n             .editSpec()\n                 .withClassName(\"org.apache.kafka.connect.file.FileStreamSinkConnector\")\n-                .addToConfig(\"topics\", TOPIC_NAME)\n+                .addToConfig(\"topics\", topicName)\n                 .addToConfig(\"file\", \"/tmp/test-file-sink.txt\")\n                 .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n                 .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n             .endSpec()\n             .build());\n \n         // Check that KafkaConnect contains created connector\n-        String connectPodName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String connectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NzA4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597257087", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String execConnectPod =  kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String execConnectPod =  kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:33:34Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -701,47 +740,49 @@ void testKafkaConnectorWithConnectAndConnectS2IWithSameName() {\n         DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(clusterName));\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(CONNECTOR_OPERATOR)\n     @Tag(INTERNAL_CLIENTS_USED)\n     @Tag(ACCEPTANCE)\n-    void testMultiNodeKafkaConnectWithConnectorCreation() {\n-        String connectClusterName = \"connect-cluster\";\n+    void testMultiNodeKafkaConnectWithConnectorCreation(ExtensionContext extensionContext) {\n+        String connectClusterName = \"connect-cluster-2\";\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        String topicName = mapWithTestTopics.get(extensionContext.getDisplayName());\n \n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3).build());\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3).build());\n         // Crate connect cluster with default connect image\n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 3)\n-                .editMetadata()\n-                    .addToAnnotations(Annotations.STRIMZI_IO_USE_CONNECTOR_RESOURCES, \"true\")\n-                .endMetadata()\n-                .editSpec()\n-                    .addToConfig(\"group.id\", connectClusterName)\n-                    .addToConfig(\"offset.storage.topic\", connectClusterName + \"-offsets\")\n-                    .addToConfig(\"config.storage.topic\", connectClusterName + \"-config\")\n-                    .addToConfig(\"status.storage.topic\", connectClusterName + \"-status\")\n-                .endSpec()\n-                .build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 3)\n+            .editMetadata()\n+                .addToAnnotations(Annotations.STRIMZI_IO_USE_CONNECTOR_RESOURCES, \"true\")\n+            .endMetadata()\n+            .editSpec()\n+                .addToConfig(\"group.id\", connectClusterName)\n+                .addToConfig(\"offset.storage.topic\", connectClusterName + \"-offsets\")\n+                .addToConfig(\"config.storage.topic\", connectClusterName + \"-config\")\n+                .addToConfig(\"status.storage.topic\", connectClusterName + \"-status\")\n+            .endSpec()\n+            .build());\n \n-        KafkaConnectorResource.createAndWaitForReadiness(KafkaConnectorResource.kafkaConnector(clusterName)\n-                .editSpec()\n-                    .withClassName(\"org.apache.kafka.connect.file.FileStreamSinkConnector\")\n-                    .addToConfig(\"topics\", TOPIC_NAME)\n-                    .addToConfig(\"file\", Constants.DEFAULT_SINK_FILE_PATH)\n-                    .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                    .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                .endSpec()\n-                .build());\n+        resourceManager.createResource(extensionContext, KafkaConnectorTemplates.kafkaConnector(clusterName)\n+            .editSpec()\n+                .withClassName(\"org.apache.kafka.connect.file.FileStreamSinkConnector\")\n+                .addToConfig(\"topics\", topicName)\n+                .addToConfig(\"file\", Constants.DEFAULT_SINK_FILE_PATH)\n+                .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+                .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n+            .endSpec()\n+            .build());\n \n         InternalKafkaClient internalKafkaClient = new InternalKafkaClient.Builder()\n             .withUsingPodName(kafkaClientsPodName)\n-            .withTopicName(TOPIC_NAME)\n+            .withTopicName(topicName)\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withListenerName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n             .build();\n \n-        String execConnectPod =  kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String execConnectPod =  kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NzM1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597257350", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:34:02Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -910,15 +955,16 @@ void testConnectAuthorizationWithWeirdUserName(String userName, SecurityProtocol\n         KafkaConnectUtils.waitForMessagesInKafkaConnectFileSink(connectorPodName, Constants.DEFAULT_SINK_FILE_PATH);\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(SCALABILITY)\n-    void testScaleConnectWithoutConnectorToZero() {\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3).build());\n+    void testScaleConnectWithoutConnectorToZero(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n \n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 2).build());\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3).build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 2).build());\n \n         String connectDeploymentName = KafkaConnectResources.deploymentName(clusterName);\n-        List<String> connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NzQ3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597257473", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:34:16Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -928,37 +974,39 @@ void testScaleConnectWithoutConnectorToZero() {\n         KafkaConnectUtils.waitForConnectReady(clusterName);\n         PodUtils.waitForPodsReady(kubeClient().getDeploymentSelectors(connectDeploymentName), 0, true);\n \n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NzgxMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597257810", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:34:56Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -928,37 +974,39 @@ void testScaleConnectWithoutConnectorToZero() {\n         KafkaConnectUtils.waitForConnectReady(clusterName);\n         PodUtils.waitForPodsReady(kubeClient().getDeploymentSelectors(connectDeploymentName), 0, true);\n \n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n         KafkaConnectStatus connectStatus = KafkaConnectResource.kafkaConnectClient().inNamespace(NAMESPACE).withName(clusterName).get().getStatus();\n \n         assertThat(connectPods.size(), is(0));\n         assertThat(connectStatus.getConditions().get(0).getType(), is(Ready.toString()));\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(SCALABILITY)\n     @Tag(CONNECTOR_OPERATOR)\n-    void testScaleConnectWithConnectorToZero() {\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3).build());\n+    void testScaleConnectWithConnectorToZero(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        String topicName = mapWithTestTopics.get(extensionContext.getDisplayName());\n \n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 2)\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3).build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 2)\n             .editMetadata()\n                 .addToAnnotations(Annotations.STRIMZI_IO_USE_CONNECTOR_RESOURCES, \"true\")\n             .endMetadata()\n             .build());\n \n-        KafkaConnectorResource.createAndWaitForReadiness(KafkaConnectorResource.kafkaConnector(clusterName)\n+        resourceManager.createResource(extensionContext, KafkaConnectorTemplates.kafkaConnector(clusterName)\n             .editSpec()\n                 .withClassName(\"org.apache.kafka.connect.file.FileStreamSinkConnector\")\n                 .addToConfig(\"file\", Constants.DEFAULT_SINK_FILE_PATH)\n                 .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n                 .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                .addToConfig(\"topics\", TOPIC_NAME)\n+                .addToConfig(\"topics\", topicName)\n             .endSpec()\n             .build());\n \n         String connectDeploymentName = KafkaConnectResources.deploymentName(clusterName);\n-        List<String> connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1NzkxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597257914", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:35:09Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -968,7 +1016,7 @@ void testScaleConnectWithConnectorToZero() {\n         KafkaConnectUtils.waitForConnectReady(clusterName);\n         PodUtils.waitForPodsReady(kubeClient().getDeploymentSelectors(connectDeploymentName), 0, true);\n \n-        connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1ODIzNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597258235", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String connectGenName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getGenerateName();\n          \n          \n            \n                    String connectGenName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getGenerateName();", "author": "im-konge", "createdAt": "2021-03-18T21:35:41Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -978,39 +1026,41 @@ void testScaleConnectWithConnectorToZero() {\n         assertThat(connectorStatus.getConditions().stream().anyMatch(condition -> condition.getMessage().contains(\"has 0 replicas\")), is(true));\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(SCALABILITY)\n     @Tag(CONNECTOR_OPERATOR)\n-    void testScaleConnectAndConnectorSubresource() {\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3).build());\n+    void testScaleConnectAndConnectorSubresource(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        String topicName = mapWithTestTopics.get(extensionContext.getDisplayName());\n \n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 1)\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3).build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 1)\n             .editMetadata()\n                 .addToAnnotations(Annotations.STRIMZI_IO_USE_CONNECTOR_RESOURCES, \"true\")\n             .endMetadata()\n             .build());\n \n-        KafkaConnectorResource.createAndWaitForReadiness(KafkaConnectorResource.kafkaConnector(clusterName)\n+        resourceManager.createResource(extensionContext, KafkaConnectorTemplates.kafkaConnector(clusterName)\n             .editSpec()\n                 .withClassName(\"org.apache.kafka.connect.file.FileStreamSinkConnector\")\n                 .addToConfig(\"file\", Constants.DEFAULT_SINK_FILE_PATH)\n                 .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n                 .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                .addToConfig(\"topics\", TOPIC_NAME)\n+                .addToConfig(\"topics\", topicName)\n             .endSpec()\n             .build());\n \n         int scaleTo = 4;\n         long connectObsGen = KafkaConnectResource.kafkaConnectClient().inNamespace(NAMESPACE).withName(clusterName).get().getStatus().getObservedGeneration();\n-        String connectGenName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getGenerateName();\n+        String connectGenName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getGenerateName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1ODMzNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597258334", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");\n          \n          \n            \n                    List<Pod> connectPods = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName));", "author": "im-konge", "createdAt": "2021-03-18T21:35:51Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -978,39 +1026,41 @@ void testScaleConnectWithConnectorToZero() {\n         assertThat(connectorStatus.getConditions().stream().anyMatch(condition -> condition.getMessage().contains(\"has 0 replicas\")), is(true));\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(SCALABILITY)\n     @Tag(CONNECTOR_OPERATOR)\n-    void testScaleConnectAndConnectorSubresource() {\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(clusterName, 3).build());\n+    void testScaleConnectAndConnectorSubresource(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        String topicName = mapWithTestTopics.get(extensionContext.getDisplayName());\n \n-        KafkaConnectResource.createAndWaitForReadiness(KafkaConnectResource.kafkaConnect(clusterName, 1)\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(clusterName, 3).build());\n+        resourceManager.createResource(extensionContext, KafkaConnectTemplates.kafkaConnect(extensionContext, clusterName, 1)\n             .editMetadata()\n                 .addToAnnotations(Annotations.STRIMZI_IO_USE_CONNECTOR_RESOURCES, \"true\")\n             .endMetadata()\n             .build());\n \n-        KafkaConnectorResource.createAndWaitForReadiness(KafkaConnectorResource.kafkaConnector(clusterName)\n+        resourceManager.createResource(extensionContext, KafkaConnectorTemplates.kafkaConnector(clusterName)\n             .editSpec()\n                 .withClassName(\"org.apache.kafka.connect.file.FileStreamSinkConnector\")\n                 .addToConfig(\"file\", Constants.DEFAULT_SINK_FILE_PATH)\n                 .addToConfig(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n                 .addToConfig(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\")\n-                .addToConfig(\"topics\", TOPIC_NAME)\n+                .addToConfig(\"topics\", topicName)\n             .endSpec()\n             .build());\n \n         int scaleTo = 4;\n         long connectObsGen = KafkaConnectResource.kafkaConnectClient().inNamespace(NAMESPACE).withName(clusterName).get().getStatus().getObservedGeneration();\n-        String connectGenName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getGenerateName();\n+        String connectGenName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getGenerateName();\n \n         LOGGER.info(\"-------> Scaling KafkaConnect subresource <-------\");\n         LOGGER.info(\"Scaling subresource replicas to {}\", scaleTo);\n         cmdKubeClient().scaleByName(KafkaConnect.RESOURCE_KIND, clusterName, scaleTo);\n         DeploymentUtils.waitForDeploymentAndPodsReady(KafkaConnectResources.deploymentName(clusterName), scaleTo);\n \n         LOGGER.info(\"Check if replicas is set to {}, observed generation is higher - for spec and status - naming prefix should be same\", scaleTo);\n-        List<String> connectPods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND);\n+        List<Pod> connectPods = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\");", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1ODQ3Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597258477", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String connectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String connectPodName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:36:07Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -1169,7 +1221,7 @@ void testMountingSecretAndConfigMapAsVolumesAndEnvVars() {\n             .endSpec()\n             .build());\n \n-        String connectPodName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String connectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI1ODYxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597258618", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String connectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();\n          \n          \n            \n                    String connectPodName = kubeClient().listPodsByPrefixInName(KafkaConnectResources.deploymentName(clusterName)).get(0).getMetadata().getName();", "author": "im-konge", "createdAt": "2021-03-18T21:36:23Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/connect/ConnectST.java", "diffHunk": "@@ -1215,18 +1269,19 @@ void testHostAliases() {\n             .endSpec()\n             .build());\n \n-        String connectPodName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaConnect.RESOURCE_KIND).get(0).getMetadata().getName();\n+        String connectPodName = kubeClient().listPodsByPrefixInName(clusterName + \"-connect\").get(0).getMetadata().getName();", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI2MTg1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597261853", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String configMapCOName = \"strimzi-cluster-operator\";\n          \n          \n            \n                    String configMapCOName = Constants.STRIMZI_DEPLOYMENT_NAME;", "author": "im-konge", "createdAt": "2021-03-18T21:42:39Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -116,7 +124,7 @@ void testJSONFormatLogging() {\n         String configMapOpName = \"json-layout-operators\";\n         String configMapZookeeperName = \"json-layout-zookeeper\";\n         String configMapKafkaName = \"json-layout-kafka\";\n-        String configMapCOName = \"json-layout-cluster-operator\";\n+        String configMapCOName = \"strimzi-cluster-operator\";", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI2MjQ0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597262443", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        .withNewMetadata()\n          \n          \n            \n                        .withName(\"external-configmap\")\n          \n          \n            \n                        .withNamespace(NAMESPACE)\n          \n          \n            \n                        .endMetadata()\n          \n          \n            \n                        .withNewMetadata()\n          \n          \n            \n                          .withName(\"external-configmap\")\n          \n          \n            \n                          .withNamespace(NAMESPACE)\n          \n          \n            \n                        .endMetadata()", "author": "im-konge", "createdAt": "2021-03-18T21:43:44Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -714,33 +708,33 @@ void testDynamicallySetKafkaLoggingLevels() {\n \n         LOGGER.info(\"Setting external logging INFO\");\n         ConfigMap configMap = new ConfigMapBuilder()\n-                .withNewMetadata()\n-                .withName(\"external-configmap\")\n-                .withNamespace(NAMESPACE)\n-                .endMetadata()\n-                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n-                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n-                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n-                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n-                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n-                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n-                        \"log4j.logger.kafka=INFO\\n\" +\n-                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n-                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n-                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n-                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n-                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n-                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n-                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n-                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n-                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n-                .build();\n+            .withNewMetadata()\n+            .withName(\"external-configmap\")\n+            .withNamespace(NAMESPACE)\n+            .endMetadata()", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI3MzUyNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597273526", "bodyText": "leftover?", "author": "im-konge", "createdAt": "2021-03-18T22:04:53Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/mirrormaker/MirrorMaker2ST.java", "diffHunk": "@@ -638,27 +655,35 @@ private void testDockerImagesForKafkaMirrorMaker2() {\n         LOGGER.info(\"Docker images verified\");\n     }\n \n-    @Test\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n     @Tag(SCALABILITY)\n-    void testScaleMirrorMaker2Subresource() {\n+    void testScaleMirrorMaker2Subresource(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        String kafkaClusterSourceName = clusterName + \"-source\";\n+        String kafkaClusterTargetName = clusterName + \"-target\";\n+\n         // Deploy source kafka\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(kafkaClusterSourceName, 1, 1).build());\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(kafkaClusterSourceName, 1, 1).build());\n         // Deploy target kafka\n-        KafkaResource.createAndWaitForReadiness(KafkaResource.kafkaEphemeral(kafkaClusterTargetName, 1, 1).build());\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaEphemeral(kafkaClusterTargetName, 1, 1).build());\n \n-        KafkaMirrorMaker2Resource.createAndWaitForReadiness(KafkaMirrorMaker2Resource.kafkaMirrorMaker2(clusterName, kafkaClusterTargetName, kafkaClusterSourceName, 1, false).build());\n+        resourceManager.createResource(extensionContext, KafkaMirrorMaker2Templates.kafkaMirrorMaker2(clusterName, kafkaClusterTargetName, kafkaClusterSourceName, 1, false).build());\n \n         int scaleTo = 4;\n         long mm2ObsGen = KafkaMirrorMaker2Resource.kafkaMirrorMaker2Client().inNamespace(NAMESPACE).withName(clusterName).get().getStatus().getObservedGeneration();\n-        String mm2GenName = kubeClient().listPods(Labels.STRIMZI_KIND_LABEL, KafkaMirrorMaker2.RESOURCE_KIND).get(0).getMetadata().getGenerateName();\n+        String mm2GenName = kubeClient().listPods(clusterName, Labels.STRIMZI_KIND_LABEL, KafkaMirrorMaker2.RESOURCE_KIND).get(0).getMetadata().getGenerateName();\n \n         LOGGER.info(\"-------> Scaling KafkaMirrorMaker2 subresource <-------\");\n         LOGGER.info(\"Scaling subresource replicas to {}\", scaleTo);\n         cmdKubeClient().scaleByName(KafkaMirrorMaker2.RESOURCE_KIND, clusterName, scaleTo);\n         DeploymentUtils.waitForDeploymentAndPodsReady(KafkaMirrorMaker2Resources.deploymentName(clusterName), scaleTo);\n \n         LOGGER.info(\"Check if replicas is set to {}, naming prefix should be same and observed generation higher\", scaleTo);\n-        List<String> mm2Pods = kubeClient().listPodNames(Labels.STRIMZI_KIND_LABEL, KafkaMirrorMaker2.RESOURCE_KIND);\n+        List<String> mm2Pods = kubeClient().listPodNames(clusterName, Labels.STRIMZI_KIND_LABEL, KafkaMirrorMaker2.RESOURCE_KIND);\n+\n+        LOGGER.info(\"================== THIS IS MM2 PODS\\n{}\", mm2Pods);\n+        // TODO: MAKE TESTUTILS...Expected :is <true>\n+        // Actual   :<false>", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI3NTY0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597275646", "bodyText": "leftover?", "author": "im-konge", "createdAt": "2021-03-18T22:09:12Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/mirrormaker/MirrorMaker2ST.java", "diffHunk": "@@ -993,82 +1046,84 @@ void testRestoreOffsetsInConsumerGroup() {\n                 .withConsumerGroup(consumerGroup)\n                 .build();\n \n-        LOGGER.info(\"Sending messages to - topic {}, cluster {} and message count of {}\",\n-                AVAILABILITY_TOPIC_SOURCE_NAME, kafkaClusterSourceName, MESSAGE_COUNT);\n-\n         LOGGER.info(\"Send & receive {} messages to/from Source cluster.\", MESSAGE_COUNT);\n-        initialInternalClientSourceJob.createAndWaitForReadiness(initialInternalClientSourceJob.producerStrimzi().build());\n+        resourceManager.createResource(extensionContext,\n+            initialInternalClientSourceJob.producerStrimzi().build(),\n+            initialInternalClientSourceJob.consumerStrimzi().build());\n+\n         ClientUtils.waitForClientSuccess(sourceProducerName, NAMESPACE, MESSAGE_COUNT);\n-        initialInternalClientSourceJob.createAndWaitForReadiness(initialInternalClientSourceJob.consumerStrimzi().build());\n         ClientUtils.waitForClientSuccess(sourceConsumerName, NAMESPACE, MESSAGE_COUNT);\n \n         JobUtils.deleteJobWithWait(NAMESPACE, sourceProducerName);\n         JobUtils.deleteJobWithWait(NAMESPACE, sourceConsumerName);\n \n         LOGGER.info(\"Send {} messages to Source cluster.\", MESSAGE_COUNT);\n         KafkaBasicExampleClients internalClientSourceJob = initialInternalClientSourceJob.toBuilder().withMessage(\"Producer B\").build();\n-        internalClientSourceJob.createAndWaitForReadiness(internalClientSourceJob.producerStrimzi().build());\n+\n+        resourceManager.createResource(extensionContext,\n+            internalClientSourceJob.producerStrimzi().build());\n         ClientUtils.waitForClientSuccess(sourceProducerName, NAMESPACE, MESSAGE_COUNT);\n \n         LOGGER.info(\"Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it\");\n \n         LOGGER.info(\"Receive {} messages from mirrored topic on Target cluster.\", MESSAGE_COUNT);\n-        initialInternalClientTargetJob.createAndWaitForReadiness(initialInternalClientTargetJob.consumerStrimzi().build());\n+        resourceManager.createResource(extensionContext,\n+            initialInternalClientTargetJob.consumerStrimzi().build());\n         ClientUtils.waitForClientSuccess(targetConsumerName, NAMESPACE, MESSAGE_COUNT);\n         JobUtils.deleteJobWithWait(NAMESPACE, sourceProducerName);\n         JobUtils.deleteJobWithWait(NAMESPACE, targetConsumerName);\n \n         LOGGER.info(\"Send 50 messages to Source cluster\");\n         internalClientSourceJob = internalClientSourceJob.toBuilder().withMessageCount(50).withMessage(\"Producer C\").build();\n-        internalClientSourceJob.createAndWaitForReadiness(internalClientSourceJob.producerStrimzi().build());\n+        resourceManager.createResource(extensionContext,\n+            internalClientSourceJob.producerStrimzi().build());\n         ClientUtils.waitForClientSuccess(sourceProducerName, NAMESPACE, 50);\n         JobUtils.deleteJobWithWait(NAMESPACE, sourceProducerName);\n \n         LOGGER.info(\"Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it\");\n         LOGGER.info(\"Receive 10 msgs from source cluster\");\n         internalClientSourceJob = internalClientSourceJob.toBuilder().withMessageCount(10).withAdditionalConfig(\"max.poll.records=10\").build();\n-        internalClientSourceJob.createAndWaitForReadiness(internalClientSourceJob.consumerStrimzi().build());\n+        resourceManager.createResource(extensionContext,\n+            internalClientSourceJob.consumerStrimzi().build());\n         ClientUtils.waitForClientSuccess(sourceConsumerName, NAMESPACE, 10);\n         JobUtils.deleteJobWithWait(NAMESPACE, sourceConsumerName);\n \n         LOGGER.info(\"Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it\");\n \n         LOGGER.info(\"Receive 40 msgs from mirrored topic on Target cluster\");\n         KafkaBasicExampleClients internalClientTargetJob = initialInternalClientTargetJob.toBuilder().withMessageCount(40).build();\n-        internalClientTargetJob.createAndWaitForReadiness(internalClientTargetJob.consumerStrimzi().build());\n+        resourceManager.createResource(extensionContext,\n+            internalClientTargetJob.consumerStrimzi().build());\n         ClientUtils.waitForClientSuccess(targetConsumerName, NAMESPACE, 40);\n         JobUtils.deleteJobWithWait(NAMESPACE, targetConsumerName);\n \n         LOGGER.info(\"There should be no more messages to read. Try to consume at least 1 message. \" +\n                 \"This client job should fail on timeout.\");\n-        initialInternalClientTargetJob.createAndWaitForReadiness(initialInternalClientTargetJob.consumerStrimzi().build());\n+        resourceManager.createResource(extensionContext,\n+            initialInternalClientTargetJob.consumerStrimzi().build());\n         ClientUtils.waitForClientTimeout(targetConsumerName, NAMESPACE, 1);\n \n         LOGGER.info(\"As it's Active-Active MM2 mode, there should be no more messages to read from Source cluster\" +\n                 \" topic. This client job should fail on timeout.\");\n-        initialInternalClientSourceJob.createAndWaitForReadiness(initialInternalClientSourceJob.consumerStrimzi().build());\n+        resourceManager.createResource(extensionContext,\n+            initialInternalClientSourceJob.consumerStrimzi().build());\n         ClientUtils.waitForClientTimeout(sourceConsumerName, NAMESPACE, 1);\n     }\n \n     @BeforeAll\n-    void setup() {\n-        kafkaClusterSourceName = clusterName + \"-source\";\n-        kafkaClusterTargetName = clusterName + \"-target\";\n-\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE, Constants.CO_OPERATION_TIMEOUT_SHORT);\n+    void setup(ExtensionContext extensionContext) {\n+        installClusterOperator(extensionContext, NAMESPACE, Constants.CO_OPERATION_TIMEOUT_SHORT);\n     }\n \n-    @AfterEach\n-    void removeResourcesAndTopics() {\n-        // force deletion of MM2 (all) components, after that remove other KafkaTopics\n-        // else they will/might be incorrectly recreated by MM2 component (TopicOperator)\n-        ResourceManager.deleteMethodResources();\n-        KafkaTopicList kafkaTopicList = KafkaTopicResource.kafkaTopicClient().inNamespace(NAMESPACE).list();\n-        kafkaTopicList.getItems().forEach(kafkaTopic -> {\n-            KafkaTopicResource.kafkaTopicClient().inNamespace(NAMESPACE).delete(kafkaTopic);\n-            LOGGER.info(\"Topic {} deleted\", kafkaTopic.getMetadata().getName());\n-            KafkaTopicUtils.waitForKafkaTopicDeletion(kafkaTopic.getMetadata().getName());\n-        });\n-    }\n+//    @AfterEach\n+//    void removeResourcesAndTopics(ExtensionContext extensionContext) throws Exception {\n+//        // force deletion of MM2 (all) components, after that remove other KafkaTopics\n+//        // else they will/might be incorrectly recreated by MM2 component (TopicOperator)\n+//        KafkaTopicList kafkaTopicList = KafkaTopicResource.kafkaTopicClient().inNamespace(NAMESPACE).list();\n+//        kafkaTopicList.getItems().forEach(kafkaTopic -> {\n+//            KafkaTopicResource.kafkaTopicClient().inNamespace(NAMESPACE).delete(kafkaTopic);\n+//            LOGGER.info(\"Topic {} deleted\", kafkaTopic.getMetadata().getName());\n+//            KafkaTopicUtils.waitForKafkaTopicDeletion(kafkaTopic.getMetadata().getName());\n+//        });\n+//    }", "originalCommit": "9464a27e96398523a474b145c5ad9e5a2016b5c2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "84371b084624edeba44f9e6563e6ddf3753a7f69", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/84371b084624edeba44f9e6563e6ddf3753a7f69", "message": "lukas commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-18T23:39:22Z", "type": "forcePushed"}, {"oid": "f2cbdaf5202e7f986985d154c7988b3dfbf54b1d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f2cbdaf5202e7f986985d154c7988b3dfbf54b1d", "message": "lukas commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-18T23:40:18Z", "type": "forcePushed"}, {"oid": "140116b073e49d5e16c4e1775931f768642d92bc", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/140116b073e49d5e16c4e1775931f768642d92bc", "message": "s\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-19T09:25:54Z", "type": "forcePushed"}, {"oid": "5c1b0ad557583d82ba32ca1385a1411f410db716", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5c1b0ad557583d82ba32ca1385a1411f410db716", "message": "azure default\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-19T10:13:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1Mzk5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597953998", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n          \n          \n            \n                            LOGGER.info(\"Resource {} in namespace {} is {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());", "author": "Frawless", "createdAt": "2021-03-19T20:22:55Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +105,147 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n-\n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n-\n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                assertTrue(waitResourceCondition(resource, type::waitForReadiness),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1NDE1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597954151", "bodyText": "Maybe use something from constants?", "author": "Frawless", "createdAt": "2021-03-19T20:23:17Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +105,147 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n-\n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n-\n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                assertTrue(waitResourceCondition(resource, type::waitForReadiness),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1NDY4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597954686", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        LOGGER.error(\"Resource {} failed converting resource to YAML: {}\", resource.getMetadata().getName(), e.getMessage());\n          \n          \n            \n                        LOGGER.error(\"Resource {} is not convertible to YAML: {}\", resource.getMetadata().getName(), e.getMessage());", "author": "Frawless", "createdAt": "2021-03-19T20:24:23Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +105,147 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n-\n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n-\n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                assertTrue(waitResourceCondition(resource, type::waitForReadiness),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);\n+            } catch (InterruptedException e) {\n+                LOGGER.info(\"Resource {} in namespace {} is not {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                Thread.currentThread().interrupt();\n+                return false;\n+            }\n+        }\n+        T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+        boolean pass = condition.test(res);\n+        if (!pass) {\n+            LOGGER.info(\"Resource {} with with type {} failed condition check: {}\", resource.getMetadata().getName(), type, resourceToString(res));\n+        }\n+        return pass;\n     }\n \n-    private static void waitForDeletion(KafkaBridge kafkaBridge) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaBridge {}\", kafkaBridge.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n+    private static final ObjectMapper MAPPER = new ObjectMapper(new YAMLFactory());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public static <T extends HasMetadata> String resourceToString(T resource) {\n+        if (resource == null) {\n+            return \"null\";\n+        }\n+        try {\n+            return MAPPER.writeValueAsString(resource);\n+        } catch (JsonProcessingException e) {\n+            LOGGER.error(\"Resource {} failed converting resource to YAML: {}\", resource.getMetadata().getName(), e.getMessage());", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1NDg5MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597954891", "bodyText": "Wouldn't be better to return null or raise something? Just asking", "author": "Frawless", "createdAt": "2021-03-19T20:24:49Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java", "diffHunk": "@@ -94,297 +105,147 @@ public static HelmClient helmClient() {\n         return KubeClusterResource.helmClusterClient();\n     }\n \n-    public static Stack<Runnable> getPointerResources() {\n-        return pointerResources;\n-    }\n-\n-    public static void setMethodResources() {\n-        LOGGER.info(\"Setting pointer to method resources\");\n-        pointerResources = methodResources;\n+    private final ResourceType<?>[] resourceTypes = new ResourceType[]{\n+        new KafkaBridgeResource(),\n+        new KafkaClientsResource(),\n+        new KafkaConnectorResource(),\n+        new KafkaConnectResource(),\n+        new KafkaConnectS2IResource(),\n+        new KafkaMirrorMaker2Resource(),\n+        new KafkaMirrorMakerResource(),\n+        new KafkaRebalanceResource(),\n+        new KafkaResource(),\n+        new KafkaTopicResource(),\n+        new KafkaUserResource(),\n+        new BundleResource(),\n+        new ClusterRoleBindingResource(),\n+        new DeploymentResource(),\n+        new JobResource(),\n+        new NetworkPolicyResource(),\n+        new RoleBindingResource(),\n+        new ServiceResource()\n+    };\n+\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, T... resources) {\n+        createResource(testContext, true, resources);\n     }\n \n-    public static void setClassResources() {\n-        LOGGER.info(\"Setting pointer to class resources\");\n-        pointerResources = classResources;\n-    }\n-\n-    public static <T extends CustomResource, L extends CustomResourceList<T>> void replaceCrdResource(Class<T> crdClass, Class<L> listClass, String resourceName, Consumer<T> editor) {\n-        Resource<T> namedResource = Crds.operation(kubeClient().getClient(), crdClass, listClass).inNamespace(kubeClient().getNamespace()).withName(resourceName);\n-        T resource = namedResource.get();\n-        editor.accept(resource);\n-        namedResource.replace(resource);\n-    }\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void createResource(ExtensionContext testContext, boolean waitReady, T... resources) {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            // Convenience for tests that create resources in non-existing namespaces. This will create and clean them up.\n+            synchronized (this) {\n+                if (resource.getMetadata().getNamespace() != null && !kubeClient().namespaceExists(resource.getMetadata().getNamespace())) {\n+                    createResource(testContext, waitReady, new NamespaceBuilder().editOrNewMetadata().withName(resource.getMetadata().getNamespace()).endMetadata().build());\n+                }\n+            }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n-    public static <T extends HasMetadata> T deleteLater(MixedOperation<T, ?, ?> operation, T resource) {\n-        LOGGER.debug(\"Scheduled deletion of {} {} in namespace {}\",\n+            LOGGER.info(\"Create/Update of {} {} in namespace {}\",\n                 resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n-        switch (resource.getKind()) {\n-            case Kafka.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((Kafka) resource);\n-                });\n-                break;\n-            case KafkaConnect.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnect) resource);\n-                });\n-                break;\n-            case KafkaConnectS2I.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaConnectS2I) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker) resource);\n-                });\n-                break;\n-            case KafkaMirrorMaker2.RESOURCE_KIND:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaMirrorMaker2) resource);\n-                });\n-                break;\n-            case KafkaBridge.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    waitForDeletion((KafkaBridge) resource);\n-                });\n-                break;\n-            case KafkaTopic.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaTopicUtils.waitForKafkaTopicDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case KafkaUser.RESOURCE_KIND:\n-                pointerResources.add(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                        resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                        .withName(resource.getMetadata().getName())\n-                        .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                        .delete();\n-                    KafkaUserUtils.waitForKafkaUserDeletion(resource.getMetadata().getName());\n-                });\n-                break;\n-            case DEPLOYMENT:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    waitForDeletion((Deployment) resource);\n-                });\n-                break;\n-            case CLUSTER_ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().clusterRoleBindings().withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case ROLE_BINDING:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {}\",\n-                            resource.getKind(), resource.getMetadata().getName());\n-                    kubeClient().getClient().rbac().roleBindings().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case SERVICE:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    kubeClient().getClient().services().inNamespace(resource.getMetadata().getNamespace()).withName(resource.getMetadata().getName()).delete();\n-                });\n-                break;\n-            case INGRESS:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                    kubeClient().deleteIngress((Ingress) resource);\n-                });\n-                break;\n-            default:\n-                pointerResources.push(() -> {\n-                    LOGGER.info(\"Deleting {} {} in namespace {}\",\n-                            resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace());\n-                    operation.inNamespace(resource.getMetadata().getNamespace())\n-                            .withName(resource.getMetadata().getName())\n-                            .withPropagationPolicy(DeletionPropagation.BACKGROUND)\n-                            .delete();\n-                });\n-        }\n-        return resource;\n-    }\n \n-    private static void waitForDeletion(Kafka kafka) {\n-        String kafkaClusterName = kafka.getMetadata().getName();\n-        LOGGER.info(\"Waiting when all the pods are terminated for Kafka {}\", kafkaClusterName);\n+            type.create(resource);\n \n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.zookeeperStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getZookeeper().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.zookeeperPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        StatefulSetUtils.waitForStatefulSetDeletion(KafkaResources.kafkaStatefulSetName(kafkaClusterName));\n-\n-        IntStream.rangeClosed(0, kafka.getSpec().getKafka().getReplicas() - 1).forEach(podIndex ->\n-                PodUtils.deletePodWithWait(KafkaResources.kafkaPodName(kafka.getMetadata().getName(), podIndex)));\n-\n-        // Wait for EO deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaResources.entityOperatorDeploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        // Wait for Kafka Exporter deletion\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaExporterResources.deploymentName(kafkaClusterName));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(KafkaExporterResources.deploymentName(kafka.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n-\n-        SecretUtils.waitForClusterSecretsDeletion(kafkaClusterName);\n-        PersistentVolumeClaimUtils.waitUntilPVCDeletion(kafkaClusterName);\n-\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.kafkaMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-        ConfigMapUtils.waitUntilConfigMapDeletion(KafkaResources.zookeeperMetricsAndLogConfigMapName(kafka.getMetadata().getName()));\n-    }\n-\n-    private static void waitForDeletion(KafkaConnect kafkaConnect) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnect {}\", kafkaConnect.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName()));\n+            synchronized (this) {\n+                STORED_RESOURCES.computeIfAbsent(testContext.getDisplayName(), k -> new Stack<>());\n+                STORED_RESOURCES.get(testContext.getDisplayName()).push(() -> deleteResource(resource));\n+            }\n+        }\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaConnectResources.deploymentName(kafkaConnect.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+        if (waitReady) {\n+            for (T resource : resources) {\n+                ResourceType<T> type = findResourceType(resource);\n+                assertTrue(waitResourceCondition(resource, type::waitForReadiness),\n+                    String.format(\"Timed out waiting for %s %s in namespace %s to be ready\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+            }\n+        }\n     }\n \n-    // Deprecation is suppressed because of KafkaConnectS2I\n-    @SuppressWarnings(\"deprecation\")\n-    private static void waitForDeletion(KafkaConnectS2I kafkaConnectS2I) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaConnectS2I {}\", kafkaConnectS2I.getMetadata().getName());\n+    @SafeVarargs\n+    public final <T extends HasMetadata> void deleteResource(T... resources) throws Exception {\n+        for (T resource : resources) {\n+            ResourceType<T> type = findResourceType(resource);\n+            if (type == null) {\n+                LOGGER.warn(\"Can't find resource type, please delete it manually\");\n+                continue;\n+            }\n \n-        DeploymentConfigUtils.waitForDeploymentConfigDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaConnectS2IResources.deploymentName(kafkaConnectS2I.getMetadata().getName()));\n+            LOGGER.info(\"Delete of {} {} in namespace {}\",\n+                resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace() == null ? \"(not set)\" : resource.getMetadata().getNamespace());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().contains(\"-connect-\"))\n-                .forEach(p -> {\n-                    LOGGER.debug(\"Deleting: {}\", p.getMetadata().getName());\n-                    kubeClient().deletePod(p);\n-                });\n+            type.delete(resource);\n+            assertTrue(waitResourceCondition(resource, Objects::isNull),\n+                String.format(\"Timed out deleting %s %s in namespace %s\", resource.getKind(), resource.getMetadata().getName(), resource.getMetadata().getNamespace()));\n+        }\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker kafkaMirrorMaker) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker {}\", kafkaMirrorMaker.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMakerResources.deploymentName(kafkaMirrorMaker.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition) {\n+        return waitResourceCondition(resource, condition, TimeoutBudget.ofDuration(Duration.ofMinutes(5)));\n     }\n \n-    private static void waitForDeletion(KafkaMirrorMaker2 kafkaMirrorMaker2) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaMirrorMaker2 {}\", kafkaMirrorMaker2.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName()));\n-\n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaMirrorMaker2Resources.deploymentName(kafkaMirrorMaker2.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public final <T extends HasMetadata> boolean waitResourceCondition(T resource, Predicate<T> condition, TimeoutBudget timeout) {\n+        assertNotNull(resource);\n+        assertNotNull(resource.getMetadata());\n+        assertNotNull(resource.getMetadata().getName());\n+        ResourceType<T> type = findResourceType(resource);\n+        assertNotNull(type);\n+\n+        while (!timeout.timeoutExpired()) {\n+            T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+            if (condition.test(res)) {\n+                LOGGER.info(\"Resource {} in namespace {} is {}}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                return true;\n+            }\n+            try {\n+                Thread.sleep(1000);\n+            } catch (InterruptedException e) {\n+                LOGGER.info(\"Resource {} in namespace {} is not {}!\", resource.getMetadata().getName(), resource.getMetadata().getNamespace(), condition.toString());\n+                Thread.currentThread().interrupt();\n+                return false;\n+            }\n+        }\n+        T res = type.get(resource.getMetadata().getNamespace(), resource.getMetadata().getName());\n+        boolean pass = condition.test(res);\n+        if (!pass) {\n+            LOGGER.info(\"Resource {} with with type {} failed condition check: {}\", resource.getMetadata().getName(), type, resourceToString(res));\n+        }\n+        return pass;\n     }\n \n-    private static void waitForDeletion(KafkaBridge kafkaBridge) {\n-        LOGGER.info(\"Waiting when all the Pods are terminated for KafkaBridge {}\", kafkaBridge.getMetadata().getName());\n-\n-        DeploymentUtils.waitForDeploymentDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n-        ReplicaSetUtils.waitForReplicaSetDeletion(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName()));\n+    private static final ObjectMapper MAPPER = new ObjectMapper(new YAMLFactory());\n \n-        kubeClient().listPods().stream()\n-                .filter(p -> p.getMetadata().getName().startsWith(KafkaBridgeResources.deploymentName(kafkaBridge.getMetadata().getName())))\n-                .forEach(p -> PodUtils.deletePodWithWait(p.getMetadata().getName()));\n+    public static <T extends HasMetadata> String resourceToString(T resource) {\n+        if (resource == null) {\n+            return \"null\";\n+        }\n+        try {\n+            return MAPPER.writeValueAsString(resource);\n+        } catch (JsonProcessingException e) {\n+            LOGGER.error(\"Resource {} failed converting resource to YAML: {}\", resource.getMetadata().getName(), e.getMessage());\n+            return \"unknown\";", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk2NzgyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597967821", "bodyText": "I think in this case it doesn't matter much.. :)", "author": "see-quick", "createdAt": "2021-03-19T20:51:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1NDg5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1Njc5NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597956795", "bodyText": "Shouldn't be there some check for status?", "author": "Frawless", "createdAt": "2021-03-19T20:28:24Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/JobResource.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.batch.Job;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+\n+\n+public class JobResource implements ResourceType<Job> {\n+\n+    @Override\n+    public String getKind() {\n+        return \"Job\";\n+    }\n+    @Override\n+    public Job get(String namespace, String name) {\n+        return ResourceManager.kubeClient().namespace(namespace).getJob(name);\n+    }\n+    @Override\n+    public void create(Job resource) {\n+        ResourceManager.kubeClient().createJob(resource);\n+    }\n+    @Override\n+    public void delete(Job resource) throws Exception {\n+        ResourceManager.kubeClient().namespace(resource.getMetadata().getNamespace()).deleteJob(resource.getMetadata().getName());\n+    }\n+    @Override\n+    public boolean waitForReadiness(Job resource) {\n+        return resource != null;", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1NjkzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597956937", "bodyText": "Constant?", "author": "Frawless", "createdAt": "2021-03-19T20:28:39Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.HasMetadata;\n+import io.fabric8.kubernetes.api.model.LabelSelector;\n+import io.fabric8.kubernetes.api.model.LabelSelectorBuilder;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicy;\n+import io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder;\n+import io.fabric8.kubernetes.client.CustomResource;\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaExporterResources;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.Spec;\n+import io.strimzi.api.kafka.model.status.Status;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.enums.DefaultNetworkPolicy;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.systemtest.templates.kubernetes.NetworkPolicyTemplates;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+import java.util.List;\n+\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public class NetworkPolicyResource implements ResourceType<NetworkPolicy> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(NetworkPolicyResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"NetworkPolicy\";", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1NzcxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597957714", "bodyText": "Constant?", "author": "Frawless", "createdAt": "2021-03-19T20:30:12Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceResource.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.Service;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+public class ServiceResource implements ResourceType<Service> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(ServiceResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"Service\";", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk1Nzc3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597957776", "bodyText": "Constant?", "author": "Frawless", "createdAt": "2021-03-19T20:30:20Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.resources.kubernetes;\n+\n+import io.fabric8.kubernetes.api.model.rbac.RoleBinding;\n+import io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.ResourceType;\n+import io.strimzi.test.TestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+public class RoleBindingResource implements ResourceType<RoleBinding> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(RoleBindingResource.class);\n+\n+    @Override\n+    public String getKind() {\n+        return \"RoleBinding\";", "originalCommit": "5c2c64428570f0e4514cc47f39c5f81667ca7d07", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dadbb0fc4c10efc7f62f3e22d7fdc92aeac38a47", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/dadbb0fc4c10efc7f62f3e22d7fdc92aeac38a47", "message": "last test\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-19T20:21:12Z", "type": "forcePushed"}, {"oid": "3b75989e9a6a94368cb3a7637317929b15dcfaf1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3b75989e9a6a94368cb3a7637317929b15dcfaf1", "message": "last Mohican?\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-19T21:37:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk5NDgzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597994833", "bodyText": "Was this not changed to Never in the past because of upgrade tests? @Frawless?", "author": "scholzj", "createdAt": "2021-03-19T21:56:48Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/kafkaclients/KafkaBasicExampleClients.java", "diffHunk": "@@ -269,7 +269,7 @@ public JobBuilder defaultConsumerStrimzi() {\n                         .withLabels(consumerLabels)\n                     .endMetadata()\n                     .withNewSpec()\n-                        .withRestartPolicy(\"Never\")\n+                        .withRestartPolicy(\"OnFailure\")", "originalCommit": "3b75989e9a6a94368cb3a7637317929b15dcfaf1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODAwOTAyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r598009027", "bodyText": "Yes, Maros will change it back. He actually already did, but it was probably reverted by some rebase", "author": "Frawless", "createdAt": "2021-03-19T22:41:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk5NDgzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk5NjUzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r597996533", "bodyText": "Should you use in these places something similar to PATH_TO_PACKAGING_EXAMPLES from Constants.java? You changed it there, maybe it can change here as well.", "author": "scholzj", "createdAt": "2021-03-19T22:01:30Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AbstractST.java", "diffHunk": "@@ -265,46 +276,46 @@ protected void teardownEnvForOperator() {\n      * @param namespace namespace where CO will be deployed to\n      * @param bindingsNamespaces list of namespaces where Bindings should be deployed to\n      */\n-    public static void applyBindings(String namespace, List<String> bindingsNamespaces) {\n+    public static void applyBindings(ExtensionContext extensionContext, String namespace, List<String> bindingsNamespaces) {\n         for (String bindingsNamespace : bindingsNamespaces) {\n-            applyClusterRoleBindings(namespace);\n-            applyRoleBindings(namespace, bindingsNamespace);\n+            applyClusterRoleBindings(extensionContext, namespace);\n+            applyRoleBindings(extensionContext, namespace, bindingsNamespace);\n         }\n     }\n \n     /**\n      * Method for apply Strimzi cluster operator specific Role and ClusterRole bindings for specific namespaces.\n      * @param namespace namespace where CO will be deployed to\n      */\n-    public static void applyBindings(String namespace) {\n-        applyBindings(namespace, Collections.singletonList(namespace));\n+    public static void applyBindings(ExtensionContext extensionContext, String namespace) {\n+        applyBindings(extensionContext, namespace, Collections.singletonList(namespace));\n     }\n \n     /**\n      * Method for apply Strimzi cluster operator specific Role and ClusterRole bindings for specific namespaces.\n      * @param namespace namespace where CO will be deployed to\n      * @param bindingsNamespaces array of namespaces where Bindings should be deployed to\n      */\n-    public static void applyBindings(String namespace, String... bindingsNamespaces) {\n-        applyBindings(namespace, Arrays.asList(bindingsNamespaces));\n+    public static void applyBindings(ExtensionContext extensionContext, String namespace, String... bindingsNamespaces) {\n+        applyBindings(extensionContext, namespace, Arrays.asList(bindingsNamespaces));\n     }\n \n-    private static void applyClusterRoleBindings(String namespace) {\n+    private static void applyClusterRoleBindings(ExtensionContext extensionContext, String namespace) {\n         // 021-ClusterRoleBinding\n-        KubernetesResource.clusterRoleBinding(TestUtils.USER_PATH + \"/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml\", namespace);\n+        ClusterRoleBindingResource.clusterRoleBinding(extensionContext, TestUtils.USER_PATH + \"/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml\", namespace);", "originalCommit": "3b75989e9a6a94368cb3a7637317929b15dcfaf1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODAwNjA3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4137#discussion_r598006073", "bodyText": "Yes, I have added it thanks.", "author": "see-quick", "createdAt": "2021-03-19T22:30:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Nzk5NjUzMw=="}], "type": "inlineReview"}, {"oid": "77873b8583d09f678430928d1a82537af6d05413", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/77873b8583d09f678430928d1a82537af6d05413", "message": "last Mohican part 2?\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T14:22:46Z", "type": "forcePushed"}, {"oid": "0eb0f4e30900b42a5792127b43575c4c8a4a05bc", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0eb0f4e30900b42a5792127b43575c4c8a4a05bc", "message": "[MO] - [1st step paralelism] - random names for all resources\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "155f768a4d2fb432ef74bd85fb0b37dedab8904f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/155f768a4d2fb432ef74bd85fb0b37dedab8904f", "message": "validate check\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "eea1e63caa5dfab0d2e4fb5e98d601727c613047", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/eea1e63caa5dfab0d2e4fb5e98d601727c613047", "message": "[MO] - [system tests] -> 3rd step (templates, re-worked resources, first tests suites works\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "e618e6c740b33a4d6d055017896b0358b7786a9c", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e618e6c740b33a4d6d055017896b0358b7786a9c", "message": "[MO] - Paralelism Bridge suites done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "1a5d750307429405461e36444e926c5cacff90c6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/1a5d750307429405461e36444e926c5cacff90c6", "message": "Bridge, Connect, Kafka Suites work concurrently\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "3900ad900adb293043f7097c133064d5af3fdeb0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3900ad900adb293043f7097c133064d5af3fdeb0", "message": "fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "fd5f69955f0a3e676c477796f99ee29970fd6cf4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fd5f69955f0a3e676c477796f99ee29970fd6cf4", "message": "connects2i, listeners, multiple listeners suites works\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "abe8b72383f021c67d6a18964623e876cb8c3ee3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/abe8b72383f021c67d6a18964623e876cb8c3ee3", "message": "BackwardsCompatibleListenersST works\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:55Z", "type": "commit"}, {"oid": "f4c9d6feaa23aeaebac804b1db488f9f9bf42db8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f4c9d6feaa23aeaebac804b1db488f9f9bf42db8", "message": "logsettingsST done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "0fdab235b34ff83ed62df2a7bc5e8f098a5713f7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0fdab235b34ff83ed62df2a7bc5e8f098a5713f7", "message": "metrics, olm, topic, user, clusterOperatorRbac, NamespaceDeletion, Recovery, NamespaceRbacScope done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "468c8aba2eb0bdb9f8d11ef7b5accaf0a6a6ec22", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/468c8aba2eb0bdb9f8d11ef7b5accaf0a6a6ec22", "message": "cluster operation, helm, spepcifc, templat, traccing, upgrade, utils, watcher, rollingupdate done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "b2f0ac4753da438f3635942b8115ebb4ac618ec7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b2f0ac4753da438f3635942b8115ebb4ac618ec7", "message": "bridge fixes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "dfd0ea9ae85b8e6865cdab719fe5f430552ce51d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/dfd0ea9ae85b8e6865cdab719fe5f430552ce51d", "message": "checkstyle\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "1d1d0e36786cce6a60278765c72cff688c211096", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/1d1d0e36786cce6a60278765c72cff688c211096", "message": "rebasing shit...\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "32506936ef43abfceaac6d3793ada7e9bf8f5603", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/32506936ef43abfceaac6d3793ada7e9bf8f5603", "message": "s\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "711c281701dc76135df257e2790da208bafb2e38", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/711c281701dc76135df257e2790da208bafb2e38", "message": "check and build done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "3914b0e2d6f2c031c8232d4ee1b1ae66f6ec02e8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3914b0e2d6f2c031c8232d4ee1b1ae66f6ec02e8", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "9a29c174bbd80e4970605a4baab11e6cc559426a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9a29c174bbd80e4970605a4baab11e6cc559426a", "message": "sds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "c6c63840ceeaa73c471500d5d2958d9a7c8d0fc2", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c6c63840ceeaa73c471500d5d2958d9a7c8d0fc2", "message": "[MO] - only tracing and securityST is not working\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "35a3b55d4c40ebeb27b4ba2a15061247e5ed0483", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/35a3b55d4c40ebeb27b4ba2a15061247e5ed0483", "message": "last fixes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "e7be5a8e65e5c8fe251d9da6167f128b3e1d7ea1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e7be5a8e65e5c8fe251d9da6167f128b3e1d7ea1", "message": "[MO] spotbugs, checkstyle etc..\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "73168e31c3917f7a82902e9a54e1b1c9e6cd3426", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/73168e31c3917f7a82902e9a54e1b1c9e6cd3426", "message": "something\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "cdac90686eb39d20be79471ac8cf57d31f888269", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/cdac90686eb39d20be79471ac8cf57d31f888269", "message": "[MO] - rebase, tracing full fix, security almost\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "5443a0e5e79b9c9d8c3ee9d1eb5f36d083a46c67", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5443a0e5e79b9c9d8c3ee9d1eb5f36d083a46c67", "message": "spotbugs + checkstyle\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "61d344cc936713e3e3c800a7d60da1450af169a4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/61d344cc936713e3e3c800a7d60da1450af169a4", "message": "rebase again\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "883482eb7728ef055ee55f16f58e72ed87bf9819", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/883482eb7728ef055ee55f16f58e72ed87bf9819", "message": "fixes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "6ef616b758e209d137f9fd839b1cc67445c7faf4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6ef616b758e209d137f9fd839b1cc67445c7faf4", "message": "check and spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "c4903e35e155c1ac2e1d2d375c25f27e0577ed7d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c4903e35e155c1ac2e1d2d375c25f27e0577ed7d", "message": "azure count paralelism\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "938b32cd8264a99460e3a474f4104aaa29898f15", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/938b32cd8264a99460e3a474f4104aaa29898f15", "message": "fix NPE\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "54ff9a4e88d5675e447ad013016dd5d26a4f60bd", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/54ff9a4e88d5675e447ad013016dd5d26a4f60bd", "message": "2 parallel\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "934a77ff823efd6c507dbeca172fb325d6770f89", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/934a77ff823efd6c507dbeca172fb325d6770f89", "message": "namespace fixed\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "627a44f201c2b17c42721d143ef8f59bac6e2722", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/627a44f201c2b17c42721d143ef8f59bac6e2722", "message": "mo - updating\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "6c748d2de6860d7dbda9606b50e1359158e23b51", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6c748d2de6860d7dbda9606b50e1359158e23b51", "message": "fucking rebase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "5c37543e92e97f3ce87933b4786a56e9e62f6974", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5c37543e92e97f3ce87933b4786a56e9e62f6974", "message": "changes after rbase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "3723a752428840b644e3ec1de52a79e8c0c8359f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3723a752428840b644e3ec1de52a79e8c0c8359f", "message": "connect, bridge, metrics done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "31ca63d30d67250bbaece8e178537c61d837615e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/31ca63d30d67250bbaece8e178537c61d837615e", "message": "checkstyle\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "f57389d28814e76dbebbcd89739987221d0a4279", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f57389d28814e76dbebbcd89739987221d0a4279", "message": "wip on fixing listeners\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:56Z", "type": "commit"}, {"oid": "e8884d79d8bd6115b419362924c50a76777cc9cf", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e8884d79d8bd6115b419362924c50a76777cc9cf", "message": "fixup! wip on fixing listeners\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "545f28aace0421f5a61aee690b351706de8c7f90", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/545f28aace0421f5a61aee690b351706de8c7f90", "message": "fixup! fixup! wip on fixing listeners\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "b0fda413f249ef48ff65adbd7808e0c9c167c8f3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b0fda413f249ef48ff65adbd7808e0c9c167c8f3", "message": "add timeout for CC topics\n\nSigned-off-by: Lukas Kral <lukywill16@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "a5c868f7d3ecc3e81b86e67e5b8297e1dfd15c1e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a5c868f7d3ecc3e81b86e67e5b8297e1dfd15c1e", "message": "WIP\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "e9e1fe3de908acacf3e126937298337380e1b8c4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e9e1fe3de908acacf3e126937298337380e1b8c4", "message": "some fixes\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "01b008ec1dab23e7c5aac66f2c44ea70888f86ee", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/01b008ec1dab23e7c5aac66f2c44ea70888f86ee", "message": "fixup! some fixes\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "48d650512d4426fe1220fe79dcadd232db5b26a3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/48d650512d4426fe1220fe79dcadd232db5b26a3", "message": "many fixes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "14be70c8a213cbfacc7e2b0d5dca1ad5ef2041e4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/14be70c8a213cbfacc7e2b0d5dca1ad5ef2041e4", "message": "more fixes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "01dc73e07569990703bde14e450201086e3b6fbe", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/01dc73e07569990703bde14e450201086e3b6fbe", "message": "Some fixes and changes\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "96fbaf0bf086768174a0406373baf130c48a334c", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/96fbaf0bf086768174a0406373baf130c48a334c", "message": "sds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "367b5dba4502df266676b602602dc195518459a8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/367b5dba4502df266676b602602dc195518459a8", "message": "[MO] - security test fixed\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "beab8658623c3d59fab3993ed52d859e2be39def", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/beab8658623c3d59fab3993ed52d859e2be39def", "message": "securityST + LogsettingsSt done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "65dcced04592be8113f1c455ff20140ec2f34ad0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/65dcced04592be8113f1c455ff20140ec2f34ad0", "message": "fix waiting for KafkaRebalance, replace ParallelTest tag with Isolated one\n\nSigned-off-by: Lukas Kral <lukywill16@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "f9a892c13d8885fdea9eea2732679ff5eeacb59d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f9a892c13d8885fdea9eea2732679ff5eeacb59d", "message": "checkstyle\n\nSigned-off-by: Lukas Kral <lukywill16@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "cbf48347c10b301130fd65d338ef68f5a2721327", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/cbf48347c10b301130fd65d338ef68f5a2721327", "message": "fixes for MultipleClusterOperatorsST\n\nSigned-off-by: Lukas Kral <lukywill16@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "8a5e323df80b5097ea2a4e83ab5eb9e2b01b347b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8a5e323df80b5097ea2a4e83ab5eb9e2b01b347b", "message": "[MO] - all tests which creating kafka moved to isolated\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "7cbb35d69b245068666e988f54d2603afd3aa85f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7cbb35d69b245068666e988f54d2603afd3aa85f", "message": "checkstyle + spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "06110d505d22e0b39fb8a4d9e8ff771e86cdad33", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/06110d505d22e0b39fb8a4d9e8ff771e86cdad33", "message": "rebased\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "8f87e5df139ac16edab056a8299be81099809f5a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8f87e5df139ac16edab056a8299be81099809f5a", "message": "Fixup path to CO file\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "adba1c05c5203ee0c47303300018067469da3438", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/adba1c05c5203ee0c47303300018067469da3438", "message": "Fixes for failures\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "32540c1017246a108bc328b1d3083f5c6fecdb7b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/32540c1017246a108bc328b1d3083f5c6fecdb7b", "message": "fixes after rebease\n\nSigned-off-by: Jakub Stejskal <xstejs24@gmail.com>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "d418f7cf486bbc4a49493d9307c31db6e2c62ebf", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d418f7cf486bbc4a49493d9307c31db6e2c62ebf", "message": "Jakub commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "455660040e1a317d004a6a16d6d28cc0169e8e06", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/455660040e1a317d004a6a16d6d28cc0169e8e06", "message": "last one? :D\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "a3b39fe6301fcb34e6b824b0d7ab02aed91ceb1b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a3b39fe6301fcb34e6b824b0d7ab02aed91ceb1b", "message": "last rebase..?\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "07834886cfae98221d4f9310142999fb971f8df0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/07834886cfae98221d4f9310142999fb971f8df0", "message": "last last one??\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "e5e8d0674f9007f3db470de8b205b7a1284fbf91", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e5e8d0674f9007f3db470de8b205b7a1284fbf91", "message": "finish\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:57Z", "type": "commit"}, {"oid": "4ae2ff9a007d4cfe7f5a5952bec6391fac9996e0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/4ae2ff9a007d4cfe7f5a5952bec6391fac9996e0", "message": "jakub sch. commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:58Z", "type": "commit"}, {"oid": "60287f95c978c1e6d2ed1f1d4637ce99ba451327", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/60287f95c978c1e6d2ed1f1d4637ce99ba451327", "message": "lukas commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:58Z", "type": "commit"}, {"oid": "393cc797407ecd4eeeb45f349805fadf928a71ee", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/393cc797407ecd4eeeb45f349805fadf928a71ee", "message": "s\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:58Z", "type": "commit"}, {"oid": "f00e86ceb981e7247c3371ffa613e3409e4d1ce7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f00e86ceb981e7247c3371ffa613e3409e4d1ce7", "message": "azure default\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:58Z", "type": "commit"}, {"oid": "cfdff3d72b74ec85a4f034872d116d7af713192a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/cfdff3d72b74ec85a4f034872d116d7af713192a", "message": "last test\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:58Z", "type": "commit"}, {"oid": "5730064b0beffa0c031e18f88f00780e0a12c638", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5730064b0beffa0c031e18f88f00780e0a12c638", "message": "last Mohican?\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:58Z", "type": "commit"}, {"oid": "3d7188b583db9c980e5566acb003f3e5e7673a05", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3d7188b583db9c980e5566acb003f3e5e7673a05", "message": "last Mohican part 2?\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:49:58Z", "type": "commit"}, {"oid": "68ac567cb92ba2592681fa660f6bd4cada056480", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/68ac567cb92ba2592681fa660f6bd4cada056480", "message": "fix fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:54:46Z", "type": "commit"}, {"oid": "989d10c4936d4ff3b14ed5b6eba6678a233342c1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/989d10c4936d4ff3b14ed5b6eba6678a233342c1", "message": "rebase...\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:56:32Z", "type": "commit"}, {"oid": "989d10c4936d4ff3b14ed5b6eba6678a233342c1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/989d10c4936d4ff3b14ed5b6eba6678a233342c1", "message": "rebase...\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T15:56:32Z", "type": "forcePushed"}, {"oid": "a7a7975f4d6c89efc129dc872445ea37d830dc81", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a7a7975f4d6c89efc129dc872445ea37d830dc81", "message": "last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2021-03-20T23:31:08Z", "type": "commit"}]}