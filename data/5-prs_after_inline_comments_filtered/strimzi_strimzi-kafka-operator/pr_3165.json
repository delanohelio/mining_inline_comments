{"pr_number": 3165, "pr_title": "Add support for scale subresource to Connect, S2I, MM1, MM2, Bridge and Connectors", "pr_createdAt": "2020-06-06T23:09:52Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjcxOTAwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r436719009", "bodyText": "They're not really representing the resource. I'm suggesting provide, but I'm not completely convinced by that either.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @Description(\"Total number of pods representing this resource.\")\n          \n          \n            \n                @Description(\"The current number of pods being used to provide this resource.\")", "author": "tombentley", "createdAt": "2020-06-08T13:50:42Z", "path": "api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectStatus.java", "diffHunk": "@@ -50,4 +53,24 @@ public void setUrl(String url) {\n     public void setConnectorPlugins(List<ConnectorPlugin> connectorPlugins) {\n         this.connectorPlugins = connectorPlugins;\n     }\n+\n+    @JsonInclude(JsonInclude.Include.NON_NULL)\n+    @Description(\"Total number of pods representing this resource.\")", "originalCommit": "d1287c310907e51c64e8303d59341d9cbc05dd39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjcxOTQ0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r436719448", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @Description(\"Label selector for pods representing this resource.\")\n          \n          \n            \n                @Description(\"Label selector for pods providing this resource.\")", "author": "tombentley", "createdAt": "2020-06-08T13:51:07Z", "path": "api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectStatus.java", "diffHunk": "@@ -50,4 +53,24 @@ public void setUrl(String url) {\n     public void setConnectorPlugins(List<ConnectorPlugin> connectorPlugins) {\n         this.connectorPlugins = connectorPlugins;\n     }\n+\n+    @JsonInclude(JsonInclude.Include.NON_NULL)\n+    @Description(\"Total number of pods representing this resource.\")\n+    public int getReplicas() {\n+        return replicas;\n+    }\n+\n+    public void setReplicas(int replicas) {\n+        this.replicas = replicas;\n+    }\n+\n+    @JsonInclude(JsonInclude.Include.NON_NULL)\n+    @Description(\"Label selector for pods representing this resource.\")", "originalCommit": "d1287c310907e51c64e8303d59341d9cbc05dd39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjcyMzM1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r436723351", "bodyText": "If you default this to the empty array you won't have to specify an empty scale subresource for things which don't have it, like Kafka.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        Scale[] scale();\n          \n          \n            \n                        Scale[] scale() default {};", "author": "tombentley", "createdAt": "2020-06-08T13:54:54Z", "path": "crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Crd.java", "diffHunk": "@@ -108,17 +110,31 @@\n          * @return The subresources of a custom resources that this is the definition for.\n          * @see <a href=\"https://v1-11.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#customresourcedefinitionversion-v1beta1-apiextensions\">Kubernetes 1.11 API documtation</a>\n          */\n-        Subresources subresources() default @Subresources(status = {});\n+        Subresources subresources() default @Subresources(\n+                status = {},\n+                scale = {}\n+                );\n \n         /**\n          * The subresources of a custom resources that this is the definition for.\n          * @see <a href=\"https://v1-11.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#customresourcedefinitionversion-v1beta1-apiextensions\">Kubernetes 1.11 API documtation</a>\n          */\n         @interface Subresources {\n             Status[] status();\n+            Scale[] scale();", "originalCommit": "d1287c310907e51c64e8303d59341d9cbc05dd39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjczNDkzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r436734936", "bodyText": "In the CRD schema status and scale are both scalar quantities. When I added support for the status subresource wanted to have a way to say \"this CRD supports status\" or not, and I chose not to use a boolean flag for that, but rather use the presence/absence of the @Status annotation to indicate that. Since you can only omit an annotation value if it has a default, and you can't default an annotation to null that forced me to make status an array. You're using the same pattern here, which I guess is fine, but I suppose we should have the CrdGenerator exit with an error if the length of the array is greater than 1.", "author": "tombentley", "createdAt": "2020-06-08T14:06:34Z", "path": "crd-generator/src/main/java/io/strimzi/crdgenerator/CrdGenerator.java", "diffHunk": "@@ -254,11 +254,27 @@ private ObjectNode buildSpec(Crd.Spec crd, Class<? extends CustomResource> crdCl\n             result.set(\"additionalPrinterColumns\", cols);\n         }\n         if (crd.subresources().status().length != 0) {\n-            ObjectNode statusNode = nf.objectNode();\n+            ObjectNode subresources = nf.objectNode();\n+\n             if (crd.subresources().status().length > 0) {\n-                statusNode.set(\"status\", nf.objectNode());\n+                subresources.set(\"status\", nf.objectNode());\n+            }\n+\n+            if (crd.subresources().scale().length > 0) {", "originalCommit": "d1287c310907e51c64e8303d59341d9cbc05dd39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAyODQzMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437028432", "bodyText": "That makes sense. I added a check and if both status or scale have length > 1 it throws RuntimeException which should stop it.", "author": "scholzj", "createdAt": "2020-06-08T22:08:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjczNDkzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r436751993", "bodyText": "This isn't correct. A Connector.taskConfigs(int maxTasks) is called by Connect to get the tasks configs, but the connector is not obliged to return maxTasks configs. Some connectors (such as the Debezium [MySQL connector}(https://debezium.io/documentation/reference/1.1/connectors/mysql.html)) return fewer task configs. So we would at least need to get the actual number of tasks that the connector is using, via the REST API, rather than copying the value from the spec.\nSomething else to be aware of: for a source connector it's possible that returning >1 task config when there's no way of partitioning the source data will result in duplicated records in the topic, because two or more tasks consume the same events from the source system. In theory people should be writing their connectors to return only 1 task config in that case, but that doesn't mean people actually do that in practice. Turning an autoscaler loose on such connectors would result in dupes. So it might be worthwhile have some way, in the CR, of opting in to having these published on the status (or maybe autoscalers already work by selecting their victims via labels, and it would be up to the user to not configure those labels for KafkaConnector resources which didn't properly support multiple tasks.", "author": "tombentley", "createdAt": "2020-06-08T14:28:00Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/AbstractConnectOperator.java", "diffHunk": "@@ -565,6 +565,7 @@ public static void updateStatus(Throwable error, KafkaConnector kafkaConnector2,\n         }\n         StatusUtils.setStatusConditionAndObservedGeneration(connector, status, error != null ? Future.failedFuture(error) : Future.succeededFuture());\n         status.setConnectorStatus(statusResult);\n+        status.setTasksMax(connector.getSpec().getTasksMax());", "originalCommit": "d1287c310907e51c64e8303d59341d9cbc05dd39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAzNjE3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437036170", "bodyText": "I actually think using the maximum number is better. Otherwise you can in theory run in loop trying to scale more than what is the number of tasks the connector actually runs. So I do not think this is an issue. If you would scale an deployment, it would actually make sure the pods are running, but will also have no control over whether they do something or whether they do duplicate work. So I do not think it is radically different.\nIn general, all what this PR allows is that you can do something like kubectl scale <kind> <name> --replicas N. So on its own this is not different from the user taking the connector and scaling it to N tasks by editing the resource. It does not so any autoscaling, it is just an enabler.\nAchieving some form of autoscaling is more complicated and needs some more steps. For the resources which have the LabalSelector (Connect, Mirror Maker, Bridge), the user can create a HorizontalPodAutoscaler resource and have it to autoscale the resources based on some metric. So the users needs to do some additional deliberate action where (s)he hopefully considers the consequences. Out of the box, the scaling can be done for example based on CPU utilization - but we know that with Kafka this is not sufficient due to consumer groups etc. So user would really need some additional tooling to deal with it.\nWith things such as KafkaConnector, the HPA as far as I know cannot be used because it does not have the label selector (and there is really nothing to select since the resource does not live as pod or any other Kubernetes resource). TBH, I'm not sure how much use would the scale subresource get here unless we (or some user) write some specific tooling around it. But it seems like a small effort and something what would not hurt and might enable interesting patterns.\nA chapter on its own would be probably something like scaling of Kafka topics which is I guess even more tricky (since you cannot scale down).", "author": "scholzj", "createdAt": "2020-06-08T22:28:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzQ5NzYxMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437497610", "bodyText": "Otherwise you can in theory run in loop trying to scale more than what is the number of tasks the connector actually runs.\n\nI'm not sure I understand this loop. But I can see a looping problem when the task.max is used as the scale in the status:\n\nSome actor decides (based on $metrics) that more scale is needed.\nThey increase the scale in the spec\nBecause the the connector has some hard coded limit about the number of tasks creates it ignores the task.max\nBut the operator updates the status to match the spec, so it looks like that many tasks have been created.\nThe $metrics in point 1 haven't changed, so the scale is increased again.", "author": "tombentley", "createdAt": "2020-06-09T15:07:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzUxODEyMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437518122", "bodyText": "Well, the loop I would be afraid of is when the scaler decides that the scale should be N and is triggering it all the time while the custom resource is ignoring it because the connector decided it will use only 1 task. The loop you are describing will normally stop at some upper boundary set in the autoscaler.\nI'm also not completely sure what your concern is. I can see how the user can screw it up. So can he screw up scaling for Kafka Consumer and many other workloads running in Kubernetes Deployment. Yet I do not think it is a reson to remove the scale resource from Deployments. In this case it is actually a bit esier since the tasks seem to be fairly virtual, so the scaler would stop at some upper limit and not even waste so much resources as with Deployments and Pods for example.\nAnyway, one of the issues is that the maxTasks are not mandatory and do not have any default in Strimzi. I'm not yet sure how to deal with it - might decide to just remove the scale subresource from the connector to make my life easier.", "author": "scholzj", "createdAt": "2020-06-09T15:24:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkyNzM5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437927390", "bodyText": "the loop I would be afraid of is when the scaler decides that the scale should be N and is triggering it all the time while the custom resource is ignoring it because the connector decided it will use only 1 task.\n\nI would argue that a scaler should observe that the status.observedGeneration matches the metadata.generation after the scaling, but the status.${scale} does not reflect the spec.${scale} and infer that, for whatever reason, the resource can't be scaled that high (or at least, not right now) and thus not try to increase the scale again. I say \"I would argue\", because until there are actually scalers for CRs like this it's a bit of a moot point. And perhaps that's an argument for not making the KafkaConnector scalable at this point. Or perhaps we should open a conversation with the KEDA folks about this. Because it would be annoying if we made what turned out to be the wrong decision here.\n\nThe loop you are describing will normally stop at some upper boundary set in the autoscaler.\n\nDoes the autoscale have such a limit and is it mandatory for it to be set?", "author": "tombentley", "createdAt": "2020-06-10T07:48:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk1NzU1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437957550", "bodyText": "I'm afraid I still don't understand the concerns you have. Yes, some connectors cannot scale. This is not unique for connectors, there are many applications which do not scale and yet they have the scale subresource in their deployments and just don't use it. This doesn't add any new features - it just makes it easier to change the .spec.tasksMax field using the scale subresource. But even today a user can go and set tasksMax to 10 for a connector which can do only one task. So for me this is something what can help some users and will not hurt others. Once again, this does not do any autoscaling unless user plugs it into some different tooling - so it will not scale to milion tasks for anyone out of the box. So what is the issue you expect?\n\nI would argue that a scaler should observe that the status.observedGeneration matches the metadata.generation after the scaling, but the status.${scale} does not reflect the spec.${scale} and infer that, for whatever reason, the resource can't be scaled that high (or at least, not right now) and thus not try to increase the scale again.\n\nNot sure what observedGeneration has to do with it. It changes one way or another. I also after my investigation don't think you can get any better number of tasks. The connector status is using very ambigous and doesn't for example allow you to determine if UNASSIGNED tasks are scaled down or if they are just temporarily unassigned because of node restarts. So for example counting all tasks in status or counting running tasks does not give any reasonable value either.\n\nI say \"I would argue\", because until there are actually scalers for CRs like this it's a bit of a moot point. And perhaps that's an argument for not making the KafkaConnector scalable at this point.\n\nI think this is very unfriendly to the ecosystem. If you wait for the tooling to exist, you basically wait for someone to implement it in a much more complicated way (by hardcoding the mechanism for editing KadkaConnector CR and changing tasksMax number). And then you would implement the scale subresource and tell them hey, you can do this in a much easier way.\n\nDoes the autoscale have such a limit and is it mandatory for it to be set?\n\nI do not think this question has a single answer. AFAIK HorizontalPodAutoscaler has maxReplicas as required value. But there are many ways how to implement autoscaling and I do not know if it applies to all of them. And as I said, HorizontalPodAutoscaler does not apply to KafkaConnector but only to pod based resources.", "author": "scholzj", "createdAt": "2020-06-10T08:38:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODAzNzc5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r438037794", "bodyText": "I'm afraid I still don't understand the concerns you have.\n\nI think you're misunderstanding my point. I'm not against adding scale subresource to KafkaConnector at all. I'm just trying to understand the consequences of having a scale in the status that does not reflect the true scale at which the connector be being run. In general having system A lie to system B about something which system B will use to automated system A seems like it could go wrong, so I want to be pretty sure that there are no negative consequences, as you assert.\n\nthere are many applications which do not scale and yet they have the scale subresource in their deployments and just don't use it.\n\nWell, if there's an established pattern of having the scale in the status not actually reflect the true scale, because it's being ignored then I guess that's fine. But it would be good to know which operators are already doing this.\n\nNot sure what observedGeneration has to do with it.\n\nBy comparing the observedGeneration with the metadata.generation the scaler can infer whether the change in scale that was requests has been actioned by the operator. If the generations match and the status scale is not the spec scale then the scaler can infer that the scaling didn't happen for some reason.\n\nIf you wait for the tooling to exist, you basically wait for someone to implement it in a much more complicated way\n\nI'm not saying we have to wait, as such. But since this is the interface that any such tooling will use I do think it's important that we implement something which will actually work when that tooling exists.\n\nthere are many ways how to implement autoscaling and I do not know if it applies to all of them\n\nSo that means that \"my\" loop is indeed potentially an issue.", "author": "tombentley", "createdAt": "2020-06-10T10:59:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODA3NDA3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r438074076", "bodyText": "Well, if there's an established pattern of having the scale in the status not actually reflect the true scale, because it's being ignored then I guess that's fine. But it would be good to know which operators are already doing this.\n\nI think that depends on the point of view. Imagine a Kafka Consumer running in a Deployment and consuming from a topic with 1 partition. Now when you scale the deployment to 5 replicas, they will be all running and the deployment will report 5 running replicas. But in reality all but one will be idle waiting for some consumer group rebalancer whihc might assign them some partition. Is the deployment lying when saying that there are 5 pod running? And Kafka is just an example - it is not the only application using some kind of locking mechanism and not being able to scale infinitely.\nAs I see it, our situation is not really different. We tell Connect / connector to run N tasks and if in reality for its internal reasons it decides to do some work only in one of them it does not differ from the idle pods in the Kafka Consumer deployment.\nThis is of course a bit my view which might differ from yours.\n\nBy comparing the observedGeneration with the metadata.generation the scaler can infer whether the change in scale that was requests has been actioned by the operator. If the generations match and the status scale is not the spec scale then the scaler can infer that the scaling didn't happen for some reason.\n\nThe main point of the scale subresource is to allow you to deal with scaling without understanding the structure of all resources. The /scale endpoint has a fixed structure given by Kubernetes. So it can be used by applications without understanding our CRDs. If you expect the application to decode the whole CR and understand it (to know where to find the right fields etc.), it would not need the scale subresource since it can directly change the .spec.tasksMax field. The scale subresource abstracts this and makes it easier to scale the resources and at the end allows you to write generic scalers. This is important, because for example for source connectors, you would need a scaler which isn't necessarily Kafka aware but rather understands the source system and maybe never heard about Strimzi.\n\nI'm not saying we have to wait, as such. But since this is the interface that any such tooling will use I do think it's important that we implement something which will actually work when that tooling exists.\n\nTo be honest, I think we will never find out if nobody uses it because we do not allow it to be used. I of course cannto guarantee how it will or will not work with different tools.\n\nSo that means that \"my\" loop is indeed potentially an issue.\n\nWell, when you include infinite number of autoscaling tools which might exist somewhere in the world there will be always some which will screw up and do something crazy ;-)", "author": "scholzj", "createdAt": "2020-06-10T12:13:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODA3NzkzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r438077939", "bodyText": "So going back to this:\n\nNot sure what observedGeneration has to do with it. It changes one way or another. I also after my investigation don't think you can get any better number of tasks. The connector status is using very ambigous and doesn't for example allow you to determine if UNASSIGNED tasks are scaled down or if they are just temporarily unassigned because of node restarts. So for example counting all tasks in status or counting running tasks does not give any reasonable value either.\n\nDo you have any better idea how to get the actual number of tasks for the status if you think that is what should be there? You worked with it more while working on the Connector operator ... so is there some actual way how to get that number?", "author": "scholzj", "createdAt": "2020-06-10T12:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwMTUyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r438101523", "bodyText": "Imagine a Kafka Consumer running in a Deployment and consuming from a topic with 1 partition. Now when you scale the deployment to 5 replicas, they will be all running and the deployment will report 5 running replicas. But in reality all but one will be idle waiting for some consumer group rebalancer whihc might assign them some partition.\n\nThat's a useful analogy, thanks for explaining it. I guess this is likely to be something that is common to all workloads which scale by sharding and where it's not possible to reshard the workload to adjust to the available processing units (in this case pods). It makes me wonder how an autoscaler can, in general cope with this. That's not our problem to solve, but I would love to know the thoughts of someone who was implementing such a scaler.\n\nDo you have any better idea how to get the actual number of tasks for the status\n\nSadly not. Your point about UNASSIGNED tasks is valid, but from the PoV of Kafka Connect I think there are good reasons why the scaled-down and mid-reassignment cases are conflated as UNASSIGNED. I guess we could investigate whether it would be possible to expose something which allowed us to distinguish. But for now I don't have any better ideas.\nI think you've persuaded me that this is the best we can do for how.", "author": "tombentley", "createdAt": "2020-06-10T12:59:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1MTk5Mw=="}], "type": "inlineReview"}, {"oid": "6a5727df14b12716df3007386f78695e355983af", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6a5727df14b12716df3007386f78695e355983af", "message": "Add scaling to Bridge and MM\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-08T21:47:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzIzMTcwMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437231702", "bodyText": "I think we are missing a full stop here", "author": "samuel-hawker", "createdAt": "2020-06-09T08:33:49Z", "path": "api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.java", "diffHunk": "@@ -39,4 +40,14 @@\n     public void setConnectorStatus(Map<String, Object> connectorStatus) {\n         this.connectorStatus = connectorStatus;\n     }\n+\n+    @JsonInclude(JsonInclude.Include.NON_NULL)\n+    @Description(\"The maximum number of tasks for the Kafka Connector\")", "originalCommit": "6a5727df14b12716df3007386f78695e355983af", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzI0OTA3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437249073", "bodyText": "The DocGenerator will add a full stop if one is missing.", "author": "tombentley", "createdAt": "2020-06-09T08:59:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzIzMTcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk0MjYzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r437942636", "bodyText": "I wonder if it is worth a small doc comment explaining why this is the 'actual' value and why tasksMax is sometimes not set?", "author": "samuel-hawker", "createdAt": "2020-06-10T08:14:35Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/AbstractConnectOperator.java", "diffHunk": "@@ -565,14 +565,28 @@ public static void updateStatus(Throwable error, KafkaConnector kafkaConnector2,\n         }\n         StatusUtils.setStatusConditionAndObservedGeneration(connector, status, error != null ? Future.failedFuture(error) : Future.succeededFuture());\n         status.setConnectorStatus(statusResult);\n-        status.setTasksMax(connector.getSpec().getTasksMax());\n+\n+        status.setTasksMax(getActualTaskCount(connector, statusResult));\n \n         return maybeUpdateStatusCommon(connectorOperator, connector, reconciliation, status,\n             (connector1, status1) -> {\n                 return new KafkaConnectorBuilder(connector1).withStatus(status1).build();\n             });\n     }\n \n+    protected int getActualTaskCount(KafkaConnector connector, Map<String, Object> statusResult)  {", "originalCommit": "745ddd648be12ca0b1dba0a9d836f9e8b763252f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ed16b7f96eb238b68a2f3fde1db14fb10b24f12b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ed16b7f96eb238b68a2f3fde1db14fb10b24f12b", "message": "Add support for scale subresource in KafkaConnect, its derivatives and KafkaConnector\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T12:58:27Z", "type": "commit"}, {"oid": "38ea61283aa2faa65c379db86d8149950251ea76", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/38ea61283aa2faa65c379db86d8149950251ea76", "message": "Apply suggestions from code review\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>\n\nCo-authored-by: Tom Bentley <tombentley@users.noreply.github.com>", "committedDate": "2020-06-10T12:58:27Z", "type": "commit"}, {"oid": "48f19a59d7f555771e1da6585512ee623f17f408", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/48f19a59d7f555771e1da6585512ee623f17f408", "message": "Review comments\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T12:58:27Z", "type": "commit"}, {"oid": "c40d93bf12492a071532c01b4e0c0c904d3ee652", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c40d93bf12492a071532c01b4e0c0c904d3ee652", "message": "Add scaling to Bridge and MM\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T12:58:27Z", "type": "commit"}, {"oid": "d3fea37eda0f24898bbae6baa579550bf12f47f7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d3fea37eda0f24898bbae6baa579550bf12f47f7", "message": "Make existing tests pass and fix some review comments\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T12:58:27Z", "type": "commit"}, {"oid": "8e4b00b8b2eafcb0de9d3e4dcfbee630ac8edfb6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8e4b00b8b2eafcb0de9d3e4dcfbee630ac8edfb6", "message": "Add CRD tests\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T12:58:27Z", "type": "commit"}, {"oid": "5587b101f93314cbe0d4ba44c02359cadbd1033e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5587b101f93314cbe0d4ba44c02359cadbd1033e", "message": "Regen of APi reference after rebase\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T13:31:35Z", "type": "commit"}, {"oid": "5587b101f93314cbe0d4ba44c02359cadbd1033e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5587b101f93314cbe0d4ba44c02359cadbd1033e", "message": "Regen of APi reference after rebase\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T13:31:35Z", "type": "forcePushed"}, {"oid": "e6f9dc3633e22e5c013143f10dbe07634099e9b9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e6f9dc3633e22e5c013143f10dbe07634099e9b9", "message": "Add more tests for setitng the right statuses in the different resources, try to fix the CRD tests on Minikube\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T15:03:44Z", "type": "commit"}, {"oid": "c2b68d0d31243d02196e626253c1bc971a900716", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c2b68d0d31243d02196e626253c1bc971a900716", "message": "Fix unused imports and improve CHANGELOG.md\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T15:08:37Z", "type": "commit"}, {"oid": "43afe7d779e757feab785e34229a2e51cde88ec4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/43afe7d779e757feab785e34229a2e51cde88ec4", "message": "Travis seems to be too fast?\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T15:49:12Z", "type": "commit"}, {"oid": "8f540ed61246bc20a75603dc3eb837a87410ac21", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8f540ed61246bc20a75603dc3eb837a87410ac21", "message": "Try to fix Travis race conditions\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T17:08:35Z", "type": "commit"}, {"oid": "0ed8a51f3ff57bd2851532966f0815e64c7b2470", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0ed8a51f3ff57bd2851532966f0815e64c7b2470", "message": "Try to fix Travis race conditions II\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T17:28:18Z", "type": "commit"}, {"oid": "e8dc917a4bd82bfaee299464593abeb10d6ee849", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e8dc917a4bd82bfaee299464593abeb10d6ee849", "message": "Debug Travis\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T17:43:52Z", "type": "commit"}, {"oid": "2dd8d70d0c5690fd287dbd1d68304edaa8f0e5e3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2dd8d70d0c5690fd287dbd1d68304edaa8f0e5e3", "message": "Bump kubectl to 1.16.0 since the issue seems ot be caused by Kubernetes bug #81342\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T18:55:21Z", "type": "commit"}, {"oid": "4a3bc1f4f4de8122145912e05c169f4cd1e0dbc3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/4a3bc1f4f4de8122145912e05c169f4cd1e0dbc3", "message": "Cleanup previous debug and fix attempts\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T19:41:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5OTkxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r438399914", "bodyText": "isn't this technically just a condition as opposed to ready as we can pass in Predicates for any user defined condition?", "author": "samuel-hawker", "createdAt": "2020-06-10T20:49:29Z", "path": "test/src/main/java/io/strimzi/test/k8s/cmdClient/KubeCmdClient.java", "diffHunk": "@@ -111,6 +123,15 @@ default K delete(String... files) {\n      */\n     ExecResult exec(boolean throwError, boolean logToOutput, String... command);\n \n+    /**\n+     * Wait for the resource with the given {@code name} to be reach the state defined by the predicate.\n+     * @param resource The resource type.\n+     * @param name The resource name.\n+     * @param ready Predicate to test if the resource is or isn't \"ready\"", "originalCommit": "4a3bc1f4f4de8122145912e05c169f4cd1e0dbc3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQxOTA4MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3165#discussion_r438419081", "bodyText": "You are right - I used the oriinal name ready which might be confusing. I renamed it to condition to give it a more corresponding name.", "author": "scholzj", "createdAt": "2020-06-10T21:29:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5OTkxNA=="}], "type": "inlineReview"}, {"oid": "c80d559352bb649d3f86d3a69b52e32d3cc5899d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c80d559352bb649d3f86d3a69b52e32d3cc5899d", "message": "Use better name in waitFor method\n\nSigned-off-by: Jakub Scholz <www@scholzj.com>", "committedDate": "2020-06-10T21:28:57Z", "type": "commit"}]}