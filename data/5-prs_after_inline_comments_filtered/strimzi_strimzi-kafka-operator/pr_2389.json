{"pr_number": 2389, "pr_title": "Dynamic configuration of Kafka brokers", "pr_createdAt": "2020-01-13T12:06:15Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389", "timeline": [{"oid": "ff8813feb862ecee12fb23dc80a3fab042ee616f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ff8813feb862ecee12fb23dc80a3fab042ee616f", "message": "squash after rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "f62acf43eec9d4d283632d4c5fd9329103fefc11", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f62acf43eec9d4d283632d4c5fd9329103fefc11", "message": "roll stucked pods\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "ef1de3c01accb3262d12d3edd08fadffd8797a66", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ef1de3c01accb3262d12d3edd08fadffd8797a66", "message": "transfer kafkaFuture to vertx one\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "f004f33ee3ad1e0164833c044b438a62c7c02d21", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f004f33ee3ad1e0164833c044b438a62c7c02d21", "message": "logging + correct assertion messages\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "b136f81a13b8ab94a7cea33dc53abf483e27d73a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b136f81a13b8ab94a7cea33dc53abf483e27d73a", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE5MDM0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r366190348", "bodyText": "was it removed by mistake or on purpose? isn't it something added recently by Jakub?", "author": "ppatierno", "createdAt": "2020-01-14T07:48:56Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1675,62 +1573,6 @@ private KafkaVersionChange getKafkaVersionChange(StatefulSet kafkaSts) {\n             return withVoid(blockingPromise.future());\n         }\n \n-        Future<ReconciliationState> kafkaNodePortExternalListenerStatus() {", "originalCommit": "2236f083d5440d12657ee849c9eef52c05761745", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE5MTI4NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r366191285", "bodyText": "This is probably some rebasing incident :( let me fix this.", "author": "sknot-rh", "createdAt": "2020-01-14T07:51:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE5MDM0OA=="}], "type": "inlineReview"}, {"oid": "02973eebfc98d1906e117d506482acba3b3c12f9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/02973eebfc98d1906e117d506482acba3b3c12f9", "message": "WIP\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-01-14T08:22:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIzMzU2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r366233562", "bodyText": "I guess we have the right list somewhere?", "author": "ppatierno", "createdAt": "2020-01-14T09:36:21Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(\n+            \"test.property.name\"));", "originalCommit": "186e05afc9f1456e18cd4187ed453567165d4a1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI2MTAxOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r366261019", "bodyText": "We have a google doc with some hints.", "author": "sknot-rh", "createdAt": "2020-01-14T10:30:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIzMzU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI0MDY1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r366240652", "bodyText": "a couple of considerations, maybe we should take a look at the AdminClient Kafka source code.\nIs it going to work if you close the admin client instance ac.close() before getting the result?\nIs it a blocking operation? should we need a more async way with Vert.x execute blocking code?", "author": "ppatierno", "createdAt": "2020-01-14T09:50:17Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaSetOperator.java", "diffHunk": "@@ -72,4 +88,55 @@ public static boolean needsRollingUpdate(StatefulSetDiff diff) {\n                 .rollingRestart(podNeedsRestart);\n     }\n \n+    /**\n+     * @param sts Stateful set to which kafka pod belongs\n+     * @param pod Specific kafka pod\n+     * @return a future which contains map with all kafka properties including their values\n+     */\n+    public Future<Map<ConfigResource, Config>> getCurrentConfig(StatefulSet sts, Pod pod) {\n+        String cluster = sts.getMetadata().getLabels().get(Labels.STRIMZI_CLUSTER_LABEL);\n+        String namespace = sts.getMetadata().getNamespace();\n+        Future<Secret> clusterCaKeySecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+        Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(cluster));\n+        int podId = Integer.parseInt(pod.getMetadata().getName().substring(pod.getMetadata().getName().lastIndexOf(\"-\") + 1));\n+        return CompositeFuture.join(clusterCaKeySecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+            Secret clusterCaKeySecret = compositeFuture.resultAt(0);\n+            if (clusterCaKeySecret == null) {\n+                return Future.failedFuture(missingSecretFuture(namespace, KafkaCluster.clusterCaKeySecretName(cluster)));\n+            }\n+            Secret coKeySecret = compositeFuture.resultAt(1);\n+            if (coKeySecret == null) {\n+                return Future.failedFuture(missingSecretFuture(namespace, ClusterOperator.secretName(cluster)));\n+            }\n+            return getCurrentConfig(podId, namespace, cluster, clusterCaKeySecret, coKeySecret);\n+        });\n+    }\n+\n+    public Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, String namespace, String cluster,\n+                                         Secret clusterCaCertSecret, Secret coKeySecret) {\n+        Promise<Map<ConfigResource, Config>> futRes = Promise.promise();\n+        String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+        AdminClient ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret);\n+        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+        DescribeConfigsResult configs = ac.describeConfigs(Collections.singletonList(resource));\n+        ac.close();\n+        Map<ConfigResource, Config> config = null;\n+        try {\n+            config = configs.all().get(1000, TimeUnit.MILLISECONDS);", "originalCommit": "186e05afc9f1456e18cd4187ed453567165d4a1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI2MDYwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r366260603", "bodyText": "I totally agree with you. Consider this piece of code to be here just for the testing purposes (you see it is not called anywhere). I used it to extract the \"current\" Kafka configuration. In the final phase we could use some of it but as you say... we have to polish it.", "author": "sknot-rh", "createdAt": "2020-01-14T10:29:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI0MDY1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxMjUxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369412511", "bodyText": "This is not so easy.\n\nIf something, shouldn't you use them out of the broker configuration helper classes which we already have?\nMany of these are not really changeable. For example ssl.keystore cannot be updated when the certificate CN / SANs change. So you need to have a special treatment for this.\nLot of the configs are not used like this in our brokers but with a prefix as  aper-listener configurations.\n\nI wonder if you should instead of this have some lists of options which are not changeable + their esceptions similarly to how we do it for the user suplied configuration.", "author": "scholzj", "createdAt": "2020-01-22T07:59:43Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxMjczOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369412739", "bodyText": "Wouldn't it make sense to leave the config map out of this and pass directly the configuration? Or what is the advantage of passing the ConfigMap here?", "author": "scholzj", "createdAt": "2020-01-22T08:00:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(\n+            // per broker\n+            \"advertised.listeners\",\n+            \"listeners\",\n+            \"principal.builder.class\",\n+            \"sasl.enabled.mechanisms\",\n+            \"sasl.jaas.config\",\n+            \"sasl.kerberos.kinit.cmd\",\n+            \"sasl.kerberos.min.time.before.relogin\",\n+            \"sasl.kerberos.principal.to.local.rules\",\n+            \"sasl.kerberos.service.name\",\n+            \"sasl.kerberos.ticket.renew.jitter\",\n+            \"sasl.kerberos.ticket.renew.window.factor\",\n+            \"sasl.login.refresh.buffer.seconds\",\n+            \"sasl.login.refresh.min.period.seconds\",\n+            \"sasl.login.refresh.window.factor\",\n+            \"sasl.login.refresh.window.jitter\",\n+            \"sasl.mechanism.inter.broker.protocol\",\n+            \"ssl.cipher.suites\",\n+            \"ssl.client.auth\",\n+            \"ssl.enabled.protocols\",\n+            \"ssl.key.password\",\n+            \"ssl.keymanager.algorithm\",\n+            \"ssl.keystore.location\",\n+            \"ssl.keystore.password\",\n+            \"ssl.keystore.type\",\n+            \"ssl.protocol\",\n+            \"ssl.provider\",\n+            \"ssl.trustmanager.algorithm\",\n+            \"ssl.truststore.location\",\n+            \"ssl.truststore.password\",\n+            \"ssl.truststore.type\",\n+            \"listener.security.protocol.map\",\n+            \"ssl.endpoint.identification.algorithm\",\n+            \"ssl.secure.random.implementation\",\n+            // cluster wide\n+            \"background.threads\",\n+            \"compression.type\",\n+            \"log.flush.interval.messages\",\n+            \"log.flush.interval.ms\",\n+            \"log.retention.bytes\",\n+            \"log.retention.ms\",\n+            \"log.roll.jitter.ms\",\n+            \"log.roll.ms\",\n+            \"log.segment.bytes\",\n+            \"log.segment.delete.delay.ms\",\n+            \"message.max.bytes\",\n+            \"min.insync.replicas\",\n+            \"num.io.threads\",\n+            \"num.network.threads\",\n+            \"num.recovery.threads.per.data.dir\",\n+            \"num.replica.fetchers\",\n+            \"unclean.leader.election.enable\",\n+            \"log.cleaner.backoff.ms\",\n+            \"log.cleaner.dedupe.buffer.size\",\n+            \"log.cleaner.delete.retention.ms\",\n+            \"log.cleaner.io.buffer.load.factor\",\n+            \"log.cleaner.io.buffer.size\",\n+            \"log.cleaner.io.max.bytes.per.second\",\n+            \"log.cleaner.max.compaction.lag.ms\",\n+            \"log.cleaner.min.cleanable.ratio\",\n+            \"log.cleaner.min.compaction.lag.ms\",\n+            \"log.cleaner.threads\",\n+            \"log.cleanup.policy\",\n+            \"log.index.interval.bytes\",\n+            \"log.index.size.max.bytes\",\n+            \"log.message.timestamp.difference.max.ms\",\n+            \"log.message.timestamp.type\",\n+            \"log.preallocate\",\n+            \"max.connections\",\n+            \"max.connections.per.ip\",\n+            \"max.connections.per.ip.overrides\",\n+            \"log.message.downconversion.enable\",\n+            \"metric.reporters\"\n+            ));\n+\n+    public KafkaConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired) {", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxMzE3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369413171", "bodyText": "Why do we need the replace(\" \", \"\") here?", "author": "scholzj", "createdAt": "2020-01-22T08:01:49Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(\n+            // per broker\n+            \"advertised.listeners\",\n+            \"listeners\",\n+            \"principal.builder.class\",\n+            \"sasl.enabled.mechanisms\",\n+            \"sasl.jaas.config\",\n+            \"sasl.kerberos.kinit.cmd\",\n+            \"sasl.kerberos.min.time.before.relogin\",\n+            \"sasl.kerberos.principal.to.local.rules\",\n+            \"sasl.kerberos.service.name\",\n+            \"sasl.kerberos.ticket.renew.jitter\",\n+            \"sasl.kerberos.ticket.renew.window.factor\",\n+            \"sasl.login.refresh.buffer.seconds\",\n+            \"sasl.login.refresh.min.period.seconds\",\n+            \"sasl.login.refresh.window.factor\",\n+            \"sasl.login.refresh.window.jitter\",\n+            \"sasl.mechanism.inter.broker.protocol\",\n+            \"ssl.cipher.suites\",\n+            \"ssl.client.auth\",\n+            \"ssl.enabled.protocols\",\n+            \"ssl.key.password\",\n+            \"ssl.keymanager.algorithm\",\n+            \"ssl.keystore.location\",\n+            \"ssl.keystore.password\",\n+            \"ssl.keystore.type\",\n+            \"ssl.protocol\",\n+            \"ssl.provider\",\n+            \"ssl.trustmanager.algorithm\",\n+            \"ssl.truststore.location\",\n+            \"ssl.truststore.password\",\n+            \"ssl.truststore.type\",\n+            \"listener.security.protocol.map\",\n+            \"ssl.endpoint.identification.algorithm\",\n+            \"ssl.secure.random.implementation\",\n+            // cluster wide\n+            \"background.threads\",\n+            \"compression.type\",\n+            \"log.flush.interval.messages\",\n+            \"log.flush.interval.ms\",\n+            \"log.retention.bytes\",\n+            \"log.retention.ms\",\n+            \"log.roll.jitter.ms\",\n+            \"log.roll.ms\",\n+            \"log.segment.bytes\",\n+            \"log.segment.delete.delay.ms\",\n+            \"message.max.bytes\",\n+            \"min.insync.replicas\",\n+            \"num.io.threads\",\n+            \"num.network.threads\",\n+            \"num.recovery.threads.per.data.dir\",\n+            \"num.replica.fetchers\",\n+            \"unclean.leader.election.enable\",\n+            \"log.cleaner.backoff.ms\",\n+            \"log.cleaner.dedupe.buffer.size\",\n+            \"log.cleaner.delete.retention.ms\",\n+            \"log.cleaner.io.buffer.load.factor\",\n+            \"log.cleaner.io.buffer.size\",\n+            \"log.cleaner.io.max.bytes.per.second\",\n+            \"log.cleaner.max.compaction.lag.ms\",\n+            \"log.cleaner.min.cleanable.ratio\",\n+            \"log.cleaner.min.compaction.lag.ms\",\n+            \"log.cleaner.threads\",\n+            \"log.cleanup.policy\",\n+            \"log.index.interval.bytes\",\n+            \"log.index.size.max.bytes\",\n+            \"log.message.timestamp.difference.max.ms\",\n+            \"log.message.timestamp.type\",\n+            \"log.preallocate\",\n+            \"max.connections\",\n+            \"max.connections.per.ip\",\n+            \"max.connections.per.ip.overrides\",\n+            \"log.message.downconversion.enable\",\n+            \"metric.reporters\"\n+            ));\n+\n+    public KafkaConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired) {\n+        this.current = current;\n+        this.desired = desired;\n+    }\n+\n+    private Properties configMap2Properties() {\n+        Properties result = new Properties();\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxNDgyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369414828", "bodyText": "Thsi is very confusing for me. Current broker configuration is a map of the actual broker configuration? If so, how do you know the first line gives you broker ID? Do you just expect it to be always on the first line? If so, this is a bit dangerous and you should also explain it in comments. But I think this code is very hard to read and nobody will properly undertsand it in few weeks. It has to be rewritten into something more readable.", "author": "scholzj", "createdAt": "2020-01-22T08:07:02Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(\n+            // per broker\n+            \"advertised.listeners\",\n+            \"listeners\",\n+            \"principal.builder.class\",\n+            \"sasl.enabled.mechanisms\",\n+            \"sasl.jaas.config\",\n+            \"sasl.kerberos.kinit.cmd\",\n+            \"sasl.kerberos.min.time.before.relogin\",\n+            \"sasl.kerberos.principal.to.local.rules\",\n+            \"sasl.kerberos.service.name\",\n+            \"sasl.kerberos.ticket.renew.jitter\",\n+            \"sasl.kerberos.ticket.renew.window.factor\",\n+            \"sasl.login.refresh.buffer.seconds\",\n+            \"sasl.login.refresh.min.period.seconds\",\n+            \"sasl.login.refresh.window.factor\",\n+            \"sasl.login.refresh.window.jitter\",\n+            \"sasl.mechanism.inter.broker.protocol\",\n+            \"ssl.cipher.suites\",\n+            \"ssl.client.auth\",\n+            \"ssl.enabled.protocols\",\n+            \"ssl.key.password\",\n+            \"ssl.keymanager.algorithm\",\n+            \"ssl.keystore.location\",\n+            \"ssl.keystore.password\",\n+            \"ssl.keystore.type\",\n+            \"ssl.protocol\",\n+            \"ssl.provider\",\n+            \"ssl.trustmanager.algorithm\",\n+            \"ssl.truststore.location\",\n+            \"ssl.truststore.password\",\n+            \"ssl.truststore.type\",\n+            \"listener.security.protocol.map\",\n+            \"ssl.endpoint.identification.algorithm\",\n+            \"ssl.secure.random.implementation\",\n+            // cluster wide\n+            \"background.threads\",\n+            \"compression.type\",\n+            \"log.flush.interval.messages\",\n+            \"log.flush.interval.ms\",\n+            \"log.retention.bytes\",\n+            \"log.retention.ms\",\n+            \"log.roll.jitter.ms\",\n+            \"log.roll.ms\",\n+            \"log.segment.bytes\",\n+            \"log.segment.delete.delay.ms\",\n+            \"message.max.bytes\",\n+            \"min.insync.replicas\",\n+            \"num.io.threads\",\n+            \"num.network.threads\",\n+            \"num.recovery.threads.per.data.dir\",\n+            \"num.replica.fetchers\",\n+            \"unclean.leader.election.enable\",\n+            \"log.cleaner.backoff.ms\",\n+            \"log.cleaner.dedupe.buffer.size\",\n+            \"log.cleaner.delete.retention.ms\",\n+            \"log.cleaner.io.buffer.load.factor\",\n+            \"log.cleaner.io.buffer.size\",\n+            \"log.cleaner.io.max.bytes.per.second\",\n+            \"log.cleaner.max.compaction.lag.ms\",\n+            \"log.cleaner.min.cleanable.ratio\",\n+            \"log.cleaner.min.compaction.lag.ms\",\n+            \"log.cleaner.threads\",\n+            \"log.cleanup.policy\",\n+            \"log.index.interval.bytes\",\n+            \"log.index.size.max.bytes\",\n+            \"log.message.timestamp.difference.max.ms\",\n+            \"log.message.timestamp.type\",\n+            \"log.preallocate\",\n+            \"max.connections\",\n+            \"max.connections.per.ip\",\n+            \"max.connections.per.ip.overrides\",\n+            \"log.message.downconversion.enable\",\n+            \"metric.reporters\"\n+            ));\n+\n+    public KafkaConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired) {\n+        this.current = current;\n+        this.desired = desired;\n+    }\n+\n+    private Properties configMap2Properties() {\n+        Properties result = new Properties();\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    public Set<String> getDiff() {\n+        Properties desiredConfig = configMap2Properties();\n+        Set<String> result = new HashSet<>();\n+        Map.Entry<ConfigResource, Config> currentBrokerConfiguration = current.entrySet().iterator().next();", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxNTg0NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369415844", "bodyText": "This comment is confusing. It suggests that the if bellow is for pod dependent eentry. But that seems to be the else. Please fix.", "author": "scholzj", "createdAt": "2020-01-22T08:09:52Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(\n+            // per broker\n+            \"advertised.listeners\",\n+            \"listeners\",\n+            \"principal.builder.class\",\n+            \"sasl.enabled.mechanisms\",\n+            \"sasl.jaas.config\",\n+            \"sasl.kerberos.kinit.cmd\",\n+            \"sasl.kerberos.min.time.before.relogin\",\n+            \"sasl.kerberos.principal.to.local.rules\",\n+            \"sasl.kerberos.service.name\",\n+            \"sasl.kerberos.ticket.renew.jitter\",\n+            \"sasl.kerberos.ticket.renew.window.factor\",\n+            \"sasl.login.refresh.buffer.seconds\",\n+            \"sasl.login.refresh.min.period.seconds\",\n+            \"sasl.login.refresh.window.factor\",\n+            \"sasl.login.refresh.window.jitter\",\n+            \"sasl.mechanism.inter.broker.protocol\",\n+            \"ssl.cipher.suites\",\n+            \"ssl.client.auth\",\n+            \"ssl.enabled.protocols\",\n+            \"ssl.key.password\",\n+            \"ssl.keymanager.algorithm\",\n+            \"ssl.keystore.location\",\n+            \"ssl.keystore.password\",\n+            \"ssl.keystore.type\",\n+            \"ssl.protocol\",\n+            \"ssl.provider\",\n+            \"ssl.trustmanager.algorithm\",\n+            \"ssl.truststore.location\",\n+            \"ssl.truststore.password\",\n+            \"ssl.truststore.type\",\n+            \"listener.security.protocol.map\",\n+            \"ssl.endpoint.identification.algorithm\",\n+            \"ssl.secure.random.implementation\",\n+            // cluster wide\n+            \"background.threads\",\n+            \"compression.type\",\n+            \"log.flush.interval.messages\",\n+            \"log.flush.interval.ms\",\n+            \"log.retention.bytes\",\n+            \"log.retention.ms\",\n+            \"log.roll.jitter.ms\",\n+            \"log.roll.ms\",\n+            \"log.segment.bytes\",\n+            \"log.segment.delete.delay.ms\",\n+            \"message.max.bytes\",\n+            \"min.insync.replicas\",\n+            \"num.io.threads\",\n+            \"num.network.threads\",\n+            \"num.recovery.threads.per.data.dir\",\n+            \"num.replica.fetchers\",\n+            \"unclean.leader.election.enable\",\n+            \"log.cleaner.backoff.ms\",\n+            \"log.cleaner.dedupe.buffer.size\",\n+            \"log.cleaner.delete.retention.ms\",\n+            \"log.cleaner.io.buffer.load.factor\",\n+            \"log.cleaner.io.buffer.size\",\n+            \"log.cleaner.io.max.bytes.per.second\",\n+            \"log.cleaner.max.compaction.lag.ms\",\n+            \"log.cleaner.min.cleanable.ratio\",\n+            \"log.cleaner.min.compaction.lag.ms\",\n+            \"log.cleaner.threads\",\n+            \"log.cleanup.policy\",\n+            \"log.index.interval.bytes\",\n+            \"log.index.size.max.bytes\",\n+            \"log.message.timestamp.difference.max.ms\",\n+            \"log.message.timestamp.type\",\n+            \"log.preallocate\",\n+            \"max.connections\",\n+            \"max.connections.per.ip\",\n+            \"max.connections.per.ip.overrides\",\n+            \"log.message.downconversion.enable\",\n+            \"metric.reporters\"\n+            ));\n+\n+    public KafkaConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired) {\n+        this.current = current;\n+        this.desired = desired;\n+    }\n+\n+    private Properties configMap2Properties() {\n+        Properties result = new Properties();\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    public Set<String> getDiff() {\n+        Properties desiredConfig = configMap2Properties();\n+        Set<String> result = new HashSet<>();\n+        Map.Entry<ConfigResource, Config> currentBrokerConfiguration = current.entrySet().iterator().next();\n+\n+        log.info(\"Diff for broker id {}\", currentBrokerConfiguration.getKey());\n+        currentBrokerConfiguration.getValue().entries().forEach(entry -> {\n+            String val = entry.value() == null ? \"null\" : entry.value();\n+\n+            if (desiredConfig.containsKey(entry.name())) {\n+                if (desiredConfig.getProperty(entry.name()).equals(val)) {\n+                    // entry is in both, equal\n+                } else {\n+                    // entry is in both, not equal -> diff\n+                    if (desiredConfig.getProperty(entry.name()) != null) {\n+                        // diff is in pod dependent entry", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxNjE4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369416182", "bodyText": "How does it work with default values? I would expect that we will need some special handling for them, when some option in the broker is set to its default value or when for example our configuration file sets something to the default value.", "author": "scholzj", "createdAt": "2020-01-22T08:10:48Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(\n+            // per broker\n+            \"advertised.listeners\",\n+            \"listeners\",\n+            \"principal.builder.class\",\n+            \"sasl.enabled.mechanisms\",\n+            \"sasl.jaas.config\",\n+            \"sasl.kerberos.kinit.cmd\",\n+            \"sasl.kerberos.min.time.before.relogin\",\n+            \"sasl.kerberos.principal.to.local.rules\",\n+            \"sasl.kerberos.service.name\",\n+            \"sasl.kerberos.ticket.renew.jitter\",\n+            \"sasl.kerberos.ticket.renew.window.factor\",\n+            \"sasl.login.refresh.buffer.seconds\",\n+            \"sasl.login.refresh.min.period.seconds\",\n+            \"sasl.login.refresh.window.factor\",\n+            \"sasl.login.refresh.window.jitter\",\n+            \"sasl.mechanism.inter.broker.protocol\",\n+            \"ssl.cipher.suites\",\n+            \"ssl.client.auth\",\n+            \"ssl.enabled.protocols\",\n+            \"ssl.key.password\",\n+            \"ssl.keymanager.algorithm\",\n+            \"ssl.keystore.location\",\n+            \"ssl.keystore.password\",\n+            \"ssl.keystore.type\",\n+            \"ssl.protocol\",\n+            \"ssl.provider\",\n+            \"ssl.trustmanager.algorithm\",\n+            \"ssl.truststore.location\",\n+            \"ssl.truststore.password\",\n+            \"ssl.truststore.type\",\n+            \"listener.security.protocol.map\",\n+            \"ssl.endpoint.identification.algorithm\",\n+            \"ssl.secure.random.implementation\",\n+            // cluster wide\n+            \"background.threads\",\n+            \"compression.type\",\n+            \"log.flush.interval.messages\",\n+            \"log.flush.interval.ms\",\n+            \"log.retention.bytes\",\n+            \"log.retention.ms\",\n+            \"log.roll.jitter.ms\",\n+            \"log.roll.ms\",\n+            \"log.segment.bytes\",\n+            \"log.segment.delete.delay.ms\",\n+            \"message.max.bytes\",\n+            \"min.insync.replicas\",\n+            \"num.io.threads\",\n+            \"num.network.threads\",\n+            \"num.recovery.threads.per.data.dir\",\n+            \"num.replica.fetchers\",\n+            \"unclean.leader.election.enable\",\n+            \"log.cleaner.backoff.ms\",\n+            \"log.cleaner.dedupe.buffer.size\",\n+            \"log.cleaner.delete.retention.ms\",\n+            \"log.cleaner.io.buffer.load.factor\",\n+            \"log.cleaner.io.buffer.size\",\n+            \"log.cleaner.io.max.bytes.per.second\",\n+            \"log.cleaner.max.compaction.lag.ms\",\n+            \"log.cleaner.min.cleanable.ratio\",\n+            \"log.cleaner.min.compaction.lag.ms\",\n+            \"log.cleaner.threads\",\n+            \"log.cleanup.policy\",\n+            \"log.index.interval.bytes\",\n+            \"log.index.size.max.bytes\",\n+            \"log.message.timestamp.difference.max.ms\",\n+            \"log.message.timestamp.type\",\n+            \"log.preallocate\",\n+            \"max.connections\",\n+            \"max.connections.per.ip\",\n+            \"max.connections.per.ip.overrides\",\n+            \"log.message.downconversion.enable\",\n+            \"metric.reporters\"\n+            ));\n+\n+    public KafkaConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired) {\n+        this.current = current;\n+        this.desired = desired;\n+    }\n+\n+    private Properties configMap2Properties() {\n+        Properties result = new Properties();\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    public Set<String> getDiff() {\n+        Properties desiredConfig = configMap2Properties();\n+        Set<String> result = new HashSet<>();\n+        Map.Entry<ConfigResource, Config> currentBrokerConfiguration = current.entrySet().iterator().next();\n+\n+        log.info(\"Diff for broker id {}\", currentBrokerConfiguration.getKey());\n+        currentBrokerConfiguration.getValue().entries().forEach(entry -> {\n+            String val = entry.value() == null ? \"null\" : entry.value();\n+\n+            if (desiredConfig.containsKey(entry.name())) {\n+                if (desiredConfig.getProperty(entry.name()).equals(val)) {\n+                    // entry is in both, equal\n+                } else {\n+                    // entry is in both, not equal -> diff\n+                    if (desiredConfig.getProperty(entry.name()) != null) {\n+                        // diff is in pod dependent entry\n+                        if (!desiredConfig.getProperty(entry.name()).matches(\".*\\\\$\\\\{.+\\\\}.*\")) {", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxNjcyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369416721", "bodyText": "How does this know from the file if it is added by user or not from these two configurations? By added by the user I expect Kafka.spec.kafka.config. Does it mean something else here? If yes, it is confusing.", "author": "scholzj", "createdAt": "2020-01-22T08:12:29Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+\n+public class KafkaConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaConfigurationDiff.class.getName());\n+    private Map<ConfigResource, Config> current;\n+    private ConfigMap desired;\n+\n+    private static final ArrayList<String> DYNAMICALLY_CHANGEABLE_ENTRIES = new ArrayList<>(Arrays.asList(\n+            // per broker\n+            \"advertised.listeners\",\n+            \"listeners\",\n+            \"principal.builder.class\",\n+            \"sasl.enabled.mechanisms\",\n+            \"sasl.jaas.config\",\n+            \"sasl.kerberos.kinit.cmd\",\n+            \"sasl.kerberos.min.time.before.relogin\",\n+            \"sasl.kerberos.principal.to.local.rules\",\n+            \"sasl.kerberos.service.name\",\n+            \"sasl.kerberos.ticket.renew.jitter\",\n+            \"sasl.kerberos.ticket.renew.window.factor\",\n+            \"sasl.login.refresh.buffer.seconds\",\n+            \"sasl.login.refresh.min.period.seconds\",\n+            \"sasl.login.refresh.window.factor\",\n+            \"sasl.login.refresh.window.jitter\",\n+            \"sasl.mechanism.inter.broker.protocol\",\n+            \"ssl.cipher.suites\",\n+            \"ssl.client.auth\",\n+            \"ssl.enabled.protocols\",\n+            \"ssl.key.password\",\n+            \"ssl.keymanager.algorithm\",\n+            \"ssl.keystore.location\",\n+            \"ssl.keystore.password\",\n+            \"ssl.keystore.type\",\n+            \"ssl.protocol\",\n+            \"ssl.provider\",\n+            \"ssl.trustmanager.algorithm\",\n+            \"ssl.truststore.location\",\n+            \"ssl.truststore.password\",\n+            \"ssl.truststore.type\",\n+            \"listener.security.protocol.map\",\n+            \"ssl.endpoint.identification.algorithm\",\n+            \"ssl.secure.random.implementation\",\n+            // cluster wide\n+            \"background.threads\",\n+            \"compression.type\",\n+            \"log.flush.interval.messages\",\n+            \"log.flush.interval.ms\",\n+            \"log.retention.bytes\",\n+            \"log.retention.ms\",\n+            \"log.roll.jitter.ms\",\n+            \"log.roll.ms\",\n+            \"log.segment.bytes\",\n+            \"log.segment.delete.delay.ms\",\n+            \"message.max.bytes\",\n+            \"min.insync.replicas\",\n+            \"num.io.threads\",\n+            \"num.network.threads\",\n+            \"num.recovery.threads.per.data.dir\",\n+            \"num.replica.fetchers\",\n+            \"unclean.leader.election.enable\",\n+            \"log.cleaner.backoff.ms\",\n+            \"log.cleaner.dedupe.buffer.size\",\n+            \"log.cleaner.delete.retention.ms\",\n+            \"log.cleaner.io.buffer.load.factor\",\n+            \"log.cleaner.io.buffer.size\",\n+            \"log.cleaner.io.max.bytes.per.second\",\n+            \"log.cleaner.max.compaction.lag.ms\",\n+            \"log.cleaner.min.cleanable.ratio\",\n+            \"log.cleaner.min.compaction.lag.ms\",\n+            \"log.cleaner.threads\",\n+            \"log.cleanup.policy\",\n+            \"log.index.interval.bytes\",\n+            \"log.index.size.max.bytes\",\n+            \"log.message.timestamp.difference.max.ms\",\n+            \"log.message.timestamp.type\",\n+            \"log.preallocate\",\n+            \"max.connections\",\n+            \"max.connections.per.ip\",\n+            \"max.connections.per.ip.overrides\",\n+            \"log.message.downconversion.enable\",\n+            \"metric.reporters\"\n+            ));\n+\n+    public KafkaConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired) {\n+        this.current = current;\n+        this.desired = desired;\n+    }\n+\n+    private Properties configMap2Properties() {\n+        Properties result = new Properties();\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    public Set<String> getDiff() {\n+        Properties desiredConfig = configMap2Properties();\n+        Set<String> result = new HashSet<>();\n+        Map.Entry<ConfigResource, Config> currentBrokerConfiguration = current.entrySet().iterator().next();\n+\n+        log.info(\"Diff for broker id {}\", currentBrokerConfiguration.getKey());\n+        currentBrokerConfiguration.getValue().entries().forEach(entry -> {\n+            String val = entry.value() == null ? \"null\" : entry.value();\n+\n+            if (desiredConfig.containsKey(entry.name())) {\n+                if (desiredConfig.getProperty(entry.name()).equals(val)) {\n+                    // entry is in both, equal\n+                } else {\n+                    // entry is in both, not equal -> diff\n+                    if (desiredConfig.getProperty(entry.name()) != null) {\n+                        // diff is in pod dependent entry\n+                        if (!desiredConfig.getProperty(entry.name()).matches(\".*\\\\$\\\\{.+\\\\}.*\")) {\n+                            result.add(entry.name());\n+                            log.debug(\"diff {} {}/{}\", entry.name(), desiredConfig.getProperty(entry.name()), entry.value());\n+                        } else {\n+                            log.debug(\"skipping pod-dependent entry {} with value {}\", entry.name(), desiredConfig.getProperty(entry.name()));\n+                        }\n+                    } else {\n+                        result.add(entry.name());\n+                        log.debug(\"diff {} {}/{}\", entry.name(), desiredConfig.getProperty(entry.name()), entry.value());\n+                    }\n+                }\n+            } else {\n+                // desired does not contain key, check default value\n+                if (entry.isDefault()) {\n+                    // value is default, do nothing\n+                } else {\n+                    // value is not default, not present in desired -> set it to the default\n+                    log.debug(\"diff {} {}/{}\", entry.name(), desiredConfig.getProperty(entry.name()), entry.value());\n+                    result.add(entry.name());\n+                }\n+            }\n+        });\n+\n+        // now we have to check entries added by user", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQxNzA2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r369417063", "bodyText": "Can we keep this somewhere else? I think it would be great to keep the Kubernetes functions and the Kafka functions separate.", "author": "scholzj", "createdAt": "2020-01-22T08:13:24Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaSetOperator.java", "diffHunk": "@@ -72,4 +89,55 @@ public static boolean needsRollingUpdate(StatefulSetDiff diff) {\n                 .rollingRestart(podNeedsRestart);\n     }\n \n+    /**\n+     * @param sts Stateful set to which kafka pod belongs\n+     * @param pod Specific kafka pod\n+     * @return a future which contains map with all kafka properties including their values\n+     */\n+    public Future<Map<ConfigResource, Config>> getCurrentConfig(StatefulSet sts, Pod pod) {", "originalCommit": "49ad07ca56b893fd47a28cb3c6bf63a5f32ad783", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f1367d7ce93c2085cb2bbafea477922bc27eea24", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f1367d7ce93c2085cb2bbafea477922bc27eea24", "message": "WIP\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-02-13T12:14:24Z", "type": "forcePushed"}, {"oid": "d9a8f00dd89b76421261f7a2e158fe3603898470", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d9a8f00dd89b76421261f7a2e158fe3603898470", "message": "tests\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-02-17T10:46:37Z", "type": "forcePushed"}, {"oid": "a911bea0e0e72cccec70f5ed4181696c6e9afede", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a911bea0e0e72cccec70f5ed4181696c6e9afede", "message": "tests\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-02-17T12:24:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMTE1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384521156", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-02-26T14:20:48Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +166,48 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /**\n+     * Returns true if the supplied value is default to kafka in kafkaVersion version\n+     * @param property name of the tested property\n+     * @param value desired value\n+     * @param kafkaVersion version of Kafka\n+     * @return true if the desired value is default\n+     */\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    public static boolean isValueOfPropertyDefault(String property, String value, KafkaVersion kafkaVersion) {\n+        value = value == null ? \"null\" : value;\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);\n+        Optional<Map.Entry<String, ConfigModel>> optProp = c.entrySet().stream().filter(entry -> entry.getKey().equals(property)).findFirst();\n+        if (optProp.isPresent()) {\n+            return optProp.get().getValue().getDefaultVal().toString()\n+                    .replace(\"\\\"\", \"\").toLowerCase(Locale.ENGLISH)\n+                    .equals(value.toLowerCase(Locale.ENGLISH));\n+        } else {\n+            // custom property\n+            return false;\n+        }\n+    }\n+\n+    /* test */\n+    public static Object getDefaultValueOfProperty(String property, KafkaVersion kafkaVersion) {\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);\n+        Optional<Map.Entry<String, ConfigModel>> optProp = c.entrySet().stream().filter(entry -> entry.getKey().equals(property)).findFirst();\n+        if (optProp.isPresent()) {\n+            return optProp.get().getValue().getDefaultVal();\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    public List<Map.Entry<String, String>> nonDefaultProperties(KafkaVersion kafkaVersion) {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMTM3Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384521377", "bodyText": "Does this have to be public?", "author": "tombentley", "createdAt": "2020-02-26T14:21:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +166,48 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /**\n+     * Returns true if the supplied value is default to kafka in kafkaVersion version\n+     * @param property name of the tested property\n+     * @param value desired value\n+     * @param kafkaVersion version of Kafka\n+     * @return true if the desired value is default\n+     */\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    public static boolean isValueOfPropertyDefault(String property, String value, KafkaVersion kafkaVersion) {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDk2MjE5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384962193", "bodyText": "Yes :( it is called from KafkaBrokerConfigurationDiff", "author": "sknot-rh", "createdAt": "2020-02-27T07:57:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMTM3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMjUwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384522508", "bodyText": "I don't think this should be needed, since you're specifying the locale in your calls to toLowerCase()", "author": "tombentley", "createdAt": "2020-02-26T14:22:54Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +166,48 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /**\n+     * Returns true if the supplied value is default to kafka in kafkaVersion version\n+     * @param property name of the tested property\n+     * @param value desired value\n+     * @param kafkaVersion version of Kafka\n+     * @return true if the desired value is default\n+     */\n+    @SuppressWarnings(\"checkstyle:Regexp\")", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMzA1Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384523057", "bodyText": "c is a Map<configName \u2192 model >, right? So why the linear search the matching model, when you could just do c.get(property)?", "author": "tombentley", "createdAt": "2020-02-26T14:23:48Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +166,48 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /**\n+     * Returns true if the supplied value is default to kafka in kafkaVersion version\n+     * @param property name of the tested property\n+     * @param value desired value\n+     * @param kafkaVersion version of Kafka\n+     * @return true if the desired value is default\n+     */\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    public static boolean isValueOfPropertyDefault(String property, String value, KafkaVersion kafkaVersion) {\n+        value = value == null ? \"null\" : value;\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);\n+        Optional<Map.Entry<String, ConfigModel>> optProp = c.entrySet().stream().filter(entry -> entry.getKey().equals(property)).findFirst();\n+        if (optProp.isPresent()) {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMzkyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384523921", "bodyText": "Why is it OK to ignore \" and why is it OK to ignore case?\nAlso why not use equalsIgnoreCase()?", "author": "tombentley", "createdAt": "2020-02-26T14:25:12Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +166,48 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /**\n+     * Returns true if the supplied value is default to kafka in kafkaVersion version\n+     * @param property name of the tested property\n+     * @param value desired value\n+     * @param kafkaVersion version of Kafka\n+     * @return true if the desired value is default\n+     */\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    public static boolean isValueOfPropertyDefault(String property, String value, KafkaVersion kafkaVersion) {\n+        value = value == null ? \"null\" : value;\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);\n+        Optional<Map.Entry<String, ConfigModel>> optProp = c.entrySet().stream().filter(entry -> entry.getKey().equals(property)).findFirst();\n+        if (optProp.isPresent()) {\n+            return optProp.get().getValue().getDefaultVal().toString()\n+                    .replace(\"\\\"\", \"\").toLowerCase(Locale.ENGLISH)", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDk2MTk2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384961963", "bodyText": "I hit a situation where the current config had https and desired HTTPS (or inverse)", "author": "sknot-rh", "createdAt": "2020-02-27T07:56:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMzkyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyNTA1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384525056", "bodyText": "The is clashes with the plural \"properties\". I would just call this allPropertiesHaveDefaultValue.", "author": "tombentley", "createdAt": "2020-02-26T14:26:59Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +166,48 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /**\n+     * Returns true if the supplied value is default to kafka in kafkaVersion version\n+     * @param property name of the tested property\n+     * @param value desired value\n+     * @param kafkaVersion version of Kafka\n+     * @return true if the desired value is default\n+     */\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    public static boolean isValueOfPropertyDefault(String property, String value, KafkaVersion kafkaVersion) {\n+        value = value == null ? \"null\" : value;\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);\n+        Optional<Map.Entry<String, ConfigModel>> optProp = c.entrySet().stream().filter(entry -> entry.getKey().equals(property)).findFirst();\n+        if (optProp.isPresent()) {\n+            return optProp.get().getValue().getDefaultVal().toString()\n+                    .replace(\"\\\"\", \"\").toLowerCase(Locale.ENGLISH)\n+                    .equals(value.toLowerCase(Locale.ENGLISH));\n+        } else {\n+            // custom property\n+            return false;\n+        }\n+    }\n+\n+    /* test */\n+    public static Object getDefaultValueOfProperty(String property, KafkaVersion kafkaVersion) {\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);\n+        Optional<Map.Entry<String, ConfigModel>> optProp = c.entrySet().stream().filter(entry -> entry.getKey().equals(property)).findFirst();\n+        if (optProp.isPresent()) {\n+            return optProp.get().getValue().getDefaultVal();\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    public List<Map.Entry<String, String>> nonDefaultProperties(KafkaVersion kafkaVersion) {\n+        List<Map.Entry<String, String>> nonDefaultProperties = asOrderedProperties().asMap().entrySet().stream().filter(entry ->\n+                !isValueOfPropertyDefault(entry.getKey(), entry.getValue(), kafkaVersion))\n+                .collect(Collectors.toList());\n+        return nonDefaultProperties;\n+    }\n+\n+    public boolean isAllPropertiesDefaultValue(KafkaVersion kafkaVersion) {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyNzgxMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384527812", "bodyText": "I think the BiFunction<ArrayList<Boolean>,... is pretty hairy. It would be better to declare a static inner class with named fields, I think.", "author": "tombentley", "createdAt": "2020-02-26T14:31:14Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1311,18 +1319,24 @@ private KafkaVersionChange getKafkaVersionChange(StatefulSet kafkaSts) {\n             return withVoid(serviceOperations.reconcile(namespace, zkCluster.getHeadlessServiceName(), zkHeadlessService));\n         }\n \n-        Future<ReconciliationState> getReconciliationStateOfConfigMap(AbstractModel cluster, ConfigMap configMap, BiFunction<Boolean, Future<ReconcileResult<ConfigMap>>, Future<ReconciliationState>> function) {\n+        Future<ReconciliationState> getReconciliationStateOfConfigMap(AbstractModel cluster, ConfigMap configMap, BiFunction<ArrayList<Boolean>, Future<ReconcileResult<ConfigMap>>, Future<ReconciliationState>> function) {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyODQ4MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384528480", "bodyText": "Why do you create a copy here? If it's for a good reason, then it's worth a comment.", "author": "tombentley", "createdAt": "2020-02-26T14:32:13Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1311,18 +1319,24 @@ private KafkaVersionChange getKafkaVersionChange(StatefulSet kafkaSts) {\n             return withVoid(serviceOperations.reconcile(namespace, zkCluster.getHeadlessServiceName(), zkHeadlessService));\n         }\n \n-        Future<ReconciliationState> getReconciliationStateOfConfigMap(AbstractModel cluster, ConfigMap configMap, BiFunction<Boolean, Future<ReconcileResult<ConfigMap>>, Future<ReconciliationState>> function) {\n+        Future<ReconciliationState> getReconciliationStateOfConfigMap(AbstractModel cluster, ConfigMap configMap, BiFunction<ArrayList<Boolean>, Future<ReconcileResult<ConfigMap>>, Future<ReconciliationState>> function) {\n             Promise<ReconciliationState> resultPromise = Promise.promise();\n \n-            vertx.createSharedWorkerExecutor(\"kubernetes-ops-pool\").<Boolean>executeBlocking(\n+            vertx.createSharedWorkerExecutor(\"kubernetes-ops-pool\").<ArrayList<Boolean>>executeBlocking(\n                 future -> {\n                     ConfigMap current = configMapOperations.get(namespace, cluster.getAncillaryConfigName());\n                     boolean onlyMetricsSettingChanged = onlyMetricsSettingChanged(current, configMap);\n-                    future.complete(onlyMetricsSettingChanged);\n+                    boolean loggingAndKafkaChanged = kafkaLoggingChanged(current, configMap);\n+                    ArrayList<Boolean> result = new ArrayList<>();\n+                    result.add(onlyMetricsSettingChanged);\n+                    result.add(loggingAndKafkaChanged);\n+                    future.complete(result);\n                 }, res -> {\n                     if (res.succeeded()) {\n-                        boolean onlyMetricsSettingChanged = res.result();\n-                        function.apply(onlyMetricsSettingChanged, configMapOperations.reconcile(namespace, cluster.getAncillaryConfigName(), configMap)).setHandler(res2 -> {\n+                        ArrayList<Boolean> booleans = new ArrayList<>();\n+                        booleans.add(res.result().get(0));\n+                        booleans.add(res.result().get(1));", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzMTUyNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384531526", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n          \n          \n            \n             * Computes a diff between current config supplied as a Map and desired config supplied as a ConfigMap\n          \n      \n    \n    \n  \n\nTBH I find this whole class difficult to understand. I suspect it can be simplified, but you should use the Javadoc to describe the algorithm it's trying to implement.", "author": "tombentley", "createdAt": "2020-02-26T14:36:49Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzMzI2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384533268", "bodyText": "Is this parsing a String in properties file format? Why not use OrderedProperties, or `Properties?", "author": "tombentley", "createdAt": "2020-02-26T14:39:38Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzMzkwNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384533906", "bodyText": "I would factor IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() into an isIgnorableProperty() method.\nAlso I don't really see much benefit of guarding the replaceAll with the contains() check. It's only for performance, not correctness, right?", "author": "tombentley", "createdAt": "2020-02-26T14:40:38Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzNTExMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384535111", "bodyText": "I guess you need to quote placeholder because it could contain regex metacharacters (e.g. .).", "author": "tombentley", "createdAt": "2020-02-26T14:42:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzNTgyNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384535825", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-02-26T14:43:32Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));\n+            }\n+        });\n+    }\n+\n+    private KafkaConfiguration computeDiff() {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzNjQyNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384536425", "bodyText": "Again, I don't think you should need this.", "author": "tombentley", "createdAt": "2020-02-26T14:44:26Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));\n+            }\n+        });\n+    }\n+\n+    private KafkaConfiguration computeDiff() {\n+        HashMap<String, String> desiredMap = configMap2Map();\n+        Map<String, String> currentMap = new HashMap<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        Map<String, String> diff = getMapDiff(currentMap, desiredMap);\n+\n+        String diffString = diff.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+\n+        return KafkaConfiguration.unvalidated(diffString);\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:Regexp\")", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzNjk4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384536989", "bodyText": "Map<String, String> difference = new HashMap<>(desired); is sufficient.", "author": "tombentley", "createdAt": "2020-02-26T14:45:16Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));\n+            }\n+        });\n+    }\n+\n+    private KafkaConfiguration computeDiff() {\n+        HashMap<String, String> desiredMap = configMap2Map();\n+        Map<String, String> currentMap = new HashMap<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        Map<String, String> diff = getMapDiff(currentMap, desiredMap);\n+\n+        String diffString = diff.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+\n+        return KafkaConfiguration.unvalidated(diffString);\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    private Map<String, String> getMapDiff(Map<String, String> current, Map<String, String> desired) {\n+        Map<String, String> difference = new HashMap<>();\n+        difference.putAll(desired);", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzNzI4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384537284", "bodyText": "equalsIgnoreCase()", "author": "tombentley", "createdAt": "2020-02-26T14:45:39Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));\n+            }\n+        });\n+    }\n+\n+    private KafkaConfiguration computeDiff() {\n+        HashMap<String, String> desiredMap = configMap2Map();\n+        Map<String, String> currentMap = new HashMap<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        Map<String, String> diff = getMapDiff(currentMap, desiredMap);\n+\n+        String diffString = diff.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+\n+        return KafkaConfiguration.unvalidated(diffString);\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    private Map<String, String> getMapDiff(Map<String, String> current, Map<String, String> desired) {\n+        Map<String, String> difference = new HashMap<>();\n+        difference.putAll(desired);\n+        desired.forEach((k, v) -> {\n+            if (IGNORABLE_PROPERTIES.matcher(k).matches() || (current.get(k) != null && current.get(k).toLowerCase(Locale.ENGLISH).equals(v.toLowerCase(Locale.ENGLISH)))) {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzODIwNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384538206", "bodyText": "There's no benefit here in using a forEach. Lambdas do not come for free, so try to use regular for loops when you aren't doing anything fancy.", "author": "tombentley", "createdAt": "2020-02-26T14:46:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));\n+            }\n+        });\n+    }\n+\n+    private KafkaConfiguration computeDiff() {\n+        HashMap<String, String> desiredMap = configMap2Map();\n+        Map<String, String> currentMap = new HashMap<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        Map<String, String> diff = getMapDiff(currentMap, desiredMap);\n+\n+        String diffString = diff.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+\n+        return KafkaConfiguration.unvalidated(diffString);\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    private Map<String, String> getMapDiff(Map<String, String> current, Map<String, String> desired) {\n+        Map<String, String> difference = new HashMap<>();\n+        difference.putAll(desired);\n+        desired.forEach((k, v) -> {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUzOTg1NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384539855", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-02-26T14:49:09Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));\n+            }\n+        });\n+    }\n+\n+    private KafkaConfiguration computeDiff() {\n+        HashMap<String, String> desiredMap = configMap2Map();\n+        Map<String, String> currentMap = new HashMap<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        Map<String, String> diff = getMapDiff(currentMap, desiredMap);\n+\n+        String diffString = diff.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+\n+        return KafkaConfiguration.unvalidated(diffString);\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    private Map<String, String> getMapDiff(Map<String, String> current, Map<String, String> desired) {\n+        Map<String, String> difference = new HashMap<>();\n+        difference.putAll(desired);\n+        desired.forEach((k, v) -> {\n+            if (IGNORABLE_PROPERTIES.matcher(k).matches() || (current.get(k) != null && current.get(k).toLowerCase(Locale.ENGLISH).equals(v.toLowerCase(Locale.ENGLISH)))) {\n+                difference.remove(k);\n+            } else {\n+                log.info(\"{} differs in '{}' ---> '{}'\", k, current.get(k), v);\n+            }\n+        });\n+\n+        current.entrySet().forEach(entry -> {\n+            // some value was set to non-default value, then the entry was removed from desired -> we want to use default value\n+            if (!desired.keySet().contains(entry.getKey()) && !isDesiredPropertyDefaultValue(entry.getKey(), entry.getValue())) {\n+                if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches()) {\n+                    String defVal = KafkaConfiguration.getDefaultValueOfProperty(entry.getKey(), kafkaVersion) == null ? \"null\" : KafkaConfiguration.getDefaultValueOfProperty(entry.getKey(), kafkaVersion).toString();\n+                    log.info(\"{} had value {} and was removed from desired. Setting {}\", entry.getKey(), entry.getValue(), defVal);\n+                    difference.put(entry.getKey(), defVal);\n+                    deletedEntries.add(entry.getKey());\n+                }\n+            }\n+        });\n+\n+        return difference;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key, String value) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return KafkaConfiguration.isValueOfPropertyDefault(entry.get().name(), value, kafkaVersion);\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        // TODO all the magic of listeners combinations\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty();\n+    }\n+\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDU0MTQ1NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384541455", "bodyText": "Do you know about ConfigEntry.ConfigSource?", "author": "tombentley", "createdAt": "2020-02-26T14:51:22Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Computes a diff between current diff supplied as a Map and desired config supplied as a ConfigMap\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        this.deletedEntries = new ArrayList<String>();\n+        Config brokerConfigs = current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            throw new RuntimeException(\"Failed to get broker \" + brokerId + \" configuration\");\n+        }\n+        this.currentEntries = brokerConfigs.entries();\n+        this.diff = computeDiff();\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+    private HashMap<String, String> configMap2Map() {\n+        HashMap<String, String> result = new HashMap<String, String>();\n+        if (desired == null || desired.getData() == null || desired.getData().get(\"server.config\") == null) {\n+            return result;\n+        }\n+        String desireConf = desired.getData().get(\"server.config\");\n+\n+        List<String> list = getLinesWithoutCommentsAndEmptyLines(desireConf);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.replace(\" \", \"\").startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line.replace(\" \", \"\"));\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> map, String placeholder, String value) {\n+        map.entrySet().forEach(entry -> {\n+            if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches() && entry.getValue().contains(\"${\" + placeholder + \"}\")) {\n+                entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + placeholder + \"\\\\}\", value));\n+            }\n+        });\n+    }\n+\n+    private KafkaConfiguration computeDiff() {\n+        HashMap<String, String> desiredMap = configMap2Map();\n+        Map<String, String> currentMap = new HashMap<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        Map<String, String> diff = getMapDiff(currentMap, desiredMap);\n+\n+        String diffString = diff.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+\n+        return KafkaConfiguration.unvalidated(diffString);\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:Regexp\")\n+    private Map<String, String> getMapDiff(Map<String, String> current, Map<String, String> desired) {\n+        Map<String, String> difference = new HashMap<>();\n+        difference.putAll(desired);\n+        desired.forEach((k, v) -> {\n+            if (IGNORABLE_PROPERTIES.matcher(k).matches() || (current.get(k) != null && current.get(k).toLowerCase(Locale.ENGLISH).equals(v.toLowerCase(Locale.ENGLISH)))) {\n+                difference.remove(k);\n+            } else {\n+                log.info(\"{} differs in '{}' ---> '{}'\", k, current.get(k), v);\n+            }\n+        });\n+\n+        current.entrySet().forEach(entry -> {\n+            // some value was set to non-default value, then the entry was removed from desired -> we want to use default value\n+            if (!desired.keySet().contains(entry.getKey()) && !isDesiredPropertyDefaultValue(entry.getKey(), entry.getValue())) {\n+                if (!IGNORABLE_PROPERTIES.matcher(entry.getKey()).matches()) {\n+                    String defVal = KafkaConfiguration.getDefaultValueOfProperty(entry.getKey(), kafkaVersion) == null ? \"null\" : KafkaConfiguration.getDefaultValueOfProperty(entry.getKey(), kafkaVersion).toString();\n+                    log.info(\"{} had value {} and was removed from desired. Setting {}\", entry.getKey(), entry.getValue(), defVal);\n+                    difference.put(entry.getKey(), defVal);\n+                    deletedEntries.add(entry.getKey());\n+                }\n+            }\n+        });\n+\n+        return difference;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key, String value) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return KafkaConfiguration.isValueOfPropertyDefault(entry.get().name(), value, kafkaVersion);", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDk3ODYwNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384978607", "bodyText": "Yes, I was looking at this, but then algorithm is dependent on which version of kafka does strimzi use, does not it? If we want to use a different version than it was built with, it could make problems, or?", "author": "sknot-rh", "createdAt": "2020-02-27T08:36:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDU0MTQ1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDk5NzEwNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384997107", "bodyText": "When you describeConfigs() you're get back (from the broker) each config and where it came from, which is what you need to know isn't it? It doesn't depend on the version of the AdminClient used by the CO.\nAlso, bear in mind that the broker might be configured with a value which == the default (e.g. DEFAULT_CONFIG), but the value is not coming from the default (e.g. it might be DYNAMIC_DEFAULT_BROKER_CONFIG or STATIC_BROKER_CONFIG). IDK how that affects your algorithm.\nAlso bear in mind that when changing a cluster-scoped config it's \"good practice\" to just set it for an indiviual broker (i.e. DYNAMIC_BROKER_CONFIG) and verify that everything is OK before applying it to the whole cluster (i.e. as DYNAMIC_DEFAULT_BROKER_CONFIG`). See https://kafka.apache.org/documentation.html#dynamicbrokerconfigs. IDK if you want to tackle that in this PR or not, but thought it worth mentioning.", "author": "tombentley", "createdAt": "2020-02-27T09:15:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDU0MTQ1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDU0MTk0OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384541949", "bodyText": "Javadoc, what is this class has responsibility for.", "author": "tombentley", "createdAt": "2020-02-26T14:52:06Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.StatefulSet;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+import static io.strimzi.operator.cluster.operator.resource.StatefulSetOperator.missingSecretFuture;\n+\n+public class KafkaBrokerConfigurationHelper {", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDU0Mzc3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r384543773", "bodyText": "If this is really necessary, then let's at least call the property defaultValue.", "author": "tombentley", "createdAt": "2020-02-26T14:54:34Z", "path": "config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModel.java", "diffHunk": "@@ -28,6 +28,7 @@\n     @JsonProperty(\"enum\")\n     private List<String> values;\n     private String pattern;\n+    private Object defaultVal;", "originalCommit": "b084b501770f18a327c0f72273b7ae6a7144a945", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0246f55d0ead3088f1b371da6d2997d776c6d989", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0246f55d0ead3088f1b371da6d2997d776c6d989", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-02-27T15:41:50Z", "type": "forcePushed"}, {"oid": "ccadce5e59da67b0c36d94b4091ee0e53d536a1e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ccadce5e59da67b0c36d94b4091ee0e53d536a1e", "message": "do not roll after change broker conf - rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-03-02T15:27:54Z", "type": "forcePushed"}, {"oid": "2ce040487a17fc1b1ed8ff1d091a2a80b3b1aa45", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2ce040487a17fc1b1ed8ff1d091a2a80b3b1aa45", "message": "another test fix\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-03-03T12:01:31Z", "type": "forcePushed"}, {"oid": "19907989ea13869d36107cbbfeae0e73ca5b9845", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/19907989ea13869d36107cbbfeae0e73ca5b9845", "message": "checkstyle + roll after logging\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-03-30T09:28:43Z", "type": "forcePushed"}, {"oid": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-03-30T12:25:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NTQ0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400555445", "bodyText": "I'm not sure this is really needed. I get it that it might safe sometimes something, but cleaner code is probably better.", "author": "scholzj", "createdAt": "2020-03-30T23:30:51Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1625,6 +1636,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        configFutures.add(\n+                            kbch.adminClient(sts, podId)\n+                                .compose(ac -> {\n+                                    Future<Map<ConfigResource, Config>> futCurrent = kbch.getCurrentConfig(finalPodId, ac);\n+                                    Future<ConfigMap> futDesired = configMapOperations.getAsync(namespace, KafkaCluster.metricAndLogConfigsName(name));\n+                                    log.debug(\"Determining kafka pod {} dynamic update ability\", finalPodId);\n+                                    return CompositeFuture.join(futCurrent, futDesired)\n+                                            .compose(res -> {\n+                                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res.result().resultAt(0), res.result().resultAt(1), kafkaCluster.getKafkaVersion(), finalPodId);\n+                                                ConfigMap kafkaCm = res.result().resultAt(1);\n+                                                boolean loggingChanged = !sts.getSpec().getTemplate().getMetadata().getAnnotations().get(AbstractModel.ANNO_STRIMZI_OLD_LOGGING_HASH).isEmpty()\n+                                                        && !sts.getSpec().getTemplate().getMetadata().getAnnotations().get(AbstractModel.ANNO_STRIMZI_OLD_LOGGING_HASH).equals(getStringHash(kafkaCm.getData().get(AbstractModel.ANCILLARY_CM_KEY_LOG_CONFIG)));\n+\n+                                                if (loggingChanged) {", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjAyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400556024", "bodyText": "Is this actually used anywhere?", "author": "scholzj", "createdAt": "2020-03-30T23:32:41Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3449,4 +3522,25 @@ private String getStringHash(String toBeHashed)  {\n             throw new RuntimeException(\"Failed to create SHA-512 MessageDigest instance\");\n         }\n     }\n+\n+    /**\n+     * @param current Current ConfigMap\n+     * @param desired Desired ConfigMap\n+     * @return Returns true if only kafka config has been changed\n+     */\n+    public boolean kafkaLoggingChanged(ConfigMap current, ConfigMap desired) {", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjcwNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400556705", "bodyText": "Why are these ignored? Because they use placeholders? It would be nice to have some clarification in coments.", "author": "scholzj", "createdAt": "2020-03-30T23:34:35Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3. Put all entries from desired to the diff\n+ *  4a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, remove entry from diff\n+ *  4b. If entry was removed from desired, add it to the diff with default value.\n+ *      If custom entry was removed, add it to the diff with 'null' value.\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private ArrayList<String> deletedEntries;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDgyNDYxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400824615", "bodyText": "I am not sure filling up the placeholders is the correct way. The values are filled after the pod is rolled", "author": "sknot-rh", "createdAt": "2020-03-31T11:02:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjcwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM2MTY3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405361673", "bodyText": "So my question here is really about where did these fields come from and if we can get some explanatory comments.", "author": "scholzj", "createdAt": "2020-04-08T08:50:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjcwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MTA1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413381052", "bodyText": "I think the comment here would be still handy. You do not need to explain every field. Just say \"these are the fields we always rool for right now because ...", "author": "scholzj", "createdAt": "2020-04-22T22:40:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjcwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjkzMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400556931", "bodyText": "Why do we need to carry this on the STS?", "author": "scholzj", "createdAt": "2020-03-30T23:35:14Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/StatefulSetDiff.java", "diffHunk": "@@ -27,6 +27,7 @@\n     private static final Pattern IGNORABLE_PATHS = Pattern.compile(\n         \"^(/spec/revisionHistoryLimit\"\n         + \"|/spec/template/metadata/annotations/\" + SHORTENED_STRIMZI_DOMAIN + \"~1generation\"\n+        + \"|/spec/template/metadata/annotations/\" + SHORTENED_STRIMZI_DOMAIN + \"~1old-logging-hash\"", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDgwMTYyNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400801626", "bodyText": "I did not find any better way how to not roll the Kafka pods when the configuration was changed dynamically and keep the rolling update triggered when the logging has changed.", "author": "sknot-rh", "createdAt": "2020-03-31T10:20:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM2MzEwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405363108", "bodyText": "Ok, maybe to put this differently: Why do we need to have this annotation if not to roll the pod when it changes? Normally we add these annotations exactly for the reason to roll the pod.", "author": "scholzj", "createdAt": "2020-04-08T08:52:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM2NzU2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405367563", "bodyText": "I extract it here https://github.com/strimzi/strimzi-kafka-operator/pull/2389/files#diff-f19c7c3bdf294affc86939228666be84R1666\nIf the logging changed, we are not interested in dynamic configuration and we want to roll.", "author": "sknot-rh", "createdAt": "2020-04-08T08:59:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQwNDUxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405404517", "bodyText": "But why? You do not change logging dynamically, or? Or did you included it in this PR?", "author": "scholzj", "createdAt": "2020-04-08T09:57:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjAzNTU1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r406035552", "bodyText": "No, I am not changing it dynamically. Let's say, the Kafka CR has changed. Basically we can divide Kafka config into three parts. Metrics, logging and Kafka itself. Nowadays we are looking just at the logging and Kafka. With this PR we are changing Kafka part. We say it is possible we won't roll because the change was done dynamically. But if we change dynamically, the STS changes and the current algorithm says we need to roll. So we need to check whether it was dynamically changed. If it was we need to bypass the condition and do not roll. But if we change Kafka and logging part, it is not (currently) applicable, so we need to roll. So we changed Kafka config, the change is doable dynamically -> don't roll, but we have also logging -> roll.\nThis explanation is maybe more confusing, let me use pseudocode to try it better.\nboolean shouldRoll;\n\n// logging part\nif (loggingChanged) {\n shouldRoll = true;\n // we do not care about Kafka config, we need to roll\n return;\n}\n\n// Kafka part\nboolean kafkaChangedDynamically = changeKafkaConfig(old, new);\nif (kafkaChangedDynamically) {\n    shouldRoll = false;\n}", "author": "sknot-rh", "createdAt": "2020-04-09T08:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NjkzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NzMyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400557328", "bodyText": "Javadoc explaining what it does would be helpful.", "author": "scholzj", "createdAt": "2020-03-30T23:36:36Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,36 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NzM5NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400557395", "bodyText": "Javadoc explaining what it does and where is it used from would be helpful.", "author": "scholzj", "createdAt": "2020-03-30T23:36:48Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,36 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line);\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    public static Map<String, String> configMap2Map(ConfigMap configMap, String key) {", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1NzUxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400557511", "bodyText": "Typo? desiredConfig?", "author": "scholzj", "createdAt": "2020-03-30T23:37:13Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,36 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line);\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    public static Map<String, String> configMap2Map(ConfigMap configMap, String key) {\n+        Map<String, String> result = new HashMap<>();\n+        if (configMap == null || configMap.getData() == null || configMap.getData().get(key) == null) {\n+            return result;\n+        }\n+        String desireConf = configMap.getData().get(key);", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDU1ODk4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r400558988", "bodyText": "I wonder if you should separate into separate method extraction fo the key from the CM and have here only a method which goes String against String.", "author": "scholzj", "createdAt": "2020-03-30T23:41:46Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,36 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+        List<String> validLines = new ArrayList<>();\n+\n+        for (String line : allLines)    {\n+            if (!line.startsWith(\"#\") && !line.isEmpty())   {\n+                validLines.add(line);\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    public static Map<String, String> configMap2Map(ConfigMap configMap, String key) {\n+        Map<String, String> result = new HashMap<>();\n+        if (configMap == null || configMap.getData() == null || configMap.getData().get(key) == null) {", "originalCommit": "ee4b0c27dab6a7310a3ed1ba2303e093a3985426", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9b0bec60773d59b26e396a18cd84ac98b05b0dea", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9b0bec60773d59b26e396a18cd84ac98b05b0dea", "message": "listeners rolling\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-03T16:32:05Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzkwMjU0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r403902547", "bodyText": "Just a note for me: unit should be ms", "author": "sknot-rh", "createdAt": "2020-04-06T08:08:44Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1653,21 +1659,34 @@ String zkConnectionString(int connectToReplicas)  {\n                                     log.debug(\"Determining kafka pod {} dynamic update ability\", finalPodId);\n                                     return CompositeFuture.join(futCurrent, futDesired)\n                                             .compose(res -> {\n-                                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res.result().resultAt(0), res.result().resultAt(1), kafkaCluster.getKafkaVersion(), finalPodId);\n                                                 ConfigMap kafkaCm = res.result().resultAt(1);\n                                                 String oldLoggingHash = sts.getSpec().getTemplate().getMetadata().getAnnotations().get(AbstractModel.ANNO_STRIMZI_OLD_LOGGING_HASH);\n                                                 String currentLoggingHash = getStringHash(kafkaCm.getData().get(AbstractModel.ANCILLARY_CM_KEY_LOG_CONFIG));\n+                                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res.result().resultAt(0), res.result().resultAt(1), kafkaCluster.getKafkaVersion(), finalPodId);\n                                                 boolean loggingChanged = !oldLoggingHash.isEmpty() && !oldLoggingHash.equals(currentLoggingHash);\n+                                                boolean nodePortChanged = kafkaCluster.isExposedWithNodePort() != Boolean.parseBoolean(sts.getSpec().getTemplate().getMetadata().getAnnotations().get(KafkaCluster.ANNO_STRIMZI_WAS_EXPOSED_WITH_NODEPORT));\n \n-                                                if (loggingChanged) {\n+                                                if (nodePortChanged) {\n+                                                    log.debug(\"nodeport listener changed, rolling\");\n+                                                    kafkaPodsUpdatedDynamically.put(finalPodId, !nodePortChanged);\n+                                                } else if (loggingChanged) {\n                                                     log.debug(\"logging changed, rolling\");\n                                                     kafkaPodsUpdatedDynamically.put(finalPodId, !loggingChanged);\n                                                 } else if (configurationDiff.getDiff().asOrderedProperties().asMap().size() > 0) {\n                                                     log.debug(\"logging not changed, kafka did dynamically:{}\", !configurationDiff.cannotBeUpdatedDynamically());\n                                                     kafkaPodsUpdatedDynamically.put(finalPodId, !configurationDiff.cannotBeUpdatedDynamically());\n-                                                    ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                                    for (Map.Entry entry: alterConfigResult.values().entrySet()) {\n+                                                        KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();\n+                                                        try {\n+                                                            log.debug(\"AlterConfig result {}\", kafkaFuture.get(operationTimeoutMs, TimeUnit.SECONDS));", "originalCommit": "9b0bec60773d59b26e396a18cd84ac98b05b0dea", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "message": "test\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-07T16:20:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM0Njc1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405346758", "bodyText": "Why do you not controlling also log.message.format.version here ?", "author": "see-quick", "createdAt": "2020-04-08T08:26:43Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MTAzNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405371035", "bodyText": "Why do you using this method? I assume you are using the dynamic configuration for instant change or?  Do we really need it?", "author": "see-quick", "createdAt": "2020-04-08T09:04:37Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxODIwMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405418201", "bodyText": "The change is not instant. It takes ~5 seconds", "author": "sknot-rh", "createdAt": "2020-04-08T10:20:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MTAzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MTg1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405371854", "bodyText": "You are verifying the same values or am I missing something ? :D", "author": "see-quick", "createdAt": "2020-04-08T09:05:54Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MjcxNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405372716", "bodyText": "Assume, that inline LOGGER needs Kafka rolling update right?", "author": "see-quick", "createdAt": "2020-04-08T09:07:12Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQyMDA2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405420061", "bodyText": "Yes. Logging needs rolling update (unless thiss is implemented #765)", "author": "sknot-rh", "createdAt": "2020-04-08T10:24:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MjcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MzA4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405373086", "bodyText": "This is not needed for the case you are testing :). I do not see any verification method for this plain listener or some exchange of messages.", "author": "see-quick", "createdAt": "2020-04-08T09:07:46Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWintExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQyMTA2NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405421064", "bodyText": "Even plain listener adds values to the advertised.listeners. I was thinking it would be good to test is together with other changes. You are right it would be better to remove it.", "author": "sknot-rh", "createdAt": "2020-04-08T10:25:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MzA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MzY0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405373641", "bodyText": "Here you can add @Tag for nodeport support only and also @Tag for using external clients.", "author": "see-quick", "createdAt": "2020-04-08T09:08:38Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3MzkxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405373918", "bodyText": "Again why it is needed?", "author": "see-quick", "createdAt": "2020-04-08T09:09:01Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWintExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3ODMzNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405378335", "bodyText": "There is no need for this waiting...If you take a look on the implementation in the tlsUser() method you will find\nprivate static KafkaUser waitFor(KafkaUser kafkaUser) {\n        LOGGER.info(\"Waiting for Kafka User {}\", kafkaUser.getMetadata().getName());\n        SecretUtils.waitForSecretReady(kafkaUser.getMetadata().getName());\n        KafkaUserUtils.waitForKafkaUserCreation(kafkaUser.getMetadata().getName());\n        LOGGER.info(\"Kafka User {} is ready\", kafkaUser.getMetadata().getName());\n        return kafkaUser;\n    }\n\nIt's already waiting on the secret in the 2nd and 3rd line.", "author": "see-quick", "createdAt": "2020-04-08T09:15:41Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWintExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() throws IOException, InterruptedException, ExecutionException, TimeoutException {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.SSL)\n+                .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClient = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+                .build();\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalNodePort()\n+                                //.withTls(false)\n+                            .endKafkaListenerExternalNodePort()\n+                            .withNewPlain()\n+                            .endPlain()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+\n+        String userName = \"alice\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+            () -> kubeClient().getSecret(userName) != null,\n+            () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQyMTU1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405421559", "bodyText": "It is copypaste error :(", "author": "sknot-rh", "createdAt": "2020-04-08T10:26:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3ODMzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3ODQzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405378437", "bodyText": "Same as previous", "author": "see-quick", "createdAt": "2020-04-08T09:15:51Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWintExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() throws IOException, InterruptedException, ExecutionException, TimeoutException {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.SSL)\n+                .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClient = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+                .build();\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalNodePort()\n+                                //.withTls(false)\n+                            .endKafkaListenerExternalNodePort()\n+                            .withNewPlain()\n+                            .endPlain()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+\n+        String userName = \"alice\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+            () -> kubeClient().getSecret(userName) != null,\n+            () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));\n+\n+        String userName2 = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName2).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+                () -> kubeClient().getSecret(userName2) != null,\n+                () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM3OTc0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405379743", "bodyText": "Again when you updating listeners you need to wait ??", "author": "see-quick", "createdAt": "2020-04-08T09:18:01Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWintExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() throws IOException, InterruptedException, ExecutionException, TimeoutException {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.SSL)\n+                .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClient = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+                .build();\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalNodePort()\n+                                //.withTls(false)\n+                            .endKafkaListenerExternalNodePort()\n+                            .withNewPlain()\n+                            .endPlain()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+\n+        String userName = \"alice\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+            () -> kubeClient().getSecret(userName) != null,\n+            () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));\n+\n+        String userName2 = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName2).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+                () -> kubeClient().getSecret(userName2) != null,\n+                () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+        basicExternalKafkaClient.setKafkaUsername(userName2);\n+\n+        assertThrows(ExecutionException.class, () -> {\n+            Future<Integer> failproducer = basicExternalKafkaClient.sendMessagesPlain(5000);\n+            Future<Integer> failconsumer = basicExternalKafkaClient.receiveMessagesPlain(5000);\n+\n+            assertThat(failproducer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+            assertThat(failconsumer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+        });\n+\n+        Future<Integer> producer = basicExternalKafkaClientTls.sendMessagesTls();\n+        Future<Integer> consumer = basicExternalKafkaClientTls.receiveMessagesTls();\n+\n+        assertThat(producer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+        assertThat(consumer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM4MDE3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405380179", "bodyText": "Why do you adding tls authentification?", "author": "see-quick", "createdAt": "2020-04-08T09:18:40Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWintExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() throws IOException, InterruptedException, ExecutionException, TimeoutException {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.SSL)\n+                .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClient = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+                .build();\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalNodePort()\n+                                //.withTls(false)\n+                            .endKafkaListenerExternalNodePort()\n+                            .withNewPlain()\n+                            .endPlain()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+\n+        String userName = \"alice\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+            () -> kubeClient().getSecret(userName) != null,\n+            () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));\n+\n+        String userName2 = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName2).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+                () -> kubeClient().getSecret(userName2) != null,\n+                () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+        basicExternalKafkaClient.setKafkaUsername(userName2);\n+\n+        assertThrows(ExecutionException.class, () -> {\n+            Future<Integer> failproducer = basicExternalKafkaClient.sendMessagesPlain(5000);\n+            Future<Integer> failconsumer = basicExternalKafkaClient.receiveMessagesPlain(5000);\n+\n+            assertThat(failproducer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+            assertThat(failconsumer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+        });\n+\n+        Future<Integer> producer = basicExternalKafkaClientTls.sendMessagesTls();\n+        Future<Integer> consumer = basicExternalKafkaClientTls.receiveMessagesTls();\n+\n+        assertThat(producer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+        assertThat(consumer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQyMjM4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405422382", "bodyText": "I need to perform various harakiri with configuration to see, whether the cluster stays functional after dynamic changes.", "author": "sknot-rh", "createdAt": "2020-04-08T10:28:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM4MDE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM4MjA1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r405382050", "bodyText": "Use our constants from the Constants class where you will find GLOBAL_CLIENTS_TIMEOUT.... and please avoid using this hard-code values.", "author": "see-quick", "createdAt": "2020-04-08T09:21:36Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2128,6 +2137,344 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWintExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.waitUntilPodsStability(kubeClient().listPodsByPrefixInName(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() throws IOException, InterruptedException, ExecutionException, TimeoutException {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.SSL)\n+                .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClient = new BasicExternalKafkaClient.Builder()\n+                .withTopicName(TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+                .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+                .build();\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalNodePort()\n+                                //.withTls(false)\n+                            .endKafkaListenerExternalNodePort()\n+                            .withNewPlain()\n+                            .endPlain()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+\n+        String userName = \"alice\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+            () -> kubeClient().getSecret(userName) != null,\n+            () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));\n+\n+        String userName2 = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName2).done();\n+        waitFor(\"Wait for secrets became available\", Constants.GLOBAL_POLL_INTERVAL, Constants.TIMEOUT_FOR_GET_SECRETS,\n+                () -> kubeClient().getSecret(userName2) != null,\n+                () -> LOGGER.error(\"Couldn't find user secret {}\", kubeClient().listSecrets()));\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+        basicExternalKafkaClient.setKafkaUsername(userName2);\n+\n+        assertThrows(ExecutionException.class, () -> {\n+            Future<Integer> failproducer = basicExternalKafkaClient.sendMessagesPlain(5000);\n+            Future<Integer> failconsumer = basicExternalKafkaClient.receiveMessagesPlain(5000);\n+\n+            assertThat(failproducer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+            assertThat(failconsumer.get(2, TimeUnit.MINUTES), is(MESSAGE_COUNT));\n+        });", "originalCommit": "2fdc5a5264d37f57282ef27fc7a4cf1bfb3b76fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "348448d95f3cc93e199dc0aa40713e39ea4f9d78", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/348448d95f3cc93e199dc0aa40713e39ea4f9d78", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-09T08:28:27Z", "type": "forcePushed"}, {"oid": "13b1bc949135c42fa61d4f542f2f3575f4b2506b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/13b1bc949135c42fa61d4f542f2f3575f4b2506b", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-09T08:30:15Z", "type": "forcePushed"}, {"oid": "9e6103618aeaea383dd2adccfb0390225a0d8a98", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9e6103618aeaea383dd2adccfb0390225a0d8a98", "message": "dynamic config\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-09T08:35:23Z", "type": "forcePushed"}, {"oid": "96b439a11603ba0ef5fa53ea7ba27e686bfca5ab", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/96b439a11603ba0ef5fa53ea7ba27e686bfca5ab", "message": "simplification\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-09T13:18:15Z", "type": "forcePushed"}, {"oid": "6726b07bee194ab789144ac8edaecfd610fd3aa6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6726b07bee194ab789144ac8edaecfd610fd3aa6", "message": "simplification\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-15T08:01:54Z", "type": "forcePushed"}, {"oid": "8e395710d224b8bf12909fc93691d6e0d0362f47", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8e395710d224b8bf12909fc93691d6e0d0362f47", "message": "fix situation with pending pods\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-16T14:00:57Z", "type": "forcePushed"}, {"oid": "2a5710be229c37fa1528d98da0969ff424b4331a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2a5710be229c37fa1528d98da0969ff424b4331a", "message": "spotbugs\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-17T20:28:37Z", "type": "forcePushed"}, {"oid": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "message": "mm\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-20T09:52:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1NTQ2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413355469", "bodyText": "Why are you adding the logger? It does not seem to be used anywhere.", "author": "scholzj", "createdAt": "2020-04-22T21:46:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -28,6 +30,8 @@\n  * Class for handling Kafka configuration passed by the user\n  */\n public class KafkaConfiguration extends AbstractConfiguration {\n+    private static final Logger log = LogManager.getLogger(KafkaConfiguration.class.getName());", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU1NDczOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413554738", "bodyText": "Oh, yeah. It is forgotten one from debugging.", "author": "sknot-rh", "createdAt": "2020-04-23T06:50:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1NTQ2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1NzI2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413357262", "bodyText": "Is this used only in test? Are we sure it will be used only in tests? Reading and parsing the resource file everytime you need to check the property sounds very expensive.", "author": "scholzj", "createdAt": "2020-04-22T21:49:59Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +164,19 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /* test */\n+    public static Object getDefaultValueOfProperty(String property, KafkaVersion kafkaVersion) {\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413358251", "bodyText": "Is it intended that this checks only the listeners field and not for exmaple advertised.listeners? Also, wasn't this class supposed to be used only for the user provided configuration options where listeners are for sure on the forbidden list?", "author": "scholzj", "createdAt": "2020-04-22T21:51:52Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -160,5 +164,19 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /* test */\n+    public static Object getDefaultValueOfProperty(String property, KafkaVersion kafkaVersion) {\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);\n+        ConfigModel optProp = c.get(property);\n+        if (optProp != null) {\n+            return optProp.getDefaultValue();\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    public boolean containsListenersChange() {\n+        return asOrderedProperties().asMap().keySet().contains(\"listeners\");", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU1OTk2MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413559960", "bodyText": "No, it is comparing config from kafka broker and the generated one from configMap.", "author": "sknot-rh", "createdAt": "2020-04-23T06:58:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM1OTA5MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414359091", "bodyText": "And the question about advertised.listeners?", "author": "tombentley", "createdAt": "2020-04-24T07:32:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ5MDE2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414490168", "bodyText": "Looking at the io.strimzi.operator.cluster.model.KafkaBrokerConfigurationBuilder#withListeners i believe advertised.listeners are changed only when listeners are. So it should be sufficient, should not it?", "author": "sknot-rh", "createdAt": "2020-04-24T11:05:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDAzNzAzNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420037034", "bodyText": "I don't think this is true. For example change to the advertised listeners overrides in external listener would for sure change only the advertised listeners field but not the actual listener.", "author": "scholzj", "createdAt": "2020-05-05T11:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDAzNzM3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420037379", "bodyText": "That said, it seems to now match listeners even in the middle - so maybe you fixed this?", "author": "scholzj", "createdAt": "2020-05-05T11:25:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDEzMDYxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420130614", "bodyText": "If we use override, the pod is rolled because of cert has changed, isn't it?", "author": "sknot-rh", "createdAt": "2020-05-05T13:58:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE0NTEyMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420145122", "bodyText": "Yeah, you are probably right. But that would still be worth checking here. But I later realized something else - the advertised.listeners contains just the placeholders. So the overrides will change in the file in the config map and no there probably.\nAnyway - with contains(...) it should now match even the advertised listeners, or? So IMHO it should be ok. I guess originaly it might have been startsWith(...)?", "author": "scholzj", "createdAt": "2020-05-05T14:17:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM1ODI1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2MDA0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413360045", "bodyText": "Can you explain to me why do we need this? The ConfigMap is a step to deliver the configuration to the broker during restarts. But I expect that the main logic of this PR should be in comparing the desired configuration generated on the fly (i.e. not the one read from the config map) against the configuration of the broker (the actual - the config map might have something but that does not say anything abotu the broker). I'm not saying this is wrong - maybe I just don't undertsand the motivation.", "author": "scholzj", "createdAt": "2020-04-22T21:55:34Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,58 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms ConfigMap data entry form String into the List of Strings, where entry = line from input.\n+     * The comments are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {\n+                    validLines.add(line);\n+                }\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    /**\n+     * Gets data from ConfigMap as a String\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return String value of ConfigMap data entry. If the key does not exist in the ConfigMap, return null\n+     */\n+    public static String getDataFromConfigMap(ConfigMap configMap, String key) {\n+        if (configMap == null || configMap.getData() == null || configMap.getData().get(key) == null) {\n+            return null;\n+        }\n+        return configMap.getData().get(key);\n+    }\n+\n+    /**\n+     * Transforms data from ConfigMap into Map\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return Map of (key, value)\n+     */\n+    public static Map<String, String> configMap2Map(ConfigMap configMap, String key) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU2MTA0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413561048", "bodyText": "As I understand this current logic:\n\nuser changes Kafka configuration\nconfigMap is generated\npod is rolled\n\nWith these changes, step 3 should be conditioned by the possibility of a dynamic update.", "author": "sknot-rh", "createdAt": "2020-04-23T07:00:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2MDA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzYwODkzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413608939", "bodyText": "Yeah. But the config map looks like a very inpractical source of the information. We should have this information available easier. Later I commendted more on the places where you make unnecessary calles to Kube APIs etc. which could be saved.", "author": "scholzj", "createdAt": "2020-04-23T08:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2MDA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2Nzk2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414367962", "bodyText": "Yeah the problem is that we need to be certain that the state of the config map is actually the state of the running pod. I've not looked at the full PR yet, but what if the CM get changed and then the CO crashes? When the new CO starts up it would infer incorrect state for the broker because were never updated (via restart of dynamically).", "author": "tombentley", "createdAt": "2020-04-24T07:47:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2MDA0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2MzQwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413363404", "bodyText": "We just reconciled the STS right before runnign this. So I wonder if we really need to get it again from Kube. It seems to be later used only to extract namespace and cluster name which is IMHO something you can get much easier form the description and do not need to request the STS from Kube API server just because of it.", "author": "scholzj", "createdAt": "2020-04-22T22:02:14Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2MzY3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413363678", "bodyText": "As I said in the other comment. If I read the code right, you pass the STS in just to get namespace and cluster name. That seems to be very wasteful. Why not just pass the two strings?", "author": "scholzj", "createdAt": "2020-04-22T22:02:50Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.StatefulSet;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private final AdminClientProvider adminClientProvider;\n+    protected final SecretOperator secretOperations;\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public KafkaBrokerConfigurationHelper(AdminClientProvider adminClientProvider, SecretOperator secretOperations) {\n+        this.adminClientProvider = adminClientProvider;\n+        this.secretOperations = secretOperations;\n+    }\n+\n+    public Future<Admin> adminClient(StatefulSet sts, int podId) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2NDU1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413364556", "bodyText": "This looks very crude for info level. Is it some left over from debugging?", "author": "scholzj", "createdAt": "2020-04-22T22:04:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.StatefulSet;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private final AdminClientProvider adminClientProvider;\n+    protected final SecretOperator secretOperations;\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public KafkaBrokerConfigurationHelper(AdminClientProvider adminClientProvider, SecretOperator secretOperations) {\n+        this.adminClientProvider = adminClientProvider;\n+        this.secretOperations = secretOperations;\n+    }\n+\n+    public Future<Admin> adminClient(StatefulSet sts, int podId) {\n+        Promise<Admin> acPromise = Promise.promise();\n+        String cluster = sts.getMetadata().getLabels().get(Labels.STRIMZI_CLUSTER_LABEL);\n+        String namespace = sts.getMetadata().getNamespace();\n+        Future<Secret> clusterCaKeySecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+        Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(cluster));\n+        String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+        return CompositeFuture.join(clusterCaKeySecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+            Secret clusterCaKeySecret = compositeFuture.resultAt(0);\n+            if (clusterCaKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaKeySecretName(cluster)));\n+            }\n+            Secret coKeySecret = compositeFuture.resultAt(1);\n+            if (coKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+            }\n+\n+            Admin ac;\n+            try {\n+                log.info(\"about to create an admin client\");", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2NjA3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413366079", "bodyText": "Since the AdminClient doesn't live inside this class ... why not have just some with static methods? Or the other way around, why not keep the AdminClient inside? (again, just asking, maybe it has some valid reasons)", "author": "scholzj", "createdAt": "2020-04-22T22:07:56Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.StatefulSet;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private final AdminClientProvider adminClientProvider;\n+    protected final SecretOperator secretOperations;\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public KafkaBrokerConfigurationHelper(AdminClientProvider adminClientProvider, SecretOperator secretOperations) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2NzY2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413367668", "bodyText": "This method is a bit confusing. It returns Future. But it seems to be synchronous. Which makes me wonder about two things:\n\nDoes it really need to return the future?\nThe describeConfigs seems to be aynchronous, but you don't use it for the method. So it seems to behave in a blocking way. Should this be wrapped in executeBlocking? Or should the KafkaFuture be linked to the Promise? and make this actually asynchronous?", "author": "scholzj", "createdAt": "2020-04-22T22:11:19Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.StatefulSet;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private final AdminClientProvider adminClientProvider;\n+    protected final SecretOperator secretOperations;\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public KafkaBrokerConfigurationHelper(AdminClientProvider adminClientProvider, SecretOperator secretOperations) {\n+        this.adminClientProvider = adminClientProvider;\n+        this.secretOperations = secretOperations;\n+    }\n+\n+    public Future<Admin> adminClient(StatefulSet sts, int podId) {\n+        Promise<Admin> acPromise = Promise.promise();\n+        String cluster = sts.getMetadata().getLabels().get(Labels.STRIMZI_CLUSTER_LABEL);\n+        String namespace = sts.getMetadata().getNamespace();\n+        Future<Secret> clusterCaKeySecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+        Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(cluster));\n+        String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+        return CompositeFuture.join(clusterCaKeySecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+            Secret clusterCaKeySecret = compositeFuture.resultAt(0);\n+            if (clusterCaKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaKeySecretName(cluster)));\n+            }\n+            Secret coKeySecret = compositeFuture.resultAt(1);\n+            if (coKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+            }\n+\n+            Admin ac;\n+            try {\n+                log.info(\"about to create an admin client\");\n+                ac = adminClientProvider.createAdminClient(hostname, clusterCaKeySecret, coKeySecret, \"cluster-operator\");\n+                acPromise.complete(ac);\n+            } catch (RuntimeException e) {\n+                log.warn(\"Failed to create Admin Client. {}\", e);\n+                acPromise.complete(null);\n+            }\n+            return acPromise.future();\n+        });\n+    }\n+\n+    public Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2ODAzMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413368031", "bodyText": "How do you know 2 seconds will be enough? Don't we need some BackOff here? Thsi seems quite agressive.", "author": "scholzj", "createdAt": "2020-04-22T22:12:13Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.StatefulSet;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private final AdminClientProvider adminClientProvider;\n+    protected final SecretOperator secretOperations;\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public KafkaBrokerConfigurationHelper(AdminClientProvider adminClientProvider, SecretOperator secretOperations) {\n+        this.adminClientProvider = adminClientProvider;\n+        this.secretOperations = secretOperations;\n+    }\n+\n+    public Future<Admin> adminClient(StatefulSet sts, int podId) {\n+        Promise<Admin> acPromise = Promise.promise();\n+        String cluster = sts.getMetadata().getLabels().get(Labels.STRIMZI_CLUSTER_LABEL);\n+        String namespace = sts.getMetadata().getNamespace();\n+        Future<Secret> clusterCaKeySecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+        Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(cluster));\n+        String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+        return CompositeFuture.join(clusterCaKeySecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+            Secret clusterCaKeySecret = compositeFuture.resultAt(0);\n+            if (clusterCaKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaKeySecretName(cluster)));\n+            }\n+            Secret coKeySecret = compositeFuture.resultAt(1);\n+            if (coKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+            }\n+\n+            Admin ac;\n+            try {\n+                log.info(\"about to create an admin client\");\n+                ac = adminClientProvider.createAdminClient(hostname, clusterCaKeySecret, coKeySecret, \"cluster-operator\");\n+                acPromise.complete(ac);\n+            } catch (RuntimeException e) {\n+                log.warn(\"Failed to create Admin Client. {}\", e);\n+                acPromise.complete(null);\n+            }\n+            return acPromise.future();\n+        });\n+    }\n+\n+    public Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+        Promise<Map<ConfigResource, Config>> futRes = Promise.promise();\n+        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+        DescribeConfigsResult configs = ac.describeConfigs(Collections.singletonList(resource));\n+        Map<ConfigResource, Config> config = new HashMap<>();\n+        try {\n+            config = configs.all().get(2000, TimeUnit.MILLISECONDS);", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM2OTAwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413369008", "bodyText": "This seems a bit like a dirty hack. Why not pass the AdminClientProvider through the KafkaAssemblyOperator and ResourceOperatorSupplier?", "author": "scholzj", "createdAt": "2020-04-22T22:14:24Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaSetOperator.java", "diffHunk": "@@ -73,4 +73,7 @@ public static boolean needsRollingUpdate(StatefulSetDiff diff) {\n                 .rollingRestart(podNeedsRestart);\n     }\n \n+    public AdminClientProvider getAdminClientProvider() {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3MDQ4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413370486", "bodyText": "Does this actually connect? Or when / why would this happen?", "author": "scholzj", "createdAt": "2020-04-22T22:17:25Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.fabric8.kubernetes.api.model.apps.StatefulSet;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.model.Labels;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private final AdminClientProvider adminClientProvider;\n+    protected final SecretOperator secretOperations;\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public KafkaBrokerConfigurationHelper(AdminClientProvider adminClientProvider, SecretOperator secretOperations) {\n+        this.adminClientProvider = adminClientProvider;\n+        this.secretOperations = secretOperations;\n+    }\n+\n+    public Future<Admin> adminClient(StatefulSet sts, int podId) {\n+        Promise<Admin> acPromise = Promise.promise();\n+        String cluster = sts.getMetadata().getLabels().get(Labels.STRIMZI_CLUSTER_LABEL);\n+        String namespace = sts.getMetadata().getNamespace();\n+        Future<Secret> clusterCaKeySecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+        Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(cluster));\n+        String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+        return CompositeFuture.join(clusterCaKeySecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+            Secret clusterCaKeySecret = compositeFuture.resultAt(0);\n+            if (clusterCaKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaKeySecretName(cluster)));\n+            }\n+            Secret coKeySecret = compositeFuture.resultAt(1);\n+            if (coKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+            }\n+\n+            Admin ac;\n+            try {\n+                log.info(\"about to create an admin client\");\n+                ac = adminClientProvider.createAdminClient(hostname, clusterCaKeySecret, coKeySecret, \"cluster-operator\");\n+                acPromise.complete(ac);\n+            } catch (RuntimeException e) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQwNDU0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414404545", "bodyText": "Yeah, we should at least document this behaviour in the AdminClientProvider.", "author": "tombentley", "createdAt": "2020-04-24T08:46:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3MDQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3MTE2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413371163", "bodyText": "I think you should plug somewhere here an readiness check. If the pod is not ready, does it make sense to be connecting to it and trying to get the configuration? Or how would you deal with pods which are crashlooping ot starting?\nThis is called right after statefulset reconciliation. So the pods will be on the beginning for sure still creating for example.", "author": "scholzj", "createdAt": "2020-04-22T22:18:46Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3Mzc4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413373782", "bodyText": "Probably related to one of my other comments. This looks like wasteful HTTP call to the API server. We have generated this ConfgiMap few milliseconds ago. You should either store the desired configuration in getKafkaAncialiaryCm (as a field in ReconciliationState for example) or you should just generate it freshly.", "author": "scholzj", "createdAt": "2020-04-22T22:24:07Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        log.warn(\"Could not create admin client.\");\n+                                        return Future.succeededFuture();\n+                                    } else {\n+                                        Future<Map<ConfigResource, Config>> futCurrent = kbch.getCurrentConfig(finalPodId, ac);\n+                                        Future<ConfigMap> futDesired = configMapOperations.getAsync(namespace, KafkaCluster.metricAndLogConfigsName(name));", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3NDYwNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413374605", "bodyText": "If this log message should stay here, it should use the reconciliation as the prefix.", "author": "scholzj", "createdAt": "2020-04-22T22:26:00Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        log.warn(\"Could not create admin client.\");", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3NDY5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413374697", "bodyText": "Same as above, use the reconciliation as the prefix.", "author": "scholzj", "createdAt": "2020-04-22T22:26:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        log.warn(\"Could not create admin client.\");\n+                                        return Future.succeededFuture();\n+                                    } else {\n+                                        Future<Map<ConfigResource, Config>> futCurrent = kbch.getCurrentConfig(finalPodId, ac);\n+                                        Future<ConfigMap> futDesired = configMapOperations.getAsync(namespace, KafkaCluster.metricAndLogConfigsName(name));\n+                                        log.debug(\"Determining kafka pod {} dynamic update ability\", finalPodId);", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3NDkwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413374903", "bodyText": "Just use the reconciliation as the prefix everywhere ;-). This log message looks weird again like some debugging left over.", "author": "scholzj", "createdAt": "2020-04-22T22:26:34Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        log.warn(\"Could not create admin client.\");\n+                                        return Future.succeededFuture();\n+                                    } else {\n+                                        Future<Map<ConfigResource, Config>> futCurrent = kbch.getCurrentConfig(finalPodId, ac);\n+                                        Future<ConfigMap> futDesired = configMapOperations.getAsync(namespace, KafkaCluster.metricAndLogConfigsName(name));\n+                                        log.debug(\"Determining kafka pod {} dynamic update ability\", finalPodId);\n+                                        return CompositeFuture.join(futCurrent, futDesired)\n+                                                .compose(res -> {\n+                                                    KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res.result().resultAt(0), res.result().resultAt(1), kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                                    if (configurationDiff.getDiff().asOrderedProperties().asMap().size() > 0) {\n+                                                        log.debug(\"kafka changed, dynamically? : {}\", !configurationDiff.cannotBeUpdatedDynamically());", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3NjMzNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413376334", "bodyText": "You are puting the same thing into the map several times on 10 lines of code. Why not try to do the update and set it only once based on the results?", "author": "scholzj", "createdAt": "2020-04-22T22:29:47Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        log.warn(\"Could not create admin client.\");\n+                                        return Future.succeededFuture();\n+                                    } else {\n+                                        Future<Map<ConfigResource, Config>> futCurrent = kbch.getCurrentConfig(finalPodId, ac);\n+                                        Future<ConfigMap> futDesired = configMapOperations.getAsync(namespace, KafkaCluster.metricAndLogConfigsName(name));\n+                                        log.debug(\"Determining kafka pod {} dynamic update ability\", finalPodId);\n+                                        return CompositeFuture.join(futCurrent, futDesired)\n+                                                .compose(res -> {\n+                                                    KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res.result().resultAt(0), res.result().resultAt(1), kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                                    if (configurationDiff.getDiff().asOrderedProperties().asMap().size() > 0) {\n+                                                        log.debug(\"kafka changed, dynamically? : {}\", !configurationDiff.cannotBeUpdatedDynamically());\n+                                                        kafkaPodsUpdatedDynamically.put(finalPodId, !configurationDiff.cannotBeUpdatedDynamically());", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3NzM5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413377390", "bodyText": "This looks really weird - as if you are catching exception from debug log. Plase make this more readable. This also looks like a blocking code which blocks potentially for 5 minutes. You need to make this either execute blocking or use the KafkaFuture better.", "author": "scholzj", "createdAt": "2020-04-22T22:32:05Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        log.warn(\"Could not create admin client.\");\n+                                        return Future.succeededFuture();\n+                                    } else {\n+                                        Future<Map<ConfigResource, Config>> futCurrent = kbch.getCurrentConfig(finalPodId, ac);\n+                                        Future<ConfigMap> futDesired = configMapOperations.getAsync(namespace, KafkaCluster.metricAndLogConfigsName(name));\n+                                        log.debug(\"Determining kafka pod {} dynamic update ability\", finalPodId);\n+                                        return CompositeFuture.join(futCurrent, futDesired)\n+                                                .compose(res -> {\n+                                                    KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res.result().resultAt(0), res.result().resultAt(1), kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                                    if (configurationDiff.getDiff().asOrderedProperties().asMap().size() > 0) {\n+                                                        log.debug(\"kafka changed, dynamically? : {}\", !configurationDiff.cannotBeUpdatedDynamically());\n+                                                        kafkaPodsUpdatedDynamically.put(finalPodId, !configurationDiff.cannotBeUpdatedDynamically());\n+                                                        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                                        for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                                            KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();\n+                                                            try {\n+                                                                log.debug(\"AlterConfig result {}\", kafkaFuture.get(operationTimeoutMs, TimeUnit.MILLISECONDS));", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3NzY0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413377641", "bodyText": "Again, weird logging statement. Not nicely written, not recomciliation -> to be fixed or removed.", "author": "scholzj", "createdAt": "2020-04-22T22:32:41Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        log.warn(\"Could not create admin client.\");\n+                                        return Future.succeededFuture();\n+                                    } else {\n+                                        Future<Map<ConfigResource, Config>> futCurrent = kbch.getCurrentConfig(finalPodId, ac);\n+                                        Future<ConfigMap> futDesired = configMapOperations.getAsync(namespace, KafkaCluster.metricAndLogConfigsName(name));\n+                                        log.debug(\"Determining kafka pod {} dynamic update ability\", finalPodId);\n+                                        return CompositeFuture.join(futCurrent, futDesired)\n+                                                .compose(res -> {\n+                                                    KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res.result().resultAt(0), res.result().resultAt(1), kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                                    if (configurationDiff.getDiff().asOrderedProperties().asMap().size() > 0) {\n+                                                        log.debug(\"kafka changed, dynamically? : {}\", !configurationDiff.cannotBeUpdatedDynamically());\n+                                                        kafkaPodsUpdatedDynamically.put(finalPodId, !configurationDiff.cannotBeUpdatedDynamically());\n+                                                        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                                        for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                                            KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();\n+                                                            try {\n+                                                                log.debug(\"AlterConfig result {}\", kafkaFuture.get(operationTimeoutMs, TimeUnit.MILLISECONDS));\n+                                                            } catch (InvalidRequestException | InterruptedException | ExecutionException | TimeoutException e) {\n+                                                                log.warn(\"Error during dynamic reconfiguration. Rolling the pod. {}\", e.getMessage());\n+                                                                kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                                            }\n+                                                        }\n+                                                    } else {\n+                                                        log.debug(\"kafka not changed\");", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3ODA5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413378099", "bodyText": "Could you fix the name also for other ancialiary? Can be in separate PR, but having each of them different is weird.", "author": "scholzj", "createdAt": "2020-04-22T22:33:42Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -2247,7 +2314,7 @@ String getCertificateThumbprint(Secret certSecret, CertAndKeySecretSource custom\n             }\n         }\n \n-        ConfigMap getKafkaAncialiaryCm()    {\n+        ConfigMap getKafkaAncillaryCm()    {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3ODYyOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413378629", "bodyText": "How are you handling changes to BROKER_ADVERTISED_PORTS_FILENAME and  BROKER_ADVERTISED_HOSTNAMES_FILENAME  if not through the hash?", "author": "scholzj", "createdAt": "2020-04-22T22:34:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -2256,19 +2323,14 @@ ConfigMap getKafkaAncialiaryCm()    {\n \n             ConfigMap brokerCm = kafkaCluster.generateAncillaryConfigMap(loggingCm, kafkaExternalAdvertisedHostnames, kafkaExternalAdvertisedPorts);\n \n-            String brokerConfiguration = brokerCm.getData().get(KafkaCluster.BROKER_CONFIGURATION_FILENAME);\n-            brokerConfiguration += brokerCm.getData().getOrDefault(KafkaCluster.BROKER_ADVERTISED_PORTS_FILENAME, \"\");", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzczMTEyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413731124", "bodyText": "reverted", "author": "sknot-rh", "createdAt": "2020-04-23T11:19:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3ODYyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM3OTEwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413379100", "bodyText": "Having some utility method for parsing the index from the pos name would be useful. I bet it is used on more places.", "author": "scholzj", "createdAt": "2020-04-22T22:36:01Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -2459,7 +2521,7 @@ StatefulSet getKafkaStatefulSet()   {\n \n         Future<ReconciliationState> kafkaRollingUpdate() {\n             return withVoid(kafkaSetOperations.maybeRollingUpdate(kafkaDiffs.resource(), pod ->\n-                    getReasonsToRestartPod(kafkaDiffs.resource(), pod, existingKafkaCertsChanged, this.clusterCa, this.clientsCa)\n+                    getReasonsToRestartPod(kafkaDiffs.resource(), pod, existingKafkaCertsChanged, kafkaPodsUpdatedDynamically.get(Integer.parseInt(pod.getMetadata().getName().substring(pod.getMetadata().getName().lastIndexOf(\"-\") + 1))), this.clusterCa, this.clientsCa)", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MDQ5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413380497", "bodyText": "You set it to null here. But I didn't found any other place where you would handle the null situation. That is a bit weird, as it seems to almost as if this skips the config reconciliation for this pod:\n\nit doesn't update it dynamically\nit doesn't do rolling update\n\nSo should this instead fail the reconciliation?", "author": "scholzj", "createdAt": "2020-04-22T22:39:01Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1648,6 +1661,60 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            KafkaBrokerConfigurationHelper kbch = new KafkaBrokerConfigurationHelper(kafkaSetOperations.getAdminClientProvider(), secretOperations);\n+            return kafkaSetOperations.getAsync(namespace, KafkaCluster.kafkaClusterName(name))\n+                .compose(sts -> {\n+                    if (sts == null) {\n+                        return Future.succeededFuture();\n+                    }\n+                    int replicas = kafkaCluster.getReplicas();\n+                    List<Future> configFutures = new ArrayList<>(replicas);\n+\n+                    for (int podId = 0; podId < replicas; podId++) {\n+                        int finalPodId = podId;\n+                        Future<Admin> acFut = kbch.adminClient(sts, podId);\n+                        configFutures.add(\n+                                acFut.compose(ac -> {\n+                                    if (ac == null) {\n+                                        kafkaPodsUpdatedDynamically.put(finalPodId, null);", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzYxMjgzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413612833", "bodyText": "I think this is going to be fixed by using podReadiness at this place. I was hitting some situations when the pod was in pending state and thus it was unable to get ac -> skip the dynamic algorithm.", "author": "sknot-rh", "createdAt": "2020-04-23T08:22:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MDQ5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzYxNjcyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413616721", "bodyText": "Well, it might not be the only reason. I think the question also still stands. What is the right way to do here? Because if I understand this code right, when the client doesn't connect you have no other way to diff the configs. So you actually don't know what should or should not be done.\nAnd because this happens for example after the fresh deployment, you cannot really fail the reconciliation either as I suggested. So I think this needs to be thought through. Maybe this has to be skipped by the initial reconciliation and fail reconciliation otherwise?\nI also suspect that in reality - with the 2 seconds timeout which I saw somewhere, this will be failing quite often even in normal situations.", "author": "scholzj", "createdAt": "2020-04-23T08:27:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MDQ5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MTUxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413381518", "bodyText": "Should this be here or not? I guess changes to log dirs would anyway require rolling update because storage changes?", "author": "scholzj", "createdAt": "2020-04-22T22:41:24Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MTczMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413381731", "bodyText": "As I said on other places, I think pullin here the config map does nto make sense.", "author": "scholzj", "createdAt": "2020-04-22T22:41:52Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MjYyNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413382626", "bodyText": "The JsonDiff seems to have issues when order of options change ... are you sure we can handle it well here?", "author": "scholzj", "createdAt": "2020-04-22T22:43:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            log.warn(\"Failed to get broker {} configuration\", brokerId);\n+        } else {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjAyOTEwNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452029105", "bodyText": "@stanlyDoge I think this question is no longer valid, but could you confirm and resolve?", "author": "tombentley", "createdAt": "2020-07-09T07:49:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MjYyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA0MTY2NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452041665", "bodyText": "Yes, sorted input solves this.", "author": "sknot-rh", "createdAt": "2020-07-09T08:12:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4MjYyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4Mjg3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413382870", "bodyText": "I think this will cause lot of questions why did or didn't this roll. I think this needs to have debug level logging similar tot he statefulset diff.", "author": "scholzj", "createdAt": "2020-04-22T22:44:30Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            //+ \"|log\\\\.dirs\"*/\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs == null) {\n+            log.warn(\"Failed to get broker {} configuration\", brokerId);\n+        } else {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4NDUxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413384514", "bodyText": "Do we have somewhere some nice human readable list of test cases we have here? It would be useful to see what we have and what tests might be missing. Also I wonder if this deserves its own test class.", "author": "scholzj", "createdAt": "2020-04-22T22:48:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/KafkaST.java", "diffHunk": "@@ -2125,6 +2131,321 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testDynamicConfiguration() {", "originalCommit": "ac8ec7fc4858273ae376faaf3d3fe10c85b56e96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzcyODM2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413728363", "bodyText": "Basically the tests are doing this\n\ndeploy clean Kafka cluster\ndynamically update some change\nupdate some change, which causes RU (listeners, logging,...)\ndynamically update some change\n\nAfter each step, it is tested whether Kafka was rolled. In the case of dynamic changes, it is tested whether the change is done in Kafka (using kafka-configs.sh to check dyn. conf of brokers)", "author": "sknot-rh", "createdAt": "2020-04-23T11:14:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4NDUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzczMjQzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413732439", "bodyText": "But the list of the changes done in 2 and 3 is what is interesting", "author": "scholzj", "createdAt": "2020-04-23T11:21:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4NDUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzczNDM5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r413734390", "bodyText": "For dynamically changeable configuration I am setting unclean.leader.election.enable property. There is test wich is changing plain listeners to tls, to external, external nodeports to external loadbalancer.", "author": "sknot-rh", "createdAt": "2020-04-23T11:25:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzM4NDUxNA=="}], "type": "inlineReview"}, {"oid": "bc587228eeb820eca5d460ec97375ae83d46548a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bc587228eeb820eca5d460ec97375ae83d46548a", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-23T10:47:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM1OTgwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414359809", "bodyText": "I don't really understand what a ConfigMap data entry is. An entry in a ConfigMap could be any (String, String) pair, but presumably you're applying this method to a particular entry which has a particular syntax, right? If so you should at least explain what the entry is so the reader knows what the syntax is supposed to be.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * This method transforms ConfigMap data entry form String into the List of Strings, where entry = line from input.\n          \n          \n            \n                 * This method transforms a ConfigMap data entry from a String into a List of Strings, where entry = line from input.", "author": "tombentley", "createdAt": "2020-04-24T07:34:03Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,58 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms ConfigMap data entry form String into the List of Strings, where entry = line from input.", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2NTE2NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414365164", "bodyText": "You're getting the key from the data twice. Use a local and do it once.", "author": "tombentley", "createdAt": "2020-04-24T07:42:48Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,58 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms ConfigMap data entry form String into the List of Strings, where entry = line from input.\n+     * The comments are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {\n+                    validLines.add(line);\n+                }\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    /**\n+     * Gets data from ConfigMap as a String\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return String value of ConfigMap data entry. If the key does not exist in the ConfigMap, return null\n+     */\n+    public static String getDataFromConfigMap(ConfigMap configMap, String key) {\n+        if (configMap == null || configMap.getData() == null || configMap.getData().get(key) == null) {\n+            return null;\n+        }\n+        return configMap.getData().get(key);", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2NTYzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414365633", "bodyText": "No need to qualify the static method since it's in the same class.", "author": "tombentley", "createdAt": "2020-04-24T07:43:32Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,58 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms ConfigMap data entry form String into the List of Strings, where entry = line from input.\n+     * The comments are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {\n+                    validLines.add(line);\n+                }\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    /**\n+     * Gets data from ConfigMap as a String\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return String value of ConfigMap data entry. If the key does not exist in the ConfigMap, return null\n+     */\n+    public static String getDataFromConfigMap(ConfigMap configMap, String key) {\n+        if (configMap == null || configMap.getData() == null || configMap.getData().get(key) == null) {\n+            return null;\n+        }\n+        return configMap.getData().get(key);\n+    }\n+\n+    /**\n+     * Transforms data from ConfigMap into Map\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return Map of (key, value)\n+     */\n+    public static Map<String, String> configMap2Map(ConfigMap configMap, String key) {\n+        Map<String, String> result = new HashMap<>();\n+        List<String> list = ModelUtils.getLinesWithoutCommentsAndEmptyLines(getDataFromConfigMap(configMap, key));", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MDQ3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414570471", "bodyText": "It is called from another class aswell", "author": "sknot-rh", "createdAt": "2020-04-24T13:20:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2NTYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2ODE0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414368145", "bodyText": "What if the value contains =?", "author": "tombentley", "createdAt": "2020-04-24T07:47:54Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,58 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms ConfigMap data entry form String into the List of Strings, where entry = line from input.\n+     * The comments are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {\n+                    validLines.add(line);\n+                }\n+            }\n+        }\n+\n+        return validLines;\n+    }\n+\n+    /**\n+     * Gets data from ConfigMap as a String\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return String value of ConfigMap data entry. If the key does not exist in the ConfigMap, return null\n+     */\n+    public static String getDataFromConfigMap(ConfigMap configMap, String key) {\n+        if (configMap == null || configMap.getData() == null || configMap.getData().get(key) == null) {\n+            return null;\n+        }\n+        return configMap.getData().get(key);\n+    }\n+\n+    /**\n+     * Transforms data from ConfigMap into Map\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return Map of (key, value)\n+     */\n+    public static Map<String, String> configMap2Map(ConfigMap configMap, String key) {\n+        Map<String, String> result = new HashMap<>();\n+        List<String> list = ModelUtils.getLinesWithoutCommentsAndEmptyLines(getDataFromConfigMap(configMap, key));\n+        for (String line: list) {\n+            String[] split = line.split(\"=\");", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2ODc1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414368754", "bodyText": "I'm guessing that should be oldKafkaReplicas.", "author": "tombentley", "createdAt": "2020-04-24T07:48:54Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -381,6 +392,7 @@ ReconciliationState createReconciliationState(Reconciliation reconciliation, Kaf\n         private Integer zkCurrentReplicas = null;\n \n         private KafkaCluster kafkaCluster = null;\n+        private int oldReplicas;", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2OTA1Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414369057", "bodyText": "For consistency should we use the term kafka, rather than broker?", "author": "tombentley", "createdAt": "2020-04-24T07:49:24Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -394,6 +406,7 @@ ReconciliationState createReconciliationState(Reconciliation reconciliation, Kaf\n         private String zkLoggingHash = \"\";\n         private String kafkaLoggingHash = \"\";\n         private String kafkaBrokerConfigurationHash = \"\";\n+        private ConfigMap brokerCm;", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM2OTUyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414369521", "bodyText": "We're accruing more and more state in this class, so let's add a comment for what this is for.", "author": "tombentley", "createdAt": "2020-04-24T07:50:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -414,6 +427,7 @@ ReconciliationState createReconciliationState(Reconciliation reconciliation, Kaf\n         // Certificate change indicators\n         private boolean existingZookeeperCertsChanged = false;\n         private boolean existingKafkaCertsChanged = false;\n+        private Map<Integer, Boolean> kafkaPodsUpdatedDynamically = new HashMap<>();", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3MjU3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414372571", "bodyText": "You know, I read this method, and the previous comment about kafkaPodsUpdatedDynamically and I wonder if we should factor out a KafkaState class (and likewise ZookeeperState, etc). That would make it clearer when during the reconciliation the various fields get set (which has been a cause of bugs in the past). It might also be a useful first step to breaking apart ReconciliationState into smaller chunks (it's grown pretty huge). It's not something to do in this PR, but it's maybe worth thinking about @scholzj @ppatierno @samuel-hawker, wdyt?", "author": "tombentley", "createdAt": "2020-04-24T07:55:07Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1638,7 +1652,7 @@ String zkConnectionString(int connectToReplicas)  {\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n                             oldReplicas = sts.getSpec().getReplicas();\n                         }", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3Mjc1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414372753", "bodyText": "Javadoc comment to outline the algorithm please", "author": "tombentley", "createdAt": "2020-04-24T07:55:28Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3MzMzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414373333", "bodyText": "As mentioned yesterday, this should be oldReplicas (or oldKafkaReplicas).", "author": "tombentley", "createdAt": "2020-04-24T07:56:22Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3NTcxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414375714", "bodyText": "Unless you're using the options you can use the 1-arg version of incrementalAlterConfigs()", "author": "tombentley", "createdAt": "2020-04-24T08:00:15Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3Njc3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414376771", "bodyText": "If you declare entry with the generic type Map.Entry<ConfigResource, KafkaFuture<Void>> then this cast will be unnecessary.", "author": "tombentley", "createdAt": "2020-04-24T08:01:56Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                    for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                        KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3NzA2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414377061", "bodyText": "This will block in a vertx worker thread, IIUC. We should factor out a helper method which takes a KafkaFuture and returns a vertx Future. That should simplify resolving this, I think and will probably be useful elsewhere too.", "author": "tombentley", "createdAt": "2020-04-24T08:02:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                    for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                        KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();\n+                                        Util.waitFor(vertx, \"KafkaFuture to be completed\",  \"updated\", 1_000, operationTimeoutMs, () -> kafkaFuture.isDone());", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTY2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414571669", "bodyText": "Umpf... I am not experienced with KafkaFutures. Are you willing to help me with this?", "author": "sknot-rh", "createdAt": "2020-04-24T13:22:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3NzA2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgzOTE3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r415839175", "bodyText": "Something like this:\n        static <T> Future<T> foo(KafkaFuture<T> kf) {\n            Promise<T> promise = Promise.promise();\n            kf.whenComplete((result, error) -> {\n                if (error != null) {\n                    promise.fail(error);\n                } else {\n                    promise.complete(result);\n                }\n            });\n            return promise.future()\n        }\nBut with a better name than foo, and somewhere where it can be reused. We could probably refactor KafkaAvailability to use it if we added a little logging.", "author": "tombentley", "createdAt": "2020-04-27T14:02:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3NzA2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM3OTIwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414379208", "bodyText": "This will also block. And you should have it in a finally.", "author": "tombentley", "createdAt": "2020-04-24T08:06:21Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                    for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                        KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();\n+                                        Util.waitFor(vertx, \"KafkaFuture to be completed\",  \"updated\", 1_000, operationTimeoutMs, () -> kafkaFuture.isDone());\n+                                        try {\n+                                            log.debug(\"{} AlterConfig result {}\", reconciliation, kafkaFuture.get());\n+                                            kafkaPodsUpdatedDynamically.put(finalPodId, true);\n+                                        } catch (InvalidRequestException | InterruptedException | ExecutionException e) {\n+                                            log.warn(\"{} Error during dynamic reconfiguration. Rolling the pod. {}\", reconciliation, e.getMessage());\n+                                            kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                        }\n+                                    }\n+                                } else {\n+                                    log.debug(\"{} Kafka configuration not changed\", reconciliation);\n+                                    kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                }\n+                                ac.close();", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4MDU2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414380562", "bodyText": "So wer're concatenating the BROKER_ADVERTISED_HOSTNAMES_FILENAME and BROKER_ADVERTISED_PORTS_FILENAME? If that's correct it deserves a comment because it seems weird.", "author": "tombentley", "createdAt": "2020-04-24T08:08:35Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -2259,19 +2318,19 @@ ConfigMap getKafkaAncialiaryCm()    {\n \n             ConfigMap brokerCm = kafkaCluster.generateAncillaryConfigMap(loggingCm, kafkaExternalAdvertisedHostnames, kafkaExternalAdvertisedPorts);\n \n-            String brokerConfiguration = brokerCm.getData().get(KafkaCluster.BROKER_CONFIGURATION_FILENAME);\n+            String brokerConfiguration = brokerCm.getData().getOrDefault(KafkaCluster.BROKER_ADVERTISED_HOSTNAMES_FILENAME, \"\");\n             brokerConfiguration += brokerCm.getData().getOrDefault(KafkaCluster.BROKER_ADVERTISED_PORTS_FILENAME, \"\");", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4MDkzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414380939", "bodyText": "Isn't there a helper method for this in KafkaCluster or KafkaResources?", "author": "tombentley", "createdAt": "2020-04-24T08:09:13Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -2309,6 +2368,10 @@ int getPodIndexFromPvcName(String pvcName)  {\n             return Integer.parseInt(pvcName.substring(pvcName.lastIndexOf(\"-\") + 1));\n         }\n \n+        int getPodIndexFromPodName(String podName)  {\n+            return Integer.parseInt(podName.substring(podName.lastIndexOf(\"-\") + 1));\n+        }", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDUxNzQzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414517437", "bodyText": "No as far as I know.", "author": "sknot-rh", "createdAt": "2020-04-24T11:54:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4MDkzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0ODgwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420048809", "bodyText": "This seems to be used on at least two other placeses. So IMHO you should either factor it into one of the Util classes or open a separate issue so that someone does it later.", "author": "scholzj", "createdAt": "2020-05-05T11:48:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4MDkzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4MjE3Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414382177", "bodyText": "Given that kafkaPodsUpdatedDynamically is a member wouldn't it be better to just let this method access it, rather than expecting the caller to get from the map?", "author": "tombentley", "createdAt": "2020-04-24T08:11:16Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3097,6 +3159,7 @@ private boolean isCustomCertUpToDate(StatefulSet sts, Pod pod, String annotation\n          */\n         private String getReasonsToRestartPod(StatefulSet sts, Pod pod,\n                                        boolean nodeCertsChange,\n+                                       Boolean kafkaPodUpdatedDynamically,", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4MzQ5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414483494", "bodyText": "I think it is getting this more complicated because we need to decide there whether it is zk or Kafka pod.", "author": "sknot-rh", "createdAt": "2020-04-24T10:53:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4MjE3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4MzMzMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414383332", "bodyText": "Think about the logging. I think you should include the pod you're operating on and the fact that this is a dynamic update.", "author": "tombentley", "createdAt": "2020-04-24T08:13:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                    for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                        KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();\n+                                        Util.waitFor(vertx, \"KafkaFuture to be completed\",  \"updated\", 1_000, operationTimeoutMs, () -> kafkaFuture.isDone());\n+                                        try {\n+                                            log.debug(\"{} AlterConfig result {}\", reconciliation, kafkaFuture.get());", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NDIyNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414384226", "bodyText": "Can you explain how InvalidRequestException can be thrown? Usually all the remote exception from the broker would be the cause of the ExecutionException.", "author": "tombentley", "createdAt": "2020-04-24T08:14:42Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                    for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                        KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();\n+                                        Util.waitFor(vertx, \"KafkaFuture to be completed\",  \"updated\", 1_000, operationTimeoutMs, () -> kafkaFuture.isDone());\n+                                        try {\n+                                            log.debug(\"{} AlterConfig result {}\", reconciliation, kafkaFuture.get());\n+                                            kafkaPodsUpdatedDynamically.put(finalPodId, true);\n+                                        } catch (InvalidRequestException | InterruptedException | ExecutionException e) {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NTE0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414385146", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            reasons.add(\"Kafka configuration changed\");\n          \n          \n            \n                            reasons.add(\"Kafka configuration changed (update not possible)\");", "author": "tombentley", "createdAt": "2020-04-24T08:16:17Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3123,6 +3186,9 @@ private String getReasonsToRestartPod(StatefulSet sts, Pod pod,\n             if (!isPodUpToDate) {\n                 reasons.add(\"Pod has old generation\");\n             }\n+            if (kafkaPodUpdatedDynamically != null && !kafkaPodUpdatedDynamically) {\n+                reasons.add(\"Kafka configuration changed\");", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NjE0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414386142", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n          \n          \n            \n             * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a ConfigMap).\n          \n      \n    \n    \n  \n\nBut use {@link}!", "author": "tombentley", "createdAt": "2020-04-24T08:17:52Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDUxOTc4NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414519785", "bodyText": "Can you be more specific, please?", "author": "sknot-rh", "createdAt": "2020-04-24T11:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NjE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4Njk0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414386945", "bodyText": "\"the entry\": Which entry? Is there a loop over entries?", "author": "tombentley", "createdAt": "2020-04-24T08:19:08Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NzI5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414387293", "bodyText": "What's a custom entry?", "author": "tombentley", "createdAt": "2020-04-24T08:19:39Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDUyMDg4NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414520885", "bodyText": "If user adds to the spec.kafka.config any custom property.", "author": "sknot-rh", "createdAt": "2020-04-24T12:00:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NzI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NzQxMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414387410", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n          \n          \n            \n                private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);", "author": "tombentley", "createdAt": "2020-04-24T08:19:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4ODQyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414388420", "bodyText": "A comment about the -909[1-4] would be useful.", "author": "tombentley", "createdAt": "2020-04-24T08:21:30Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4OTI2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414389269", "bodyText": "Any reason not to use Collections.emptyMap()?", "author": "tombentley", "createdAt": "2020-04-24T08:22:56Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5MDY3Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414390677", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-04-24T08:25:09Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5MDc3Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414390772", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-04-24T08:25:19Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5MDg1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414390851", "bodyText": "Javadoc", "author": "tombentley", "createdAt": "2020-04-24T08:25:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5MjMzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414392339", "bodyText": "currentEntries.stream().collect(Collectors.toMap(configEntry -> configEntry.name(), configEntry -> configEntry.value()));", "author": "tombentley", "createdAt": "2020-04-24T08:27:51Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5MzExNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414393115", "bodyText": "Let's not get into the habit of using 2 to mean to. That way lies the madness of coding in textspeak.", "author": "tombentley", "createdAt": "2020-04-24T08:28:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5MzkxMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414393912", "bodyText": "Swap these and compute the value of pathValueWithoutSlash from the value of pathValue.", "author": "tombentley", "createdAt": "2020-04-24T08:30:18Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5NDQzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414394438", "bodyText": "Can we use a better name than e?", "author": "tombentley", "createdAt": "2020-04-24T08:31:05Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5NTgyNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414395825", "bodyText": "I would factor out entry.get() into a local as the first stmt in this block.", "author": "tombentley", "createdAt": "2020-04-24T08:33:14Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5NjU3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414396571", "bodyText": "Why not compare with the ConfigSource.DEFAULT_CONFIG enum member, rather than converting to text?\nAlso, missing Trace logging in this block?", "author": "tombentley", "createdAt": "2020-04-24T08:34:23Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (!entry.get().source().name().equals(\"DEFAULT_CONFIG\")) {", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5NzcxMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414397710", "bodyText": "Log that it's ignorable", "author": "tombentley", "createdAt": "2020-04-24T08:36:07Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (!entry.get().source().name().equals(\"DEFAULT_CONFIG\")) {\n+                        // we are deleting custom option\n+                        difference.put(pathValueWithoutSlash, \"deleted entry\");\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.get().value()), AlterConfigOp.OpType.DELETE));\n+                    } else if (entry.get().isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip\n+                        log.trace(\"{} not set in desired, using default value\", entry.get().name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            difference.put(pathValueWithoutSlash, null);\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.get().name(), null);\n+                        }", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5ODI3Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414398272", "bodyText": "We do that even if it's an ignorable path? If so it at least deserves a comment. If not, and we should guard with an ignorable check then we should add an else with trace logging. Also in that latter case it will make sense to pull out the ignorable check put outside this if/else chain, since all the branches would have the same logging for the ignorability case.", "author": "tombentley", "createdAt": "2020-04-24T08:36:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (!entry.get().source().name().equals(\"DEFAULT_CONFIG\")) {\n+                        // we are deleting custom option\n+                        difference.put(pathValueWithoutSlash, \"deleted entry\");\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.get().value()), AlterConfigOp.OpType.DELETE));", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU1MzAzMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414553031", "bodyText": "I think custom property cannot be ignorable.", "author": "sknot-rh", "createdAt": "2020-04-24T12:54:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5ODI3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5ODU1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414398554", "bodyText": "Again, log the ignorability in an else.", "author": "tombentley", "createdAt": "2020-04-24T08:37:20Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (!entry.get().source().name().equals(\"DEFAULT_CONFIG\")) {\n+                        // we are deleting custom option\n+                        difference.put(pathValueWithoutSlash, \"deleted entry\");\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.get().value()), AlterConfigOp.OpType.DELETE));\n+                    } else if (entry.get().isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip\n+                        log.trace(\"{} not set in desired, using default value\", entry.get().name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            difference.put(pathValueWithoutSlash, null);\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.get().name(), null);\n+                        }\n+                    }\n+                } else if (d.get(\"op\").asText().equals(\"replace\")) {\n+                    // entry is in the current, desired is updated value\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        log.trace(\"{} has new desired value {}\", entry.get().name(), desiredMap.get(entry.get().name()));\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        difference.put(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash));\n+                    }", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5ODYyNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414398625", "bodyText": "Again, log the ignorability in an else.", "author": "tombentley", "createdAt": "2020-04-24T08:37:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (!entry.get().source().name().equals(\"DEFAULT_CONFIG\")) {\n+                        // we are deleting custom option\n+                        difference.put(pathValueWithoutSlash, \"deleted entry\");\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.get().value()), AlterConfigOp.OpType.DELETE));\n+                    } else if (entry.get().isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip\n+                        log.trace(\"{} not set in desired, using default value\", entry.get().name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            difference.put(pathValueWithoutSlash, null);\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.get().name(), null);\n+                        }\n+                    }\n+                } else if (d.get(\"op\").asText().equals(\"replace\")) {\n+                    // entry is in the current, desired is updated value\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        log.trace(\"{} has new desired value {}\", entry.get().name(), desiredMap.get(entry.get().name()));\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        difference.put(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash));\n+                    }\n+                }\n+            } else {\n+                if (d.get(\"op\").asText().equals(\"add\")) {\n+                    // entry is not in the current, it is added\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        log.trace(\"add new {} {}\", pathValueWithoutSlash, d.get(\"op\").asText());\n+                        difference.put(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash));\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                    }", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQwMjIxMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414402212", "bodyText": "This is ugly. Why serialize to a String only to parse it back into a map (which is what KafkaConfiguration is doing)? If you need a KafkaConfiguration.unvalidated(Map<String, String>) then write one!", "author": "tombentley", "createdAt": "2020-04-24T08:42:54Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (!entry.get().source().name().equals(\"DEFAULT_CONFIG\")) {\n+                        // we are deleting custom option\n+                        difference.put(pathValueWithoutSlash, \"deleted entry\");\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.get().value()), AlterConfigOp.OpType.DELETE));\n+                    } else if (entry.get().isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip\n+                        log.trace(\"{} not set in desired, using default value\", entry.get().name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            difference.put(pathValueWithoutSlash, null);\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.get().name(), null);\n+                        }\n+                    }\n+                } else if (d.get(\"op\").asText().equals(\"replace\")) {\n+                    // entry is in the current, desired is updated value\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        log.trace(\"{} has new desired value {}\", entry.get().name(), desiredMap.get(entry.get().name()));\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        difference.put(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash));\n+                    }\n+                }\n+            } else {\n+                if (d.get(\"op\").asText().equals(\"add\")) {\n+                    // entry is not in the current, it is added\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        log.trace(\"add new {} {}\", pathValueWithoutSlash, d.get(\"op\").asText());\n+                        difference.put(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash));\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker Config Differs : {}\", d);\n+            log.debug(\"Current Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+\n+\n+        difference.entrySet().forEach(e -> {\n+            log.info(\"{} broker conf differs: '{}' -> '{}'\", e.getKey(), currentMap.get(e.getKey()), e.getValue());\n+        });\n+\n+        updated.put(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)), updatedCE);\n+\n+        String diffString = difference.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+        return KafkaConfiguration.unvalidated(diffString);", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQwMzExNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414403114", "bodyText": "I would add an isEmpty() to KafkaConfiguration.", "author": "tombentley", "createdAt": "2020-04-24T08:44:13Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between current config supplied as a Map ConfigResource, Config and desired config supplied as a ConfigMap\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with default value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class.getName());\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|super\\\\.users\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, ConfigMap desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = emptyKafkaConf(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private KafkaConfiguration emptyKafkaConf() {\n+        Map<String, Object> conf = new HashMap<>();\n+        return new KafkaConfiguration(conf.entrySet());\n+    }\n+\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    public KafkaConfiguration getDiff() {\n+        return this.diff;\n+    }\n+\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean cannotBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return false;\n+        } else return diff.anyReadOnly(kafkaVersion)\n+                || !diff.unknownConfigs(kafkaVersion).isEmpty()\n+                || diff.containsListenersChange();\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getUpdatedConfig() {\n+        return updated;\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public KafkaConfiguration computeDiff() {\n+        Map<String, String> currentMap = new HashMap<>();\n+        Map<String, String> difference = new HashMap<>();\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentEntries.stream().forEach(e -> {\n+            currentMap.put(e.name(), e.value());\n+        });\n+\n+        Map<String, String> desiredMap = ModelUtils.configMap2Map(desired, \"server.config\");\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValueWithoutSlash = d.get(\"path\").asText().substring(1);\n+            String pathValue = d.get(\"path\").asText();\n+\n+            Optional<ConfigEntry> entry = currentEntries.stream().filter(e -> e.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (entry.isPresent()) {\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (!entry.get().source().name().equals(\"DEFAULT_CONFIG\")) {\n+                        // we are deleting custom option\n+                        difference.put(pathValueWithoutSlash, \"deleted entry\");\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.get().value()), AlterConfigOp.OpType.DELETE));\n+                    } else if (entry.get().isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip\n+                        log.trace(\"{} not set in desired, using default value\", entry.get().name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            difference.put(pathValueWithoutSlash, null);\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.get().name(), null);\n+                        }\n+                    }\n+                } else if (d.get(\"op\").asText().equals(\"replace\")) {\n+                    // entry is in the current, desired is updated value\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        log.trace(\"{} has new desired value {}\", entry.get().name(), desiredMap.get(entry.get().name()));\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        difference.put(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash));\n+                    }\n+                }\n+            } else {\n+                if (d.get(\"op\").asText().equals(\"add\")) {\n+                    // entry is not in the current, it is added\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        log.trace(\"add new {} {}\", pathValueWithoutSlash, d.get(\"op\").asText());\n+                        difference.put(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash));\n+                        updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker Config Differs : {}\", d);\n+            log.debug(\"Current Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+\n+\n+        difference.entrySet().forEach(e -> {\n+            log.info(\"{} broker conf differs: '{}' -> '{}'\", e.getKey(), currentMap.get(e.getKey()), e.getValue());\n+        });\n+\n+        updated.put(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)), updatedCE);\n+\n+        String diffString = difference.toString();\n+        diffString = diffString.substring(1, diffString.length() - 1).replace(\", \", \"\\n\");\n+        return KafkaConfiguration.unvalidated(diffString);\n+    }\n+\n+    @Override\n+    public boolean isEmpty() {\n+        return diff.asOrderedProperties().asMap().size() == 0;", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQwMzk0OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414403949", "bodyText": "This is very similar to the code for rolling. I suspect we can factor it out somewhere common.", "author": "tombentley", "createdAt": "2020-04-24T08:45:32Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import io.vertx.core.Vertx;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public static Future<Admin> adminClient(SecretOperator secretOperations, AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+        Promise<Admin> acPromise = Promise.promise();\n+        Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+        Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(cluster));\n+        String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+        return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+            Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+            if (clusterCaCertSecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+            }\n+            Secret coKeySecret = compositeFuture.resultAt(1);\n+            if (coKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+            }\n+\n+            Admin ac;\n+            try {\n+                ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                acPromise.complete(ac);\n+            } catch (RuntimeException e) {\n+                log.warn(\"Failed to create Admin Client. {}\", e);\n+                acPromise.complete(null);\n+            }\n+            return acPromise.future();", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQwNjQ0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414406443", "bodyText": "I think you should be able to use KafkaFuture.whenComplete() to avoid needing the waitFor(). That's what we do in the TO.", "author": "tombentley", "createdAt": "2020-04-24T08:49:18Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationHelper.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.operator.cluster.ClusterOperator;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.operator.resource.SecretOperator;\n+import io.vertx.core.CompositeFuture;\n+import io.vertx.core.Future;\n+import io.vertx.core.Promise;\n+import io.vertx.core.Vertx;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+\n+/**\n+ * This class contains methods for getting current configuration from the kafka brokers asynchronously.\n+ */\n+public class KafkaBrokerConfigurationHelper {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationHelper.class);\n+\n+    public static Future<Admin> adminClient(SecretOperator secretOperations, AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+        Promise<Admin> acPromise = Promise.promise();\n+        Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+        Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(cluster));\n+        String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+        return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+            Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+            if (clusterCaCertSecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+            }\n+            Secret coKeySecret = compositeFuture.resultAt(1);\n+            if (coKeySecret == null) {\n+                return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+            }\n+\n+            Admin ac;\n+            try {\n+                ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                acPromise.complete(ac);\n+            } catch (RuntimeException e) {\n+                log.warn(\"Failed to create Admin Client. {}\", e);\n+                acPromise.complete(null);\n+            }\n+            return acPromise.future();\n+        });\n+    }\n+\n+    public static Future<Map<ConfigResource, Config>> getCurrentConfig(Vertx vertx, long operationTimeoutMs, int podId, Admin ac) {\n+        Promise<Map<ConfigResource, Config>> futRes = Promise.promise();\n+        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+        DescribeConfigsResult configs = ac.describeConfigs(Collections.singletonList(resource));\n+        Map<ConfigResource, Config> config = new HashMap<>();\n+\n+        KafkaFuture<Map<ConfigResource, Config>> kafkaFuture = configs.all();\n+        Util.waitFor(vertx, \"KafkaFuture to complete\", \"fetched\", 1_000, operationTimeoutMs, () -> kafkaFuture.isDone());", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU2NjE2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414566169", "bodyText": "When does whenComplete() timeout?", "author": "sknot-rh", "createdAt": "2020-04-24T13:14:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQwNjQ0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ1ODY5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414458690", "bodyText": "if passing default options, I think you can call the other method overload where the options parameter is not provided and it use the default ones automatically.", "author": "ppatierno", "createdAt": "2020-04-24T10:10:31Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ2NTAxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414465015", "bodyText": "this is all blocking, I guess we should use do an executeBlocking for doing the entire processing. @tombentley @scholzj wdyt?", "author": "ppatierno", "createdAt": "2020-04-24T10:21:04Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1651,6 +1665,51 @@ String zkConnectionString(int connectToReplicas)  {\n                     });\n         }\n \n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            int replicas = Math.min(oldReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(replicas);\n+\n+            for (int podId = 0; podId < replicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            KafkaBrokerConfigurationHelper.adminClient(secretOperations, adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka pod {} dynamic update ability\", reconciliation, finalPodId);\n+                            return KafkaBrokerConfigurationHelper.getCurrentConfig(vertx, operationTimeoutMs, finalPodId, ac).compose(res -> {\n+                                KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.brokerCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+\n+                                if (!configurationDiff.isEmpty()) {\n+                                    AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig(), new AlterConfigsOptions());\n+                                    for (Map.Entry entry : alterConfigResult.values().entrySet()) {\n+                                        KafkaFuture kafkaFuture = (KafkaFuture) entry.getValue();", "originalCommit": "a3e9f608f3cfb8eb3ecf9432811b8c5bddcdf861", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2MjAzMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414662031", "bodyText": "Why public? And what not just desiredConfiguration()", "author": "tombentley", "createdAt": "2020-04-24T15:24:43Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2MjUwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414662508", "bodyText": "use a try-with-resources", "author": "tombentley", "createdAt": "2020-04-24T15:25:18Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2NDA0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414664047", "bodyText": "Using a ConfigMapBuilder and its addToData() methods would be clearer.", "author": "tombentley", "createdAt": "2020-04-24T15:27:20Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2NDU1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414664551", "bodyText": "currentConfiguration", "author": "tombentley", "createdAt": "2020-04-24T15:28:02Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2NDc5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414664796", "bodyText": "try with resource", "author": "tombentley", "createdAt": "2020-04-24T15:28:22Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {\n+        Map<ConfigResource, Config> current = new HashMap<>();\n+        ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, \"0\");\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\");", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2NTY3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414665679", "bodyText": "How does this work when the JVM OS line separator differs from the separator used in the conf file?", "author": "tombentley", "createdAt": "2020-04-24T15:29:34Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {\n+        Map<ConfigResource, Config> current = new HashMap<>();\n+        ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, \"0\");\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\");\n+\n+        List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU2MTI2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r415561261", "bodyText": "Did not test :(", "author": "sknot-rh", "createdAt": "2020-04-27T07:03:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2NTY3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY2Njg0OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414666849", "bodyText": "Should \"0\" be brokerId?", "author": "tombentley", "createdAt": "2020-04-24T15:31:13Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {\n+        Map<ConfigResource, Config> current = new HashMap<>();\n+        ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, \"0\");", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY3MDk0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414670947", "bodyText": "The case of a customer property is worth discussing: Kafka supports Reconfigurable interface which extends Configurable. IDK how many places where a plugin is used actually support reconfiguration via Reconfigurable. And I guess right now it's moot because Strimzi doesn't really support plugins (at least not very well). But in theory Kafka might be able to support this, sometimes. I guess you might be able to tell by trying to make a config change, but with AlterConfigsOptions.validateOnly() set and seeing if you got an error. I guess it's OK to leave our support for this to a later PR.", "author": "tombentley", "createdAt": "2020-04-24T15:36:54Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {\n+        Map<ConfigResource, Config> current = new HashMap<>();\n+        ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, \"0\");\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\");\n+\n+        List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+        configList.forEach(entry -> {\n+            String[] split = entry.split(\"=\");\n+            String val = split.length == 1 ? \"\" : split[1];\n+            ConfigEntry ce = new ConfigEntry(split[0].replace(\"\\n\", \"\"), val, true, true, false);\n+            entryList.add(ce);\n+        });\n+        for (ConfigEntry ce: additional) {\n+            entryList.add(ce);\n+        }\n+\n+        Config config = new Config(entryList);\n+        current.put(cr, config);\n+        return current;\n+    }\n+\n+    @Test\n+    public void testDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(new ArrayList<>()), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.isDesiredPropertyDefaultValue(\"offset.metadata.max.bytes\"), is(true));\n+    }\n+\n+    @Test\n+    public void testNonDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"offset.metadata.max.bytes\", \"4097\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(ces), getTestingDesiredConfiguration(new ArrayList<>()), kafkaVersion, brokerId);\n+        assertThat(kcd.isDesiredPropertyDefaultValue(\"offset.metadata.max.bytes\"), is(false));\n+    }\n+\n+    @Test\n+    public void testCustomPropertyAdded() {", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwMDI1Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r455000257", "bodyText": "Is this covered with some issue if we are leaving this for later?", "author": "scholzj", "createdAt": "2020-07-15T12:05:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY3MDk0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA1NDYzNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r455054634", "bodyText": "I did not create one. Did you @tombentley ?", "author": "sknot-rh", "createdAt": "2020-07-15T13:33:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY3MDk0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4MTA3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r455081079", "bodyText": "I didn't. Looks like MetricsReporter (configured via kafka.metrics.reporters) is currently the only user-exposed plugin which supports this right now. @stanlyDoge can you open an issue to figure out how to do this generically?", "author": "tombentley", "createdAt": "2020-07-15T14:09:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY3MDk0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY3MTc4MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414671780", "bodyText": "What about the case where the custom config is kept but is changed?", "author": "tombentley", "createdAt": "2020-04-24T15:38:11Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {\n+        Map<ConfigResource, Config> current = new HashMap<>();\n+        ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, \"0\");\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\");\n+\n+        List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+        configList.forEach(entry -> {\n+            String[] split = entry.split(\"=\");\n+            String val = split.length == 1 ? \"\" : split[1];\n+            ConfigEntry ce = new ConfigEntry(split[0].replace(\"\\n\", \"\"), val, true, true, false);\n+            entryList.add(ce);\n+        });\n+        for (ConfigEntry ce: additional) {\n+            entryList.add(ce);\n+        }\n+\n+        Config config = new Config(entryList);\n+        current.put(cr, config);\n+        return current;\n+    }\n+\n+    @Test\n+    public void testDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(new ArrayList<>()), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.isDesiredPropertyDefaultValue(\"offset.metadata.max.bytes\"), is(true));\n+    }\n+\n+    @Test\n+    public void testNonDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"offset.metadata.max.bytes\", \"4097\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(ces), getTestingDesiredConfiguration(new ArrayList<>()), kafkaVersion, brokerId);\n+        assertThat(kcd.isDesiredPropertyDefaultValue(\"offset.metadata.max.bytes\"), is(false));\n+    }\n+\n+    @Test\n+    public void testCustomPropertyAdded() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"custom.property\", \"42\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(new ArrayList<>()), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.getDiff().unknownConfigs(kafkaVersion).size(), is(1));\n+        assertThat(kcd.cannotBeUpdatedDynamically(), is(true));\n+    }\n+\n+    @Test\n+    public void testCustomPropertyRemoved() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"custom.property\", \"42\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(ces), getTestingDesiredConfiguration(new ArrayList<>()), kafkaVersion, brokerId);\n+        assertThat(kcd.getDiff().unknownConfigs(kafkaVersion).size(), is(1));\n+        assertThat(kcd.cannotBeUpdatedDynamically(), is(true));\n+    }\n+\n+    @Test\n+    public void testCustomPropertyKept() {", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY3MzgxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414673815", "bodyText": "For all these tests it would be so much easier to read if:\n\nYou used singletonList(), rather than creating a new ArrayList().\nYou inlined ces.\nYou broker the long 3rd line", "author": "tombentley", "createdAt": "2020-04-24T15:41:02Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {\n+        Map<ConfigResource, Config> current = new HashMap<>();\n+        ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, \"0\");\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\");\n+\n+        List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+        configList.forEach(entry -> {\n+            String[] split = entry.split(\"=\");\n+            String val = split.length == 1 ? \"\" : split[1];\n+            ConfigEntry ce = new ConfigEntry(split[0].replace(\"\\n\", \"\"), val, true, true, false);\n+            entryList.add(ce);\n+        });\n+        for (ConfigEntry ce: additional) {\n+            entryList.add(ce);\n+        }\n+\n+        Config config = new Config(entryList);\n+        current.put(cr, config);\n+        return current;\n+    }\n+\n+    @Test\n+    public void testDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(new ArrayList<>()), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.isDesiredPropertyDefaultValue(\"offset.metadata.max.bytes\"), is(true));\n+    }\n+\n+    @Test\n+    public void testNonDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"offset.metadata.max.bytes\", \"4097\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(ces), getTestingDesiredConfiguration(new ArrayList<>()), kafkaVersion, brokerId);", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY3NDk5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r414674994", "bodyText": "When the diff is non empty we should also assert on what the diff actually is. Just concert it to a map and assertEquals() against singletonMap.", "author": "tombentley", "createdAt": "2020-04-24T15:42:50Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.4.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    public ConfigMap getTestingDesiredConfiguration(ArrayList<ConfigEntry> additional) {\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\");\n+        String desiredConfigString = TestUtils.readResource(is);\n+\n+        for (ConfigEntry ce: additional) {\n+            desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+        }\n+\n+        ConfigMap configMap = new ConfigMap();\n+\n+        HashMap<String, String> data = new HashMap();\n+        data.put(\"server.config\", desiredConfigString);\n+        configMap.setData(data);\n+        return configMap;\n+    }\n+\n+    public Map<ConfigResource, Config> getTestingCurrentConfiguration(ArrayList<ConfigEntry> additional) {\n+        Map<ConfigResource, Config> current = new HashMap<>();\n+        ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, \"0\");\n+        InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\");\n+\n+        List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+        configList.forEach(entry -> {\n+            String[] split = entry.split(\"=\");\n+            String val = split.length == 1 ? \"\" : split[1];\n+            ConfigEntry ce = new ConfigEntry(split[0].replace(\"\\n\", \"\"), val, true, true, false);\n+            entryList.add(ce);\n+        });\n+        for (ConfigEntry ce: additional) {\n+            entryList.add(ce);\n+        }\n+\n+        Config config = new Config(entryList);\n+        current.put(cr, config);\n+        return current;\n+    }\n+\n+    @Test\n+    public void testDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(new ArrayList<>()), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.isDesiredPropertyDefaultValue(\"offset.metadata.max.bytes\"), is(true));\n+    }\n+\n+    @Test\n+    public void testNonDefaultValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"offset.metadata.max.bytes\", \"4097\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(ces), getTestingDesiredConfiguration(new ArrayList<>()), kafkaVersion, brokerId);\n+        assertThat(kcd.isDesiredPropertyDefaultValue(\"offset.metadata.max.bytes\"), is(false));\n+    }\n+\n+    @Test\n+    public void testCustomPropertyAdded() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"custom.property\", \"42\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(new ArrayList<>()), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.getDiff().unknownConfigs(kafkaVersion).size(), is(1));\n+        assertThat(kcd.cannotBeUpdatedDynamically(), is(true));\n+    }\n+\n+    @Test\n+    public void testCustomPropertyRemoved() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"custom.property\", \"42\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(ces), getTestingDesiredConfiguration(new ArrayList<>()), kafkaVersion, brokerId);\n+        assertThat(kcd.getDiff().unknownConfigs(kafkaVersion).size(), is(1));\n+        assertThat(kcd.cannotBeUpdatedDynamically(), is(true));\n+    }\n+\n+    @Test\n+    public void testCustomPropertyKept() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"custom.property\", \"42\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(ces), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.getDiff().unknownConfigs(kafkaVersion).size(), is(0));\n+        assertThat(kcd.cannotBeUpdatedDynamically(), is(false));\n+    }\n+\n+    @Test\n+    public void testChangedPresentValue() {\n+        ArrayList<ConfigEntry> ces = new ArrayList<>();\n+        ces.add(new ConfigEntry(\"min.insync.replicas\", \"2\", false, true, false));\n+        KafkaBrokerConfigurationDiff kcd = new KafkaBrokerConfigurationDiff(getTestingCurrentConfiguration(new ArrayList<>()), getTestingDesiredConfiguration(ces), kafkaVersion, brokerId);\n+        assertThat(kcd.getDiff().asOrderedProperties().asMap().size(), is(1));", "originalCommit": "9253c09d9e9f72a0f64ba218c10248fe66ff1289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU2NjA0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r416566040", "bodyText": "It is impossible to close AdminClient and I have no idea why.", "author": "sknot-rh", "createdAt": "2020-04-28T12:19:40Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1717,11 +1713,18 @@ String zkConnectionString(int connectToReplicas)  {\n                                     log.debug(\"{} Kafka broker {} configuration not changed\", reconciliation, finalPodId);\n                                     kafkaPodsUpdatedDynamically.put(finalPodId, null);\n                                 }\n-                                if (!acClosed) {\n-                                    ac.close();\n-                                }\n                                 return Future.succeededFuture();\n-                            });\n+                            }); /*.setHandler(ign -> {", "originalCommit": "a15a86f15246099918562b978560b9f86e3859f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e2f990c96ad6bed384a8e0ea0ec545452dcf1c47", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e2f990c96ad6bed384a8e0ea0ec545452dcf1c47", "message": "transfer kafkaFuture to vertx one\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-28T13:34:56Z", "type": "forcePushed"}, {"oid": "cabb65bf0774c3ec58c71bbd0482d6157f3e8cc8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/cabb65bf0774c3ec58c71bbd0482d6157f3e8cc8", "message": "Tom is the best\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-04-30T07:11:34Z", "type": "forcePushed"}, {"oid": "11dfe935b1f4e679ba569fd290594258b0377087", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/11dfe935b1f4e679ba569fd290594258b0377087", "message": "maybefix\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-05-04T10:15:08Z", "type": "forcePushed"}, {"oid": "ea24acdc1ae8da235c9481308c104e38ab0089b8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ea24acdc1ae8da235c9481308c104e38ab0089b8", "message": "maybefix\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-05-04T10:16:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDAzOTc1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420039754", "bodyText": "This looks suspitios. are you really loooking for running replicas before reconciliation or after reocnciliation? I wonder if it is just badly chosen variable name (in which case you should rename it) or if it is some bug in the code.", "author": "scholzj", "createdAt": "2020-05-05T11:30:26Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1654,19 +1672,131 @@ String zkConnectionString(int connectToReplicas)  {\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0MDMwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420040300", "bodyText": "I still don't understand why do you need to go configuration -> config map -> configuration instead of just using the configuration. Also, all the methods refer to it as AncillaryCm, so it should be probably reflected here as well.", "author": "scholzj", "createdAt": "2020-05-05T11:31:29Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -405,6 +419,7 @@ ReconciliationState createReconciliationState(Reconciliation reconciliation, Kaf\n         private String zkLoggingHash = \"\";\n         private String kafkaLoggingHash = \"\";\n         private String kafkaBrokerConfigurationHash = \"\";\n+        private ConfigMap kafkaCm;", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0MDM4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420040382", "bodyText": "I still don't understand why do you need to go configuration -> config map -> configuration instead of just using the configuration.", "author": "scholzj", "createdAt": "2020-05-05T11:31:39Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -434,4 +435,57 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms a String into a List of Strings, where entry = line from input.\n+     * The lines beginning with '#' (comments) are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {\n+                    validLines.add(line);\n+                }\n+            }\n+        }\n+        return validLines;\n+    }\n+\n+    /**\n+     * Gets data from ConfigMap as a String\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return String value of ConfigMap data entry. If the key does not exist in the ConfigMap, return null\n+     */\n+    public static String getDataFromConfigMap(ConfigMap configMap, String key) {", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0NDI3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420044279", "bodyText": "Question: What does the result contain? The whole configuration? Some Success / failure statement?", "author": "scholzj", "createdAt": "2020-05-05T11:39:43Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1654,19 +1672,131 @@ String zkConnectionString(int connectToReplicas)  {\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningReplicas);\n+\n+            for (int podId = 0; podId < runningReplicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            adminClient(adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka broker {} dynamic update ability\", reconciliation, finalPodId);\n+                            Promise<Void> p = Promise.promise();\n+                            getCurrentConfig(finalPodId, ac)\n+                                    .compose(res -> {\n+                                        log.trace(\"Broker description {}\", res);\n+                                        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.kafkaCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+                                        if (!configurationDiff.isEmpty()) {\n+                                            try {\n+                                                AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig());\n+                                                KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(finalPodId)));\n+                                                return kafkaFutureToVertxFuture(brokerConfigFuture).compose(result -> {\n+                                                    log.debug(\"{} Dynamic AlterConfig result for broker {} {}\", reconciliation, finalPodId, result);", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA2MDQxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420060414", "bodyText": "get() returns KafkaFuture<Void> so I think it will be always null.", "author": "sknot-rh", "createdAt": "2020-05-05T12:11:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0NDI3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDEwMDE1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420100152", "bodyText": "So I guess it should be updated?", "author": "scholzj", "createdAt": "2020-05-05T13:16:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0NDI3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0NDY3Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420044673", "bodyText": "Typoe broekr => broker", "author": "scholzj", "createdAt": "2020-05-05T11:40:26Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1654,19 +1672,131 @@ String zkConnectionString(int connectToReplicas)  {\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningReplicas);\n+\n+            for (int podId = 0; podId < runningReplicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            adminClient(adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka broker {} dynamic update ability\", reconciliation, finalPodId);\n+                            Promise<Void> p = Promise.promise();\n+                            getCurrentConfig(finalPodId, ac)\n+                                    .compose(res -> {\n+                                        log.trace(\"Broker description {}\", res);\n+                                        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.kafkaCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+                                        if (!configurationDiff.isEmpty()) {\n+                                            try {\n+                                                AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig());\n+                                                KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(finalPodId)));\n+                                                return kafkaFutureToVertxFuture(brokerConfigFuture).compose(result -> {\n+                                                    log.debug(\"{} Dynamic AlterConfig result for broker {} {}\", reconciliation, finalPodId, result);\n+                                                    kafkaPodsUpdatedDynamically.put(finalPodId, true);\n+                                                    return Future.<Void>succeededFuture();\n+                                                },\n+                                                    error -> {\n+                                                        log.debug(\"{} Error during dynamic reconfiguration. Rolling the broekr {}. {}\", reconciliation, finalPodId, error);", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0NDk0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420044946", "bodyText": "Left over debug statement?", "author": "scholzj", "createdAt": "2020-05-05T11:40:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1654,19 +1672,131 @@ String zkConnectionString(int connectToReplicas)  {\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningReplicas);\n+\n+            for (int podId = 0; podId < runningReplicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            adminClient(adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka broker {} dynamic update ability\", reconciliation, finalPodId);\n+                            Promise<Void> p = Promise.promise();\n+                            getCurrentConfig(finalPodId, ac)\n+                                    .compose(res -> {\n+                                        log.trace(\"Broker description {}\", res);\n+                                        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.kafkaCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+                                        if (!configurationDiff.isEmpty()) {\n+                                            try {\n+                                                AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig());\n+                                                KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(finalPodId)));\n+                                                return kafkaFutureToVertxFuture(brokerConfigFuture).compose(result -> {\n+                                                    log.debug(\"{} Dynamic AlterConfig result for broker {} {}\", reconciliation, finalPodId, result);\n+                                                    kafkaPodsUpdatedDynamically.put(finalPodId, true);\n+                                                    return Future.<Void>succeededFuture();\n+                                                },\n+                                                    error -> {\n+                                                        log.debug(\"{} Error during dynamic reconfiguration. Rolling the broekr {}. {}\", reconciliation, finalPodId, error);\n+                                                        kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                                        return Future.<Void>succeededFuture();\n+                                                    });\n+                                            } catch (InvalidRequestException e) {\n+                                                log.debug(\"{} Could not dynamically update broker {} configuration. Reason {}\", reconciliation, finalPodId, e);\n+                                                kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                            }\n+                                        } else {\n+                                            log.debug(\"{} Kafka broker {} configuration not changed\", reconciliation, finalPodId);\n+                                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        }\n+                                        return Future.<Void>succeededFuture();\n+                                    }).setHandler(ign -> {\n+                                        if (ac != null) {\n+                                            try {\n+                                                log.debug(\"closing ac instance\");", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0NDk5Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420044993", "bodyText": "Left over debug statement?", "author": "scholzj", "createdAt": "2020-05-05T11:41:03Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1654,19 +1672,131 @@ String zkConnectionString(int connectToReplicas)  {\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningReplicas);\n+\n+            for (int podId = 0; podId < runningReplicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            adminClient(adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka broker {} dynamic update ability\", reconciliation, finalPodId);\n+                            Promise<Void> p = Promise.promise();\n+                            getCurrentConfig(finalPodId, ac)\n+                                    .compose(res -> {\n+                                        log.trace(\"Broker description {}\", res);\n+                                        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.kafkaCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+                                        if (!configurationDiff.isEmpty()) {\n+                                            try {\n+                                                AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig());\n+                                                KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(finalPodId)));\n+                                                return kafkaFutureToVertxFuture(brokerConfigFuture).compose(result -> {\n+                                                    log.debug(\"{} Dynamic AlterConfig result for broker {} {}\", reconciliation, finalPodId, result);\n+                                                    kafkaPodsUpdatedDynamically.put(finalPodId, true);\n+                                                    return Future.<Void>succeededFuture();\n+                                                },\n+                                                    error -> {\n+                                                        log.debug(\"{} Error during dynamic reconfiguration. Rolling the broekr {}. {}\", reconciliation, finalPodId, error);\n+                                                        kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                                        return Future.<Void>succeededFuture();\n+                                                    });\n+                                            } catch (InvalidRequestException e) {\n+                                                log.debug(\"{} Could not dynamically update broker {} configuration. Reason {}\", reconciliation, finalPodId, e);\n+                                                kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                            }\n+                                        } else {\n+                                            log.debug(\"{} Kafka broker {} configuration not changed\", reconciliation, finalPodId);\n+                                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        }\n+                                        return Future.<Void>succeededFuture();\n+                                    }).setHandler(ign -> {\n+                                        if (ac != null) {\n+                                            try {\n+                                                log.debug(\"closing ac instance\");\n+                                                ac.close(Duration.ofMinutes(2));\n+                                                log.debug(\"ac instance closed\");", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA0NTE5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420045196", "bodyText": "Could you add the reconciliation to the start of the log message?", "author": "scholzj", "createdAt": "2020-05-05T11:41:26Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1654,19 +1672,131 @@ String zkConnectionString(int connectToReplicas)  {\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningReplicas);\n+\n+            for (int podId = 0; podId < runningReplicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, podId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            adminClient(adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{} Could not create admin client.\", reconciliation);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{} Determining kafka broker {} dynamic update ability\", reconciliation, finalPodId);\n+                            Promise<Void> p = Promise.promise();\n+                            getCurrentConfig(finalPodId, ac)\n+                                    .compose(res -> {\n+                                        log.trace(\"Broker description {}\", res);\n+                                        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(res, this.kafkaCm, kafkaCluster.getKafkaVersion(), finalPodId);\n+                                        if (!configurationDiff.isEmpty()) {\n+                                            try {\n+                                                AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getUpdatedConfig());\n+                                                KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(finalPodId)));\n+                                                return kafkaFutureToVertxFuture(brokerConfigFuture).compose(result -> {\n+                                                    log.debug(\"{} Dynamic AlterConfig result for broker {} {}\", reconciliation, finalPodId, result);\n+                                                    kafkaPodsUpdatedDynamically.put(finalPodId, true);\n+                                                    return Future.<Void>succeededFuture();\n+                                                },\n+                                                    error -> {\n+                                                        log.debug(\"{} Error during dynamic reconfiguration. Rolling the broekr {}. {}\", reconciliation, finalPodId, error);\n+                                                        kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                                        return Future.<Void>succeededFuture();\n+                                                    });\n+                                            } catch (InvalidRequestException e) {\n+                                                log.debug(\"{} Could not dynamically update broker {} configuration. Reason {}\", reconciliation, finalPodId, e);\n+                                                kafkaPodsUpdatedDynamically.put(finalPodId, false);\n+                                            }\n+                                        } else {\n+                                            log.debug(\"{} Kafka broker {} configuration not changed\", reconciliation, finalPodId);\n+                                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                                        }\n+                                        return Future.<Void>succeededFuture();\n+                                    }).setHandler(ign -> {\n+                                        if (ac != null) {\n+                                            try {\n+                                                log.debug(\"closing ac instance\");\n+                                                ac.close(Duration.ofMinutes(2));\n+                                                log.debug(\"ac instance closed\");\n+                                            } catch (Throwable e) {\n+                                                log.warn(\"Ignoring exception when closing admin client\", e);", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA1MDI2Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420050267", "bodyText": "Do we need to filer it like this? Wouldn't listener.[.*].ssl... work equally well and be more resilient against any changes to the listener names?", "author": "scholzj", "createdAt": "2020-05-05T11:51:34Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a ConfigMap).\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA1MDQ5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r420050496", "bodyText": "All others seem to use \\\\., just this one uses simply . ... is it needed or not?", "author": "scholzj", "createdAt": "2020-05-05T11:51:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.kubernetes.api.model.ConfigMap;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a ConfigMap).\n+ * An algorithm:\n+ *  1. Create map from supplied desired ConfigMap\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private ConfigMap desired;\n+    private KafkaConfiguration diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4].ssl.keystore.location\"", "originalCommit": "65faa83bd2932e10e704d0ee4cade90e93ea2570", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fa0d68a7a48ad332e94b5b05bd4fbfa22e37e502", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fa0d68a7a48ad332e94b5b05bd4fbfa22e37e502", "message": "review comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-05-07T14:26:23Z", "type": "forcePushed"}, {"oid": "8100c8ad10c2735f4266d27ee9c25165fd63d118", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8100c8ad10c2735f4266d27ee9c25165fd63d118", "message": "custom properties\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-05-11T09:15:58Z", "type": "forcePushed"}, {"oid": "3c3b5b25b1e631df8125c72c4d9436de00aff1eb", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3c3b5b25b1e631df8125c72c4d9436de00aff1eb", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-05-13T11:34:48Z", "type": "forcePushed"}, {"oid": "3414eb3c03d833947f7945e01128d83a8c62165d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3414eb3c03d833947f7945e01128d83a8c62165d", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-05-26T05:53:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQ5OTM0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433499341", "bodyText": "This method does not seem to be used anymore (finally) ... so I guess you can remove it?", "author": "scholzj", "createdAt": "2020-06-01T21:25:52Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -510,4 +511,56 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms a String into a List of Strings, where entry = line from input.\n+     * The lines beginning with '#' (comments) are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {\n+                    validLines.add(line);\n+                }\n+            }\n+        }\n+        return validLines;\n+    }\n+\n+    /**\n+     * Gets data from ConfigMap as a String\n+     * @param configMap ConfigMap to get data from\n+     * @param key key to get data from ConfigMap\n+     * @return String value of ConfigMap data entry. If the key does not exist in the ConfigMap, return null\n+     */\n+    public static String getDataFromConfigMap(ConfigMap configMap, String key) {", "originalCommit": "f4c77552402f24943786e9ba466a54fa149aaaa7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQ5OTk1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433499954", "bodyText": "Is this todo still valid? Can you either remove it or convert it to an issue?", "author": "scholzj", "createdAt": "2020-06-01T21:27:14Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1614,19 +1633,135 @@ String zkConnectionString(int connectToReplicas, Function<Integer, String> zkNod\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningBeforeAndAfterReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningBeforeAndAfterReplicas);\n+\n+            for (int podId = 0; podId < runningBeforeAndAfterReplicas; podId++) {\n+                // TODO improvement: if this.kafkaBrokerConfigurationHash changed, we can skip entire comparing algorithm", "originalCommit": "f4c77552402f24943786e9ba466a54fa149aaaa7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMDI0OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433500249", "bodyText": "Typo?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        // if BROKER_ADVERTISED_HOSTNAMES_FILENAME or BROKER_ADVERTISED_PORTS_FILENAME changes, compute a hash and put it into annotationsF\n          \n          \n            \n                        // if BROKER_ADVERTISED_HOSTNAMES_FILENAME or BROKER_ADVERTISED_PORTS_FILENAME changes, compute a hash and put it into annotation", "author": "scholzj", "createdAt": "2020-06-01T21:27:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -2226,19 +2361,23 @@ String getCertificateThumbprint(Secret certSecret, CertAndKeySecretSource custom\n             }\n         }\n \n-        ConfigMap getKafkaAncialiaryCm()    {\n+        ConfigMap getKafkaAncillaryCm()    {\n             ConfigMap loggingCm = null;\n \n             if (kafkaCluster.getLogging() instanceof ExternalLogging) {\n                 loggingCm = configMapOperations.get(kafkaAssembly.getMetadata().getNamespace(), ((ExternalLogging) kafkaCluster.getLogging()).getName());\n             }\n \n             ConfigMap brokerCm = kafkaCluster.generateAncillaryConfigMap(loggingCm, kafkaExternalAdvertisedHostnames, kafkaExternalAdvertisedPorts);\n+            this.kafkaConfig = kafkaCluster.getBrokersConfiguration();\n \n-            String brokerConfiguration = brokerCm.getData().get(KafkaCluster.BROKER_CONFIGURATION_FILENAME);\n+            // if BROKER_ADVERTISED_HOSTNAMES_FILENAME or BROKER_ADVERTISED_PORTS_FILENAME changes, compute a hash and put it into annotationsF", "originalCommit": "f4c77552402f24943786e9ba466a54fa149aaaa7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "92c1da8420df41aae636e8908b16580f293ea103", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/92c1da8420df41aae636e8908b16580f293ea103", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-02T11:34:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2MDcwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433860709", "bodyText": "Let's add a sentence of Javadoc if we're making it public.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\n          \n          \n            \n                /** \n          \n          \n            \n                 * Gets the config model for the given version of the Kafka broker.\n          \n          \n            \n                 * @param kafkaVersion The broker version.\n          \n          \n            \n                 * @return The config model for that broker version.\n          \n          \n            \n                 */\n          \n          \n            \n                public static Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {", "author": "tombentley", "createdAt": "2020-06-02T13:08:41Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -83,7 +95,7 @@ public static KafkaConfiguration unvalidated(String string) {\n         return errors;\n     }\n \n-    private Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\n+    public static Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2MjgxMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433862812", "bodyText": "I know this is hypocrisy of the first order, but can we avoid the single letter variable names?", "author": "tombentley", "createdAt": "2020-06-02T13:11:56Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaConfiguration.java", "diffHunk": "@@ -159,5 +171,25 @@ public boolean anyReadOnly(KafkaVersion kafkaVersion) {\n         return result;\n     }\n \n+    /**\n+     * Return the config properties with their values in this KafkaConfiguration which are not known broker configs.\n+     * These might be consumed by broker plugins.\n+     * @param kafkaVersion The broker version.\n+     * @return The unknown configs.\n+     */\n+    public Set<String> unknownConfigsWithValues(KafkaVersion kafkaVersion) {\n+        Map<String, ConfigModel> c = readConfigModel(kafkaVersion);", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2ODY0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433868642", "bodyText": "We should be able to use new OrderedProperties().addStringPairs() to do this, right?", "author": "tombentley", "createdAt": "2020-06-02T13:20:36Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -510,4 +510,43 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms a String into a List of Strings, where entry = line from input.\n+     * The lines beginning with '#' (comments) are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {\n+                    validLines.add(line);\n+                }\n+            }\n+        }\n+        return validLines;\n+    }\n+\n+    /**\n+     * Transforms data from String into Map. The entries are line separated tuples of key=value.\n+     * @param string string to get data from\n+     * @return Map of (key, value)\n+     */\n+    public static Map<String, String> stringToMap(String string) {\n+        Map<String, String> result = new HashMap<>();\n+        List<String> list = ModelUtils.getLinesWithoutCommentsAndEmptyLines(string);\n+        for (String line: list) {\n+            String[] split = line.split(\"=\", 2);\n+            if (split.length == 1) {\n+                result.put(split[0], \"\");\n+            } else {\n+                result.put(split[0], split[1]);\n+            }\n+        }\n+        return result;", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjAyNzI0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452027248", "bodyText": "Did you look into using OrderedProperties for doing this? I would prefer to avoid having methods for dealing with properties files in faviour forms both here and in OrderedProperties. It seems better to me to have us much as possible in OrderedProperties and this method as getLinesWithoutCommentsAndEmptyLines seem like they belong there.", "author": "tombentley", "createdAt": "2020-07-09T07:45:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2ODY0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA2Mjg3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452062878", "bodyText": "It seems I did as you suggested in the previous comment and just forgot to delete this unused code. Sorry.", "author": "sknot-rh", "createdAt": "2020-07-09T08:48:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2ODY0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4MDg1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433880853", "bodyText": "I think all this code ends up running on the vertx event loop thread (because secretOperations.getAsync() completes its future from that thread, so the composite's handler also gets executed there). But adminClientProvider.createAdminClient blocks (because it writes the key and trust stores). So strictly speaking we should be running this on a worker thread.", "author": "tombentley", "createdAt": "2020-06-02T13:37:48Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1604,19 +1623,134 @@ String zkConnectionString(int connectToReplicas, Function<Integer, String> zkNod\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4MTM2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433881368", "bodyText": "This method it very long and difficult to review. Please can you refactor it into a number of smaller methods.", "author": "tombentley", "createdAt": "2020-06-02T13:38:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1604,19 +1623,134 @@ String zkConnectionString(int connectToReplicas, Function<Integer, String> zkNod\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        protected Future<Map<ConfigResource, Config>> getCurrentConfig(int podId, Admin ac) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(podId));\n+            return kafkaFutureToVertxFuture(ac.describeConfigs(singletonList(resource)).all());\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Promise<Admin> acPromise = Promise.promise();\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Admin ac;\n+                try {\n+                    ac = adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\");\n+                    acPromise.complete(ac);\n+                } catch (RuntimeException e) {\n+                    log.warn(\"Failed to create Admin Client. {}\", e);\n+                    acPromise.complete(null);\n+                }\n+                return acPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is running, change its configuration dynamically if it is possible (and needed).\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4NDI1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433884254", "bodyText": "This logging doesn't contain any contextual info, so it's not very useful except in local debugging. If you want to keep it you should include the Reconciliation which will at least let you correlate what reconciliation it was a part of.", "author": "tombentley", "createdAt": "2020-06-02T13:42:13Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3457,4 +3602,21 @@ private String getStringHash(String toBeHashed)  {\n             throw new RuntimeException(\"Failed to create SHA-512 MessageDigest instance\");\n         }\n     }\n+\n+    public <T> Future<T> kafkaFutureToVertxFuture(KafkaFuture<T> kf) {\n+        Promise<T> promise = Promise.promise();\n+        kf.whenComplete((result, error) -> {\n+            vertx.runOnContext(ignored -> {\n+                if (error != null) {\n+                    log.debug(\"Kafkafuture failed \", error);", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4NjM0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433886345", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * An algorithm:\n          \n          \n            \n             *  1. Create map from supplied desired String\n          \n          \n            \n             *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n          \n          \n            \n             *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n          \n          \n            \n             *      else add it to the diff\n          \n          \n            \n             *  3b. If entry was removed from desired, add it to the diff with null value.\n          \n          \n            \n             *  3c. If custom entry was removed, delete property\n          \n          \n            \n             * The algorithm:\n          \n          \n            \n             *  1. Create a map from the supplied desired String\n          \n          \n            \n             *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map as the broker's {@code kafka_config_generator.sh} would\n          \n          \n            \n             *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n          \n          \n            \n             *      else add it to the diff\n          \n          \n            \n             *  3b. If entry was removed from desired, add it to the diff with null value.\n          \n          \n            \n             *  3c. If custom entry was removed, delete property", "author": "tombentley", "createdAt": "2020-06-02T13:45:07Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5MjY0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433892642", "bodyText": "There's no need for this to be a field, since it's only used locally in the constructor.", "author": "tombentley", "createdAt": "2020-06-02T13:53:30Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5Mjk0Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433892943", "bodyText": "Why is this public if it's called from the constructor?\nPersonally I would make this private static. In fact if you give it this signature:\nprivate static Map<ConfigResource, Collection<AlterConfigOp>> computeDiff(int brokerId, String desired, Config brokerConfigs, Map<String, ConfigModel> configModel)\nI think that would go a long way to resolving some of the other comments I've made about this class.", "author": "tombentley", "createdAt": "2020-06-02T13:53:55Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = Collections.emptyMap(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @return true if property in desired map has a default value\n+     */\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return true;\n+        } else {\n+            AtomicBoolean tempResult = new AtomicBoolean(true);\n+            diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).forEach(entry -> {\n+                if (isEntryReadOnly(entry.configEntry())) {\n+                    tempResult.set(false);\n+                }\n+            });\n+            return tempResult.get();\n+        }\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     *\n+     * @return size of the broker config difference\n+     */\n+    public int getDiffSize() {\n+        return diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).size();\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> computeDiff() {", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5NDA2OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433894068", "bodyText": "No need for this to be a field either, since it's only used in the ctor and an computeDiff, which is called from the ctor.", "author": "tombentley", "createdAt": "2020-06-02T13:55:18Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5NDE4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433894187", "bodyText": "This isn't used anywhere", "author": "tombentley", "createdAt": "2020-06-02T13:55:25Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private KafkaVersion kafkaVersion;", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5NTk3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433895979", "bodyText": "If it is null, surely that's a programming error. It can never be null when this class is being used correctly, right? Since the caller is having to supply both the current and the brokerId, why not force them to do the lookup in current, and have the ctor just take a Config?", "author": "tombentley", "createdAt": "2020-06-02T13:57:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = Collections.emptyMap(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5NzQzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433897439", "bodyText": "You only ever call stream() on this field, but I don't think the cost of Config.entries() is so huge that it's worth having a field to cache the result.", "author": "tombentley", "createdAt": "2020-06-02T13:59:52Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5OTU4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433899588", "bodyText": "This can be static and package access.", "author": "tombentley", "createdAt": "2020-06-02T14:01:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = Collections.emptyMap(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @return true if property in desired map has a default value\n+     */\n+    public boolean isDesiredPropertyDefaultValue(String key) {", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzkwMDUwNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433900507", "bodyText": "Wrap in Collections.unmodifiableMap()", "author": "tombentley", "createdAt": "2020-06-02T14:02:55Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = Collections.emptyMap(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @return true if property in desired map has a default value\n+     */\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return true;\n+        } else {\n+            AtomicBoolean tempResult = new AtomicBoolean(true);\n+            diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).forEach(entry -> {\n+                if (isEntryReadOnly(entry.configEntry())) {\n+                    tempResult.set(false);\n+                }\n+            });\n+            return tempResult.get();\n+        }\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     *\n+     * @return size of the broker config difference\n+     */\n+    public int getDiffSize() {\n+        return diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).size();\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> computeDiff() {\n+        Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+        Map<String, String> currentMap;\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentMap = currentEntries.stream().collect(Collectors.toMap(configEntry -> configEntry.name(), configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        Map<String, String> desiredMap = ModelUtils.stringToMap(desired);\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (isEntryCustom(entry)) {\n+                        // we are deleting custom option\n+                        //updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.value()), AlterConfigOp.OpType.DELETE));\n+                        log.trace(\"removing custom property {}\", entry.name());\n+                    } else if (entry.isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip.\n+                        // Some default properties do not have set ConfigEntry.ConfigSource.DEFAULT_CONFIG and thus\n+                        // we are removing property. That might cause redundant RU. To fix this we would have to add defaultValue\n+                        // to the configModel\n+                        log.trace(\"{} not set in desired, using default value\", entry.name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.name(), \"deleted entry\");\n+                        } else {\n+                            log.trace(\"{} is ignorable, not considering as removed\");\n+                        }\n+                    }\n+                } else if (d.get(\"op\").asText().equals(\"replace\")) {\n+                    // entry is in the current, desired is updated value\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        if (isEntryCustom(entry)) {\n+                            log.trace(\"custom property {} has new desired value {}\", entry.name(), desiredMap.get(entry.name()));\n+                        } else {\n+                            log.trace(\"property {} has new desired value {}\", entry.name(), desiredMap.get(entry.name()));\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        }\n+                    } else {\n+                        log.trace(\"{} is ignorable, not considering as replaced\");\n+                    }\n+                }\n+            } else {\n+                if (d.get(\"op\").asText().equals(\"add\")) {\n+                    // entry is not in the current, it is added\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        if (isEntryCustom(pathValueWithoutSlash)) {\n+                            log.trace(\"add new custom property {} {}\", pathValueWithoutSlash, d.get(\"op\").asText());\n+                        } else {\n+                            log.trace(\"add new {} {}\", pathValueWithoutSlash, d.get(\"op\").asText());\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        }\n+                    } else {\n+                        log.trace(\"{} is ignorable, not considering as added\");\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+\n+        updated.put(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)), updatedCE);\n+        return updated;", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzkwNTQ5NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433905495", "bodyText": "This class presents an API for callers which allows them to do all dangerous things, like calling computeDiff() for themselves. Having public methods which mutate the instance makes it much harder to reason about whether the code is correct, because I have to know whether any of those mutating methods is ever called. Using the API of the class to enforce immutability means I know, just from reading the classes code, that callers can't screw up the computed state. The API it should present is that the constructor computes the diff and various things about that are exposed immutably to the caller via accessor methods.", "author": "tombentley", "createdAt": "2020-06-02T14:07:49Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzkxMjAxMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433912013", "bodyText": "You can factor this out as a separate method", "author": "tombentley", "createdAt": "2020-06-02T14:16:36Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = Collections.emptyMap(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @return true if property in desired map has a default value\n+     */\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return true;\n+        } else {\n+            AtomicBoolean tempResult = new AtomicBoolean(true);\n+            diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).forEach(entry -> {\n+                if (isEntryReadOnly(entry.configEntry())) {\n+                    tempResult.set(false);\n+                }\n+            });\n+            return tempResult.get();\n+        }\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     *\n+     * @return size of the broker config difference\n+     */\n+    public int getDiffSize() {\n+        return diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).size();\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> computeDiff() {\n+        Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+        Map<String, String> currentMap;\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentMap = currentEntries.stream().collect(Collectors.toMap(configEntry -> configEntry.name(), configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        Map<String, String> desiredMap = ModelUtils.stringToMap(desired);\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (isEntryCustom(entry)) {\n+                        // we are deleting custom option\n+                        //updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.value()), AlterConfigOp.OpType.DELETE));\n+                        log.trace(\"removing custom property {}\", entry.name());\n+                    } else if (entry.isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip.\n+                        // Some default properties do not have set ConfigEntry.ConfigSource.DEFAULT_CONFIG and thus\n+                        // we are removing property. That might cause redundant RU. To fix this we would have to add defaultValue\n+                        // to the configModel\n+                        log.trace(\"{} not set in desired, using default value\", entry.name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.name(), \"deleted entry\");\n+                        } else {\n+                            log.trace(\"{} is ignorable, not considering as removed\");\n+                        }\n+                    }", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzkxMjY5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r433912696", "bodyText": "This is almost identical to the d.get(\"op\").asText().equals(\"replace\") case, you can factor out a single method to encapsulate both.", "author": "tombentley", "createdAt": "2020-06-02T14:17:28Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.ModelUtils;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ * Computes a diff between the current config (supplied as a Map ConfigResource) and the desired config (supplied as a String).\n+ * An algorithm:\n+ *  1. Create map from supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ *\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Map<ConfigResource, Config> current;\n+    private Collection<ConfigEntry> currentEntries;\n+    private String desired;\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private KafkaVersion kafkaVersion;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Map<ConfigResource, Config> current, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.current = current;\n+        this.diff = Collections.emptyMap(); // init\n+        this.desired = desired;\n+        this.kafkaVersion = kafkaVersion;\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        Config brokerConfigs = this.current.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        if (brokerConfigs != null) {\n+            this.currentEntries = brokerConfigs.entries();\n+            this.diff = computeDiff();\n+        }\n+    }\n+\n+    private void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @return true if property in desired map has a default value\n+     */\n+    public boolean isDesiredPropertyDefaultValue(String key) {\n+        Optional<ConfigEntry> entry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return true;\n+        } else {\n+            AtomicBoolean tempResult = new AtomicBoolean(true);\n+            diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).forEach(entry -> {\n+                if (isEntryReadOnly(entry.configEntry())) {\n+                    tempResult.set(false);\n+                }\n+            });\n+            return tempResult.get();\n+        }\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return Map object which is used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     *\n+     * @return size of the broker config difference\n+     */\n+    public int getDiffSize() {\n+        return diff.get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId))).size();\n+    }\n+\n+    private boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> computeDiff() {\n+        Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+        Map<String, String> currentMap;\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentMap = currentEntries.stream().collect(Collectors.toMap(configEntry -> configEntry.name(), configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        Map<String, String> desiredMap = ModelUtils.stringToMap(desired);\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = currentEntries.stream().filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    if (isEntryCustom(entry)) {\n+                        // we are deleting custom option\n+                        //updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.value()), AlterConfigOp.OpType.DELETE));\n+                        log.trace(\"removing custom property {}\", entry.name());\n+                    } else if (entry.isDefault()) {\n+                        // entry is in current, is not in desired, is default -> it uses default value, skip.\n+                        // Some default properties do not have set ConfigEntry.ConfigSource.DEFAULT_CONFIG and thus\n+                        // we are removing property. That might cause redundant RU. To fix this we would have to add defaultValue\n+                        // to the configModel\n+                        log.trace(\"{} not set in desired, using default value\", entry.name());\n+                    } else {\n+                        // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+                        // if the entry was custom, it should be deleted\n+                        if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                            log.trace(\"{} not set in desired, unsetting back to default {}\", entry.name(), \"deleted entry\");\n+                        } else {\n+                            log.trace(\"{} is ignorable, not considering as removed\");\n+                        }\n+                    }\n+                } else if (d.get(\"op\").asText().equals(\"replace\")) {\n+                    // entry is in the current, desired is updated value\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        if (isEntryCustom(entry)) {\n+                            log.trace(\"custom property {} has new desired value {}\", entry.name(), desiredMap.get(entry.name()));\n+                        } else {\n+                            log.trace(\"property {} has new desired value {}\", entry.name(), desiredMap.get(entry.name()));\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        }\n+                    } else {\n+                        log.trace(\"{} is ignorable, not considering as replaced\");\n+                    }\n+                }\n+            } else {\n+                if (d.get(\"op\").asText().equals(\"add\")) {\n+                    // entry is not in the current, it is added\n+                    if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                        if (isEntryCustom(pathValueWithoutSlash)) {\n+                            log.trace(\"add new custom property {} {}\", pathValueWithoutSlash, d.get(\"op\").asText());\n+                        } else {\n+                            log.trace(\"add new {} {}\", pathValueWithoutSlash, d.get(\"op\").asText());\n+                            updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, desiredMap.get(pathValueWithoutSlash)), AlterConfigOp.OpType.SET));\n+                        }\n+                    } else {\n+                        log.trace(\"{} is ignorable, not considering as added\");\n+                    }", "originalCommit": "92c1da8420df41aae636e8908b16580f293ea103", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c29e843363918264eddc76999a84e3a6dc4fb8da", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c29e843363918264eddc76999a84e3a6dc4fb8da", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-03T07:09:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQyODQ4Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434428483", "bodyText": "This method will try to update all the brokers concurrently. That's might be fast when everything goes well, but if the change does something which breaks the brokers we're basically bringing down the cluster. Off the top of my head I don't know which configs can be changed dynamically (obviously it's a subset of the Kafka dynamic configs) to decide how possible/likely it is for a user to break their cluster in this way. However, the algorithm probably needs to do something similar to the KafkaRoller, updating each broker sequentially and performing some kind of check after an update that the broker is still OK.", "author": "tombentley", "createdAt": "2020-06-03T09:20:32Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1604,19 +1623,145 @@ String zkConnectionString(int connectToReplicas, Function<Integer, String> zkNod\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        /**\n+         * Returns a Future which completes with the config of the given broker.\n+         * @param ac The admin client\n+         * @param brokerId The id of the broker.\n+         * @return a Future which completes with the config of the given broker.\n+         */\n+        protected Future<Config> brokerConfig(Admin ac, int brokerId) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+            return kafkaFutureToVertxFuture(reconciliation, ac.describeConfigs(singletonList(resource)).values().get(resource));\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Promise<Admin> blockingPromise = Promise.promise();\n+                vertx.createSharedWorkerExecutor(\"kubernetes-ops-pool\").executeBlocking(\n+                    future -> {\n+                        try {\n+                            future.complete(adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\"));\n+                        } catch (RuntimeException e) {\n+                            future.fail(e);\n+                        }\n+                    }, blockingPromise);\n+                return blockingPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is ready, change its configuration dynamically if both necessary and possible.\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningBeforeAndAfterReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningBeforeAndAfterReplicas);\n+\n+            for (int podId = 0; podId < runningBeforeAndAfterReplicas; podId++) {", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQyOTIzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434429233", "bodyText": "It's very difficult to see that this recover() recovering from the readiness check. It looks like it's recovering from the recover() above. Can we do something with indent to make it clearer?", "author": "tombentley", "createdAt": "2020-06-03T09:21:47Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1604,19 +1623,145 @@ String zkConnectionString(int connectToReplicas, Function<Integer, String> zkNod\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        /**\n+         * Returns a Future which completes with the config of the given broker.\n+         * @param ac The admin client\n+         * @param brokerId The id of the broker.\n+         * @return a Future which completes with the config of the given broker.\n+         */\n+        protected Future<Config> brokerConfig(Admin ac, int brokerId) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+            return kafkaFutureToVertxFuture(reconciliation, ac.describeConfigs(singletonList(resource)).values().get(resource));\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Promise<Admin> blockingPromise = Promise.promise();\n+                vertx.createSharedWorkerExecutor(\"kubernetes-ops-pool\").executeBlocking(\n+                    future -> {\n+                        try {\n+                            future.complete(adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\"));\n+                        } catch (RuntimeException e) {\n+                            future.fail(e);\n+                        }\n+                    }, blockingPromise);\n+                return blockingPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is ready, change its configuration dynamically if both necessary and possible.\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningBeforeAndAfterReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningBeforeAndAfterReplicas);\n+\n+            for (int podId = 0; podId < runningBeforeAndAfterReplicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, finalPodId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            adminClient(adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{}: Could not create admin client for pod {}.\", reconciliation, finalPodId);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{}: Determining whether kafka broker {} can be update dynamically\", reconciliation, finalPodId);\n+                            Promise<Void> p = Promise.promise();\n+                            brokerConfig(ac, finalPodId)\n+                                    .compose(brokerConfig -> {\n+                                        log.trace(\"{}: Broker description {}\", reconciliation, brokerConfig);\n+                                        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(brokerConfig, this.kafkaConfig, kafkaCluster.getKafkaVersion(), finalPodId);\n+                                        if (!configurationDiff.isEmpty()) {\n+                                            if (!configurationDiff.canBeUpdatedDynamically()) {\n+                                                kafkaPodsUpdatedDynamically.put(finalPodId, \"dynamic update not possible\");\n+                                                return Future.succeededFuture();\n+                                            } else {\n+                                                return updateBrokerConfigDynamically(finalPodId, ac, configurationDiff);\n+                                            }\n+                                        } else {\n+                                            log.debug(\"{}: Kafka broker {} configuration is unchanged\", reconciliation, finalPodId);\n+                                            kafkaPodsUpdatedDynamically.put(finalPodId, \"\");\n+                                        }\n+                                        return Future.succeededFuture();\n+                                    }).onComplete(ign -> {\n+                                        if (ac != null) {\n+                                            try {\n+                                                ac.close(Duration.ofMinutes(2));\n+                                            } catch (Throwable e) {\n+                                                log.warn(\"{}: Ignoring exception when closing admin client\", reconciliation, e);\n+                                            }\n+                                        }\n+                                        p.handle(ign);\n+                                    });\n+                            return p.future();\n+                        }\n+                    }).recover(ignore2 -> {\n+                        log.debug(\"{}: recovering from the failed dynamic updating {}\", reconciliation, ignore2);\n+                        kafkaPodsUpdatedDynamically.put(finalPodId, \"dynamic update failed\");\n+                        return Future.succeededFuture();\n+                    })).recover(fail -> {\n+                        log.warn(\"{}: kafka pod {} not ready\", reconciliation, finalPodId);\n+                        kafkaPodsUpdatedDynamically.put(finalPodId, \"\");\n+                        return Future.succeededFuture();\n+                    }));", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNDI4Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434434283", "bodyText": "this.kafkaConfig is quite a large String containing the Kafka properties which got built by KafkaBrokerConfigurationBuilder and will be used to update the ancillary CM. By retaining a reference to it we're holding it in memory for a long time (so it will likely survive a few GCs and get promoted to a different JVM GC generation, meaning a major GC will be needed to eventually collect it). It's also duplicating quite a lot of the data which is also held in the KafkaCluster (i.e. wasting memory). Finally, because it's a String you've had to go to the effort of parsing it in KafkaBrokerConfigurationDiff in order to have structured data to compute the diff.\nAll in all, I think this could be improved by:\n\nNo caching the String on the ReconciliationState, but computing it fresh each time it's needed (which can easily be done in this PR)\nAllowing the KafkaBrokerConfigurationBuilder to be able to build a Map or other structured representation of the config, avoiding the need to parse it (which is a bigger refactoring which can be done in another PR).", "author": "tombentley", "createdAt": "2020-06-03T09:30:21Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1604,19 +1623,145 @@ String zkConnectionString(int connectToReplicas, Function<Integer, String> zkNod\n                     .compose(sts -> {\n                         Storage oldStorage = getOldStorage(sts);\n \n-                        int oldReplicas = 0;\n+                        oldKafkaReplicas = 0;\n                         if (sts != null && sts.getSpec() != null)   {\n-                            oldReplicas = sts.getSpec().getReplicas();\n+                            oldKafkaReplicas = sts.getSpec().getReplicas();\n                         }\n \n-                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldReplicas);\n+                        this.kafkaCluster = KafkaCluster.fromCrd(kafkaAssembly, versions, oldStorage, oldKafkaReplicas);\n                         this.kafkaService = kafkaCluster.generateService();\n                         this.kafkaHeadlessService = kafkaCluster.generateHeadlessService();\n \n                         return Future.succeededFuture(this);\n                     });\n         }\n \n+        /**\n+         * Returns a Future which completes with the config of the given broker.\n+         * @param ac The admin client\n+         * @param brokerId The id of the broker.\n+         * @return a Future which completes with the config of the given broker.\n+         */\n+        protected Future<Config> brokerConfig(Admin ac, int brokerId) {\n+            ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+            return kafkaFutureToVertxFuture(reconciliation, ac.describeConfigs(singletonList(resource)).values().get(resource));\n+        }\n+\n+        protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                    namespace, KafkaResources.clusterCaCertificateSecretName(cluster));\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                    namespace, ClusterOperator.secretName(cluster));\n+            String hostname = KafkaCluster.podDnsName(namespace, cluster, KafkaCluster.kafkaPodName(cluster, podId)) + \":\" + KafkaCluster.REPLICATION_PORT;\n+\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture).compose(compositeFuture -> {\n+                Secret clusterCaCertSecret = compositeFuture.resultAt(0);\n+                if (clusterCaCertSecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(cluster)));\n+                }\n+                Secret coKeySecret = compositeFuture.resultAt(1);\n+                if (coKeySecret == null) {\n+                    return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(cluster)));\n+                }\n+\n+                Promise<Admin> blockingPromise = Promise.promise();\n+                vertx.createSharedWorkerExecutor(\"kubernetes-ops-pool\").executeBlocking(\n+                    future -> {\n+                        try {\n+                            future.complete(adminClientProvider.createAdminClient(hostname, clusterCaCertSecret, coKeySecret, \"cluster-operator\"));\n+                        } catch (RuntimeException e) {\n+                            future.fail(e);\n+                        }\n+                    }, blockingPromise);\n+                return blockingPromise.future();\n+            });\n+        }\n+\n+        /**\n+         * If the kafka pod is ready, change its configuration dynamically if both necessary and possible.\n+         * If it is not possible set the flag for specific podID an it will be rolled in {@code kafkaRollingUpdate}.\n+         * @return Succeeded future\n+         */\n+        Future<ReconciliationState> kafkaBrokerDynamicConfiguration() {\n+            // patches scale up/down situations\n+            int runningBeforeAndAfterReplicas = Math.min(oldKafkaReplicas, kafkaCluster.getReplicas());\n+            List<Future> configFutures = new ArrayList<>(runningBeforeAndAfterReplicas);\n+\n+            for (int podId = 0; podId < runningBeforeAndAfterReplicas; podId++) {\n+                int finalPodId = podId;\n+                configFutures.add(podOperations.readiness(namespace, KafkaResources.kafkaPodName(name, finalPodId), 1_000, operationTimeoutMs)\n+                    .compose(ignore ->\n+                            adminClient(adminClientProvider, namespace, name, finalPodId)\n+                    .compose(ac -> {\n+                        if (ac == null) {\n+                            kafkaPodsUpdatedDynamically.put(finalPodId, null);\n+                            log.warn(\"{}: Could not create admin client for pod {}.\", reconciliation, finalPodId);\n+                            return Future.succeededFuture();\n+                        } else {\n+                            log.debug(\"{}: Determining whether kafka broker {} can be update dynamically\", reconciliation, finalPodId);\n+                            Promise<Void> p = Promise.promise();\n+                            brokerConfig(ac, finalPodId)\n+                                    .compose(brokerConfig -> {\n+                                        log.trace(\"{}: Broker description {}\", reconciliation, brokerConfig);\n+                                        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(brokerConfig, this.kafkaConfig, kafkaCluster.getKafkaVersion(), finalPodId);", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNTA1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434435052", "bodyText": "This probably belongs in a Utils class, and can be static.", "author": "tombentley", "createdAt": "2020-06-03T09:31:35Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3456,4 +3612,21 @@ private String getStringHash(String toBeHashed)  {\n             throw new RuntimeException(\"Failed to create SHA-512 MessageDigest instance\");\n         }\n     }\n+\n+    public <T> Future<T> kafkaFutureToVertxFuture(Reconciliation reconciliation, KafkaFuture<T> kf) {\n+        Promise<T> promise = Promise.promise();\n+        kf.whenComplete((result, error) -> {\n+            vertx.runOnContext(ignored -> {\n+                if (error != null) {\n+                    log.debug(\"{} Kafkafuture failed \", reconciliation, error);\n+                    promise.fail(error);\n+                } else {\n+                    log.debug(\"{} KafkaFuture succeeded\", reconciliation);\n+                    promise.complete(result);\n+                }\n+            });\n+        });\n+        return promise.future();\n+    }", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNjE5Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434436192", "bodyText": "Remove, or use logging", "author": "tombentley", "createdAt": "2020-06-03T09:33:29Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/assembly/DynamicUpdateTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.assembly;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaBuilder;\n+import io.strimzi.operator.cluster.ClusterOperatorConfig;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.ResourceUtils;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.cluster.operator.resource.ResourceOperatorSupplier;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.vertx.core.Future;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Timeout;\n+import io.vertx.junit5.VertxExtension;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.when;\n+\n+@ExtendWith(VertxExtension.class)\n+public class DynamicUpdateTest {\n+\n+    private io.debezium.kafka.KafkaCluster kafkaCluster;\n+    Vertx vertx = Vertx.vertx();\n+\n+    @BeforeEach\n+    public void before() throws IOException {\n+        kafkaCluster = new io.debezium.kafka.KafkaCluster()\n+                .addBrokers(1)\n+                .deleteDataPriorToStartup(true)\n+                .deleteDataUponShutdown(true)\n+                .usingDirectory(Files.createTempDirectory(getClass().getName()).toFile())\n+                .withKafkaConfiguration(new Properties());\n+        kafkaCluster.startup();\n+    }\n+\n+    @AfterEach\n+    public void after() {\n+        kafkaCluster.shutdown();\n+    }\n+\n+    @Test\n+    @Timeout(value = 2, timeUnit = TimeUnit.MINUTES)\n+    public void simpleTest() throws InterruptedException {\n+        Kafka kafka = new KafkaBuilder().withNewMetadata()\n+                .withName(\"k\")\n+                .withNamespace(\"ns\")\n+                .endMetadata()\n+                .withNewSpec()\n+                .withNewKafka()\n+                .withReplicas(1)\n+                .endKafka()\n+                .endSpec()\n+                .build();\n+        ClusterOperatorConfig config = ResourceUtils.dummyClusterOperatorConfig(KafkaVersionTestUtils.getKafkaVersionLookup());\n+        ResourceOperatorSupplier supplier = ResourceUtils.supplierWithMocks(false);\n+        Future<KafkaAssemblyOperator.ReconciliationState> f = new KafkaAssemblyOperator(vertx, null, null, null,\n+                supplier,\n+                config) {\n+\n+        }.new ReconciliationState(null, kafka) {\n+            {\n+                oldKafkaReplicas = 1;\n+                kafkaCluster = KafkaCluster.fromCrd(kafka, config.versions());\n+                when(supplier.podOperations.readiness(any(), any(), anyLong(), anyLong())).thenReturn(Future.succeededFuture());\n+            }\n+\n+            @Override\n+            protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+                Properties p = new Properties();\n+                String bootstrap = DynamicUpdateTest.this.kafkaCluster.brokerList();\n+                System.err.println(\"Using \" + bootstrap);\n+                p.setProperty(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrap);\n+                p.setProperty(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, \"30000\");\n+                System.err.println(p.toString());", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNjI4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434436287", "bodyText": "Remove, or use logging", "author": "tombentley", "createdAt": "2020-06-03T09:33:39Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/assembly/DynamicUpdateTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.assembly;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaBuilder;\n+import io.strimzi.operator.cluster.ClusterOperatorConfig;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.ResourceUtils;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.cluster.operator.resource.ResourceOperatorSupplier;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.vertx.core.Future;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Timeout;\n+import io.vertx.junit5.VertxExtension;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.when;\n+\n+@ExtendWith(VertxExtension.class)\n+public class DynamicUpdateTest {\n+\n+    private io.debezium.kafka.KafkaCluster kafkaCluster;\n+    Vertx vertx = Vertx.vertx();\n+\n+    @BeforeEach\n+    public void before() throws IOException {\n+        kafkaCluster = new io.debezium.kafka.KafkaCluster()\n+                .addBrokers(1)\n+                .deleteDataPriorToStartup(true)\n+                .deleteDataUponShutdown(true)\n+                .usingDirectory(Files.createTempDirectory(getClass().getName()).toFile())\n+                .withKafkaConfiguration(new Properties());\n+        kafkaCluster.startup();\n+    }\n+\n+    @AfterEach\n+    public void after() {\n+        kafkaCluster.shutdown();\n+    }\n+\n+    @Test\n+    @Timeout(value = 2, timeUnit = TimeUnit.MINUTES)\n+    public void simpleTest() throws InterruptedException {\n+        Kafka kafka = new KafkaBuilder().withNewMetadata()\n+                .withName(\"k\")\n+                .withNamespace(\"ns\")\n+                .endMetadata()\n+                .withNewSpec()\n+                .withNewKafka()\n+                .withReplicas(1)\n+                .endKafka()\n+                .endSpec()\n+                .build();\n+        ClusterOperatorConfig config = ResourceUtils.dummyClusterOperatorConfig(KafkaVersionTestUtils.getKafkaVersionLookup());\n+        ResourceOperatorSupplier supplier = ResourceUtils.supplierWithMocks(false);\n+        Future<KafkaAssemblyOperator.ReconciliationState> f = new KafkaAssemblyOperator(vertx, null, null, null,\n+                supplier,\n+                config) {\n+\n+        }.new ReconciliationState(null, kafka) {\n+            {\n+                oldKafkaReplicas = 1;\n+                kafkaCluster = KafkaCluster.fromCrd(kafka, config.versions());\n+                when(supplier.podOperations.readiness(any(), any(), anyLong(), anyLong())).thenReturn(Future.succeededFuture());\n+            }\n+\n+            @Override\n+            protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+                Properties p = new Properties();\n+                String bootstrap = DynamicUpdateTest.this.kafkaCluster.brokerList();\n+                System.err.println(\"Using \" + bootstrap);", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNjYwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434436600", "bodyText": "Fail the test", "author": "tombentley", "createdAt": "2020-06-03T09:34:10Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/assembly/DynamicUpdateTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.assembly;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaBuilder;\n+import io.strimzi.operator.cluster.ClusterOperatorConfig;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.ResourceUtils;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.cluster.operator.resource.ResourceOperatorSupplier;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.vertx.core.Future;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Timeout;\n+import io.vertx.junit5.VertxExtension;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.when;\n+\n+@ExtendWith(VertxExtension.class)\n+public class DynamicUpdateTest {\n+\n+    private io.debezium.kafka.KafkaCluster kafkaCluster;\n+    Vertx vertx = Vertx.vertx();\n+\n+    @BeforeEach\n+    public void before() throws IOException {\n+        kafkaCluster = new io.debezium.kafka.KafkaCluster()\n+                .addBrokers(1)\n+                .deleteDataPriorToStartup(true)\n+                .deleteDataUponShutdown(true)\n+                .usingDirectory(Files.createTempDirectory(getClass().getName()).toFile())\n+                .withKafkaConfiguration(new Properties());\n+        kafkaCluster.startup();\n+    }\n+\n+    @AfterEach\n+    public void after() {\n+        kafkaCluster.shutdown();\n+    }\n+\n+    @Test\n+    @Timeout(value = 2, timeUnit = TimeUnit.MINUTES)\n+    public void simpleTest() throws InterruptedException {\n+        Kafka kafka = new KafkaBuilder().withNewMetadata()\n+                .withName(\"k\")\n+                .withNamespace(\"ns\")\n+                .endMetadata()\n+                .withNewSpec()\n+                .withNewKafka()\n+                .withReplicas(1)\n+                .endKafka()\n+                .endSpec()\n+                .build();\n+        ClusterOperatorConfig config = ResourceUtils.dummyClusterOperatorConfig(KafkaVersionTestUtils.getKafkaVersionLookup());\n+        ResourceOperatorSupplier supplier = ResourceUtils.supplierWithMocks(false);\n+        Future<KafkaAssemblyOperator.ReconciliationState> f = new KafkaAssemblyOperator(vertx, null, null, null,\n+                supplier,\n+                config) {\n+\n+        }.new ReconciliationState(null, kafka) {\n+            {\n+                oldKafkaReplicas = 1;\n+                kafkaCluster = KafkaCluster.fromCrd(kafka, config.versions());\n+                when(supplier.podOperations.readiness(any(), any(), anyLong(), anyLong())).thenReturn(Future.succeededFuture());\n+            }\n+\n+            @Override\n+            protected Future<Admin> adminClient(AdminClientProvider adminClientProvider, String namespace, String cluster, int podId) {\n+                Properties p = new Properties();\n+                String bootstrap = DynamicUpdateTest.this.kafkaCluster.brokerList();\n+                System.err.println(\"Using \" + bootstrap);\n+                p.setProperty(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrap);\n+                p.setProperty(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, \"30000\");\n+                System.err.println(p.toString());\n+                return Future.succeededFuture(Admin.create(p));\n+            }\n+        }.kafkaBrokerDynamicConfiguration();\n+        CountDownLatch l = new CountDownLatch(1);\n+        f.setHandler(ar -> {\n+            if (ar.failed()) {\n+                ar.cause().printStackTrace();", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNjc3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434436774", "bodyText": "This test doesn't seem to make any assertions.", "author": "tombentley", "createdAt": "2020-06-03T09:34:27Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/assembly/DynamicUpdateTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.assembly;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import io.strimzi.api.kafka.model.Kafka;\n+import io.strimzi.api.kafka.model.KafkaBuilder;\n+import io.strimzi.operator.cluster.ClusterOperatorConfig;\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.ResourceUtils;\n+import io.strimzi.operator.cluster.model.KafkaCluster;\n+import io.strimzi.operator.cluster.operator.resource.ResourceOperatorSupplier;\n+import io.strimzi.operator.common.AdminClientProvider;\n+import io.vertx.core.Future;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Timeout;\n+import io.vertx.junit5.VertxExtension;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.when;\n+\n+@ExtendWith(VertxExtension.class)\n+public class DynamicUpdateTest {\n+\n+    private io.debezium.kafka.KafkaCluster kafkaCluster;\n+    Vertx vertx = Vertx.vertx();\n+\n+    @BeforeEach\n+    public void before() throws IOException {\n+        kafkaCluster = new io.debezium.kafka.KafkaCluster()\n+                .addBrokers(1)\n+                .deleteDataPriorToStartup(true)\n+                .deleteDataUponShutdown(true)\n+                .usingDirectory(Files.createTempDirectory(getClass().getName()).toFile())\n+                .withKafkaConfiguration(new Properties());\n+        kafkaCluster.startup();\n+    }\n+\n+    @AfterEach\n+    public void after() {\n+        kafkaCluster.shutdown();\n+    }\n+\n+    @Test\n+    @Timeout(value = 2, timeUnit = TimeUnit.MINUTES)\n+    public void simpleTest() throws InterruptedException {", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNzQ0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434437448", "bodyText": "Rethrow a RuntimeException. Honestly, printStackTrace() is never the right thing to do, especially in a test.", "author": "tombentley", "createdAt": "2020-06-03T09:35:28Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.singletonList;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.5.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\")) {\n+            String desiredConfigString = TestUtils.readResource(is);\n+\n+            for (ConfigEntry ce : additional) {\n+                desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+            }\n+\n+            return desiredConfigString;\n+        } catch (IOException e) {\n+            e.printStackTrace();", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjAzMTIyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452031220", "bodyText": "This still needs to be addressed.", "author": "tombentley", "createdAt": "2020-07-09T07:53:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNzQ0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzODUwNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434438506", "bodyText": "I've seen new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)) so many times in this PR. It would be better if it was factored into a Util.brokerConfigResource(brokerId).", "author": "tombentley", "createdAt": "2020-06-03T09:37:11Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.singletonList;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.5.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\")) {\n+            String desiredConfigString = TestUtils.readResource(is);\n+\n+            for (ConfigEntry ce : additional) {\n+                desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+            }\n+\n+            return desiredConfigString;\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+        return \"\";\n+    }\n+\n+    private Config getCurrentConfiguration(List<ConfigEntry> additional) {\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\")) {\n+\n+            List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));\n+            configList.forEach(entry -> {\n+                String[] split = entry.split(\"=\");\n+                String val = split.length == 1 ? \"\" : split[1];\n+                ConfigEntry ce = new ConfigEntry(split[0].replace(\"\\n\", \"\"), val, true, true, false);\n+                entryList.add(ce);\n+            });\n+            for (ConfigEntry ce : additional) {\n+                entryList.add(ce);\n+            }\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        Config config = new Config(entryList);\n+        return config;\n+    }\n+\n+    private void assertConfig(KafkaBrokerConfigurationDiff kcd, ConfigEntry ce) {\n+        Collection<AlterConfigOp> brokerDiffConf = kcd.getConfigDiff().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzOTY2OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434439669", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertThat(en.get().configEntry().name().equals(ce.name()), is(true));\n          \n          \n            \n                    assertThat(en.get().configEntry().value().equals(ce.value()), is(true));\n          \n          \n            \n                    assertThat(en.get().configEntry().name(), is(ce.name()));\n          \n          \n            \n                    assertThat(en.get().configEntry().value(), is(ce.value()));", "author": "tombentley", "createdAt": "2020-06-03T09:38:58Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.singletonList;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.5.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\")) {\n+            String desiredConfigString = TestUtils.readResource(is);\n+\n+            for (ConfigEntry ce : additional) {\n+                desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+            }\n+\n+            return desiredConfigString;\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+        return \"\";\n+    }\n+\n+    private Config getCurrentConfiguration(List<ConfigEntry> additional) {\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\")) {\n+\n+            List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));\n+            configList.forEach(entry -> {\n+                String[] split = entry.split(\"=\");\n+                String val = split.length == 1 ? \"\" : split[1];\n+                ConfigEntry ce = new ConfigEntry(split[0].replace(\"\\n\", \"\"), val, true, true, false);\n+                entryList.add(ce);\n+            });\n+            for (ConfigEntry ce : additional) {\n+                entryList.add(ce);\n+            }\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        Config config = new Config(entryList);\n+        return config;\n+    }\n+\n+    private void assertConfig(KafkaBrokerConfigurationDiff kcd, ConfigEntry ce) {\n+        Collection<AlterConfigOp> brokerDiffConf = kcd.getConfigDiff().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        Optional<AlterConfigOp> en = brokerDiffConf.stream().filter(entry -> entry.configEntry().name().equals(ce.name())).findFirst();\n+        assertThat(en.isPresent(), is(true));\n+        assertThat(en.get().configEntry().name().equals(ce.name()), is(true));\n+        assertThat(en.get().configEntry().value().equals(ce.value()), is(true));", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ0MDI3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r434440270", "bodyText": "We should assert that the given name only happens once in the stream too.", "author": "tombentley", "createdAt": "2020-06-03T09:39:56Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.operator.cluster.KafkaVersionTestUtils;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.singletonList;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+public class KafkaBrokerConfigurationDiffTest {\n+\n+    private static final KafkaVersion.Lookup VERSIONS = KafkaVersionTestUtils.getKafkaVersionLookup();\n+    private static final String KAFKA_VERSION = \"2.5.0\";\n+    KafkaVersion kafkaVersion = VERSIONS.version(KAFKA_VERSION);\n+    private int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker.conf\")) {\n+            String desiredConfigString = TestUtils.readResource(is);\n+\n+            for (ConfigEntry ce : additional) {\n+                desiredConfigString += \"\\n\" + ce.name() + \"=\" + ce.value();\n+            }\n+\n+            return desiredConfigString;\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+        return \"\";\n+    }\n+\n+    private Config getCurrentConfiguration(List<ConfigEntry> additional) {\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker.conf\")) {\n+\n+            List<String> configList = Arrays.asList(TestUtils.readResource(is).split(System.getProperty(\"line.separator\")));\n+            configList.forEach(entry -> {\n+                String[] split = entry.split(\"=\");\n+                String val = split.length == 1 ? \"\" : split[1];\n+                ConfigEntry ce = new ConfigEntry(split[0].replace(\"\\n\", \"\"), val, true, true, false);\n+                entryList.add(ce);\n+            });\n+            for (ConfigEntry ce : additional) {\n+                entryList.add(ce);\n+            }\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        Config config = new Config(entryList);\n+        return config;\n+    }\n+\n+    private void assertConfig(KafkaBrokerConfigurationDiff kcd, ConfigEntry ce) {\n+        Collection<AlterConfigOp> brokerDiffConf = kcd.getConfigDiff().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(brokerId)));\n+        Optional<AlterConfigOp> en = brokerDiffConf.stream().filter(entry -> entry.configEntry().name().equals(ce.name())).findFirst();", "originalCommit": "36cfad7a0a0530d7441888af6f86424ab85ca2eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "029ac3ddbf617143c59094e5f90215534a1142c5", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/029ac3ddbf617143c59094e5f90215534a1142c5", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-04T13:31:14Z", "type": "forcePushed"}, {"oid": "411b37b747b4df6294e4ace46cc031d45574d7d9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/411b37b747b4df6294e4ace46cc031d45574d7d9", "message": "sequentially done changes\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-05T13:32:04Z", "type": "forcePushed"}, {"oid": "9e15c48ca1f07e99f7b9a71c572f536d857f258b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9e15c48ca1f07e99f7b9a71c572f536d857f258b", "message": "sequentially done changes\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-05T17:09:39Z", "type": "forcePushed"}, {"oid": "97a484c9909a64feedb15c71be1a63cd040c02a3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/97a484c9909a64feedb15c71be1a63cd040c02a3", "message": "sequentially done changes\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-08T06:49:42Z", "type": "forcePushed"}, {"oid": "94c7bd1ece6fef7f7139c16527feba976b66c668", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/94c7bd1ece6fef7f7139c16527feba976b66c668", "message": "sequentially done changes\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-08T06:59:27Z", "type": "forcePushed"}, {"oid": "4c74eaa32342beaf91aa75908bd02ff798a2cfbf", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/4c74eaa32342beaf91aa75908bd02ff798a2cfbf", "message": "tests\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-08T12:08:22Z", "type": "forcePushed"}, {"oid": "59b216650a0a7a3aa373d44f625b7a2b21a2c69e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/59b216650a0a7a3aa373d44f625b7a2b21a2c69e", "message": "tests\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-15T08:01:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDE1NjIxMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r440156212", "bodyText": "I think this comment is incorrect now that we always add BrokerConfigChange in the KAO.", "author": "tombentley", "createdAt": "2020-06-15T12:57:41Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -305,6 +327,63 @@ private void restartIfNecessary(int podId, RestartContext restartContext)\n         }\n     }\n \n+    /**\n+     * Returns a Future which completes with the config of the given broker.\n+     * @param ac The admin client\n+     * @param brokerId The id of the broker.\n+     * @return a Future which completes with the config of the given broker.\n+     */\n+    protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n+        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n+            30, TimeUnit.SECONDS,\n+            error -> new ForceableProblem(\"Error getting broker config\", error)\n+        );\n+    }\n+\n+    private boolean updateBrokerConfigDynamically(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff) {\n+        try {\n+            AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getConfigDiff());\n+            KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(podId)));\n+            await(Util.kafkaFutureToVertxFuture(vertx, brokerConfigFuture), 30, TimeUnit.SECONDS,\n+                error -> new ForceableProblem(\"Error doing dynamic update\", error));\n+            log.debug(\"Dynamic AlterConfig result for broker {}. Can be updated dynamically\", podId);\n+            // TODO probably don't want a _kube_ readiness check here, but need something to check that the broker is still OK\n+            // awaitReadiness(null, 30, TimeUnit.SECONDS);\n+            return true;\n+        } catch (Exception e) {\n+            log.debug(\"Could not dynamically update broker {} configuration. Reason {}\", podId, e);\n+            return false;\n+        }\n+    }\n+\n+    /**\n+     * Determine whether the broker running in the given {@code podId} can be reconfigured dynamically,\n+     * and if so try to reconfigure it and return true, otherwise (if the broker changes cannot be done dynamically, or result in an error) then return false.\n+     * @param ac AdminClient\n+     * @param podId Broker id\n+     * @param kafkaConfig The broker config to be applied (as a String)\n+     * @param kafkaVersion The version of Kafka running in the given pod.\n+     * @return true iff the broker way reconfigured dynamically\n+     * @throws ForceableProblem\n+     * @throws InterruptedException\n+     */\n+    protected boolean maybeReconfigureBroker(Admin ac, int podId, String kafkaConfig, KafkaVersion kafkaVersion) throws ForceableProblem, InterruptedException {\n+        Config brokerConfig = brokerConfig(ac, podId);\n+        log.trace(\"Broker {}: description {}\", podId, brokerConfig);\n+        KafkaBrokerConfigurationDiff configurationDiff = new KafkaBrokerConfigurationDiff(brokerConfig, kafkaConfig, kafkaVersion, podId);\n+        if (!configurationDiff.isEmpty()) {\n+            if (configurationDiff.canBeUpdatedDynamically()) {\n+                return updateBrokerConfigDynamically(podId, ac, configurationDiff);\n+            } else {\n+                return false;\n+            }\n+        } else {\n+            // In theory this can never happen", "originalCommit": "59b216650a0a7a3aa373d44f625b7a2b21a2c69e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDE1Nzc2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r440157761", "bodyText": "Is it public for a reason? We should probablly document at least why a subclass is necessary. And I think it might be cleaner froma layering PoV if:\n\nRestartReason was made topic level\nBrokerConfigChange was moved to the same package as KSO.", "author": "tombentley", "createdAt": "2020-06-15T13:00:27Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3456,4 +3473,37 @@ private String getStringHash(String toBeHashed)  {\n             throw new RuntimeException(\"Failed to create SHA-512 MessageDigest instance\");\n         }\n     }\n+\n+    public static class BrokerConfigChange extends RestartReason {", "originalCommit": "59b216650a0a7a3aa373d44f625b7a2b21a2c69e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDE1ODYzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r440158638", "bodyText": "Would be good to see a test for the good case, where the dynamic update can be attempted and is successful, and assert that the broker is not rolled.", "author": "tombentley", "createdAt": "2020-06-15T13:01:58Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaRollerTest.java", "diffHunk": "@@ -341,9 +343,37 @@ public void testControllerNeverRollable(VertxTestContext testContext) throws Int\n                 emptyList());\n     }\n \n+    @Test\n+    public void testRollHandlesErrorWhenGettingConfig(VertxTestContext testContext) {\n+        PodOperator podOps = mockPodOps(podId -> succeededFuture());\n+        StatefulSet sts = buildStatefulSet();\n+        TestingKafkaRoller kafkaRoller = new TestingKafkaRoller(sts, null, null, podOps,\n+                null, null,\n+                null, null, new KafkaRoller.ForceableProblem(\"could not get config exception\"),\n+            brokerId -> succeededFuture(true), 2);\n+        // The algorithm should carry on rolling the pods\n+        doSuccessfulRollingRestart(testContext, kafkaRoller,\n+                asList(0, 1, 2, 3, 4),\n+                asList(0, 1, 3, 4, 2));\n+    }\n+\n+    @Test\n+    public void testRollHandlesErrorWhenAlteringConfig(VertxTestContext testContext) {\n+        PodOperator podOps = mockPodOps(podId -> succeededFuture());\n+        StatefulSet sts = buildStatefulSet();\n+        TestingKafkaRoller kafkaRoller = new TestingKafkaRoller(sts, null, null, podOps,\n+                null, null,\n+                null, new KafkaRoller.ForceableProblem(\"could not get alter exception\"), null,\n+            brokerId -> succeededFuture(true), 2);\n+        // The algorithm should carry on rolling the pods\n+        doSuccessfulRollingRestart(testContext, kafkaRoller,\n+                asList(0, 1, 2, 3, 4),\n+                asList(0, 1, 3, 4, 2));\n+    }\n+", "originalCommit": "59b216650a0a7a3aa373d44f625b7a2b21a2c69e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgyMzI1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r440823251", "bodyText": "is it a rebase problem or you just noticed the wrong lines position and moved them?", "author": "ppatierno", "createdAt": "2020-06-16T12:49:43Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -111,13 +111,14 @@\n     @Deprecated\n     public static final String ANNO_CO_STRIMZI_IO_DELETE_CLAIM = ClusterOperator.STRIMZI_CLUSTER_OPERATOR_DOMAIN + \"/delete-claim\";\n \n-    private static final String ENV_VAR_HTTP_PROXY = \"HTTP_PROXY\";\n-    private static final String ENV_VAR_HTTPS_PROXY = \"HTTPS_PROXY\";\n-    private static final String ENV_VAR_NO_PROXY = \"NO_PROXY\";\n     /**\n      * Configure HTTP/HTTPS Proxy env vars\n      * These are set in the Cluster Operator and then passed to all created containers\n      */\n+    private static final String ENV_VAR_HTTP_PROXY = \"HTTP_PROXY\";\n+    private static final String ENV_VAR_HTTPS_PROXY = \"HTTPS_PROXY\";\n+    private static final String ENV_VAR_NO_PROXY = \"NO_PROXY\";\n+", "originalCommit": "c551a2064c705639193dc320730ae4450e786b83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgyODc5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r440828796", "bodyText": "it seems to be a specialization of RestartReason not StatefulSetOperator", "author": "ppatierno", "createdAt": "2020-06-16T12:58:34Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/BrokerConfigChange.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import java.util.Objects;\n+\n+/**\n+ * Specialization of {@link StatefulSetOperator} for StatefulSets of Kafka brokers\n+ */\n+public class BrokerConfigChange extends StatefulSetOperator.RestartReason {", "originalCommit": "c551a2064c705639193dc320730ae4450e786b83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgzMjczMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r440832731", "bodyText": "remove commented line", "author": "ppatierno", "createdAt": "2020-06-16T13:04:42Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,253 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ The algorithm:\n+ *  1. Create a map from the supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map as the broker's {@code kafka_config_generator.sh} would\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private Map<ConfigResource, Collection<AlterConfigOp>> diff;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Config brokerConfigs, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.diff = Collections.emptyMap(); // init\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        this.diff = computeDiff(brokerId, desired, brokerConfigs, configModel);\n+    }\n+\n+    private static void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @param config Config which contains property\n+     * @return true if property in desired map has a default value\n+     */\n+    boolean isDesiredPropertyDefaultValue(String key, Config config) {\n+        Optional<ConfigEntry> entry = config.entries().stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        if (diff == null) {\n+            return true;\n+        } else {\n+            AtomicBoolean tempResult = new AtomicBoolean(true);\n+            diff.get(Util.getBrokersConfig(brokerId)).forEach(entry -> {\n+                if (isEntryReadOnly(entry.configEntry())) {\n+                    tempResult.set(false);\n+                }\n+            });\n+            return tempResult.get();\n+        }\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return A map which can be used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.get(Util.getBrokersConfig(brokerId)).size();\n+    }\n+\n+    private static boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @param brokerId id of compared broker\n+     * @param desired desired configuration\n+     * @param brokerConfigs current configuration\n+     * @param configModel default configuration for {@code kafkaVersion} of broker\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    private static Map<ConfigResource, Collection<AlterConfigOp>> computeDiff(int brokerId, String desired,\n+                                                                       Config brokerConfigs,\n+                                                                       Map<String, ConfigModel> configModel) {\n+        Map<ConfigResource, Collection<AlterConfigOp>> updated = new HashMap<>();\n+        if (brokerConfigs == null) {\n+            return updated;\n+        }\n+        Map<String, String> currentMap;\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentMap = brokerConfigs.entries().stream().collect(Collectors.toMap(configEntry -> configEntry.name(), configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream().filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash)).findFirst();\n+\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (d.get(\"op\").asText().equals(\"remove\")) {\n+                    removeProperty(configModel, updatedCE, pathValueWithoutSlash, entry);\n+                } else if (d.get(\"op\").asText().equals(\"replace\")) {\n+                    // entry is in the current, desired is updated value\n+                    updateOrAdd(entry.name(), configModel, desiredMap, updatedCE);\n+                }\n+            } else {\n+                if (d.get(\"op\").asText().equals(\"add\")) {\n+                    // entry is not in the current, it is added\n+                    updateOrAdd(pathValueWithoutSlash, configModel, desiredMap, updatedCE);\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+\n+        updated.put(Util.getBrokersConfig(brokerId), updatedCE);\n+        return Collections.unmodifiableMap(updated);\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, ConfigModel> configModel, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!isIgnorableProperty(propertyName)) {\n+            if (isEntryCustom(propertyName, configModel)) {\n+                log.trace(\"custom property {} has been updated/added {}\", propertyName, desiredMap.get(propertyName));\n+            } else {\n+                log.trace(\"property {} has been updated/added {}\", propertyName, desiredMap.get(propertyName));\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, desiredMap.get(propertyName)), AlterConfigOp.OpType.SET));\n+            }\n+        } else {\n+            log.trace(\"{} is ignorable, not considering\");\n+        }\n+    }\n+\n+    private static void removeProperty(Map<String, ConfigModel> configModel, Collection<AlterConfigOp> updatedCE, String pathValueWithoutSlash, ConfigEntry entry) {\n+        if (isEntryCustom(entry.name(), configModel)) {\n+            // we are deleting custom option\n+            //updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, entry.value()), AlterConfigOp.OpType.DELETE));", "originalCommit": "c551a2064c705639193dc320730ae4450e786b83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgzMzM5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r440833390", "bodyText": "what about these two comments? leftovers?", "author": "ppatierno", "createdAt": "2020-06-16T13:05:45Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -305,6 +326,62 @@ private void restartIfNecessary(int podId, RestartContext restartContext)\n         }\n     }\n \n+    /**\n+     * Returns a Future which completes with the config of the given broker.\n+     * @param ac The admin client\n+     * @param brokerId The id of the broker.\n+     * @return a Future which completes with the config of the given broker.\n+     */\n+    protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n+        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n+            30, TimeUnit.SECONDS,\n+            error -> new ForceableProblem(\"Error getting broker config\", error)\n+        );\n+    }\n+\n+    private boolean updateBrokerConfigDynamically(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff) {\n+        try {\n+            AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configurationDiff.getConfigDiff());\n+            KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(podId)));\n+            await(Util.kafkaFutureToVertxFuture(vertx, brokerConfigFuture), 30, TimeUnit.SECONDS,\n+                error -> new ForceableProblem(\"Error doing dynamic update\", error));\n+            log.debug(\"Dynamic AlterConfig result for broker {}. Can be updated dynamically\", podId);\n+            // TODO probably don't want a _kube_ readiness check here, but need something to check that the broker is still OK\n+            // awaitReadiness(null, 30, TimeUnit.SECONDS);", "originalCommit": "c551a2064c705639193dc320730ae4450e786b83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dd262317ea07a31800c67488b6639ddfecc409ff", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/dd262317ea07a31800c67488b6639ddfecc409ff", "message": "empty bug\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-18T07:59:29Z", "type": "forcePushed"}, {"oid": "8c74ba520ea1f4eac1075250cfda7c872e91c560", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/8c74ba520ea1f4eac1075250cfda7c872e91c560", "message": "npe\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-27T16:12:57Z", "type": "forcePushed"}, {"oid": "d7afe57dcb93d2bb00a06d021411d352b1fa14e8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d7afe57dcb93d2bb00a06d021411d352b1fa14e8", "message": "certTest needs to be fixed\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-30T10:56:18Z", "type": "forcePushed"}, {"oid": "2f8b6aca52ba6d68b95b01426bfcb73497ab91f9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2f8b6aca52ba6d68b95b01426bfcb73497ab91f9", "message": "certTest needs to be fixed\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-06-30T10:58:36Z", "type": "forcePushed"}, {"oid": "ab582900efec733af8ef234ac8544227ae62fb14", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ab582900efec733af8ef234ac8544227ae62fb14", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-01T07:44:05Z", "type": "forcePushed"}, {"oid": "ec998a3b7b530f35b8e896989a10190260c979ac", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ec998a3b7b530f35b8e896989a10190260c979ac", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-01T08:47:32Z", "type": "forcePushed"}, {"oid": "a19e6afccb0442ebc9f90b8cbc5e714d5984b9e3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a19e6afccb0442ebc9f90b8cbc5e714d5984b9e3", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-01T08:59:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5MDAzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r448490037", "bodyText": "Can we remove this please.", "author": "tombentley", "createdAt": "2020-07-01T16:45:23Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -210,39 +232,78 @@ public String toString() {\n         RestartContext ctx = podToContext.computeIfAbsent(podId,\n             k -> new RestartContext(backoffSupplier));\n         singleExecutor.schedule(() -> {\n-            log.debug(\"Considering restart of pod {} after delay of {} {}\", podId, delay, unit);\n+            log.debug(\"{} Considering restart of pod {} after delay of {} {}\", reconciliation, podId, delay, unit);\n             try {\n                 restartIfNecessary(podId, ctx);\n                 ctx.promise.complete();\n             } catch (InterruptedException e) {\n                 // Let the executor deal with interruption.\n                 Thread.currentThread().interrupt();\n             } catch (FatalProblem e) {\n-                log.info(\"Could not restart pod {}, giving up after {} attempts/{}ms\",\n-                        podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n+                log.info(\"{} Could not restart pod {}, giving up after {} attempts/{}ms\",\n+                        reconciliation, podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n                 ctx.promise.fail(e);\n                 singleExecutor.shutdownNow();\n                 podToContext.forEachValue(Integer.MAX_VALUE, f -> {\n                     f.promise.tryFail(e);\n                 });\n             } catch (Exception e) {\n                 if (ctx.backOff.done()) {\n-                    log.info(\"Could not roll pod {}, giving up after {} attempts/{}ms\",\n-                            podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n+                    log.info(\"{} Could not roll pod {}, giving up after {} attempts/{}ms\",\n+                            reconciliation, podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n                     ctx.promise.fail(e instanceof TimeoutException ?\n                             new io.strimzi.operator.common.operator.resource.TimeoutException() :\n                             e);\n                 } else {\n                     long delay1 = ctx.backOff.delayMs();\n-                    log.info(\"Could not roll pod {} due to {}, retrying after at least {}ms\",\n-                            podId, e, delay1);\n+                    e.printStackTrace();", "originalCommit": "2a188dd6a06e9eafd6086214425b323d0ea72ad4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5MDI4MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r448490280", "bodyText": "We need a better name for this.", "author": "tombentley", "createdAt": "2020-07-01T16:45:48Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -210,39 +232,78 @@ public String toString() {\n         RestartContext ctx = podToContext.computeIfAbsent(podId,\n             k -> new RestartContext(backoffSupplier));\n         singleExecutor.schedule(() -> {\n-            log.debug(\"Considering restart of pod {} after delay of {} {}\", podId, delay, unit);\n+            log.debug(\"{} Considering restart of pod {} after delay of {} {}\", reconciliation, podId, delay, unit);\n             try {\n                 restartIfNecessary(podId, ctx);\n                 ctx.promise.complete();\n             } catch (InterruptedException e) {\n                 // Let the executor deal with interruption.\n                 Thread.currentThread().interrupt();\n             } catch (FatalProblem e) {\n-                log.info(\"Could not restart pod {}, giving up after {} attempts/{}ms\",\n-                        podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n+                log.info(\"{} Could not restart pod {}, giving up after {} attempts/{}ms\",\n+                        reconciliation, podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n                 ctx.promise.fail(e);\n                 singleExecutor.shutdownNow();\n                 podToContext.forEachValue(Integer.MAX_VALUE, f -> {\n                     f.promise.tryFail(e);\n                 });\n             } catch (Exception e) {\n                 if (ctx.backOff.done()) {\n-                    log.info(\"Could not roll pod {}, giving up after {} attempts/{}ms\",\n-                            podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n+                    log.info(\"{} Could not roll pod {}, giving up after {} attempts/{}ms\",\n+                            reconciliation, podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n                     ctx.promise.fail(e instanceof TimeoutException ?\n                             new io.strimzi.operator.common.operator.resource.TimeoutException() :\n                             e);\n                 } else {\n                     long delay1 = ctx.backOff.delayMs();\n-                    log.info(\"Could not roll pod {} due to {}, retrying after at least {}ms\",\n-                            podId, e, delay1);\n+                    e.printStackTrace();\n+                    log.info(\"{} Could not roll pod {} due to {}, retrying after at least {}ms\",\n+                            reconciliation, podId, e, delay1);\n                     schedule(podId, delay1, TimeUnit.MILLISECONDS);\n                 }\n             }\n         }, delay, unit);\n         return ctx.promise.future();\n     }\n \n+    class RestartPlan {\n+        private final boolean needsRestart;\n+        private final boolean needsReconfig;\n+        private final KafkaBrokerConfigurationDiff diff;\n+        private Admin adminClient;\n+        private final int podId;\n+\n+        public RestartPlan(Admin adminClient, int podId, boolean needsRestart, boolean needsReconfig, KafkaBrokerConfigurationDiff diff) {\n+            this.adminClient = adminClient;\n+            this.podId = podId;\n+            this.needsRestart = needsRestart;\n+            this.needsReconfig = needsReconfig;\n+            this.diff = diff;\n+        }\n+\n+        Admin adminClient() throws ForceableProblem, FatalProblem {\n+            if (adminClient == null) {\n+                adminClient = KafkaRoller.this.adminClient(podId, false);\n+            }\n+            return adminClient;\n+        }\n+\n+        private void closeLoggingAnyError() {\n+            if (adminClient != null) {\n+                closeLoggingAnyError2(adminClient);\n+                adminClient = null;\n+            }\n+        }\n+    }\n+\n+    private static void closeLoggingAnyError2(Admin admin) {", "originalCommit": "2a188dd6a06e9eafd6086214425b323d0ea72ad4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5MDkyMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r448490922", "bodyText": "I don't think we need this class now that we're not using the hack of always passing BrokerConfigChange. This means the Function<Pod, List<RestartReason>> could revert to Function<Pod, List<String>> or Function<Pod, String>.", "author": "tombentley", "createdAt": "2020-07-01T16:46:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/RestartReason.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import java.util.Objects;\n+\n+public class RestartReason {", "originalCommit": "2a188dd6a06e9eafd6086214425b323d0ea72ad4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f34d7771be456b02e638b8b8240e3ba37db384ae", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f34d7771be456b02e638b8b8240e3ba37db384ae", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-02T14:30:19Z", "type": "forcePushed"}, {"oid": "a63020bbada493daf2586ded83738400bf6a8032", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a63020bbada493daf2586ded83738400bf6a8032", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-02T14:36:42Z", "type": "forcePushed"}, {"oid": "530e9af8d0a765b8eaf2c460ce0c6a98980eed51", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/530e9af8d0a765b8eaf2c460ce0c6a98980eed51", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-02T14:43:24Z", "type": "forcePushed"}, {"oid": "5f37c53e686658a7e86809b24c8086cf7ab40e27", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5f37c53e686658a7e86809b24c8086cf7ab40e27", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-07T07:06:25Z", "type": "forcePushed"}, {"oid": "942c3d6e8506651cbf0bd463074120a5c36e5dbd", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/942c3d6e8506651cbf0bd463074120a5c36e5dbd", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-08T07:26:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjAyOTU5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452029599", "bodyText": "I think in the KAO we use \"{}: \" for interpolating the reconciliation. It would be good to be consistent.", "author": "tombentley", "createdAt": "2020-07-09T07:50:26Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -150,7 +172,7 @@\n             // only for it not to become ready and thus drive the cluster to a worse state.\n             podIds.add(podOperations.isReady(namespace, podName(podId)) ? podIds.size() : 0, podId);\n         }\n-        log.debug(\"Initial order for rolling restart {}\", podIds);\n+        log.debug(\"{} Initial order for rolling restart {}\", reconciliation, podIds);", "originalCommit": "942c3d6e8506651cbf0bd463074120a5c36e5dbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjAzMDI5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452030296", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                class RestartPlan {\n          \n          \n            \n                /** Described how the \"restart\" (which might actually just be a reconfigure) will be performed. */\n          \n          \n            \n                class RestartPlan {", "author": "tombentley", "createdAt": "2020-07-09T07:51:42Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -210,39 +232,77 @@ public String toString() {\n         RestartContext ctx = podToContext.computeIfAbsent(podId,\n             k -> new RestartContext(backoffSupplier));\n         singleExecutor.schedule(() -> {\n-            log.debug(\"Considering restart of pod {} after delay of {} {}\", podId, delay, unit);\n+            log.debug(\"{} Considering restart of pod {} after delay of {} {}\", reconciliation, podId, delay, unit);\n             try {\n                 restartIfNecessary(podId, ctx);\n                 ctx.promise.complete();\n             } catch (InterruptedException e) {\n                 // Let the executor deal with interruption.\n                 Thread.currentThread().interrupt();\n             } catch (FatalProblem e) {\n-                log.info(\"Could not restart pod {}, giving up after {} attempts/{}ms\",\n-                        podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n+                log.info(\"{} Could not restart pod {}, giving up after {} attempts/{}ms\",\n+                        reconciliation, podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n                 ctx.promise.fail(e);\n                 singleExecutor.shutdownNow();\n                 podToContext.forEachValue(Integer.MAX_VALUE, f -> {\n                     f.promise.tryFail(e);\n                 });\n             } catch (Exception e) {\n                 if (ctx.backOff.done()) {\n-                    log.info(\"Could not roll pod {}, giving up after {} attempts/{}ms\",\n-                            podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n+                    log.info(\"{} Could not roll pod {}, giving up after {} attempts/{}ms\",\n+                            reconciliation, podId, ctx.backOff.maxAttempts(), ctx.backOff.totalDelayMs(), e);\n                     ctx.promise.fail(e instanceof TimeoutException ?\n                             new io.strimzi.operator.common.operator.resource.TimeoutException() :\n                             e);\n                 } else {\n                     long delay1 = ctx.backOff.delayMs();\n-                    log.info(\"Could not roll pod {} due to {}, retrying after at least {}ms\",\n-                            podId, e, delay1);\n+                    log.info(\"{} Could not roll pod {} due to {}, retrying after at least {}ms\",\n+                            reconciliation, podId, e, delay1);\n                     schedule(podId, delay1, TimeUnit.MILLISECONDS);\n                 }\n             }\n         }, delay, unit);\n         return ctx.promise.future();\n     }\n \n+    class RestartPlan {", "originalCommit": "942c3d6e8506651cbf0bd463074120a5c36e5dbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA3NTg1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452075852", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * This method transforms a String into a List of Strings, where entry = line from input.\n          \n          \n            \n                 * This method transforms a String into a List of Strings, where each entry is an uncommented line of input.", "author": "samuel-hawker", "createdAt": "2020-07-09T09:10:02Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -551,4 +551,25 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms a String into a List of Strings, where entry = line from input.", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA3NjUzMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452076532", "bodyText": "I wonder if it is possible that a line might be  #... e.g. a whitespace at the beginning of a line, and if we should catch this and count this as a comment line.\nAlso could a comment be appended to the end of an input line?", "author": "samuel-hawker", "createdAt": "2020-07-09T09:11:11Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/ModelUtils.java", "diffHunk": "@@ -551,4 +551,25 @@ public static String getJavaSystemPropertiesToString(List<SystemProperty> javaSy\n         }\n         return String.join(\" \", javaSystemPropertiesList);\n     }\n+\n+    /**\n+     * This method transforms a String into a List of Strings, where entry = line from input.\n+     * The lines beginning with '#' (comments) are ignored.\n+     * @param config ConfigMap data as a String\n+     * @return List of String key=value\n+     */\n+    public static List<String> getLinesWithoutCommentsAndEmptyLines(String config) {\n+        List<String> validLines = new ArrayList<>();\n+        if (config != null) {\n+            List<String> allLines = Arrays.asList(config.split(\"\\\\r?\\\\n\"));\n+\n+            for (String line : allLines) {\n+                if (!line.startsWith(\"#\") && !line.isEmpty()) {", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE0NTE1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452145154", "bodyText": "What comment do you mind?", "author": "sknot-rh", "createdAt": "2020-07-09T11:19:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA3NjUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA4ODEyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452088123", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    KafkaCluster kafkaCluster = null;\n          \n          \n            \n                    /* test */ KafkaCluster kafkaCluster = null;\n          \n      \n    \n    \n  \n\nIf you're omitting private, do you need it public for testing?", "author": "samuel-hawker", "createdAt": "2020-07-09T09:30:36Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -386,7 +394,8 @@ ReconciliationState createReconciliationState(Reconciliation reconciliation, Kaf\n         /* test */ ReconcileResult<StatefulSet> zkDiffs;\n         private Integer zkCurrentReplicas = null;\n \n-        private KafkaCluster kafkaCluster = null;\n+        KafkaCluster kafkaCluster = null;", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA4ODIxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452088218", "bodyText": "ditto", "author": "samuel-hawker", "createdAt": "2020-07-09T09:30:46Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -386,7 +394,8 @@ ReconciliationState createReconciliationState(Reconciliation reconciliation, Kaf\n         /* test */ ReconcileResult<StatefulSet> zkDiffs;\n         private Integer zkCurrentReplicas = null;\n \n-        private KafkaCluster kafkaCluster = null;\n+        KafkaCluster kafkaCluster = null;\n+        int oldKafkaReplicas;", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjA5NTIyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452095228", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Future<Void> maybeKafkaRoll(StatefulSet sts, Function<Pod, List<String>> podNeedsRestart) {\n          \n          \n            \n                    Future<Void> maybeRollKafka(StatefulSet sts, Function<Pod, List<String>> podNeedsRestart) {\n          \n      \n    \n    \n  \n\nSmall suggested name change, a doc comment explaining why this is so highly configurable would be useful too.", "author": "samuel-hawker", "createdAt": "2020-07-09T09:42:39Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -1228,17 +1237,46 @@ private KafkaVersionChange getKafkaVersionChange(StatefulSet kafkaSts) {\n                             resultSts = (StatefulSet) ((ReconcileResult) result.resultAt(0)).resource();\n                         }\n \n-                        return kafkaSetOperations.maybeRollingUpdate(sts, pod -> {\n+                        Function<Pod, List<String>> fn = pod -> {\n                             log.info(\"{}: Downgrade: Patch + rolling update of {}: Pod {}\", reconciliation, stsName, pod.getMetadata().getName());\n-                            return \"Downgrade phase 1 of \" + phases + \": Patch + rolling update of \" + name + \": Pod \" + pod.getMetadata().getName();\n-                        }).map(resultSts);\n+                            return singletonList(\"Downgrade phase 1 of \" + phases + \": Patch + rolling update of \" + name + \": Pod \" + pod.getMetadata().getName());\n+                        };\n+                        return maybeKafkaRoll(sts, fn).map(resultSts);\n                     })\n                     .compose(ss2 -> {\n                         log.info(\"{}: {}, phase 1 of {} completed\", reconciliation, versionChange, phases);\n                         return Future.succeededFuture(ss2);\n                     });\n         }\n \n+        protected CompositeFuture adminClientSecrets() {\n+            Future<Secret> clusterCaCertSecretFuture = secretOperations.getAsync(\n+                namespace, KafkaResources.clusterCaCertificateSecretName(name)).compose(secret -> {\n+                    if (secret == null) {\n+                        return Future.failedFuture(Util.missingSecretException(namespace, KafkaCluster.clusterCaCertSecretName(name)));\n+                    } else {\n+                        return Future.succeededFuture(secret);\n+                    }\n+                });\n+            Future<Secret> coKeySecretFuture = secretOperations.getAsync(\n+                namespace, ClusterOperator.secretName(name)).compose(secret -> {\n+                    if (secret == null) {\n+                        return Future.failedFuture(Util.missingSecretException(namespace, ClusterOperator.secretName(name)));\n+                    } else {\n+                        return Future.succeededFuture(secret);\n+                    }\n+                });\n+            return CompositeFuture.join(clusterCaCertSecretFuture, coKeySecretFuture);\n+        }\n+\n+        Future<Void> maybeKafkaRoll(StatefulSet sts, Function<Pod, List<String>> podNeedsRestart) {", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwMDQ0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452100448", "bodyText": "Did you mean to leave in this comment?", "author": "samuel-hawker", "createdAt": "2020-07-09T09:51:39Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperatorCustomCertTest.java", "diffHunk": "@@ -102,51 +114,29 @@ public static void before() {\n     @BeforeEach\n     public void setup() {\n         kafka = createKafka();\n-        kafkaCluster = KafkaCluster.fromCrd(kafka, VERSIONS);\n \n-        supplier = ResourceUtils.supplierWithMocks(false);\n+        client = new MockKube()\n+                .withCustomResourceDefinition(Crds.kafka(), Kafka.class, KafkaList.class, DoneableKafka.class).end()\n+                .build();\n+        Crds.kafkaOperation(client).inNamespace(namespace).create(kafka);\n+        client.secrets().inNamespace(namespace).create(getTlsSecret());\n+        client.secrets().inNamespace(namespace).create(getExternalSecret());\n+        client.secrets().inNamespace(namespace).createNew().withNewMetadata()\n+                .withNamespace(namespace)\n+                .withName(\"testkafka-cluster-operator-certs\")\n+                .endMetadata()\n+                .addToData(\"foo\", \"bar\")\n+                .done();\n+        ResourceOperatorSupplier supplier = new ResourceOperatorSupplier(vertx, client, mock(ZookeeperLeaderFinder.class),\n+                mock(AdminClientProvider.class), mock(ZookeeperScalerProvider.class),\n+                mock(MetricsProvider.class), new PlatformFeaturesAvailability(false, KubernetesVersion.V1_14), 10000);\n+        //supplier = ResourceUtils.supplierWithMocks(false);", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwMTE5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452101194", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                protected void updateBrokerConfigDynamically(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)\n          \n          \n            \n                protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)", "author": "samuel-hawker", "createdAt": "2020-07-09T09:52:55Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -261,58 +322,149 @@ private void restartIfNecessary(int podId, RestartContext restartContext)\n             throw new UnforceableProblem(\"Error getting pod \" + podName(podId), e);\n         }\n \n-        String reasonToRestartPod = podNeedsRestart.apply(pod);\n-        if (reasonToRestartPod != null && !reasonToRestartPod.isEmpty()) {\n-            log.info(\"Pod {} needs to be restarted. Reason: {}\", podId, reasonToRestartPod);\n-            Admin adminClient = null;\n-            try {\n-                try {\n-                    adminClient = adminClient(podId);\n-                    Integer controller = controller(podId, adminClient, operationTimeoutMs, TimeUnit.MILLISECONDS, restartContext);\n-                    int stillRunning = podToContext.reduceValuesToInt(100, v -> v.promise.future().isComplete() ? 0 : 1,\n-                            0, Integer::sum);\n-                    if (controller == podId && stillRunning > 1) {\n-                        log.debug(\"Pod {} is controller and there are other pods to roll\", podId);\n-                        throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n-                    } else {\n-                        if (canRoll(adminClient, podId, 60_000, TimeUnit.MILLISECONDS)) {\n-                            log.debug(\"Pod {} can be rolled now\", podId);\n+        RestartPlan restartPlan = null;\n+        try {\n+            restartPlan = restartPlan(podId, pod);\n+            if (restartPlan.needsRestart || restartPlan.needsReconfig) {\n+\n+                Integer controller = controller(podId, restartPlan.adminClient(), operationTimeoutMs, TimeUnit.MILLISECONDS, restartContext);\n+                int stillRunning = podToContext.reduceValuesToInt(100, v -> v.promise.future().isComplete() ? 0 : 1,\n+                        0, Integer::sum);\n+                if (controller == podId && stillRunning > 1) {\n+                    log.debug(\"{}: Pod {} is controller and there are other pods to roll\", reconciliation, podId);\n+                    throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n+                } else {\n+                    if (canRoll(restartPlan.adminClient(), podId, 60_000, TimeUnit.MILLISECONDS)) {\n+                        // Check for rollability before trying a dynamic update so that if the dynamic update fails we can go to a full restart\n+                        boolean updatedDynamically;\n+\n+                        if (restartPlan.needsReconfig) {\n+                            try {\n+                                updateBrokerConfigDynamically(podId, restartPlan.adminClient(), restartPlan.diff);\n+                                updatedDynamically = true;\n+                            } catch (ForceableProblem e) {\n+                                log.debug(\"{}: Pod {} could not be updated dynamically ({}), will restart\", reconciliation, podId, e);\n+                                updatedDynamically = false;\n+                            }\n+                        } else {\n+                            updatedDynamically = false;\n+                        }\n+                        if (!updatedDynamically) {\n+                            log.debug(\"{}: Pod {} can be rolled now\", reconciliation, podId);\n                             restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n                         } else {\n-                            log.debug(\"Pod {} cannot be rolled right now\", podId);\n-                            throw new UnforceableProblem(\"Pod \" + podName(podId) + \" is currently not rollable\");\n+                            // TODO do we need some check here that the broker is still OK?\n+                            awaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n                         }\n+                    } else {\n+                        log.debug(\"{}: Pod {} cannot be rolled right now\", reconciliation, podId);\n+                        throw new UnforceableProblem(\"Pod \" + podName(podId) + \" is currently not rollable\");\n                     }\n-                } finally {\n-                    closeLoggingAnyError(adminClient);\n-                }\n-            } catch (ForceableProblem e) {\n-                if (restartContext.backOff.done() || e.forceNow) {\n-                    log.warn(\"Pod {} will be force-rolled\", podName(podId));\n-                    restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n-                } else {\n-                    throw e;\n                 }\n+            } else {\n+                // By testing even pods which don't need needsRestart for readiness we prevent successive reconciliations\n+                // from taking out a pod each time (due, e.g. to a configuration error).\n+                // We rely on Kube to try restarting such pods.\n+                log.debug(\"{}: Pod {} does not need to be restarted\", reconciliation, podId);\n+                log.debug(\"{}: Waiting for non-restarted pod {} to become ready\", reconciliation, podId);\n+                await(isReady(namespace, KafkaCluster.kafkaPodName(cluster, podId)), operationTimeoutMs, TimeUnit.MILLISECONDS, e -> new FatalProblem(\"Error while waiting for non-restarted pod \" + podName(podId) + \" to become ready\", e));\n+                log.debug(\"{}: Pod {} is now ready\", reconciliation, podId);\n+            }\n+        } catch (ForceableProblem e) {\n+            if (restartContext.backOff.done() || e.forceNow) {\n+                log.warn(\"{}: Pod {} will be force-rolled\", reconciliation, podName(podId));\n+                restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n+            } else {\n+                throw e;\n+            }\n+        } finally {\n+            if (restartPlan != null) {\n+                restartPlan.closeLoggingAnyError();\n             }\n-        } else {\n-            // By testing even pods which don't need restart for readiness we prevent successive reconciliations\n-            // from taking out a pod each time (due, e.g. to a configuration error).\n-            // We rely on Kube to try restarting such pods.\n-            log.debug(\"Pod {} does not need to be restarted\", podId);\n-            log.debug(\"Waiting for non-restarted pod {} to become ready\", podId);\n-            await(isReady(namespace, KafkaCluster.kafkaPodName(cluster, podId)), operationTimeoutMs, TimeUnit.MILLISECONDS, e -> new FatalProblem(\"Error while waiting for non-restarted pod \" + podName(podId) + \" to become ready\", e));\n-            log.debug(\"Pod {} is now ready\", podId);\n         }\n     }\n \n-    private void closeLoggingAnyError(Admin adminClient) {\n-        if (adminClient != null) {\n+    private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, InterruptedException, FatalProblem {\n+        List<String> reasonToRestartPod = podNeedsRestart.apply(pod);\n+        // Unless the annotation is present, check the pod is at least ready.\n+        boolean needsRestart = reasonToRestartPod != null && !reasonToRestartPod.isEmpty();\n+        KafkaBrokerConfigurationDiff diff = null;\n+        boolean needsReconfig = false;\n+        Admin adminClient = null;\n+        if (!needsRestart) {\n+            /*\n+            First time we do the restart, the pod is Unschedulable, the post-restart livesness check fails with a timeout and the reconciliation ends\n+              - lastProbeTime: null\n+                lastTransitionTime: \"2020-06-19T10:25:54Z\"\n+                message: '0/1 nodes are available: 1 Insufficient cpu.'\n+                reason: Unschedulable\n+                status: \"False\"\n+                type: PodScheduled\n+\n+            Next reconciliation there's no reason to restart, so we try to determine whether we need to reconfigure.\n+            We get CE. If this were not fatal we'd try the next broker (TEST FAIL).\n+            In the old algo, because no change was necessary we never tried to open an AC\n+\n+            When pod later becomes schedulable due to spec change we had a stale pod, so we don't open the AC here,\n+            but rather when getting the controller.\n+            If we treat CE as fatal there we never restart the pod, so it never gets fixed.\n+             */\n+\n+            // ConFigException is fatal here because otherwise we bring down the cluster by trying to roll other brokers\n             try {\n-                adminClient.close(Duration.ofMinutes(2));\n-            } catch (Exception e) {\n-                log.warn(\"Ignoring exception when closing admin client\", e);\n+                adminClient = adminClient(podId, true);\n+                diff = diff(adminClient, podId);\n+                if (diff.getDiffSize() > 0) {\n+                    if (diff.canBeUpdatedDynamically()) {\n+                        log.info(\"{}: Pod {} needs to be reconfigured.\", reconciliation, podId);\n+                        needsReconfig = true;\n+                    } else {\n+                        log.info(\"{}: Pod {} needs to be restarted, because reconfiguration cannot be done dynamically\", reconciliation, podId);\n+                        needsRestart = true;\n+                    }\n+                }\n+            } catch (RuntimeException | Error e) {\n+                if (adminClient != null) {\n+                    closeAdminClient(adminClient);\n+                }\n+                throw e;\n             }\n+        } else {\n+            log.info(\"{}: Pod {} needs to be restarted. Reason: {}\", reconciliation, podId, reasonToRestartPod);\n         }\n+        return new RestartPlan(adminClient, podId, needsRestart, needsReconfig, diff);\n+    }\n+\n+    /**\n+     * Returns a Future which completes with the config of the given broker.\n+     * @param ac The admin client\n+     * @param brokerId The id of the broker.\n+     * @return a Future which completes with the config of the given broker.\n+     */\n+    protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n+        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n+            30, TimeUnit.SECONDS,\n+            error -> new ForceableProblem(\"Error getting broker config\", error)\n+        );\n+    }\n+\n+    protected void updateBrokerConfigDynamically(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwMjQzMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452102430", "bodyText": "CE? ConfigException?", "author": "samuel-hawker", "createdAt": "2020-07-09T09:55:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -261,58 +322,149 @@ private void restartIfNecessary(int podId, RestartContext restartContext)\n             throw new UnforceableProblem(\"Error getting pod \" + podName(podId), e);\n         }\n \n-        String reasonToRestartPod = podNeedsRestart.apply(pod);\n-        if (reasonToRestartPod != null && !reasonToRestartPod.isEmpty()) {\n-            log.info(\"Pod {} needs to be restarted. Reason: {}\", podId, reasonToRestartPod);\n-            Admin adminClient = null;\n-            try {\n-                try {\n-                    adminClient = adminClient(podId);\n-                    Integer controller = controller(podId, adminClient, operationTimeoutMs, TimeUnit.MILLISECONDS, restartContext);\n-                    int stillRunning = podToContext.reduceValuesToInt(100, v -> v.promise.future().isComplete() ? 0 : 1,\n-                            0, Integer::sum);\n-                    if (controller == podId && stillRunning > 1) {\n-                        log.debug(\"Pod {} is controller and there are other pods to roll\", podId);\n-                        throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n-                    } else {\n-                        if (canRoll(adminClient, podId, 60_000, TimeUnit.MILLISECONDS)) {\n-                            log.debug(\"Pod {} can be rolled now\", podId);\n+        RestartPlan restartPlan = null;\n+        try {\n+            restartPlan = restartPlan(podId, pod);\n+            if (restartPlan.needsRestart || restartPlan.needsReconfig) {\n+\n+                Integer controller = controller(podId, restartPlan.adminClient(), operationTimeoutMs, TimeUnit.MILLISECONDS, restartContext);\n+                int stillRunning = podToContext.reduceValuesToInt(100, v -> v.promise.future().isComplete() ? 0 : 1,\n+                        0, Integer::sum);\n+                if (controller == podId && stillRunning > 1) {\n+                    log.debug(\"{}: Pod {} is controller and there are other pods to roll\", reconciliation, podId);\n+                    throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n+                } else {\n+                    if (canRoll(restartPlan.adminClient(), podId, 60_000, TimeUnit.MILLISECONDS)) {\n+                        // Check for rollability before trying a dynamic update so that if the dynamic update fails we can go to a full restart\n+                        boolean updatedDynamically;\n+\n+                        if (restartPlan.needsReconfig) {\n+                            try {\n+                                updateBrokerConfigDynamically(podId, restartPlan.adminClient(), restartPlan.diff);\n+                                updatedDynamically = true;\n+                            } catch (ForceableProblem e) {\n+                                log.debug(\"{}: Pod {} could not be updated dynamically ({}), will restart\", reconciliation, podId, e);\n+                                updatedDynamically = false;\n+                            }\n+                        } else {\n+                            updatedDynamically = false;\n+                        }\n+                        if (!updatedDynamically) {\n+                            log.debug(\"{}: Pod {} can be rolled now\", reconciliation, podId);\n                             restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n                         } else {\n-                            log.debug(\"Pod {} cannot be rolled right now\", podId);\n-                            throw new UnforceableProblem(\"Pod \" + podName(podId) + \" is currently not rollable\");\n+                            // TODO do we need some check here that the broker is still OK?\n+                            awaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n                         }\n+                    } else {\n+                        log.debug(\"{}: Pod {} cannot be rolled right now\", reconciliation, podId);\n+                        throw new UnforceableProblem(\"Pod \" + podName(podId) + \" is currently not rollable\");\n                     }\n-                } finally {\n-                    closeLoggingAnyError(adminClient);\n-                }\n-            } catch (ForceableProblem e) {\n-                if (restartContext.backOff.done() || e.forceNow) {\n-                    log.warn(\"Pod {} will be force-rolled\", podName(podId));\n-                    restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n-                } else {\n-                    throw e;\n                 }\n+            } else {\n+                // By testing even pods which don't need needsRestart for readiness we prevent successive reconciliations\n+                // from taking out a pod each time (due, e.g. to a configuration error).\n+                // We rely on Kube to try restarting such pods.\n+                log.debug(\"{}: Pod {} does not need to be restarted\", reconciliation, podId);\n+                log.debug(\"{}: Waiting for non-restarted pod {} to become ready\", reconciliation, podId);\n+                await(isReady(namespace, KafkaCluster.kafkaPodName(cluster, podId)), operationTimeoutMs, TimeUnit.MILLISECONDS, e -> new FatalProblem(\"Error while waiting for non-restarted pod \" + podName(podId) + \" to become ready\", e));\n+                log.debug(\"{}: Pod {} is now ready\", reconciliation, podId);\n+            }\n+        } catch (ForceableProblem e) {\n+            if (restartContext.backOff.done() || e.forceNow) {\n+                log.warn(\"{}: Pod {} will be force-rolled\", reconciliation, podName(podId));\n+                restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n+            } else {\n+                throw e;\n+            }\n+        } finally {\n+            if (restartPlan != null) {\n+                restartPlan.closeLoggingAnyError();\n             }\n-        } else {\n-            // By testing even pods which don't need restart for readiness we prevent successive reconciliations\n-            // from taking out a pod each time (due, e.g. to a configuration error).\n-            // We rely on Kube to try restarting such pods.\n-            log.debug(\"Pod {} does not need to be restarted\", podId);\n-            log.debug(\"Waiting for non-restarted pod {} to become ready\", podId);\n-            await(isReady(namespace, KafkaCluster.kafkaPodName(cluster, podId)), operationTimeoutMs, TimeUnit.MILLISECONDS, e -> new FatalProblem(\"Error while waiting for non-restarted pod \" + podName(podId) + \" to become ready\", e));\n-            log.debug(\"Pod {} is now ready\", podId);\n         }\n     }\n \n-    private void closeLoggingAnyError(Admin adminClient) {\n-        if (adminClient != null) {\n+    private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, InterruptedException, FatalProblem {\n+        List<String> reasonToRestartPod = podNeedsRestart.apply(pod);\n+        // Unless the annotation is present, check the pod is at least ready.\n+        boolean needsRestart = reasonToRestartPod != null && !reasonToRestartPod.isEmpty();\n+        KafkaBrokerConfigurationDiff diff = null;\n+        boolean needsReconfig = false;\n+        Admin adminClient = null;\n+        if (!needsRestart) {\n+            /*\n+            First time we do the restart, the pod is Unschedulable, the post-restart livesness check fails with a timeout and the reconciliation ends\n+              - lastProbeTime: null\n+                lastTransitionTime: \"2020-06-19T10:25:54Z\"\n+                message: '0/1 nodes are available: 1 Insufficient cpu.'\n+                reason: Unschedulable\n+                status: \"False\"\n+                type: PodScheduled\n+\n+            Next reconciliation there's no reason to restart, so we try to determine whether we need to reconfigure.\n+            We get CE. If this were not fatal we'd try the next broker (TEST FAIL).", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjExODIzNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452118235", "bodyText": "Most probably. @tombentley ^", "author": "sknot-rh", "createdAt": "2020-07-09T10:24:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwMjQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjIwNzQwOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452207409", "bodyText": "CE=ConfigException. The problem we found was:\n\nIf we treated a ConfigException from the AdminClient as fatal then we weren't able to proceed with the restarting in the case where a running Pod is just not responding\nIf we deleted a Pod and the new one was not schedulable and the resulting ConfigException was not fatal then we would end up taking down the cluster, one pod at a time, because\n\nThe solution I used here was to defer opening the AdminClient in the case where we already know we have reasons to restart (which includes the not schedulable test case). This meant there were two different code paths where we opened the AC and I could treat the exception differently in each. This was enough to pass the test case.\nBut thinking about it now, I realise that's flawed. The test causes the non-schedulability of a Pod by changing the CPU requirement to something unfulfillable. That requires a rolling restart because the Pod is stale wrt the STS pod template. But there are other causes of pods being unschedulable which don't result in stale pods, and those would go via the deferred AC creation path, and thus treat CE as non-fatal.\nI think the correct way to solve it would be:\n\nLook at the Pod status prior to deciding to open an AC. Unschedulable => fatal exception (which will exit the reconciliation)\nWe can then remove the deferred AC creation, since it's a complication we don't need.\n\nI feel stupid, because it's blindingly obvious when I put it like this \ud83d\ude1e @stanlyDoge is this a change you're happy to make (given this very brief description)?", "author": "tombentley", "createdAt": "2020-07-09T13:14:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwMjQzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwMjY0OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452102649", "bodyText": "Don't know if info about old algoirthm is necessary here..", "author": "samuel-hawker", "createdAt": "2020-07-09T09:55:32Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -261,58 +322,149 @@ private void restartIfNecessary(int podId, RestartContext restartContext)\n             throw new UnforceableProblem(\"Error getting pod \" + podName(podId), e);\n         }\n \n-        String reasonToRestartPod = podNeedsRestart.apply(pod);\n-        if (reasonToRestartPod != null && !reasonToRestartPod.isEmpty()) {\n-            log.info(\"Pod {} needs to be restarted. Reason: {}\", podId, reasonToRestartPod);\n-            Admin adminClient = null;\n-            try {\n-                try {\n-                    adminClient = adminClient(podId);\n-                    Integer controller = controller(podId, adminClient, operationTimeoutMs, TimeUnit.MILLISECONDS, restartContext);\n-                    int stillRunning = podToContext.reduceValuesToInt(100, v -> v.promise.future().isComplete() ? 0 : 1,\n-                            0, Integer::sum);\n-                    if (controller == podId && stillRunning > 1) {\n-                        log.debug(\"Pod {} is controller and there are other pods to roll\", podId);\n-                        throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n-                    } else {\n-                        if (canRoll(adminClient, podId, 60_000, TimeUnit.MILLISECONDS)) {\n-                            log.debug(\"Pod {} can be rolled now\", podId);\n+        RestartPlan restartPlan = null;\n+        try {\n+            restartPlan = restartPlan(podId, pod);\n+            if (restartPlan.needsRestart || restartPlan.needsReconfig) {\n+\n+                Integer controller = controller(podId, restartPlan.adminClient(), operationTimeoutMs, TimeUnit.MILLISECONDS, restartContext);\n+                int stillRunning = podToContext.reduceValuesToInt(100, v -> v.promise.future().isComplete() ? 0 : 1,\n+                        0, Integer::sum);\n+                if (controller == podId && stillRunning > 1) {\n+                    log.debug(\"{}: Pod {} is controller and there are other pods to roll\", reconciliation, podId);\n+                    throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n+                } else {\n+                    if (canRoll(restartPlan.adminClient(), podId, 60_000, TimeUnit.MILLISECONDS)) {\n+                        // Check for rollability before trying a dynamic update so that if the dynamic update fails we can go to a full restart\n+                        boolean updatedDynamically;\n+\n+                        if (restartPlan.needsReconfig) {\n+                            try {\n+                                updateBrokerConfigDynamically(podId, restartPlan.adminClient(), restartPlan.diff);\n+                                updatedDynamically = true;\n+                            } catch (ForceableProblem e) {\n+                                log.debug(\"{}: Pod {} could not be updated dynamically ({}), will restart\", reconciliation, podId, e);\n+                                updatedDynamically = false;\n+                            }\n+                        } else {\n+                            updatedDynamically = false;\n+                        }\n+                        if (!updatedDynamically) {\n+                            log.debug(\"{}: Pod {} can be rolled now\", reconciliation, podId);\n                             restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n                         } else {\n-                            log.debug(\"Pod {} cannot be rolled right now\", podId);\n-                            throw new UnforceableProblem(\"Pod \" + podName(podId) + \" is currently not rollable\");\n+                            // TODO do we need some check here that the broker is still OK?\n+                            awaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n                         }\n+                    } else {\n+                        log.debug(\"{}: Pod {} cannot be rolled right now\", reconciliation, podId);\n+                        throw new UnforceableProblem(\"Pod \" + podName(podId) + \" is currently not rollable\");\n                     }\n-                } finally {\n-                    closeLoggingAnyError(adminClient);\n-                }\n-            } catch (ForceableProblem e) {\n-                if (restartContext.backOff.done() || e.forceNow) {\n-                    log.warn(\"Pod {} will be force-rolled\", podName(podId));\n-                    restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n-                } else {\n-                    throw e;\n                 }\n+            } else {\n+                // By testing even pods which don't need needsRestart for readiness we prevent successive reconciliations\n+                // from taking out a pod each time (due, e.g. to a configuration error).\n+                // We rely on Kube to try restarting such pods.\n+                log.debug(\"{}: Pod {} does not need to be restarted\", reconciliation, podId);\n+                log.debug(\"{}: Waiting for non-restarted pod {} to become ready\", reconciliation, podId);\n+                await(isReady(namespace, KafkaCluster.kafkaPodName(cluster, podId)), operationTimeoutMs, TimeUnit.MILLISECONDS, e -> new FatalProblem(\"Error while waiting for non-restarted pod \" + podName(podId) + \" to become ready\", e));\n+                log.debug(\"{}: Pod {} is now ready\", reconciliation, podId);\n+            }\n+        } catch (ForceableProblem e) {\n+            if (restartContext.backOff.done() || e.forceNow) {\n+                log.warn(\"{}: Pod {} will be force-rolled\", reconciliation, podName(podId));\n+                restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n+            } else {\n+                throw e;\n+            }\n+        } finally {\n+            if (restartPlan != null) {\n+                restartPlan.closeLoggingAnyError();\n             }\n-        } else {\n-            // By testing even pods which don't need restart for readiness we prevent successive reconciliations\n-            // from taking out a pod each time (due, e.g. to a configuration error).\n-            // We rely on Kube to try restarting such pods.\n-            log.debug(\"Pod {} does not need to be restarted\", podId);\n-            log.debug(\"Waiting for non-restarted pod {} to become ready\", podId);\n-            await(isReady(namespace, KafkaCluster.kafkaPodName(cluster, podId)), operationTimeoutMs, TimeUnit.MILLISECONDS, e -> new FatalProblem(\"Error while waiting for non-restarted pod \" + podName(podId) + \" to become ready\", e));\n-            log.debug(\"Pod {} is now ready\", podId);\n         }\n     }\n \n-    private void closeLoggingAnyError(Admin adminClient) {\n-        if (adminClient != null) {\n+    private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, InterruptedException, FatalProblem {\n+        List<String> reasonToRestartPod = podNeedsRestart.apply(pod);\n+        // Unless the annotation is present, check the pod is at least ready.\n+        boolean needsRestart = reasonToRestartPod != null && !reasonToRestartPod.isEmpty();\n+        KafkaBrokerConfigurationDiff diff = null;\n+        boolean needsReconfig = false;\n+        Admin adminClient = null;\n+        if (!needsRestart) {\n+            /*\n+            First time we do the restart, the pod is Unschedulable, the post-restart livesness check fails with a timeout and the reconciliation ends\n+              - lastProbeTime: null\n+                lastTransitionTime: \"2020-06-19T10:25:54Z\"\n+                message: '0/1 nodes are available: 1 Insufficient cpu.'\n+                reason: Unschedulable\n+                status: \"False\"\n+                type: PodScheduled\n+\n+            Next reconciliation there's no reason to restart, so we try to determine whether we need to reconfigure.\n+            We get CE. If this were not fatal we'd try the next broker (TEST FAIL).\n+            In the old algo, because no change was necessary we never tried to open an AC", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwNDAyNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452104026", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static boolean isEntryCustom(String entryName, Map<String, ConfigModel> configModel) {\n          \n          \n            \n                private static boolean isCustomEntry(String entryName, Map<String, ConfigModel> configModel) {", "author": "samuel-hawker", "createdAt": "2020-07-09T09:57:51Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ The algorithm:\n+ *  1. Create a map from the supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map as the broker's {@code kafka_config_generator.sh} would\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Config brokerConfigs, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        this.diff = computeDiff(brokerId, desired, brokerConfigs, configModel);\n+    }\n+\n+    private static void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @param config Config which contains property\n+     * @return true if property in desired map has a default value\n+     */\n+    boolean isDesiredPropertyDefaultValue(String key, Config config) {\n+        Optional<ConfigEntry> entry = config.entries().stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        boolean result = true;\n+        for (AlterConfigOp entry : diff) {\n+            if (isEntryReadOnly(entry.configEntry())) {\n+                result = false;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return A map which can be used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return Collections.singletonMap(Util.getBrokersConfig(brokerId), diff);\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    private static boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @param brokerId id of compared broker\n+     * @param desired desired configuration\n+     * @param brokerConfigs current configuration\n+     * @param configModel default configuration for {@code kafkaVersion} of broker\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> computeDiff(int brokerId, String desired,\n+                                                         Config brokerConfigs,\n+                                                         Map<String, ConfigModel> configModel) {\n+        if (brokerConfigs == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(configModel, updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    updateOrAdd(entry.name(), configModel, desiredMap, updatedCE);\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    // entry is not in the current, it is added\n+                    updateOrAdd(pathValueWithoutSlash, configModel, desiredMap, updatedCE);\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+\n+        return updatedCE;\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, ConfigModel> configModel, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!isIgnorableProperty(propertyName)) {\n+            if (isEntryCustom(propertyName, configModel)) {\n+                log.trace(\"custom property {} has been updated/added {}\", propertyName, desiredMap.get(propertyName));\n+            } else {\n+                log.trace(\"property {} has been updated/added {}\", propertyName, desiredMap.get(propertyName));\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, desiredMap.get(propertyName)), AlterConfigOp.OpType.SET));\n+            }\n+        } else {\n+            log.trace(\"{} is ignorable, not considering\");\n+        }\n+    }\n+\n+    private static void removeProperty(Map<String, ConfigModel> configModel, Collection<AlterConfigOp> updatedCE, String pathValueWithoutSlash, ConfigEntry entry) {\n+        if (isEntryCustom(entry.name(), configModel)) {\n+            // we are deleting custom option\n+            log.trace(\"removing custom property {}\", entry.name());\n+        } else if (entry.isDefault()) {\n+            // entry is in current, is not in desired, is default -> it uses default value, skip.\n+            // Some default properties do not have set ConfigEntry.ConfigSource.DEFAULT_CONFIG and thus\n+            // we are removing property. That might cause redundant RU. To fix this we would have to add defaultValue\n+            // to the configModel\n+            log.trace(\"{} not set in desired, using default value\", entry.name());\n+        } else {\n+            // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+            // if the entry was custom, it should be deleted\n+            if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                log.trace(\"{} not set in desired, unsetting back to default {}\", entry.name(), \"deleted entry\");\n+            } else {\n+                log.trace(\"{} is ignorable, not considering as removed\");\n+            }\n+        }\n+    }\n+\n+    /**\n+     * @return whether the current config and the desired config are identical (thus, no update is necessary).\n+     */\n+    @Override\n+    public boolean isEmpty() {\n+        return  diff.size() == 0;\n+    }\n+\n+    /**\n+     * For some reason not all default entries have set ConfigEntry.ConfigSource.DEFAULT_CONFIG so we need to compare\n+     * @param entryName tested ConfigEntry\n+     * @param configModel configModel\n+     * @return true if entry is default (not custom)\n+     */\n+    private static boolean isEntryCustom(String entryName, Map<String, ConfigModel> configModel) {", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwNDUxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452104517", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * @return true if entry is default (not custom)\n          \n          \n            \n                 * @return true if entry is custom (not default)", "author": "samuel-hawker", "createdAt": "2020-07-09T09:58:39Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ The algorithm:\n+ *  1. Create a map from the supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map as the broker's {@code kafka_config_generator.sh} would\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Config brokerConfigs, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        this.diff = computeDiff(brokerId, desired, brokerConfigs, configModel);\n+    }\n+\n+    private static void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @param config Config which contains property\n+     * @return true if property in desired map has a default value\n+     */\n+    boolean isDesiredPropertyDefaultValue(String key, Config config) {\n+        Optional<ConfigEntry> entry = config.entries().stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        boolean result = true;\n+        for (AlterConfigOp entry : diff) {\n+            if (isEntryReadOnly(entry.configEntry())) {\n+                result = false;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return A map which can be used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return Collections.singletonMap(Util.getBrokersConfig(brokerId), diff);\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    private static boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @param brokerId id of compared broker\n+     * @param desired desired configuration\n+     * @param brokerConfigs current configuration\n+     * @param configModel default configuration for {@code kafkaVersion} of broker\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> computeDiff(int brokerId, String desired,\n+                                                         Config brokerConfigs,\n+                                                         Map<String, ConfigModel> configModel) {\n+        if (brokerConfigs == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n+\n+        JsonNode source = patchMapper().valueToTree(currentMap);\n+        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(configModel, updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    updateOrAdd(entry.name(), configModel, desiredMap, updatedCE);\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    // entry is not in the current, it is added\n+                    updateOrAdd(pathValueWithoutSlash, configModel, desiredMap, updatedCE);\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+\n+        return updatedCE;\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, ConfigModel> configModel, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!isIgnorableProperty(propertyName)) {\n+            if (isEntryCustom(propertyName, configModel)) {\n+                log.trace(\"custom property {} has been updated/added {}\", propertyName, desiredMap.get(propertyName));\n+            } else {\n+                log.trace(\"property {} has been updated/added {}\", propertyName, desiredMap.get(propertyName));\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, desiredMap.get(propertyName)), AlterConfigOp.OpType.SET));\n+            }\n+        } else {\n+            log.trace(\"{} is ignorable, not considering\");\n+        }\n+    }\n+\n+    private static void removeProperty(Map<String, ConfigModel> configModel, Collection<AlterConfigOp> updatedCE, String pathValueWithoutSlash, ConfigEntry entry) {\n+        if (isEntryCustom(entry.name(), configModel)) {\n+            // we are deleting custom option\n+            log.trace(\"removing custom property {}\", entry.name());\n+        } else if (entry.isDefault()) {\n+            // entry is in current, is not in desired, is default -> it uses default value, skip.\n+            // Some default properties do not have set ConfigEntry.ConfigSource.DEFAULT_CONFIG and thus\n+            // we are removing property. That might cause redundant RU. To fix this we would have to add defaultValue\n+            // to the configModel\n+            log.trace(\"{} not set in desired, using default value\", entry.name());\n+        } else {\n+            // entry is in current, is not in desired, is not default -> it was using non-default value and was removed\n+            // if the entry was custom, it should be deleted\n+            if (!isIgnorableProperty(pathValueWithoutSlash)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, null), AlterConfigOp.OpType.DELETE));\n+                log.trace(\"{} not set in desired, unsetting back to default {}\", entry.name(), \"deleted entry\");\n+            } else {\n+                log.trace(\"{} is ignorable, not considering as removed\");\n+            }\n+        }\n+    }\n+\n+    /**\n+     * @return whether the current config and the desired config are identical (thus, no update is necessary).\n+     */\n+    @Override\n+    public boolean isEmpty() {\n+        return  diff.size() == 0;\n+    }\n+\n+    /**\n+     * For some reason not all default entries have set ConfigEntry.ConfigSource.DEFAULT_CONFIG so we need to compare\n+     * @param entryName tested ConfigEntry\n+     * @param configModel configModel\n+     * @return true if entry is default (not custom)", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjEwNTQ0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r452105440", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static Collection<AlterConfigOp> computeDiff(int brokerId, String desired,\n          \n          \n            \n                private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n          \n      \n    \n    \n  \n\nI think that it is inferred that a call to diff would compute a diff, just a. recommended change", "author": "samuel-hawker", "createdAt": "2020-07-09T10:00:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,249 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.Scope;\n+import io.strimzi.operator.cluster.model.KafkaConfiguration;\n+import io.strimzi.operator.cluster.model.KafkaVersion;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+/**\n+ The algorithm:\n+ *  1. Create a map from the supplied desired String\n+ *  2. Fill placeholders (e.g. ${BROKER_ID}) in desired map as the broker's {@code kafka_config_generator.sh} would\n+ *  3a. Loop over all entries. If the entry is in IGNORABLE_PROPERTIES or entry.value from desired is equal to entry.value from current, do nothing\n+ *      else add it to the diff\n+ *  3b. If entry was removed from desired, add it to the diff with null value.\n+ *  3c. If custom entry was removed, delete property\n+ */\n+public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+    private Map<String, ConfigModel> configModel;\n+\n+    /**\n+     * These options are skipped because they contain placeholders\n+     * 909[1-4] is for skipping all (internal, plain, secured, external) listeners properties\n+     */\n+    public static final Pattern IGNORABLE_PROPERTIES = Pattern.compile(\n+            \"^(broker\\\\.id\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.keystore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.location\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.password\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.truststore\\\\.type\"\n+            + \"|.*-909[1-4]\\\\.ssl\\\\.client\\\\.auth\"\n+            + \"|.*-909[1-4]\\\\.scram-sha-512\\\\.sasl\\\\.jaas\\\\.config\"\n+            + \"|.*-909[1-4]\\\\.sasl\\\\.enabled\\\\.mechanisms\"\n+            + \"|advertised\\\\.listeners\"\n+            + \"|zookeeper\\\\.connect\"\n+            + \"|broker\\\\.rack)$\");\n+\n+    public KafkaBrokerConfigurationDiff(Config brokerConfigs, String desired, KafkaVersion kafkaVersion, int brokerId) {\n+        this.configModel = KafkaConfiguration.readConfigModel(kafkaVersion);\n+        this.brokerId = brokerId;\n+        this.diff = computeDiff(brokerId, desired, brokerConfigs, configModel);\n+    }\n+\n+    private static void fillPlaceholderValue(Map<String, String> orderedProperties, String placeholder, String value) {\n+        orderedProperties.entrySet().forEach(entry -> {\n+            entry.setValue(entry.getValue().replaceAll(\"\\\\$\\\\{\" + Pattern.quote(placeholder) + \"\\\\}\", value));\n+        });\n+    }\n+\n+    /**\n+     * Returns true if property in desired map has a default value\n+     * @param key name of the property\n+     * @param config Config which contains property\n+     * @return true if property in desired map has a default value\n+     */\n+    boolean isDesiredPropertyDefaultValue(String key, Config config) {\n+        Optional<ConfigEntry> entry = config.entries().stream().filter(configEntry -> configEntry.name().equals(key)).findFirst();\n+        if (entry.isPresent()) {\n+            return entry.get().isDefault();\n+        }\n+        return false;\n+    }\n+\n+    public boolean canBeUpdatedDynamically() {\n+        boolean result = true;\n+        for (AlterConfigOp entry : diff) {\n+            if (isEntryReadOnly(entry.configEntry())) {\n+                result = false;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    /**\n+     * @param entry tested ConfigEntry\n+     * @return true if the entry is READ_ONLY\n+     */\n+    private boolean isEntryReadOnly(ConfigEntry entry) {\n+        return configModel.get(entry.name()).getScope().equals(Scope.READ_ONLY);\n+    }\n+\n+    /**\n+     * @return A map which can be used for dynamic configuration of kafka broker\n+     */\n+    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n+        return Collections.singletonMap(Util.getBrokersConfig(brokerId), diff);\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    private static boolean isIgnorableProperty(String key) {\n+        return IGNORABLE_PROPERTIES.matcher(key).matches();\n+    }\n+\n+    /**\n+     * Computes diff between two maps. Entries in IGNORABLE_PROPERTIES are skipped\n+     * @param brokerId id of compared broker\n+     * @param desired desired configuration\n+     * @param brokerConfigs current configuration\n+     * @param configModel default configuration for {@code kafkaVersion} of broker\n+     * @return KafkaConfiguration containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> computeDiff(int brokerId, String desired,", "originalCommit": "cf38dc6fcbca159001401a2e954fdef92ea0bd24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5b836d561ecb375a6a3a4e18d30361250baa6a4d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5b836d561ecb375a6a3a4e18d30361250baa6a4d", "message": "remove restart plan class\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-09T17:27:26Z", "type": "forcePushed"}, {"oid": "2e21148d6b6f733648a0b2b497b69ee33395c69c", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2e21148d6b6f733648a0b2b497b69ee33395c69c", "message": "build fix\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-10T18:44:19Z", "type": "forcePushed"}, {"oid": "bef2937aca03853763bc0d804458ef12847bb47c", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bef2937aca03853763bc0d804458ef12847bb47c", "message": "build fix\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-10T18:47:27Z", "type": "forcePushed"}, {"oid": "d78bef926e9e3216c75fcfd8a3c3f16daa1ce62a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d78bef926e9e3216c75fcfd8a3c3f16daa1ce62a", "message": "java11 and spotbugs\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-13T10:48:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMDk4Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r454910983", "bodyText": "I think it looks a bit weird when it both generates the configuration into some internal field but also returns it. Why do we need this?", "author": "scholzj", "createdAt": "2020-07-15T09:16:55Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/KafkaCluster.java", "diffHunk": "@@ -2514,6 +2515,12 @@ private String generateBrokerConfiguration()   {\n                 .withCruiseControl(cluster, cruiseControlSpec, ccNumPartitions, ccReplicationFactor)\n                 .withUserConfiguration(configuration)\n                 .build().trim();\n+        this.brokersConfiguration = result;\n+        return result;", "originalCommit": "d78bef926e9e3216c75fcfd8a3c3f16daa1ce62a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2MDA3Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r454960077", "bodyText": "How does this differ from zkCurrentReplicas? If it has the same meaning, it should follow the same naming I think. Having it as Integer with null value to identify no cluster might be also better if we in the future move to allowing 0 replicas.", "author": "scholzj", "createdAt": "2020-07-15T10:44:32Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -387,6 +396,7 @@ ReconciliationState createReconciliationState(Reconciliation reconciliation, Kaf\n         private Integer zkCurrentReplicas = null;\n \n         private KafkaCluster kafkaCluster = null;\n+        private int oldKafkaReplicas;", "originalCommit": "d78bef926e9e3216c75fcfd8a3c3f16daa1ce62a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5Nzg4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r454997887", "bodyText": "What about this TODO? Should it be implemented? Or removed?", "author": "scholzj", "createdAt": "2020-07-15T12:00:39Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -261,58 +315,147 @@ private void restartIfNecessary(int podId, RestartContext restartContext)\n             throw new UnforceableProblem(\"Error getting pod \" + podName(podId), e);\n         }\n \n-        String reasonToRestartPod = podNeedsRestart.apply(pod);\n-        if (reasonToRestartPod != null && !reasonToRestartPod.isEmpty()) {\n-            log.info(\"Pod {} needs to be restarted. Reason: {}\", podId, reasonToRestartPod);\n-            Admin adminClient = null;\n-            try {\n-                try {\n-                    adminClient = adminClient(podId);\n-                    Integer controller = controller(podId, adminClient, operationTimeoutMs, TimeUnit.MILLISECONDS, restartContext);\n-                    int stillRunning = podToContext.reduceValuesToInt(100, v -> v.promise.future().isComplete() ? 0 : 1,\n-                            0, Integer::sum);\n-                    if (controller == podId && stillRunning > 1) {\n-                        log.debug(\"Pod {} is controller and there are other pods to roll\", podId);\n-                        throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n-                    } else {\n-                        if (canRoll(adminClient, podId, 60_000, TimeUnit.MILLISECONDS)) {\n-                            log.debug(\"Pod {} can be rolled now\", podId);\n+        try (RestartPlan restartPlan = restartPlan(podId, pod)) {\n+            if (restartPlan.needsRestart || restartPlan.needsReconfig) {\n+                if (deferController(podId, restartContext, restartPlan)) {\n+                    log.debug(\"{}: Pod {} is controller and there are other pods to roll\", reconciliation, podId);\n+                    throw new ForceableProblem(\"Pod \" + podName(podId) + \" is currently the controller and there are other pods still to roll\");\n+                } else {\n+                    if (canRoll(restartPlan.adminClient(), podId, 60_000, TimeUnit.MILLISECONDS)) {\n+                        // Check for rollability before trying a dynamic update so that if the dynamic update fails we can go to a full restart\n+                        if (!maybeDynamicUpdateBrokerConfig(podId, restartPlan)) {\n+                            log.debug(\"{}: Pod {} can be rolled now\", reconciliation, podId);\n                             restartAndAwaitReadiness(pod, operationTimeoutMs, TimeUnit.MILLISECONDS);\n                         } else {\n-                            log.debug(\"Pod {} cannot be rolled right now\", podId);\n-                            throw new UnforceableProblem(\"Pod \" + podName(podId) + \" is currently not rollable\");\n+                            // TODO do we need some check here that the broker is still OK?", "originalCommit": "d78bef926e9e3216c75fcfd8a3c3f16daa1ce62a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA1NTM4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r455055382", "bodyText": "@tombentley ^", "author": "sknot-rh", "createdAt": "2020-07-15T13:34:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5Nzg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA3NDE4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r455074184", "bodyText": "I do think that we might want to have some deeper insight into whether the dynamic update has borked the broker. But it's difficult to really say for sure because I don't know what configs that are user-owned are dynamically updateable, and thus know which might come with the possibility of borkage.\nIn the longer term it's also possible we use this mechanism for certain broker-owned configs, where there's significantly higher chance of borkage.\n\nBut your question is really about the // TODO, not questioning what the // TODO is about. @stanlyDoge can you open an issue for this?", "author": "tombentley", "createdAt": "2020-07-15T14:00:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5Nzg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMDg5NTAwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2389#discussion_r610895004", "bodyText": "@tombentley Concerning the TODO for this, the brokerConfig is checked during the restartPlan creation a bit before this if/else statement within the same method. Is that not sufficient for what's being asked? Are you looking for that code to be reused again in that else statement? If not, do you have an example you could point me too for some clarification on this issue?", "author": "npecka", "createdAt": "2021-04-09T20:43:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5Nzg4Nw=="}], "type": "inlineReview"}, {"oid": "ff8813feb862ecee12fb23dc80a3fab042ee616f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ff8813feb862ecee12fb23dc80a3fab042ee616f", "message": "squash after rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "0082deecfd2c2f2506f960b3f5d1baabe75c322d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0082deecfd2c2f2506f960b3f5d1baabe75c322d", "message": "merge tom's work\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "b9ba125213723c3912b183a2150cdb745c3a1596", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b9ba125213723c3912b183a2150cdb745c3a1596", "message": "dynamic config\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "093d6297f3e31e4e1d872406b88517e7872caa9a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/093d6297f3e31e4e1d872406b88517e7872caa9a", "message": "simplification\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "6fe2d2cee281de69552bee8bbe94171ff4222ed9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6fe2d2cee281de69552bee8bbe94171ff4222ed9", "message": "fix situation with pending pods\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "b52a5ae78531ed0acef0cf03e6f4fd86a544d2e8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b52a5ae78531ed0acef0cf03e6f4fd86a544d2e8", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "5bc77773729c33f8b70bca1a2194cc912424a482", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5bc77773729c33f8b70bca1a2194cc912424a482", "message": "rebase\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "f62acf43eec9d4d283632d4c5fd9329103fefc11", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f62acf43eec9d4d283632d4c5fd9329103fefc11", "message": "roll stucked pods\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "1d2703ed50f6b4c1a6ecb43180b0a2113e11f8b9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/1d2703ed50f6b4c1a6ecb43180b0a2113e11f8b9", "message": "Tom's comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "ef1de3c01accb3262d12d3edd08fadffd8797a66", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ef1de3c01accb3262d12d3edd08fadffd8797a66", "message": "transfer kafkaFuture to vertx one\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "602c79c971e1e5445639131b4848185cc38190b3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/602c79c971e1e5445639131b4848185cc38190b3", "message": "Tom is the best\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "6583cb6c9ed84293e9bcb9306bdea368991527c3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6583cb6c9ed84293e9bcb9306bdea368991527c3", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "9f4225575e61cd4cb57b1660ab3fac5252504970", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9f4225575e61cd4cb57b1660ab3fac5252504970", "message": "Restart reasons\n\nSigned-off-by: Tom Bentley <tbentley@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "d35b6cf57eba6ef7d8dd52fbccbc8ca403bd84ba", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d35b6cf57eba6ef7d8dd52fbccbc8ca403bd84ba", "message": "Move dynamic update logic into the KafkaRoller\n\nSigned-off-by: Tom Bentley <tbentley@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "2eeebdd9b29069f5681832f9d23cb458bd53abad", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2eeebdd9b29069f5681832f9d23cb458bd53abad", "message": "Fix conflict\n\nSigned-off-by: Tom Bentley <tbentley@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}, {"oid": "e9f5069915e9967352de0444bc292ab096075d55", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e9f5069915e9967352de0444bc292ab096075d55", "message": "comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-07-15T18:38:49Z", "type": "commit"}]}