{"pr_number": 2185, "pr_title": "HADOOP-15891. provide Regex Based Mount Point In Inode Tree", "pr_createdAt": "2020-08-03T04:24:54Z", "pr_url": "https://github.com/apache/hadoop/pull/2185", "timeline": [{"oid": "65c151a5680e015d6d2a143a99fe197f92b6e1da", "url": "https://github.com/apache/hadoop/commit/65c151a5680e015d6d2a143a99fe197f92b6e1da", "message": "HADOOP-15891: provide Regex Based Mount Point In Inode Tree\n\nDifferential Revision: https://phabricator.twitter.biz/D526212", "committedDate": "2020-08-04T00:49:00Z", "type": "forcePushed"}, {"oid": "1fe566506bf0978a39b49194e622da52ea70cbb4", "url": "https://github.com/apache/hadoop/commit/1fe566506bf0978a39b49194e622da52ea70cbb4", "message": "HADOOP-15891: provide Regex Based Mount Point In Inode Tree\n\nDifferential Revision: https://phabricator.twitter.biz/D526212", "committedDate": "2020-08-04T03:11:52Z", "type": "forcePushed"}, {"oid": "3ae248e04f985df5c838720d6f798191ffc0b465", "url": "https://github.com/apache/hadoop/commit/3ae248e04f985df5c838720d6f798191ffc0b465", "message": "HADOOP-15891: provide Regex Based Mount Point In Inode Tree\n\nDifferential Revision: https://phabricator.twitter.biz/D526212", "committedDate": "2020-08-04T17:53:19Z", "type": "forcePushed"}, {"oid": "44f3909fca1d3a0e10b83d9c43a241022a4b0edf", "url": "https://github.com/apache/hadoop/commit/44f3909fca1d3a0e10b83d9c43a241022a4b0edf", "message": "HADOOP-15891: provide Regex Based Mount Point In Inode Tree\n\nDifferential Revision: https://phabricator.twitter.biz/D526212", "committedDate": "2020-08-04T23:27:27Z", "type": "forcePushed"}, {"oid": "d3e5b8331000f9189c7d2073d08a281532f36a8d", "url": "https://github.com/apache/hadoop/commit/d3e5b8331000f9189c7d2073d08a281532f36a8d", "message": "HADOOP-15891: provide Regex Based Mount Point In Inode Tree\n\nDifferential Revision: https://phabricator.twitter.biz/D526212", "committedDate": "2020-08-06T18:42:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE3NjY2MQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r467176661", "bodyText": "It looks complex. But the major change here is simple.\nBefore:\nif (key.startsWith(mountTablePrefix)) {\n// hundred line of code\n}\nAfter:\nif (!key.startsWith(mountTablePrefix)) {\ncontinue\n}\n// hundred line of code", "author": "JohnZZGithub", "createdAt": "2020-08-07T17:33:57Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -486,84 +506,113 @@ protected InodeTree(final Configuration config, final String viewName,\n     final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n     for (Entry<String, String> si : config) {\n       final String key = si.getKey();\n-      if (key.startsWith(mountTablePrefix)) {\n-        gotMountTableEntry = true;\n-        LinkType linkType;\n-        String src = key.substring(mountTablePrefix.length());\n-        String settings = null;\n-        if (src.startsWith(linkPrefix)) {\n-          src = src.substring(linkPrefix.length());\n-          if (src.equals(SlashPath.toString())) {\n-            throw new UnsupportedFileSystemException(\"Unexpected mount table \"\n-                + \"link entry '\" + key + \"'. Use \"\n-                + Constants.CONFIG_VIEWFS_LINK_MERGE_SLASH  + \" instead!\");\n-          }\n-          linkType = LinkType.SINGLE;\n-        } else if (src.startsWith(linkFallbackPrefix)) {\n-          if (src.length() != linkFallbackPrefix.length()) {\n-            throw new IOException(\"ViewFs: Mount points initialization error.\" +\n-                \" Invalid \" + Constants.CONFIG_VIEWFS_LINK_FALLBACK +\n-                \" entry in config: \" + src);\n-          }\n-          linkType = LinkType.SINGLE_FALLBACK;\n-        } else if (src.startsWith(linkMergePrefix)) { // A merge link\n-          src = src.substring(linkMergePrefix.length());\n-          linkType = LinkType.MERGE;\n-        } else if (src.startsWith(linkMergeSlashPrefix)) {\n-          // This is a LinkMergeSlash entry. This entry should\n-          // not have any additional source path.\n-          if (src.length() != linkMergeSlashPrefix.length()) {\n-            throw new IOException(\"ViewFs: Mount points initialization error.\" +\n-                \" Invalid \" + Constants.CONFIG_VIEWFS_LINK_MERGE_SLASH +\n-                \" entry in config: \" + src);\n-          }\n-          linkType = LinkType.MERGE_SLASH;\n-        } else if (src.startsWith(Constants.CONFIG_VIEWFS_LINK_NFLY)) {\n-          // prefix.settings.src\n-          src = src.substring(Constants.CONFIG_VIEWFS_LINK_NFLY.length() + 1);\n-          // settings.src\n-          settings = src.substring(0, src.indexOf('.'));\n-          // settings\n-\n-          // settings.src\n-          src = src.substring(settings.length() + 1);\n-          // src\n-\n-          linkType = LinkType.NFLY;\n-        } else if (src.startsWith(Constants.CONFIG_VIEWFS_HOMEDIR)) {\n-          // ignore - we set home dir from config\n-          continue;\n-        } else {\n-          throw new IOException(\"ViewFs: Cannot initialize: Invalid entry in \" +\n-              \"Mount table in config: \" + src);\n-        }\n+      if (!key.startsWith(mountTablePrefix)) {", "originalCommit": "d3e5b8331000f9189c7d2073d08a281532f36a8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c58b945a32a978c470be07ef47e9ed7edfff428c", "url": "https://github.com/apache/hadoop/commit/c58b945a32a978c470be07ef47e9ed7edfff428c", "message": "HADOOP-15891: provide Regex Based Mount Point In Inode Tree\n\nDifferential Revision: https://phabricator.twitter.biz/D526212", "committedDate": "2020-08-10T21:03:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcxMDcwNQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r467710705", "bodyText": "you mean fs.viewfs.mounttable.<mnt_tbl_name>.linkRegex ?", "author": "umamaheswararao", "createdAt": "2020-08-10T06:36:44Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -226,7 +239,14 @@ void addLink(final String pathComponent, final INodeLink<T> link)\n      * Config prefix: fs.viewfs.mounttable.<mnt_tbl_name>.linkNfly\n      * Refer: {@link Constants#CONFIG_VIEWFS_LINK_NFLY}\n      */\n-    NFLY;\n+    NFLY,\n+    /**\n+     * Link entry which source are regex exrepssions and target refer matched\n+     * group from source\n+     * Config prefix: fs.viewfs.mounttable.<mnt_tbl_name>.linkMerge", "originalCommit": "d3e5b8331000f9189c7d2073d08a281532f36a8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM2MDczNA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r472360734", "bodyText": "Yes, nice catch.", "author": "JohnZZGithub", "createdAt": "2020-08-18T17:24:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcxMDcwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI2ODI2NQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471268265", "bodyText": "Below params javadoc does not help anything I think. If you want to add, please add description( most of that params seems self explanatory ), otherwise just remove params part.", "author": "umamaheswararao", "createdAt": "2020-08-17T06:59:51Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "diffHunk": "@@ -166,6 +166,41 @@ public static void addLinkNfly(final Configuration conf, final String src,\n     addLinkNfly(conf, getDefaultMountTableName(conf), src, null, targets);\n   }\n \n+\n+  /**\n+   * Add a LinkRegex to the config for the specified mount table.", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNTgyOA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471825828", "bodyText": "Make sense, thanks.", "author": "JohnZZGithub", "createdAt": "2020-08-17T23:18:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI2ODI2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwNjE3NQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471306175", "bodyText": "below key construction can be outside of lock?", "author": "umamaheswararao", "createdAt": "2020-08-17T07:57:54Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+\n+  private void addResolveResultToCache(final String pathStr,\n+      final Boolean resolveLastComponent,\n+      final ResolveResult<T> resolveResult) {\n+    try {\n+      cacheRWLock.writeLock().lock();", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwNjM4Nw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471306387", "bodyText": "same as above.", "author": "umamaheswararao", "createdAt": "2020-08-17T07:58:20Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+\n+  private void addResolveResultToCache(final String pathStr,\n+      final Boolean resolveLastComponent,\n+      final ResolveResult<T> resolveResult) {\n+    try {\n+      cacheRWLock.writeLock().lock();\n+      String key = getResolveCacheKeyStr(pathStr, resolveLastComponent);\n+      pathResolutionCache.put(key, resolveResult);\n+    } finally {\n+      cacheRWLock.writeLock().unlock();\n+    }\n+  }\n+\n+  private ResolveResult<T> getResolveResultFromCache(final String pathStr,\n+      final Boolean resolveLastComponent) {\n+    if (pathResolutionCacheCapacity <= 0) {\n+      return null;\n+    }\n+    try {\n+      cacheRWLock.readLock().lock();", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNjQwMw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471316403", "bodyText": "Please move annotation to separate line below.", "author": "umamaheswararao", "createdAt": "2020-08-17T08:17:27Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+\n+  private void addResolveResultToCache(final String pathStr,\n+      final Boolean resolveLastComponent,\n+      final ResolveResult<T> resolveResult) {\n+    try {\n+      cacheRWLock.writeLock().lock();\n+      String key = getResolveCacheKeyStr(pathStr, resolveLastComponent);\n+      pathResolutionCache.put(key, resolveResult);\n+    } finally {\n+      cacheRWLock.writeLock().unlock();\n+    }\n+  }\n+\n+  private ResolveResult<T> getResolveResultFromCache(final String pathStr,\n+      final Boolean resolveLastComponent) {\n+    if (pathResolutionCacheCapacity <= 0) {\n+      return null;\n+    }\n+    try {\n+      cacheRWLock.readLock().lock();\n+      String key = getResolveCacheKeyStr(pathStr, resolveLastComponent);\n+      return (ResolveResult<T>) pathResolutionCache.get(key);\n+    } finally {\n+      cacheRWLock.readLock().unlock();\n+    }\n+  }\n+\n+  public static String getResolveCacheKeyStr(final String path,\n+      Boolean resolveLastComp) {\n+    return path + \",resolveLastComp\" + resolveLastComp;\n+  }\n+", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxODI3MA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471318270", "bodyText": "nit: typo: give space after comma", "author": "umamaheswararao", "createdAt": "2020-08-17T08:20:56Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.InodeTree.SlashPath;\n+\n+/**\n+ * Regex mount point is build to implement regex based mount point.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPoint<T> {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(RegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private String srcPathRegex;\n+  private Pattern srcPattern;\n+  private String dstPath;\n+  private String interceptorSettingsString;\n+  private List<RegexMountPointInterceptor> interceptorList;\n+\n+  public static final String SETTING_SRCREGEX_SEP = \"#.\";\n+  public static final char INTERCEPTOR_SEP = ';';\n+  public static final char INTERCEPTOR_INTERNAL_SEP = ':';\n+  // ${var},$var\n+  public static final Pattern VAR_PATTERN_IN_DEST =\n+      Pattern.compile(\"\\\\$((\\\\{\\\\w+\\\\})|(\\\\w+))\");\n+\n+  // key => $key or key = > ${key}\n+  private Map<String, Set<String>> varInDestPathMap;\n+\n+  public Map<String, Set<String>> getVarInDestPathMap() {\n+    return varInDestPathMap;\n+  }\n+\n+  RegexMountPoint(InodeTree inodeTree, String sourcePathRegex,\n+      String destPath, String settingsStr) {\n+    this.inodeTree = inodeTree;\n+    this.srcPathRegex = sourcePathRegex;\n+    this.dstPath = destPath;\n+    this.interceptorSettingsString = settingsStr;\n+    this.interceptorList = new ArrayList<>();\n+  }\n+\n+  /**\n+   * Initialize regex mount point.\n+   *\n+   * @throws IOException\n+   */\n+  public void initialize() throws IOException {\n+    try {\n+      srcPattern = Pattern.compile(srcPathRegex);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Failed to initialized mount point due to bad src path regex:\"\n+              + srcPathRegex + \", dstPath:\" + dstPath, ex);\n+    }\n+    varInDestPathMap = getVarListInString(dstPath);\n+    initializeInterceptors();\n+  }\n+\n+  private void initializeInterceptors() throws IOException {\n+    if (interceptorSettingsString == null\n+        || interceptorSettingsString.isEmpty()) {\n+      return;\n+    }\n+    String[] interceptorStrArray =\n+        StringUtils.split(interceptorSettingsString, INTERCEPTOR_SEP);\n+    for (String interceptorStr : interceptorStrArray) {\n+      RegexMountPointInterceptor interceptor =\n+          RegexMountPointInterceptorFactory.create(interceptorStr);\n+      if (interceptor == null) {\n+        throw new IOException(\n+            \"Illegal settings String \" + interceptorSettingsString);\n+      }\n+      interceptor.initialize();\n+      interceptorList.add(interceptor);\n+    }\n+  }\n+\n+  /**\n+   * Get $var1 and $var2 style variables in string.\n+   *\n+   * @param input\n+   * @return\n+   */\n+  public static Map<String, Set<String>> getVarListInString(String input) {\n+    Map<String, Set<String>> varMap = new HashMap<>();\n+    Matcher matcher = VAR_PATTERN_IN_DEST.matcher(input);\n+    while (matcher.find()) {\n+      // $var or ${var}\n+      String varName = matcher.group(0);\n+      // var or {var}\n+      String strippedVarName = matcher.group(1);\n+      if (strippedVarName.startsWith(\"{\")) {\n+        // {varName} = > varName\n+        strippedVarName =\n+            strippedVarName.substring(1, strippedVarName.length() - 1);\n+      }\n+      varMap.putIfAbsent(strippedVarName, new HashSet<>());\n+      varMap.get(strippedVarName).add(varName);\n+    }\n+    return varMap;\n+  }\n+\n+  public String getSrcPathRegex() {\n+    return srcPathRegex;\n+  }\n+\n+  public Pattern getSrcPattern() {\n+    return srcPattern;\n+  }\n+\n+  public String getDstPath() {\n+    return dstPath;\n+  }\n+\n+  public static Pattern getVarPatternInDest() {\n+    return VAR_PATTERN_IN_DEST;\n+  }\n+\n+  /**\n+   * Get resolved path from regex mount points.\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  public InodeTree.ResolveResult<T> resolve(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    String pathStrToResolve = srcPath;\n+    if (!resolveLastComponent) {\n+      int lastSlashIndex = srcPath.lastIndexOf(SlashPath.toString());\n+      if (lastSlashIndex == -1) {\n+        return null;\n+      }\n+      pathStrToResolve = srcPath.substring(0, lastSlashIndex);\n+    }\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      pathStrToResolve = interceptor.interceptSource(pathStrToResolve);\n+    }", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTYwOTYyMA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471609620", "bodyText": "Since this patch already big and this cache improvement seems to be generic for all cases, can we file separate Sub JIRA for this cache part?", "author": "umamaheswararao", "createdAt": "2020-08-17T16:51:50Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java", "diffHunk": "@@ -646,102 +714,222 @@ boolean isInternalDir() {\n   }\n \n   /**\n-   * Resolve the pathname p relative to root InodeDir\n+   * Resolve the pathname p relative to root InodeDir.\n    * @param p - input path\n    * @param resolveLastComponent\n    * @return ResolveResult which allows further resolution of the remaining path\n    * @throws FileNotFoundException\n    */\n   ResolveResult<T> resolve(final String p, final boolean resolveLastComponent)\n       throws FileNotFoundException {\n-    String[] path = breakIntoPathComponents(p);\n-    if (path.length <= 1) { // special case for when path is \"/\"\n-      T targetFs = root.isInternalDir() ?\n-          getRootDir().getInternalDirFs() : getRootLink().getTargetFileSystem();\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-          targetFs, root.fullPath, SlashPath);\n-      return res;\n-    }\n+    ResolveResult<T> resolveResult = null;\n+    resolveResult = getResolveResultFromCache(p, resolveLastComponent);\n+    if (resolveResult != null) {\n+      return resolveResult;\n+    }\n+\n+    try {\n+      String[] path = breakIntoPathComponents(p);\n+      if (path.length <= 1) { // special case for when path is \"/\"\n+        T targetFs = root.isInternalDir() ?\n+            getRootDir().getInternalDirFs()\n+            : getRootLink().getTargetFileSystem();\n+        resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+            targetFs, root.fullPath, SlashPath);\n+        return resolveResult;\n+      }\n \n-    /**\n-     * linkMergeSlash has been configured. The root of this mount table has\n-     * been linked to the root directory of a file system.\n-     * The first non-slash path component should be name of the mount table.\n-     */\n-    if (root.isLink()) {\n-      Path remainingPath;\n-      StringBuilder remainingPathStr = new StringBuilder();\n-      // ignore first slash\n-      for (int i = 1; i < path.length; i++) {\n-        remainingPathStr.append(\"/\").append(path[i]);\n+      /**\n+       * linkMergeSlash has been configured. The root of this mount table has\n+       * been linked to the root directory of a file system.\n+       * The first non-slash path component should be name of the mount table.\n+       */\n+      if (root.isLink()) {\n+        Path remainingPath;\n+        StringBuilder remainingPathStr = new StringBuilder();\n+        // ignore first slash\n+        for (int i = 1; i < path.length; i++) {\n+          remainingPathStr.append(\"/\").append(path[i]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+        resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+            getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n-      ResolveResult<T> res = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-          getRootLink().getTargetFileSystem(), root.fullPath, remainingPath);\n-      return res;\n-    }\n-    Preconditions.checkState(root.isInternalDir());\n-    INodeDir<T> curInode = getRootDir();\n+      Preconditions.checkState(root.isInternalDir());\n+      INodeDir<T> curInode = getRootDir();\n \n-    int i;\n-    // ignore first slash\n-    for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n-      INode<T> nextInode = curInode.resolveInternal(path[i]);\n-      if (nextInode == null) {\n-        if (hasFallbackLink()) {\n-          return new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-              getRootFallbackLink().getTargetFileSystem(),\n-              root.fullPath, new Path(p));\n-        } else {\n-          StringBuilder failedAt = new StringBuilder(path[0]);\n-          for (int j = 1; j <= i; ++j) {\n-            failedAt.append('/').append(path[j]);\n+      // Try to resolve path in the regex mount point\n+      resolveResult = tryResolveInRegexMountpoint(p, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n+      }\n+\n+      int i;\n+      // ignore first slash\n+      for (i = 1; i < path.length - (resolveLastComponent ? 0 : 1); i++) {\n+        INode<T> nextInode = curInode.resolveInternal(path[i]);\n+        if (nextInode == null) {\n+          if (hasFallbackLink()) {\n+            resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+                getRootFallbackLink().getTargetFileSystem(), root.fullPath,\n+                new Path(p));\n+            return resolveResult;\n+          } else {\n+            StringBuilder failedAt = new StringBuilder(path[0]);\n+            for (int j = 1; j <= i; ++j) {\n+              failedAt.append('/').append(path[j]);\n+            }\n+            throw (new FileNotFoundException(\n+                \"File/Directory does not exist: \" + failedAt.toString()));\n           }\n-          throw (new FileNotFoundException(\n-              \"File/Directory does not exist: \" + failedAt.toString()));\n         }\n-      }\n \n-      if (nextInode.isLink()) {\n-        final INodeLink<T> link = (INodeLink<T>) nextInode;\n-        final Path remainingPath;\n-        if (i >= path.length - 1) {\n-          remainingPath = SlashPath;\n-        } else {\n-          StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i + 1]);\n-          for (int j = i + 2; j < path.length; ++j) {\n-            remainingPathStr.append('/').append(path[j]);\n+        if (nextInode.isLink()) {\n+          final INodeLink<T> link = (INodeLink<T>) nextInode;\n+          final Path remainingPath;\n+          if (i >= path.length - 1) {\n+            remainingPath = SlashPath;\n+          } else {\n+            StringBuilder remainingPathStr =\n+                new StringBuilder(\"/\" + path[i + 1]);\n+            for (int j = i + 2; j < path.length; ++j) {\n+              remainingPathStr.append('/').append(path[j]);\n+            }\n+            remainingPath = new Path(remainingPathStr.toString());\n           }\n-          remainingPath = new Path(remainingPathStr.toString());\n+          resolveResult = new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n+              link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n+          return resolveResult;\n+        } else if (nextInode.isInternalDir()) {\n+          curInode = (INodeDir<T>) nextInode;\n         }\n-        final ResolveResult<T> res =\n-            new ResolveResult<T>(ResultKind.EXTERNAL_DIR,\n-                link.getTargetFileSystem(), nextInode.fullPath, remainingPath);\n-        return res;\n-      } else if (nextInode.isInternalDir()) {\n-        curInode = (INodeDir<T>) nextInode;\n+      }\n+\n+      // We have resolved to an internal dir in mount table.\n+      Path remainingPath;\n+      if (resolveLastComponent) {\n+        remainingPath = SlashPath;\n+      } else {\n+        // note we have taken care of when path is \"/\" above\n+        // for internal dirs rem-path does not start with / since the lookup\n+        // that follows will do a children.get(remaningPath) and will have to\n+        // strip-out the initial /\n+        StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n+        for (int j = i + 1; j < path.length; ++j) {\n+          remainingPathStr.append('/').append(path[j]);\n+        }\n+        remainingPath = new Path(remainingPathStr.toString());\n+      }\n+      resolveResult = new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n+          curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n+      return resolveResult;\n+    } finally {\n+      if (pathResolutionCacheCapacity > 0 && resolveResult != null) {\n+        addResolveResultToCache(p, resolveLastComponent, resolveResult);\n       }\n     }\n+  }\n \n-    // We have resolved to an internal dir in mount table.\n-    Path remainingPath;\n-    if (resolveLastComponent) {\n-      remainingPath = SlashPath;\n-    } else {\n-      // note we have taken care of when path is \"/\" above\n-      // for internal dirs rem-path does not start with / since the lookup\n-      // that follows will do a children.get(remaningPath) and will have to\n-      // strip-out the initial /\n-      StringBuilder remainingPathStr = new StringBuilder(\"/\" + path[i]);\n-      for (int j = i + 1; j < path.length; ++j) {\n-        remainingPathStr.append('/').append(path[j]);\n+  /**\n+   * Walk through all regex mount points to see\n+   * whether the path match any regex expressions.\n+   *\n+   * @param srcPath\n+   * @param resolveLastComponent\n+   * @return\n+   */\n+  protected ResolveResult<T> tryResolveInRegexMountpoint(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    for (RegexMountPoint regexMountPoint : regexMountPointList) {\n+      ResolveResult resolveResult =\n+          regexMountPoint.resolve(srcPath, resolveLastComponent);\n+      if (resolveResult != null) {\n+        return resolveResult;\n       }\n-      remainingPath = new Path(remainingPathStr.toString());\n     }\n-    final ResolveResult<T> res =\n-        new ResolveResult<T>(ResultKind.INTERNAL_DIR,\n-            curInode.getInternalDirFs(), curInode.fullPath, remainingPath);\n-    return res;\n+    return null;\n+  }\n+\n+  /**\n+   * Build resolve result return to caller.\n+   *\n+   * @param resultKind\n+   * @param resolvedPathStr\n+   * @param targetOfResolvedPathStr\n+   * @param remainingPath\n+   * @return\n+   */\n+  protected ResolveResult<T> buildResolveResultForRegexMountPoint(\n+      ResultKind resultKind, String resolvedPathStr,\n+      String targetOfResolvedPathStr, Path remainingPath) {\n+    try {\n+      T targetFs = getTargetFileSystem(new URI(targetOfResolvedPathStr));\n+      return new ResolveResult<T>(resultKind, targetFs, resolvedPathStr,\n+          remainingPath);\n+    } catch (IOException ex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          ex);\n+      return null;\n+    } catch (URISyntaxException uex) {\n+      LOGGER.error(String.format(\n+          \"Got Exception while build resolve result.\"\n+              + \" ResultKind:%s, resolvedPathStr:%s,\"\n+              + \" targetOfResolvedPathStr:%s, remainingPath:%s,\"\n+              + \" will return null.\",\n+          resultKind, resolvedPathStr, targetOfResolvedPathStr, remainingPath),\n+          uex);\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Return resolution cache capacity.\n+   *\n+   * @return\n+   */\n+  public int getPathResolutionCacheCapacity() {\n+    return pathResolutionCacheCapacity;\n+  }\n+", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM4NTQwNA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r472385404", "bodyText": "Sure, make sense.", "author": "JohnZZGithub", "createdAt": "2020-08-18T18:06:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTYwOTYyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1ODE1NQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471658155", "bodyText": "same as above comment", "author": "umamaheswararao", "createdAt": "2020-08-17T17:48:16Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2Njg5OA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471866898", "bodyText": "Good point", "author": "JohnZZGithub", "createdAt": "2020-08-18T01:48:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1ODE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1ODgwMQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471658801", "bodyText": "you may want to close vfs as well?", "author": "umamaheswararao", "createdAt": "2020-08-17T17:49:20Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1OTI2MQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471659261", "bodyText": "same as above...please take care of other similar places.", "author": "umamaheswararao", "createdAt": "2020-08-17T17:50:04Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2Njg2NQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471866865", "bodyText": "Good point", "author": "JohnZZGithub", "createdAt": "2020-08-18T01:48:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1OTI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY2NjYxMA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471666610", "bodyText": "Could we assert the results of listStatus. Probably create few files under that expected target path. So, the ls will get some file and assert to make sure it's getting from target.", "author": "umamaheswararao", "createdAt": "2020-08-17T18:03:26Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    dstPathStr = targetTestRoot + \"${firstDir}\";\n+    srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping2\");\n+    expectedResolveResult = new Path(dstPathStr\n+        .replace(\"${firstDir}\", \"testConfLinkRegexNamedGroupMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MDEwOQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471670109", "bodyText": "seems like no change here. Can we avoid it?", "author": "umamaheswararao", "createdAt": "2020-08-17T18:09:11Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "diffHunk": "@@ -1417,7 +1418,8 @@ public void testGetContentSummaryWithFileInLocalFS() throws Exception {\n       fos.write(expected.getBytes());\n     }\n     ConfigUtil.addLink(conf,\n-        \"/internalDir/internalDir2/linkToLocalFile\", localFile.toURI());", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MTAzMw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471671033", "bodyText": "Nits: please add annotation in separate line above. You may want to change for all such lines below and above.", "author": "umamaheswararao", "createdAt": "2020-08-17T18:10:40Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test public void testInterceptSource() {", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MzA4OQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471673089", "bodyText": "Not about this test, In general on inner cache disabling: There was a suggestion for avoiding explicitly user disabling it. If that not possible, you may want to have check while analyzing mount points itself that, if cache is enabled and using regex mounts, then you may want to fail fs initialization itself?", "author": "umamaheswararao", "createdAt": "2020-08-17T18:14:41Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemBaseTest.java", "diffHunk": "@@ -1430,4 +1432,49 @@ public void testGetContentSummaryWithFileInLocalFS() throws Exception {\n           summaryAfter.getLength());\n     }\n   }\n+\n+  @Test\n+  public void testMountPointCache() throws Exception {\n+    conf.setInt(Constants.CONFIG_VIEWFS_PATH_RESOLUTION_CACHE_CAPACITY, 1);\n+    conf.setBoolean(\"fs.viewfs.impl.disable.cache\", true);", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgzMDYzMA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471830630", "bodyText": "@umamaheswararao  This is a good point. Let me add some preconditions check.\nBTW, now the inner cache assumes every filesystem is created while InodeTree is constructed and never changed. Do you think it's reasonable to change it to a concurrent hash map?", "author": "JohnZZGithub", "createdAt": "2020-08-17T23:34:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MzA4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5Mjk2Nw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471892967", "bodyText": "One more thing to clarify, I guess this config is for per schema level cache. Regex mount point is OK with it. Mount table is not good inner cache inside ViewFileSystem.java.", "author": "JohnZZGithub", "createdAt": "2020-08-18T03:30:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY3MzA4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcyMDQ0NA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471720444", "bodyText": "Do you mind writing some description test steps in javadoc. It will be helpful to better understand about scenarios.", "author": "umamaheswararao", "createdAt": "2020-08-17T19:12:39Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.Constants.CONFIG_VIEWFS_ENABLE_INNER_CACHE;\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexIndexMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // (^/(\\w+),/targetTestRoot/$1)\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping1\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$1\";\n+    Path srcPath = new Path(\"/testConfLinkRegexIndexMapping1\");\n+    Path expectedResolveResult =\n+        new Path(dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    // ^/(\\w+, /targetTestRoot/${1})\n+    // => /targetTestRoot/testConfLinkRegexIndexMapping2\n+    dstPathStr = targetTestRoot + \"${1}\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping2\");\n+    expectedResolveResult =\n+        new Path(dstPathStr.replace(\"${1}\", \"testConfLinkRegexIndexMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping3/file1\n+    dstPathStr = targetTestRoot + \"$1\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping3/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping3/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    //(^/(\\w+)/file1, /targetTestRoot/$1/)\n+    // = > /targetTestRoot/testConfLinkRegexIndexMapping4/file1\n+    dstPathStr = targetTestRoot + \"$1/\";\n+    srcPath = new Path(\"/testConfLinkRegexIndexMapping4/file1\");\n+    expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$1\", \"testConfLinkRegexIndexMapping4/file1\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexNamedGroupMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    // ^/(?<firstDir>\\\\w+) = > /targetTestRoot/$firstDir\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/(?<firstDir>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$firstDir\";\n+    Path srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping1\");\n+    Path expectedResolveResult = new Path(\n+        dstPathStr.replace(\"$firstDir\", \"testConfLinkRegexNamedGroupMapping1\"));\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+\n+    // Test ${1} format\n+    dstPathStr = targetTestRoot + \"${firstDir}\";\n+    srcPath = new Path(\"/testConfLinkRegexNamedGroupMapping2\");\n+    expectedResolveResult = new Path(dstPathStr\n+        .replace(\"${firstDir}\", \"testConfLinkRegexNamedGroupMapping2\"));\n+    outputStream = fsTarget.create(expectedResolveResult);\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexFixedDestMapping() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/\\\\w+\";\n+    String dstPathStr =\n+        targetTestRoot + \"testConfLinkRegexFixedDestMappingFile\";\n+    Path expectedResolveResult = new Path(dstPathStr);\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil.addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, null);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(\n+        expectedResolveResult.equals(vfs.resolvePath(new Path(\"/misc1\"))));\n+    Assert.assertTrue(\n+        expectedResolveResult.equals(vfs.resolvePath(new Path(\"/misc2\"))));\n+  }\n+\n+  @Test\n+  public void testConfLinkRegexWithSingleInterceptor() throws Exception {\n+    conf.setBoolean(CONFIG_VIEWFS_ENABLE_INNER_CACHE, false);\n+    URI viewFsUri =\n+        new URI(FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = targetTestRoot + \"$username\";\n+    // Replace \"_\" with \"-\"\n+    String settingString = buildReplaceInterceptorSettingString(\"_\", \"-\");\n+    Path srcPath = new Path(\"/user/hadoop_user1/hadoop_file1\");\n+    Path expectedResolveResult =\n+        new Path(targetTestRoot, \"hadoop-user1/hadoop_file1\");\n+    FSDataOutputStream outputStream = fsTarget.create((expectedResolveResult));\n+    fsTarget.listStatus(expectedResolveResult);\n+    outputStream.close();\n+    ConfigUtil\n+        .addLinkRegex(conf, CLUSTER_NAME, regexStr, dstPathStr, settingString);\n+    FileSystem vfs = FileSystem.get(viewFsUri, conf);\n+    Assert.assertTrue(expectedResolveResult.equals(vfs.resolvePath(srcPath)));\n+    Assert.assertEquals(0L, vfs.getFileStatus(srcPath).getLen());\n+  }\n+", "originalCommit": "c58b945a32a978c470be07ef47e9ed7edfff428c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5MjA1OA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r471892058", "bodyText": "Sure.", "author": "JohnZZGithub", "createdAt": "2020-08-18T03:27:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcyMDQ0NA=="}], "type": "inlineReview"}, {"oid": "809b9a12153679cc304436013038ddfeeedd8fdb", "url": "https://github.com/apache/hadoop/commit/809b9a12153679cc304436013038ddfeeedd8fdb", "message": "HADOOP-15891. address comments.", "committedDate": "2020-08-18T03:26:14Z", "type": "forcePushed"}, {"oid": "31a68cd94f7abe79bb91fc1e223c4f9340dab708", "url": "https://github.com/apache/hadoop/commit/31a68cd94f7abe79bb91fc1e223c4f9340dab708", "message": "HADOOP-15891. address comments.", "committedDate": "2020-08-18T03:46:13Z", "type": "forcePushed"}, {"oid": "f9c61ef653133d40e889c6ec81f05cb073d54de8", "url": "https://github.com/apache/hadoop/commit/f9c61ef653133d40e889c6ec81f05cb073d54de8", "message": "HADOOP-15891. address comments.", "committedDate": "2020-08-18T17:59:42Z", "type": "forcePushed"}, {"oid": "74b0ac5ed9027f3c99fb7be09f5c9a4da591b5ed", "url": "https://github.com/apache/hadoop/commit/74b0ac5ed9027f3c99fb7be09f5c9a4da591b5ed", "message": "HADOOP-15891. address comments.", "committedDate": "2020-08-18T19:32:58Z", "type": "forcePushed"}, {"oid": "a36f64e5a4de6d2f120fc822b74b2f74751fdf3e", "url": "https://github.com/apache/hadoop/commit/a36f64e5a4de6d2f120fc822b74b2f74751fdf3e", "message": "HADOOP-15891. fix formats.", "committedDate": "2020-08-19T17:48:40Z", "type": "forcePushed"}, {"oid": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "url": "https://github.com/apache/hadoop/commit/3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "message": "HADOOP-15891. fix formats.", "committedDate": "2020-08-20T01:44:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDc5NTM0Mg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480795342", "bodyText": "Below method seems like unused method? Please removed it.", "author": "umamaheswararao", "createdAt": "2020-09-01T04:44:30Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.InodeTree.SlashPath;\n+\n+/**\n+ * Regex mount point is build to implement regex based mount point.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPoint<T> {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(RegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private String srcPathRegex;\n+  private Pattern srcPattern;\n+  private String dstPath;\n+  private String interceptorSettingsString;\n+  private List<RegexMountPointInterceptor> interceptorList;\n+\n+  public static final String SETTING_SRCREGEX_SEP = \"#.\";\n+  public static final char INTERCEPTOR_SEP = ';';\n+  public static final char INTERCEPTOR_INTERNAL_SEP = ':';\n+  // ${var},$var\n+  public static final Pattern VAR_PATTERN_IN_DEST =\n+      Pattern.compile(\"\\\\$((\\\\{\\\\w+\\\\})|(\\\\w+))\");\n+\n+  // Same var might have different representations.\n+  // e.g.\n+  // key => $key or key = > ${key}\n+  private Map<String, Set<String>> varInDestPathMap;\n+\n+  public Map<String, Set<String>> getVarInDestPathMap() {\n+    return varInDestPathMap;\n+  }\n+\n+  RegexMountPoint(InodeTree inodeTree, String sourcePathRegex,\n+      String destPath, String settingsStr) {\n+    this.inodeTree = inodeTree;\n+    this.srcPathRegex = sourcePathRegex;\n+    this.dstPath = destPath;\n+    this.interceptorSettingsString = settingsStr;\n+    this.interceptorList = new ArrayList<>();\n+  }\n+\n+  /**\n+   * Initialize regex mount point.\n+   *\n+   * @throws IOException\n+   */\n+  public void initialize() throws IOException {\n+    try {\n+      srcPattern = Pattern.compile(srcPathRegex);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Failed to initialized mount point due to bad src path regex:\"\n+              + srcPathRegex + \", dstPath:\" + dstPath, ex);\n+    }\n+    varInDestPathMap = getVarListInString(dstPath);\n+    initializeInterceptors();\n+  }\n+\n+  private void initializeInterceptors() throws IOException {\n+    if (interceptorSettingsString == null\n+        || interceptorSettingsString.isEmpty()) {\n+      return;\n+    }\n+    String[] interceptorStrArray =\n+        StringUtils.split(interceptorSettingsString, INTERCEPTOR_SEP);\n+    for (String interceptorStr : interceptorStrArray) {\n+      RegexMountPointInterceptor interceptor =\n+          RegexMountPointInterceptorFactory.create(interceptorStr);\n+      if (interceptor == null) {\n+        throw new IOException(\n+            \"Illegal settings String \" + interceptorSettingsString);\n+      }\n+      interceptor.initialize();\n+      interceptorList.add(interceptor);\n+    }\n+  }\n+\n+  /**\n+   * Get $var1 and $var2 style variables in string.\n+   *\n+   * @param input - the string to be process.\n+   * @return\n+   */\n+  public static Map<String, Set<String>> getVarListInString(String input) {\n+    Map<String, Set<String>> varMap = new HashMap<>();\n+    Matcher matcher = VAR_PATTERN_IN_DEST.matcher(input);\n+    while (matcher.find()) {\n+      // $var or ${var}\n+      String varName = matcher.group(0);\n+      // var or {var}\n+      String strippedVarName = matcher.group(1);\n+      if (strippedVarName.startsWith(\"{\")) {\n+        // {varName} = > varName\n+        strippedVarName =\n+            strippedVarName.substring(1, strippedVarName.length() - 1);\n+      }\n+      varMap.putIfAbsent(strippedVarName, new HashSet<>());\n+      varMap.get(strippedVarName).add(varName);\n+    }\n+    return varMap;\n+  }\n+\n+  public String getSrcPathRegex() {\n+    return srcPathRegex;\n+  }\n+\n+  public Pattern getSrcPattern() {\n+    return srcPattern;\n+  }\n+\n+  public String getDstPath() {\n+    return dstPath;\n+  }\n+\n+  public static Pattern getVarPatternInDest() {\n+    return VAR_PATTERN_IN_DEST;\n+  }\n+\n+  /**\n+   * Get resolved path from regex mount points.\n+   *  E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n+   *  srcPath: is /user/hadoop/dir1\n+   *  resolveLastComponent: true\n+   *  then return value is s3://hadoop.apache.com/_hadoop\n+   * @param srcPath - the src path to resolve\n+   * @param resolveLastComponent - whether resolve the path after last `/`\n+   * @return mapped path of the mount point.\n+   */\n+  public InodeTree.ResolveResult<T> resolve(final String srcPath,\n+      final boolean resolveLastComponent) {\n+    String pathStrToResolve = getPathToResolve(srcPath, resolveLastComponent);\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      pathStrToResolve = interceptor.interceptSource(pathStrToResolve);\n+    }\n+    LOGGER.debug(\"Path to resolve:\" + pathStrToResolve + \", srcPattern:\"\n+        + getSrcPathRegex());\n+    Matcher srcMatcher = getSrcPattern().matcher(pathStrToResolve);\n+    String parsedDestPath = getDstPath();\n+    int mappedCount = 0;\n+    String resolvedPathStr = \"\";\n+    while (srcMatcher.find()) {\n+      resolvedPathStr = pathStrToResolve.substring(0, srcMatcher.end());\n+      Map<String, Set<String>> varMap = getVarInDestPathMap();\n+      for (Map.Entry<String, Set<String>> entry : varMap.entrySet()) {\n+        String regexGroupNameOrIndexStr = entry.getKey();\n+        Set<String> groupRepresentationStrSetInDest = entry.getValue();\n+        parsedDestPath = replaceRegexCaptureGroupInPath(\n+            parsedDestPath, srcMatcher,\n+            regexGroupNameOrIndexStr, groupRepresentationStrSetInDest);\n+      }\n+      ++mappedCount;\n+    }\n+    if (0 == mappedCount) {\n+      return null;\n+    }\n+    Path remainingPath = getRemainingPathStr(srcPath, resolvedPathStr);\n+    for (RegexMountPointInterceptor interceptor : interceptorList) {\n+      parsedDestPath = interceptor.interceptResolvedDestPathStr(parsedDestPath);\n+      remainingPath =\n+          interceptor.interceptRemainingPath(remainingPath);\n+    }\n+    InodeTree.ResolveResult resolveResult = inodeTree\n+        .buildResolveResultForRegexMountPoint(InodeTree.ResultKind.EXTERNAL_DIR,\n+            resolvedPathStr, parsedDestPath, remainingPath);\n+    return resolveResult;\n+  }\n+\n+  private Path getRemainingPathStr(\n+      String srcPath,\n+      String resolvedPathStr) {\n+    String remainingPathStr = srcPath.substring(resolvedPathStr.length());\n+    if (!remainingPathStr.startsWith(\"/\")) {\n+      remainingPathStr = \"/\" + remainingPathStr;\n+    }\n+    return new Path(remainingPathStr);\n+  }\n+\n+  private String getPathToResolve(\n+      String srcPath, boolean resolveLastComponent) {\n+    if (resolveLastComponent) {\n+      return srcPath;\n+    }\n+    int lastSlashIndex = srcPath.lastIndexOf(SlashPath.toString());\n+    if (lastSlashIndex == -1) {\n+      return null;\n+    }\n+    return srcPath.substring(0, lastSlashIndex);\n+  }\n+\n+  /**\n+   * Use capture group named regexGroupNameOrIndexStr in mather to replace\n+   * parsedDestPath.\n+   * E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n+   * srcMatcher is from /user/hadoop.\n+   * Then the params will be like following.\n+   * parsedDestPath: s3://$user.apache.com/_${user},\n+   * regexGroupNameOrIndexStr: user\n+   * groupRepresentationStrSetInDest: {user:$user; user:${user}}\n+   * return value will be s3://hadoop.apache.com/_hadoop\n+   * @param parsedDestPath\n+   * @param srcMatcher\n+   * @param regexGroupNameOrIndexStr\n+   * @param groupRepresentationStrSetInDest\n+   * @return return parsedDestPath while ${var},$var replaced or\n+   * parsedDestPath nothing found.\n+   */\n+  private String replaceRegexCaptureGroupInPath(\n+      String parsedDestPath,\n+      Matcher srcMatcher,\n+      String regexGroupNameOrIndexStr,\n+      Set<String> groupRepresentationStrSetInDest) {\n+    String groupValue = getRegexGroupValueFromMather(\n+        srcMatcher, regexGroupNameOrIndexStr);\n+    if (groupValue == null) {\n+      return parsedDestPath;\n+    }\n+    for (String varName : groupRepresentationStrSetInDest) {\n+      parsedDestPath = parsedDestPath.replace(varName, groupValue);\n+      LOGGER.debug(\"parsedDestPath value is:\" + parsedDestPath);\n+    }\n+    return parsedDestPath;\n+  }\n+\n+  /**\n+   * Get matched capture group value from regex matched string. E.g.\n+   * Regex: ^/user/(?<username>\\\\w+), regexGroupNameOrIndexStr: userName\n+   * then /user/hadoop should return hadoop while call\n+   * getRegexGroupValueFromMather(matcher, usersName)\n+   * or getRegexGroupValueFromMather(matcher, 1)\n+   *\n+   * @param srcMatcher - the matcher to be use\n+   * @param regexGroupNameOrIndexStr - the regex group name or index\n+   * @return - Null if no matched group named regexGroupNameOrIndexStr found.\n+   */\n+  private String getRegexGroupValueFromMather(\n+      Matcher srcMatcher, String regexGroupNameOrIndexStr) {\n+    if (regexGroupNameOrIndexStr.matches(\"\\\\d+\")) {\n+      // group index\n+      int groupIndex = Integer.parseUnsignedInt(regexGroupNameOrIndexStr);\n+      if (groupIndex >= 0 && groupIndex <= srcMatcher.groupCount()) {\n+        return srcMatcher.group(groupIndex);\n+      }\n+    } else {\n+      // named group in regex\n+      return srcMatcher.group(regexGroupNameOrIndexStr);\n+    }\n+    return null;\n+  }\n+\n+  /**", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1MDIyOA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481350228", "bodyText": "Good catch.", "author": "JohnZZGithub", "createdAt": "2020-09-01T18:35:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDc5NTM0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480836939", "bodyText": "below comment can be corrected? regex base mount point will not use caching now.\nOtherwise people could confuse and may tend to disable. IIUC, even if they enable this, RegexBasedMountPoints will continue to work right?", "author": "umamaheswararao", "createdAt": "2020-09-01T05:36:33Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/Constants.java", "diffHunk": "@@ -86,12 +86,21 @@\n    */\n   String CONFIG_VIEWFS_LINK_MERGE_SLASH = \"linkMergeSlash\";\n \n+  /**\n+   * Config variable for specifying a regex link which uses regular expressions\n+   * as source and target could use group captured in src.\n+   * E.g. (^/(?<firstDir>\\\\w+), /prefix-${firstDir}) =>\n+   *   (/path1/file1 => /prefix-path1/file1)\n+   */\n+  String CONFIG_VIEWFS_LINK_REGEX = \"linkRegex\";\n+\n   FsPermission PERMISSION_555 = new FsPermission((short) 0555);\n \n   String CONFIG_VIEWFS_RENAME_STRATEGY = \"fs.viewfs.rename.strategy\";\n \n   /**\n    * Enable ViewFileSystem to cache all children filesystems in inner cache.", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMyODkxMg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481328912", "bodyText": "Good cache, I should clarify it. ViewFileSystem will work if people enable inner cache. However, regex based mounts won't be cache.", "author": "JohnZZGithub", "createdAt": "2020-09-01T17:56:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzNTU4NA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485335584", "bodyText": "Is the below comment is still valid?", "author": "umamaheswararao", "createdAt": "2020-09-09T04:48:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0Njk1NA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346954", "bodyText": "Nice catch.", "author": "JohnZZGithub", "createdAt": "2020-09-09T05:28:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDgzNjkzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg0NzcxOA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480847718", "bodyText": "you may want to remove the below return ? or specify what it's returning.", "author": "umamaheswararao", "createdAt": "2020-09-01T05:53:13Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * The interceptor factory used to create RegexMountPoint interceptors.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+final class RegexMountPointInterceptorFactory {\n+\n+  private RegexMountPointInterceptorFactory() {\n+\n+  }\n+\n+  /**\n+   * interceptorSettingsString string should be like ${type}:${string},\n+   * e.g. replaceresolveddstpath:word1,word2.\n+   *\n+   * @param interceptorSettingsString", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480851021", "bodyText": "What are you asserting in this case?", "author": "umamaheswararao", "createdAt": "2020-09-01T05:58:12Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUyMTExMQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481521111", "bodyText": "I meant to have a safegurad when users want to change the interceptor settings format. But I'm OK to remove it.", "author": "JohnZZGithub", "createdAt": "2020-09-02T01:14:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzOTA0MQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481539041", "bodyText": "I mean we just need to assert I think. Ex: Test saying \"InterceptSource\". SO, just validate what it should return. In this case, it will return same string. It may not have much value now, but if some one changes it, it can just catch it.\nCurrently this test will never fail.", "author": "umamaheswararao", "createdAt": "2020-09-02T01:46:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDAxMzQxNA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r484013414", "bodyText": "Ah, gotcha. I thought the comment was for testSerialization().", "author": "JohnZZGithub", "createdAt": "2020-09-06T02:53:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjE3Mg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r484092172", "bodyText": "No worries. Thanks for addressing. :-)", "author": "umamaheswararao", "createdAt": "2020-09-06T16:58:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MTAyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1MjM2Ng==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480852366", "bodyText": "You want to use assertEquals instead?", "author": "umamaheswararao", "createdAt": "2020-09-01T06:00:10Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(\n+        interceptor.getSrcRegexPattern().toString().equals(srcRegex));\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringBadCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    serializedString = serializedString + \":ddd\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertEquals(interceptor, null);\n+  }\n+\n+  @Test\n+  public void testSerialization() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    Assert.assertEquals(interceptor.serializeToString(), serializedString);\n+  }\n+\n+  @Test\n+  public void testInterceptSource() {\n+    String srcRegex = \"word1\";\n+    String replaceString = \"word2\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    String sourcePath = \"/a/b/l3/dd\";\n+    sourcePath = interceptor.interceptSource(sourcePath);\n+  }\n+\n+  @Test\n+  public void testInterceptResolve() throws IOException {\n+    String pathAfterResolution = \"/user-hadoop\";\n+    Path remainingPath = new Path(\"/ad-data\");\n+\n+    String srcRegex = \"hadoop\";\n+    String replaceString = \"hdfs\";\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(srcRegex,\n+            replaceString);\n+    interceptor.initialize();\n+    Assert.assertTrue(", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1Mjg3Ng==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480852876", "bodyText": "You may want to use assertEquals directly?", "author": "umamaheswararao", "createdAt": "2020-09-01T06:00:57Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Test RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+public class TestRegexMountPointResolvedDstPathReplaceInterceptor {\n+\n+  public String createSerializedString(String regex, String replaceString) {\n+    return REPLACE_RESOLVED_DST_PATH.getConfigName()\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + regex\n+        + RegexMountPoint.INTERCEPTOR_INTERNAL_SEP + replaceString;\n+  }\n+\n+  @Test\n+  public void testDeserializeFromStringNormalCase() throws IOException {\n+    String srcRegex = \"-\";\n+    String replaceString = \"_\";\n+    String serializedString = createSerializedString(srcRegex, replaceString);\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        RegexMountPointResolvedDstPathReplaceInterceptor\n+            .deserializeFromString(serializedString);\n+    Assert.assertTrue(interceptor.getSrcRegexString().equals(srcRegex));\n+    Assert.assertTrue(interceptor.getReplaceString().equals(replaceString));\n+    Assert.assertTrue(interceptor.getSrcRegexPattern() == null);\n+    interceptor.initialize();\n+    Assert.assertTrue(", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1NDQyOQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480854429", "bodyText": "You may want to use assertEquals ? The advantage would be that, when assertion fails it will tell you what's mismatching.  There are lot of asserts like this, please change if possible.", "author": "umamaheswararao", "createdAt": "2020-09-01T06:03:23Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPoint.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Test Regex Mount Point.\n+ */\n+public class TestRegexMountPoint {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestRegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private Configuration conf;\n+\n+  class TestRegexMountPointFileSystem {\n+    public URI getUri() {\n+      return uri;\n+    }\n+\n+    private URI uri;\n+\n+    TestRegexMountPointFileSystem(URI uri) {\n+      String uriStr = uri == null ? \"null\" : uri.toString();\n+      LOGGER.info(\"Create TestRegexMountPointFileSystem Via URI:\" + uriStr);\n+      this.uri = uri;\n+    }\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    ConfigUtil.addLink(conf, TestRegexMountPoint.class.getName(), \"/mnt\",\n+        URI.create(\"file:///\"));\n+\n+    inodeTree = new InodeTree<TestRegexMountPointFileSystem>(conf,\n+        TestRegexMountPoint.class.getName(), null, false) {\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final URI uri) {\n+        return new TestRegexMountPointFileSystem(uri);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final URI uri, boolean enableCache) {\n+        return new TestRegexMountPointFileSystem(uri);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final INodeDir<TestRegexMountPointFileSystem> dir) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final String settings, final URI[] mergeFsURIList) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+    };\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    inodeTree = null;\n+  }\n+\n+  @Test\n+  public void testGetVarListInString() throws IOException {\n+    String srcRegex = \"/(\\\\w+)\";\n+    String target = \"/$0/${1}/$1/${2}/${2}\";\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, srcRegex, target, null);\n+    regexMountPoint.initialize();\n+    Map<String, Set<String>> varMap = regexMountPoint.getVarInDestPathMap();\n+    Assert.assertEquals(varMap.size(), 3);\n+    Assert.assertEquals(varMap.get(\"0\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"0\").contains(\"$0\"));\n+    Assert.assertEquals(varMap.get(\"1\").size(), 2);\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"${1}\"));\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"$1\"));\n+    Assert.assertEquals(varMap.get(\"2\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"2\").contains(\"${2}\"));\n+  }\n+\n+  @Test\n+  public void testResolve() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    String settingsStr = null;\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop/file1\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop\"));\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1NTg0Mw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480855843", "bodyText": "you can use assertNull", "author": "umamaheswararao", "createdAt": "2020-09-01T06:05:32Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPointInterceptorFactory.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Test Regex Mount Point Interceptor Factory.\n+ */\n+public class TestRegexMountPointInterceptorFactory {\n+\n+  @Test\n+  public void testCreateNormalCase() {\n+    String replaceInterceptorStr =\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + Character.toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP)\n+            + \"src\" + Character\n+            .toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP) + \"replace\";\n+    RegexMountPointInterceptor interceptor =\n+        RegexMountPointInterceptorFactory.create(replaceInterceptorStr);\n+    Assert.assertTrue(\n+        interceptor\n+            instanceof RegexMountPointResolvedDstPathReplaceInterceptor);\n+  }\n+\n+  @Test\n+  public void testCreateBadCase() {\n+    String replaceInterceptorStr =\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + \"___\" + Character\n+            .toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP) + \"src\"\n+            + Character.toString(RegexMountPoint.INTERCEPTOR_INTERNAL_SEP)\n+            + \"replace\";\n+    RegexMountPointInterceptor interceptor =\n+        RegexMountPointInterceptorFactory.create(replaceInterceptorStr);", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1OTMyNg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480859326", "bodyText": "Seems like we don't do anything here. Could you please add that in Javadoc. Say src will not be intercepted in this impl, it's only for dst?", "author": "umamaheswararao", "createdAt": "2020-09-01T06:10:56Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzMTQ1Mw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481531453", "bodyText": "Sure.", "author": "JohnZZGithub", "createdAt": "2020-09-02T01:34:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg1OTMyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MTk5Mg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480861992", "bodyText": "same a interceptSrc doc.", "author": "umamaheswararao", "createdAt": "2020-09-01T06:14:52Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.\n+   *\n+   * @param source\n+   * @return\n+   */\n+  @Override\n+  public String interceptSource(String source) {\n+    return source;\n+  }\n+\n+  /**\n+   * Intercept resolved path, e.g.\n+   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n+   * If incoming path is /user1/home/tmp/job1,\n+   * then the resolved path str will be user1.\n+   *\n+   * @return intercepted string\n+   */\n+  @Override public String interceptResolvedDestPathStr(\n+      String parsedDestPathStr) {\n+    Matcher matcher = srcRegexPattern.matcher(parsedDestPathStr);\n+    return matcher.replaceAll(replaceString);\n+  }\n+\n+  /**\n+   * Intercept remaining path.\n+   *", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUzMjg4Mg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481532882", "bodyText": "Done.", "author": "JohnZZGithub", "createdAt": "2020-09-02T01:36:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MTk5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MzE2Nw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r480863167", "bodyText": "shouldn't it interceptSrc and remainingPath follow same pattern to intercept as they both split from src path only?\nWhats the issue if I use same interceptSource method? Do we have some concerns? Could you explain me if I miss something here?", "author": "umamaheswararao", "createdAt": "2020-09-01T06:16:40Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH;\n+\n+/**\n+ * Implementation of RegexMountPointResolvedDstPathReplaceInterceptor.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class RegexMountPointResolvedDstPathReplaceInterceptor\n+    implements RegexMountPointInterceptor {\n+\n+  private String srcRegexString;\n+  private String replaceString;\n+  private Pattern srcRegexPattern;\n+\n+  RegexMountPointResolvedDstPathReplaceInterceptor(String srcRegex,\n+      String replaceString) {\n+    this.srcRegexString = srcRegex;\n+    this.replaceString = replaceString;\n+    this.srcRegexPattern = null;\n+  }\n+\n+  public String getSrcRegexString() {\n+    return srcRegexString;\n+  }\n+\n+  public String getReplaceString() {\n+    return replaceString;\n+  }\n+\n+  public Pattern getSrcRegexPattern() {\n+    return srcRegexPattern;\n+  }\n+\n+  @Override\n+  public void initialize() throws IOException {\n+    try {\n+      srcRegexPattern = Pattern.compile(srcRegexString);\n+    } catch (PatternSyntaxException ex) {\n+      throw new IOException(\n+          \"Initialize interceptor failed, srcRegx:\" + srcRegexString, ex);\n+    }\n+  }\n+\n+  /**\n+   * Intercept source before resolution.\n+   *\n+   * @param source\n+   * @return\n+   */\n+  @Override\n+  public String interceptSource(String source) {\n+    return source;\n+  }\n+\n+  /**\n+   * Intercept resolved path, e.g.\n+   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n+   * If incoming path is /user1/home/tmp/job1,\n+   * then the resolved path str will be user1.\n+   *\n+   * @return intercepted string\n+   */\n+  @Override public String interceptResolvedDestPathStr(\n+      String parsedDestPathStr) {\n+    Matcher matcher = srcRegexPattern.matcher(parsedDestPathStr);\n+    return matcher.replaceAll(replaceString);\n+  }\n+\n+  /**\n+   * Intercept remaining path.\n+   *\n+   * @return intercepted path\n+   */\n+  @Override public Path interceptRemainingPath(Path remainingPath) {", "originalCommit": "3d9b3db8e9df4d5fe43f20880e54c22f1916fdb4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTU0MzQ0Ng==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481543446", "bodyText": "This is a great question.\nHere's an example:\n/user/$userName => s3://$userName.apache.org\n/user/hadoop/dir1/dir2/dir3/file3 will be mapped to s3://user.hadoop.apche.org/dir1/dir2/dir3/file3\ninterceptSource(/user/hadoop/) take sourcePath and won't know which part will be replaced in the final path.\nAfter mapping, the dest will be s3://user.hadoop.apche.org/dir1/dir2/dir3/file3\ninterceptRemainingPath(dir1/dir2/dir3/file3) could enable users to do process the remaining part (dir1/dir2/dir3/file3) after the mounted path. E.g. take care of bad characters.\nBut I'm good to remove it if we feel it's cleaner.", "author": "JohnZZGithub", "createdAt": "2020-09-02T01:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MzE2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTg4MzIzNw==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r481883237", "bodyText": "The splitting of remaining part is kind implementation details I feel. User may not clearly have idea on that, unless they are very advanced users.\nMy question was what if we use same API for remaining part also?  in ur example interceptSource((dir1/dir2/dir3/file3)). What will happen here? Intercepter are just for replacing some pattern with other right. Let me know if I am missing.\nThanks for your efforts on fixing other comments..\nAlso please take a look at check-style warnings reported in latest report.", "author": "umamaheswararao", "createdAt": "2020-09-02T08:28:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MzE2Nw=="}], "type": "inlineReview"}, {"oid": "6a47ea225d20083e05913678cbcbfeeb3a0a272a", "url": "https://github.com/apache/hadoop/commit/6a47ea225d20083e05913678cbcbfeeb3a0a272a", "message": "HADOOP-15891. address comments.", "committedDate": "2020-09-02T02:00:02Z", "type": "forcePushed"}, {"oid": "2dcf31f19bcc05ee4433bdefdd77776ee450fb1f", "url": "https://github.com/apache/hadoop/commit/2dcf31f19bcc05ee4433bdefdd77776ee450fb1f", "message": "HADOOP-15891. address comments.", "committedDate": "2020-09-02T07:53:17Z", "type": "forcePushed"}, {"oid": "01a2419e9c942c0a34c40a14d4074a7420cd4420", "url": "https://github.com/apache/hadoop/commit/01a2419e9c942c0a34c40a14d4074a7420cd4420", "message": "HADOOP-15891: provide Regex Based Mount Point In Inode Tree\n\nDifferential Revision: https://phabricator.twitter.biz/D526212", "committedDate": "2020-09-08T20:34:55Z", "type": "commit"}, {"oid": "d66b86b5e5d9fd5b7e8a40f43fedc93c33f44a24", "url": "https://github.com/apache/hadoop/commit/d66b86b5e5d9fd5b7e8a40f43fedc93c33f44a24", "message": "HADOOP-15891. address comments.", "committedDate": "2020-09-08T20:34:55Z", "type": "commit"}, {"oid": "2f3ae037e3533bc6512d8b497683d36cd559f7ad", "url": "https://github.com/apache/hadoop/commit/2f3ae037e3533bc6512d8b497683d36cd559f7ad", "message": "HADOOP-15891. remove inode tree mounpoints resolution cache.", "committedDate": "2020-09-08T20:34:55Z", "type": "commit"}, {"oid": "d540c6b0ffa00351f7e232b72a790f7bb52a8a07", "url": "https://github.com/apache/hadoop/commit/d540c6b0ffa00351f7e232b72a790f7bb52a8a07", "message": "HADOOP-15891. address comments.", "committedDate": "2020-09-08T20:34:55Z", "type": "commit"}, {"oid": "c609d13f77950d082525bb06a931540fe9119cb9", "url": "https://github.com/apache/hadoop/commit/c609d13f77950d082525bb06a931540fe9119cb9", "message": "HADOOP-15891: address comments. Add inner cache support.", "committedDate": "2020-09-08T20:34:55Z", "type": "commit"}, {"oid": "e25945d5da8ecd58ba3eb1e9f28715291c4273f1", "url": "https://github.com/apache/hadoop/commit/e25945d5da8ecd58ba3eb1e9f28715291c4273f1", "message": "Add more comments.", "committedDate": "2020-09-08T20:34:55Z", "type": "commit"}, {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002", "url": "https://github.com/apache/hadoop/commit/8786ee8fc0e1b1d617eabd398c4d05a1ff656002", "message": "Fix checkstyle and rebase.", "committedDate": "2020-09-08T20:43:56Z", "type": "commit"}, {"oid": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002", "url": "https://github.com/apache/hadoop/commit/8786ee8fc0e1b1d617eabd398c4d05a1ff656002", "message": "Fix checkstyle and rebase.", "committedDate": "2020-09-08T20:43:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjUxOA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485332518", "bodyText": "Do you see any issue if we make it true? If no issues, we can simply clean it on close right instead of having another config?\nSeems like this is an improvement to existing code. If you want, I am ok to file small JIRA and fix this cleanup thing.( I am assuming it's not necessarily needed with this.)", "author": "umamaheswararao", "createdAt": "2020-09-09T04:37:33Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -217,6 +239,7 @@ public Path getMountedOnPath() {\n   Path homeDir = null;\n   private boolean enableInnerCache = false;\n   private InnerCache cache;\n+  private boolean evictCacheOnClose = false;", "originalCommit": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0NjQ1OA==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346458", "bodyText": "@umamaheswararao Thanks for the kindness. I didn't see a problem with making it true. Just mean to be more cautious, let me remove it.", "author": "JohnZZGithub", "createdAt": "2020-09-09T05:27:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjg4MQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485332881", "bodyText": "Still this asserts can use assertEquals method? Please use them all places.", "author": "umamaheswararao", "createdAt": "2020-09-09T04:39:09Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestRegexMountPoint.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Test Regex Mount Point.\n+ */\n+public class TestRegexMountPoint {\n+  private static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestRegexMountPoint.class.getName());\n+\n+  private InodeTree inodeTree;\n+  private Configuration conf;\n+\n+  class TestRegexMountPointFileSystem {\n+    public URI getUri() {\n+      return uri;\n+    }\n+\n+    private URI uri;\n+\n+    TestRegexMountPointFileSystem(URI uri) {\n+      String uriStr = uri == null ? \"null\" : uri.toString();\n+      LOGGER.info(\"Create TestRegexMountPointFileSystem Via URI:\" + uriStr);\n+      this.uri = uri;\n+    }\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    ConfigUtil.addLink(conf, TestRegexMountPoint.class.getName(), \"/mnt\",\n+        URI.create(\"file:///\"));\n+\n+    inodeTree = new InodeTree<TestRegexMountPointFileSystem>(conf,\n+        TestRegexMountPoint.class.getName(), null, false) {\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final URI uri) {\n+        return new TestRegexMountPointFileSystem(uri);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final INodeDir<TestRegexMountPointFileSystem> dir) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+\n+      @Override\n+      protected TestRegexMountPointFileSystem getTargetFileSystem(\n+          final String settings, final URI[] mergeFsURIList) {\n+        return new TestRegexMountPointFileSystem(null);\n+      }\n+    };\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    inodeTree = null;\n+  }\n+\n+  @Test\n+  public void testGetVarListInString() throws IOException {\n+    String srcRegex = \"/(\\\\w+)\";\n+    String target = \"/$0/${1}/$1/${2}/${2}\";\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, srcRegex, target, null);\n+    regexMountPoint.initialize();\n+    Map<String, Set<String>> varMap = regexMountPoint.getVarInDestPathMap();\n+    Assert.assertEquals(varMap.size(), 3);\n+    Assert.assertEquals(varMap.get(\"0\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"0\").contains(\"$0\"));\n+    Assert.assertEquals(varMap.get(\"1\").size(), 2);\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"${1}\"));\n+    Assert.assertTrue(varMap.get(\"1\").contains(\"$1\"));\n+    Assert.assertEquals(varMap.get(\"2\").size(), 1);\n+    Assert.assertTrue(varMap.get(\"2\").contains(\"${2}\"));\n+  }\n+\n+  @Test\n+  public void testResolve() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    String settingsStr = null;\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop/file1\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop\"));\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(\n+        ((TestRegexMountPointFileSystem) resolveResult.targetFileSystem)\n+            .getUri().toString().equals(\"/namenode1/testResolve/hadoop\"));\n+    Assert.assertTrue(resolveResult.remainingPath.toString().equals(\"/file1\"));\n+  }\n+\n+  @Test\n+  public void testResolveWithInterceptor() throws IOException {\n+    String regexStr = \"^/user/(?<username>\\\\w+)\";\n+    String dstPathStr = \"/namenode1/testResolve/$username\";\n+    // Replace \"_\" with \"-\"\n+    RegexMountPointResolvedDstPathReplaceInterceptor interceptor =\n+        new RegexMountPointResolvedDstPathReplaceInterceptor(\"_\", \"-\");\n+    // replaceresolvedpath:_:-\n+    String settingsStr = interceptor.serializeToString();\n+    RegexMountPoint regexMountPoint =\n+        new RegexMountPoint(inodeTree, regexStr, dstPathStr, settingsStr);\n+    regexMountPoint.initialize();\n+    InodeTree.ResolveResult resolveResult =\n+        regexMountPoint.resolve(\"/user/hadoop_user1/file_index\", true);\n+    Assert.assertEquals(resolveResult.kind, InodeTree.ResultKind.EXTERNAL_DIR);\n+    Assert.assertTrue(\n+        resolveResult.targetFileSystem\n+            instanceof TestRegexMountPointFileSystem);\n+    Assert.assertTrue(resolveResult.resolvedPath.equals(\"/user/hadoop_user1\"));", "originalCommit": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0NjU0Mg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346542", "bodyText": "Nice catch, let me clean them up.", "author": "JohnZZGithub", "createdAt": "2020-09-09T05:27:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMjg4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMzc4MQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485333781", "bodyText": "small suggestion here:\nMove above lines out of try block. Then use try(FileSystem vfs = = FileSystem.get(viewFsUri, config)){\n}\nThis should close automatically after block. So, we can remove finally block below?", "author": "umamaheswararao", "createdAt": "2020-09-09T04:42:27Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkRegex.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileSystemTestHelper;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.fs.viewfs.RegexMountPoint.INTERCEPTOR_INTERNAL_SEP;\n+import static org.junit.Assert.assertSame;\n+\n+/**\n+ * Test linkRegex node type for view file system.\n+ */\n+public class TestViewFileSystemLinkRegex extends ViewFileSystemBaseTest {\n+  public static final Logger LOGGER =\n+      LoggerFactory.getLogger(TestViewFileSystemLinkRegex.class);\n+\n+  private static FileSystem fsDefault;\n+  private static MiniDFSCluster cluster;\n+  private static Configuration clusterConfig;\n+  private static final int NAME_SPACES_COUNT = 3;\n+  private static final int DATA_NODES_COUNT = 3;\n+  private static final int FS_INDEX_DEFAULT = 0;\n+  private static final FileSystem[] FS_HDFS = new FileSystem[NAME_SPACES_COUNT];\n+  private static final String CLUSTER_NAME =\n+      \"TestViewFileSystemLinkRegexCluster\";\n+  private static final File TEST_DIR = GenericTestUtils\n+      .getTestDir(TestViewFileSystemLinkRegex.class.getSimpleName());\n+  private static final String TEST_BASE_PATH =\n+      \"/tmp/TestViewFileSystemLinkRegex\";\n+\n+  @Override\n+  protected FileSystemTestHelper createFileSystemHelper() {\n+    return new FileSystemTestHelper(TEST_BASE_PATH);\n+  }\n+\n+  @BeforeClass\n+  public static void clusterSetupAtBeginning() throws IOException {\n+    SupportsBlocks = true;\n+    clusterConfig = ViewFileSystemTestSetup.createConfig();\n+    clusterConfig.setBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,\n+        true);\n+    cluster = new MiniDFSCluster.Builder(clusterConfig).nnTopology(\n+        MiniDFSNNTopology.simpleFederatedTopology(NAME_SPACES_COUNT))\n+        .numDataNodes(DATA_NODES_COUNT).build();\n+    cluster.waitClusterUp();\n+\n+    for (int i = 0; i < NAME_SPACES_COUNT; i++) {\n+      FS_HDFS[i] = cluster.getFileSystem(i);\n+    }\n+    fsDefault = FS_HDFS[FS_INDEX_DEFAULT];\n+  }\n+\n+  @AfterClass\n+  public static void clusterShutdownAtEnd() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Override\n+  @Before\n+  public void setUp() throws Exception {\n+    fsTarget = fsDefault;\n+    super.setUp();\n+  }\n+\n+  /**\n+   * Override this so that we don't set the targetTestRoot to any path under the\n+   * root of the FS, and so that we don't try to delete the test dir, but rather\n+   * only its contents.\n+   */\n+  @Override\n+  void initializeTargetTestRoot() throws IOException {\n+    targetTestRoot = fsDefault.makeQualified(new Path(\"/\"));\n+    for (FileStatus status : fsDefault.listStatus(targetTestRoot)) {\n+      fsDefault.delete(status.getPath(), true);\n+    }\n+  }\n+\n+  @Override\n+  void setupMountPoints() {\n+    super.setupMountPoints();\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCount() {\n+    return 1; // all point to the same fs so 1 unique token\n+  }\n+\n+  @Override\n+  int getExpectedDelegationTokenCountWithCredentials() {\n+    return 1;\n+  }\n+\n+  public String buildReplaceInterceptorSettingString(String srcRegex,\n+      String replaceString) {\n+    return\n+        RegexMountPointInterceptorType.REPLACE_RESOLVED_DST_PATH.getConfigName()\n+            + INTERCEPTOR_INTERNAL_SEP + srcRegex + INTERCEPTOR_INTERNAL_SEP\n+            + replaceString;\n+  }\n+\n+  public String linkInterceptorSettings(\n+      List<String> interceptorSettingStrList) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+    int listSize = interceptorSettingStrList.size();\n+    for (int i = 0; i < listSize; ++i) {\n+      stringBuilder.append(interceptorSettingStrList.get(i));\n+      if (i < listSize - 1) {\n+        stringBuilder.append(RegexMountPoint.INTERCEPTOR_SEP);\n+      }\n+    }\n+    return stringBuilder.toString();\n+  }\n+\n+  private void createDirWithChildren(\n+      FileSystem fileSystem, Path dir, List<Path> childrenFiles)\n+      throws IOException {\n+    Assert.assertTrue(fileSystem.mkdirs(dir));\n+    int index = 0;\n+    for (Path childFile : childrenFiles) {\n+      createFile(fileSystem, childFile, index, true);\n+    }\n+  }\n+\n+  private void createFile(\n+      FileSystem fileSystem, Path file, int dataLenToWrite, boolean overwrite)\n+      throws IOException {\n+    FSDataOutputStream outputStream = null;\n+    try {\n+      outputStream = fileSystem.create(file, overwrite);\n+      for (int i = 0; i < dataLenToWrite; ++i) {\n+        outputStream.writeByte(i);\n+      }\n+      outputStream.close();\n+    } finally {\n+      if (outputStream != null) {\n+        outputStream.close();\n+      }\n+    }\n+  }\n+\n+  private void createDirWithChildren(\n+      FileSystem fileSystem, Path dir, int childrenFilesCnt)\n+      throws IOException {\n+    List<Path> childrenFiles = new ArrayList<>(childrenFilesCnt);\n+    for (int i = 0; i < childrenFilesCnt; ++i) {\n+      childrenFiles.add(new Path(dir, \"file\" + i));\n+    }\n+    createDirWithChildren(fileSystem, dir, childrenFiles);\n+  }\n+\n+  /**\n+   * The function used to test regex mountpoints.\n+   * @param config - get mountable config from this conf\n+   * @param regexStr - the src path regex expression that applies to this config\n+   * @param dstPathStr - the string of target path\n+   * @param interceptorSettings - the serialized interceptor string to be\n+   *                           applied while resolving the mapping\n+   * @param dirPathBeforeMountPoint - the src path user passed in to be mapped.\n+   * @param expectedResolveResult - the expected path after resolve\n+   *                             dirPathBeforeMountPoint via regex mountpint.\n+   * @param childrenFilesCnt - the child files under dirPathBeforeMountPoint to\n+   *                         be created\n+   * @throws IOException\n+   * @throws URISyntaxException\n+   */\n+  private void testRegexMountpoint(\n+      Configuration config,\n+      String regexStr,\n+      String dstPathStr,\n+      String interceptorSettings,\n+      Path dirPathBeforeMountPoint,\n+      Path expectedResolveResult,\n+      int childrenFilesCnt)\n+      throws IOException, URISyntaxException {\n+    FileSystem vfs = null;\n+    try {\n+      // Set up test env\n+      createDirWithChildren(\n+          fsTarget, expectedResolveResult, childrenFilesCnt);\n+      ConfigUtil.addLinkRegex(\n+          config, CLUSTER_NAME, regexStr, dstPathStr, interceptorSettings);\n+\n+      // Asserts\n+      URI viewFsUri = new URI(\n+          FsConstants.VIEWFS_SCHEME, CLUSTER_NAME, \"/\", null, null);\n+      vfs = FileSystem.get(viewFsUri, config);", "originalCommit": "8786ee8fc0e1b1d617eabd398c4d05a1ff656002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM0NjY5MQ==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r485346691", "bodyText": "Good call", "author": "JohnZZGithub", "createdAt": "2020-09-09T05:28:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMzc4MQ=="}], "type": "inlineReview"}, {"oid": "1cb9b50d18cfaba2d2b4dd2fb84913567ab29201", "url": "https://github.com/apache/hadoop/commit/1cb9b50d18cfaba2d2b4dd2fb84913567ab29201", "message": "HADOOP-15891: fix comments.", "committedDate": "2020-09-09T06:00:26Z", "type": "commit"}, {"oid": "4c543eae3410afb35553a4701ae911919c1f009c", "url": "https://github.com/apache/hadoop/commit/4c543eae3410afb35553a4701ae911919c1f009c", "message": "Fix checkstyle.", "committedDate": "2020-09-09T15:54:15Z", "type": "commit"}, {"oid": "4c543eae3410afb35553a4701ae911919c1f009c", "url": "https://github.com/apache/hadoop/commit/4c543eae3410afb35553a4701ae911919c1f009c", "message": "Fix checkstyle.", "committedDate": "2020-09-09T15:54:15Z", "type": "forcePushed"}, {"oid": "fa30c5ec629e54379368db91dcf51f2414b03d28", "url": "https://github.com/apache/hadoop/commit/fa30c5ec629e54379368db91dcf51f2414b03d28", "message": "Fix checkstyle", "committedDate": "2020-09-09T22:04:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA2MzMxMg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r486063312", "bodyText": "This method not used anywhere?", "author": "umamaheswararao", "createdAt": "2020-09-10T04:57:06Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java", "diffHunk": "@@ -166,6 +166,42 @@ public static void addLinkNfly(final Configuration conf, final String src,\n     addLinkNfly(conf, getDefaultMountTableName(conf), src, null, targets);\n   }\n \n+\n+  /**\n+   * Add a LinkRegex to the config for the specified mount table.\n+   * @param conf - get mountable config from this conf\n+   * @param mountTableName - the mountable name of the regex config item\n+   * @param srcRegex - the src path regex expression that applies to this config\n+   * @param targetStr - the string of target path\n+   */", "originalCommit": "fa30c5ec629e54379368db91dcf51f2414b03d28", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA3MDg5Mg==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r486070892", "bodyText": "Good catch, I guess the next addLinkRegex is used but not this one.", "author": "JohnZZGithub", "createdAt": "2020-09-10T05:17:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA2MzMxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA3MTM1Ng==", "url": "https://github.com/apache/hadoop/pull/2185#discussion_r486071356", "bodyText": "Removed.", "author": "JohnZZGithub", "createdAt": "2020-09-10T05:19:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA2MzMxMg=="}], "type": "inlineReview"}, {"oid": "c494bfe25c6c71d813194c3ed4561866db196f37", "url": "https://github.com/apache/hadoop/commit/c494bfe25c6c71d813194c3ed4561866db196f37", "message": "Remove useless method.", "committedDate": "2020-09-10T05:18:44Z", "type": "commit"}, {"oid": "226f8f3358bdcc7742219dc5dbd69e4f1e476d8c", "url": "https://github.com/apache/hadoop/commit/226f8f3358bdcc7742219dc5dbd69e4f1e476d8c", "message": "Address comments.", "committedDate": "2020-09-10T06:19:45Z", "type": "commit"}]}