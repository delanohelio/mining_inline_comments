{"pr_number": 2052, "pr_title": "HDFS-15386. ReplicaNotFoundException keeps happening in DN after removing multiple DN's data directories.", "pr_createdAt": "2020-06-04T03:00:58Z", "pr_url": "https://github.com/apache/hadoop/pull/2052", "timeline": [{"oid": "200c7c149195f9b17f936eb6fa23f188729936ce", "url": "https://github.com/apache/hadoop/commit/200c7c149195f9b17f936eb6fa23f188729936ce", "message": "HDFS-15386 ReplicaNotFoundException keeps happening in DN after removing multiple DN's data directories", "committedDate": "2020-06-04T07:57:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTA2ODI3MA==", "url": "https://github.com/apache/hadoop/pull/2052#discussion_r435068270", "bodyText": "If you like, you can change this block to below (Java 8 feature):\nList<ReplicaInfo> blocks = blkToInvalidate.computeIfAbsent(bpid, (k) -> new ArrayList<>());\n\nThis will return the existing key if it exists, or create a new one it does not.", "author": "sodonnel", "createdAt": "2020-06-04T08:06:34Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java", "diffHunk": "@@ -578,7 +578,13 @@ public void removeVolumes(\n           // Unlike updating the volumeMap in addVolume(), this operation does\n           // not scan disks.\n           for (String bpid : volumeMap.getBlockPoolList()) {\n-            List<ReplicaInfo> blocks = new ArrayList<>();\n+            List<ReplicaInfo> blocks;\n+            if (blkToInvalidate.containsKey(bpid)) {\n+              blocks = blkToInvalidate.get(bpid);\n+            } else {\n+              blocks = new ArrayList<>();\n+              blkToInvalidate.put(bpid, blocks);\n+            }", "originalCommit": "200c7c149195f9b17f936eb6fa23f188729936ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTE4ODc1NQ==", "url": "https://github.com/apache/hadoop/pull/2052#discussion_r435188755", "bodyText": "Just thinking - this fix will need to go on all branches back into the 2.8 branch, which I think still uses Java 7. Therefore it would be ideal to have the computeIfAbsent method for trunk and 3.x but we will need a patch like this existing one for branch 2.", "author": "sodonnel", "createdAt": "2020-06-04T11:39:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTA2ODI3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTIyMzk0Mw==", "url": "https://github.com/apache/hadoop/pull/2052#discussion_r435223943", "bodyText": "@sodonnel Thank you for reviewing this!\nOkay, I will change the code for your review here. And after this is committed to trunk, I will make a new PR for branch 2, which is like the original code.", "author": "brfrn169", "createdAt": "2020-06-04T12:44:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTA2ODI3MA=="}], "type": "inlineReview"}, {"oid": "207182829ae493e696c49b3baa6a4e7e36c21c35", "url": "https://github.com/apache/hadoop/commit/207182829ae493e696c49b3baa6a4e7e36c21c35", "message": "HDFS-15386 ReplicaNotFoundException keeps happening in DN after removing multiple DN's data directories", "committedDate": "2020-06-04T12:48:47Z", "type": "commit"}, {"oid": "207182829ae493e696c49b3baa6a4e7e36c21c35", "url": "https://github.com/apache/hadoop/commit/207182829ae493e696c49b3baa6a4e7e36c21c35", "message": "HDFS-15386 ReplicaNotFoundException keeps happening in DN after removing multiple DN's data directories", "committedDate": "2020-06-04T12:48:47Z", "type": "forcePushed"}]}