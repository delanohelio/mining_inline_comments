{"pr_number": 2021, "pr_title": "Hadoop-17015. ABFS: Handling Rename and Delete idempotency", "pr_createdAt": "2020-05-13T17:34:01Z", "pr_url": "https://github.com/apache/hadoop/pull/2021", "timeline": [{"oid": "5a11d4d5c8edd1feb9260afddd8ed8319f0347de", "url": "https://github.com/apache/hadoop/commit/5a11d4d5c8edd1feb9260afddd8ed8319f0347de", "message": "rename changes", "committedDate": "2020-04-29T16:35:12Z", "type": "commit"}, {"oid": "6471661ab59e297c8f476bf6ece229b8cc5a4bcf", "url": "https://github.com/apache/hadoop/commit/6471661ab59e297c8f476bf6ece229b8cc5a4bcf", "message": "Some refactoring", "committedDate": "2020-05-13T03:39:44Z", "type": "commit"}, {"oid": "16f0b7bf826691e02ca7bafd79c3b1f4651197c1", "url": "https://github.com/apache/hadoop/commit/16f0b7bf826691e02ca7bafd79c3b1f4651197c1", "message": "Merge from master", "committedDate": "2020-05-13T04:31:13Z", "type": "commit"}, {"oid": "c158597fa2efc7e75cbfd09ea3ff48b0e0b8904a", "url": "https://github.com/apache/hadoop/commit/c158597fa2efc7e75cbfd09ea3ff48b0e0b8904a", "message": "Checkstyle and findbugs fixes", "committedDate": "2020-05-13T16:23:47Z", "type": "commit"}, {"oid": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "url": "https://github.com/apache/hadoop/commit/77f54cb8323ef8137df430aff3e16caf6ab3497f", "message": "Removing redundant new lines", "committedDate": "2020-05-13T16:32:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYzOTY1MA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424639650", "bodyText": "I think private is enough here.", "author": "bilaharith", "createdAt": "2020-05-13T18:18:49Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/DateTimeUtils.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.Locale;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public final class DateTimeUtils {\n+  public static final Logger LOG = LoggerFactory.getLogger(DateTimeUtils.class);", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1NjQyMw==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424956423", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-05-14T08:23:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYzOTY1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0MTY4Ng==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424641686", "bodyText": "This line exceeds checkstyle limit of 80 chars", "author": "bilaharith", "createdAt": "2020-05-13T18:22:13Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/DateTimeUtils.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.Locale;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public final class DateTimeUtils {\n+  public static final Logger LOG = LoggerFactory.getLogger(DateTimeUtils.class);\n+  private static final String DATE_TIME_PATTERN = \"E, dd MMM yyyy HH:mm:ss z\";\n+\n+  public static long parseLastModifiedTime(final String lastModifiedTime) {\n+    long parsedTime = 0;\n+    try {\n+      Date utcDate = new SimpleDateFormat(DATE_TIME_PATTERN, Locale.US).parse(lastModifiedTime);", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1NjUxNA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424956514", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-05-14T08:23:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0MTY4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0MzY0MA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424643640", "bodyText": "Import order", "author": "bilaharith", "createdAt": "2020-05-13T18:25:21Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelete.java", "diffHunk": "@@ -28,20 +28,36 @@\n \n import org.junit.Test;\n \n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+import org.apache.hadoop.fs.azurebfs.services.ExponentialRetryPolicy;\n+import org.apache.hadoop.fs.azurebfs.services.SharedKeyCredentials;\n import org.apache.hadoop.fs.FileAlreadyExistsException;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n \n+import static java.net.HttpURLConnection.HTTP_NOT_FOUND;\n+import static java.net.HttpURLConnection.HTTP_OK;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DOT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_DELETE_CONSIDERED_IDEMPOTENT;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertDeleted;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\n import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.mockito.Mockito.mock;", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1NjU3OA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424956578", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-05-14T08:23:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0MzY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0NDcwNQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424644705", "bodyText": "CheckStyle: I think this line exceeds 80 chars limit", "author": "bilaharith", "createdAt": "2020-05-13T18:27:06Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -748,7 +747,7 @@ public FileStatus getFileStatus(final Path path) throws IOException {\n           long contentLength = entry.contentLength() == null ? 0 : entry.contentLength();\n           boolean isDirectory = entry.isDirectory() == null ? false : entry.isDirectory();\n           if (entry.lastModified() != null && !entry.lastModified().isEmpty()) {\n-            lastModifiedMillis = parseLastModifiedTime(entry.lastModified());\n+            lastModifiedMillis = DateTimeUtils.parseLastModifiedTime(entry.lastModified());", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1NjY0Nw==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424956647", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-05-14T08:23:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0NDcwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NzQxNQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424657415", "bodyText": "Would it be better to move the idempotency related methods to a separate Utility/Helper class?", "author": "bilaharith", "createdAt": "2020-05-13T18:48:38Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -321,9 +324,81 @@ public AbfsRestOperation renamePath(String source, final String destination, fin\n             url,\n             requestHeaders);\n     op.execute();\n+\n+    if (op.getResult().getStatusCode() != HttpURLConnection.HTTP_OK) {\n+      return renameIdempotencyCheckOp(op, destination);\n+    }\n+\n     return op;\n   }\n \n+  /**", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1NzY0OQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424957649", "bodyText": "The changes are not utility related and are insync with the handling of the ABFS response. The reason they were included as separate methods was to enable mock testing which was otherwise not possible. Retaining the change as such.", "author": "snvijaya", "createdAt": "2020-05-14T08:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NzQxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxNzQ0Nw==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425317447", "bodyText": "One option is restrict the method accessibility tp package level and use @VisibleForTesting annotation. Keeping a method public solely for testing doesn't look good practice.\nAlso the idea is, If you have methods 'assisting', chances are the class is actually doing too much. Moving these methods into separate classes with public interfaces keeps the class with the assisting methods responsible for one thing and one thing only (see Single Responsibility Principle). This move into separte classes automatically makes for a testable structure as your methods must be made public\nhttps://softwareengineering.stackexchange.com/questions/274937/is-it-bad-practice-to-make-methods-public-solely-for-the-sake-of-unit-testing", "author": "bilaharith", "createdAt": "2020-05-14T17:40:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NzQxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYyNTkxNA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425625914", "bodyText": "AbfsClient class handles triggering of requests to Store backend and returning the AbfsRestOperation back. For Rename and Delete, the response to return is not determined if request was re-tried by the idempotent logic. It will be not right to consider these methods as \"assisting\" or providing a utility service and are part of the actual flow.", "author": "snvijaya", "createdAt": "2020-05-15T07:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NzQxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1OTEyNw==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424659127", "bodyText": "Does it convey the right meaning by keeping DEFAULT prefix(as the same is not a configurable flag)?", "author": "bilaharith", "createdAt": "2020-05-13T18:51:26Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -81,5 +81,7 @@\n   public static final String DEFAULT_FS_AZURE_USER_AGENT_PREFIX = EMPTY_STRING;\n   public static final String DEFAULT_VALUE_UNKNOWN = \"UNKNOWN\";\n \n+  public static final boolean DEFAULT_DELETE_CONSIDERED_IDEMPOTENT = true;", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1ODc5OQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424958799", "bodyText": "DEFAULT prefix is to highlight the default behaviour of these fields in ABFS driver. There do not mandate the need to be backed by user modifiable config in AbfsConfiguration. AbfsConfiguration is the consumer of these defaults.", "author": "snvijaya", "createdAt": "2020-05-14T08:27:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1OTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyOTkyNA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425329924", "bodyText": "As you said keeping the DEFAULT prefix to highlight the default behaviour it is assumed that there are non default behaviours associated with this feature, are there any?", "author": "bilaharith", "createdAt": "2020-05-14T17:59:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1OTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU0NTU4OA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425545588", "bodyText": "Definition of defaults need not be confused with what should be given an option to override using core-site configs.\nThe behaviour of delete idempotency is being defined by default now, and the non default behaviour (where handling is absent) would be its not idempotent.", "author": "snvijaya", "createdAt": "2020-05-15T03:26:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1OTEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2MjQzMg==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424662432", "bodyText": "Better to declare constant as static", "author": "bilaharith", "createdAt": "2020-05-13T18:56:56Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelete.java", "diffHunk": "@@ -28,20 +28,36 @@\n \n import org.junit.Test;\n \n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+import org.apache.hadoop.fs.azurebfs.services.ExponentialRetryPolicy;\n+import org.apache.hadoop.fs.azurebfs.services.SharedKeyCredentials;\n import org.apache.hadoop.fs.FileAlreadyExistsException;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n \n+import static java.net.HttpURLConnection.HTTP_NOT_FOUND;\n+import static java.net.HttpURLConnection.HTTP_OK;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DOT;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_DELETE_CONSIDERED_IDEMPOTENT;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertDeleted;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\n import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n /**\n  * Test delete operation.\n  */\n public class ITestAzureBlobFileSystemDelete extends\n     AbstractAbfsIntegrationTest {\n \n+  private final int reducedRetryCount = 1;", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1ODg3MA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424958870", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-05-14T08:27:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2MjQzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDQzOA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424664438", "bodyText": "Try to use the better alternative assertJ.", "author": "bilaharith", "createdAt": "2020-05-13T19:00:31Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelete.java", "diffHunk": "@@ -130,4 +146,49 @@ public Void call() throws Exception {\n     assertPathDoesNotExist(fs, \"deleted\", dir);\n \n   }\n+\n+  @Test\n+  public void testDeleteIdempotency() throws Exception {\n+    org.junit.Assume.assumeTrue(DEFAULT_DELETE_CONSIDERED_IDEMPOTENT);\n+    // Config to reduce the retry and maxBackoff time for test run\n+    AbfsConfiguration abfsConfig = getConfiguration();\n+    abfsConfig.setMaxIoRetries(reducedRetryCount);\n+    abfsConfig.setMaxBackoffIntervalMilliseconds(reducedMaxBackoffIntervalMs);\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    AbfsClient abfsClient = fs.getAbfsStore().getClient();\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\",\n+        this.getAccountName(),\n+        abfsConfig);\n+\n+    // Create test AbfsClient\n+    AbfsClient testClient = new AbfsClient(\n+        abfsClient.getBaseUrl(),\n+        new SharedKeyCredentials(abfsConfig.getAccountName().substring(0,\n+            abfsConfig.getAccountName().indexOf(DOT)),\n+            abfsConfig.getStorageAccountKey()),\n+        abfsConfig,\n+        new ExponentialRetryPolicy(reducedRetryCount),\n+        abfsConfig.getTokenProvider(),\n+        tracker);\n+\n+    // Mock instance of AbfsRestOperation\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    // Set retryCount to non-zero\n+    when(op.getRetryCount()).thenReturn(reducedRetryCount);\n+\n+    // Mock instance of Http Operation response. This will return HTTP:Not Found\n+    AbfsHttpOperation http404Op = mock(AbfsHttpOperation.class);\n+    when(http404Op.getStatusCode()).thenReturn(HTTP_NOT_FOUND);\n+\n+    // Mock delete response to 404\n+    when(op.getResult()).thenReturn(http404Op);\n+\n+    assertTrue(", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1ODkyMQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424958921", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-05-14T08:27:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDQzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDc4NQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424664785", "bodyText": "Import order", "author": "bilaharith", "createdAt": "2020-05-13T19:01:07Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java", "diffHunk": "@@ -28,20 +28,37 @@\n import org.junit.Assert;\n import org.junit.Test;\n \n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+import org.apache.hadoop.fs.azurebfs.services.ExponentialRetryPolicy;\n+import org.apache.hadoop.fs.azurebfs.services.SharedKeyCredentials;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n \n+import static java.util.UUID.randomUUID;\n+import static java.net.HttpURLConnection.HTTP_BAD_REQUEST;\n+import static java.net.HttpURLConnection.HTTP_NOT_FOUND;\n+import static java.net.HttpURLConnection.HTTP_OK;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DOT;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertMkdirs;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertRenameOutcome;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertIsFile;\n+import static org.mockito.Mockito.mock;", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1OTAwMA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424959000", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-05-14T08:27:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDc4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDk5Mg==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424664992", "bodyText": "Better to keep constants as static", "author": "bilaharith", "createdAt": "2020-05-13T19:01:30Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java", "diffHunk": "@@ -28,20 +28,37 @@\n import org.junit.Assert;\n import org.junit.Test;\n \n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+import org.apache.hadoop.fs.azurebfs.services.ExponentialRetryPolicy;\n+import org.apache.hadoop.fs.azurebfs.services.SharedKeyCredentials;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n \n+import static java.util.UUID.randomUUID;\n+import static java.net.HttpURLConnection.HTTP_BAD_REQUEST;\n+import static java.net.HttpURLConnection.HTTP_NOT_FOUND;\n+import static java.net.HttpURLConnection.HTTP_OK;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.DOT;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertMkdirs;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertRenameOutcome;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertIsFile;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n /**\n  * Test rename operation.\n  */\n public class ITestAzureBlobFileSystemRename extends\n     AbstractAbfsIntegrationTest {\n \n+  private final int reducedRetryCount = 1;", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1OTA2Mg==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424959062", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-05-14T08:27:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDk5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3MjE0NA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424672144", "bodyText": "Couldn't find it reasonable to keep all the cases into the same method with if-else. I think separating the same could improve readability of the test cases.", "author": "bilaharith", "createdAt": "2020-05-13T19:14:35Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java", "diffHunk": "@@ -149,4 +166,135 @@ public void testPosixRenameDirectory() throws Exception {\n     assertTrue(fs.exists(new Path(\"testDir2/test4/test3\")));\n     assertFalse(fs.exists(new Path(\"testDir2/test1/test2/test3\")));\n   }\n+\n+  @Test\n+  public void testRenameRetryFailureAsHTTP400() throws Exception {\n+    // Rename failed as Bad Request\n+    // RenameIdempotencyCheck should throw back the rename failure Op\n+    testRenameTimeout(HTTP_BAD_REQUEST, HTTP_BAD_REQUEST, false);\n+  }\n+\n+  @Test\n+  public void testRenameRetryFailureAsHTTP404() throws Exception {\n+    // Rename failed as FileNotFound and the destination LMT is\n+    // within TimespanForIdentifyingRecentOperationThroughLMT\n+    testRenameTimeout(HTTP_NOT_FOUND, HTTP_OK, false);\n+  }\n+\n+  @Test\n+  public void testRenameRetryFailureWithDestOldLMT() throws Exception {\n+    // Rename failed as FileNotFound and the destination LMT is\n+    // older than TimespanForIdentifyingRecentOperationThroughLMT\n+    testRenameTimeout(HTTP_NOT_FOUND, HTTP_NOT_FOUND, true);\n+  }\n+\n+  private void testRenameTimeout(", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk2MDMyNg==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424960326", "bodyText": "Over the past PRs we have had several instances of reusable code being duplicated. These test cases share a lot of reusable code, hence the reusable code is put into a method. Have made some minor updates which might help improve readability.", "author": "snvijaya", "createdAt": "2020-05-14T08:29:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3MjE0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0Mzg0MA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425343840", "bodyText": "I see common code as (1. Creating FS instance, 2. Creating testClient instance, 3. Creating mock AbfsRestOperation  instance, 4. Common assertion). This is clubbed with the non common codes (1. Creating http400Op  instance for one particular test case, 2. Creating http404Op  instance for another particular test case)\nShall we move the common instances that are required to a separate private methods and call those methods in each test case? That would help improve readability and solved the problem of code duplication.\nI think we should relook and address the issues with the old PRs if those are recent, where resusable code is duplicated. Could you please create a workitem for this.", "author": "bilaharith", "createdAt": "2020-05-14T18:23:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3MjE0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYyNzU3Nw==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425627577", "bodyText": "Creating a method, passing over all indiividual mock instances created back to test method will actually render code more unreadable. Will leave this comment open if any other reviewer feels the code is unreadable too.\nFor issues that have come across for my code changes, i have made modifications with the respective PRs. Should definitely keep a look out for any that come across.", "author": "snvijaya", "createdAt": "2020-05-15T07:51:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3MjE0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3MjkwMA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424672900", "bodyText": "this. prefix?", "author": "bilaharith", "createdAt": "2020-05-13T19:15:54Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -157,7 +175,7 @@ void execute() throws AzureBlobFileSystemException {\n       requestHeaders.add(httpHeader);\n     }\n \n-    int retryCount = 0;\n+    retryCount = 0;", "originalCommit": "77f54cb8323ef8137df430aff3e16caf6ab3497f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk2MDk2NA==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r424960964", "bodyText": "this. usage has been advised against many times in previous PRs by Steve to align with diff IDE. Retaining current code.", "author": "snvijaya", "createdAt": "2020-05-14T08:30:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3MjkwMA=="}], "type": "inlineReview"}, {"oid": "be7c0240a9507f4eb80f52f5a3f2c3f7899f249b", "url": "https://github.com/apache/hadoop/commit/be7c0240a9507f4eb80f52f5a3f2c3f7899f249b", "message": "Review comments", "committedDate": "2020-05-14T07:24:59Z", "type": "commit"}, {"oid": "4c9cdbcaa9e7c9530ce986523776d52ec1484684", "url": "https://github.com/apache/hadoop/commit/4c9cdbcaa9e7c9530ce986523776d52ec1484684", "message": "checkstyle fix", "committedDate": "2020-05-14T08:21:57Z", "type": "commit"}, {"oid": "db5c57826fd39894ad88a2ea0628ebbe84b48091", "url": "https://github.com/apache/hadoop/commit/db5c57826fd39894ad88a2ea0628ebbe84b48091", "message": "Change LMT recent detection logic", "committedDate": "2020-05-14T17:11:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyNDM3Mw==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425324373", "bodyText": "Widening the method accessibility by making it public solely for testing is not a good practice. See if at least the acces can be restricted to package level and use @VisibleForTesting annotation.", "author": "bilaharith", "createdAt": "2020-05-14T17:50:12Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -131,7 +135,7 @@ public String getFileSystem() {\n     return filesystem;\n   }\n \n-  protected AbfsPerfTracker getAbfsPerfTracker() {\n+  public AbfsPerfTracker getAbfsPerfTracker() {", "originalCommit": "db5c57826fd39894ad88a2ea0628ebbe84b48091", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzMzMyMQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425633321", "bodyText": "This wont be necessary as test code is refactored.", "author": "snvijaya", "createdAt": "2020-05-15T08:02:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyNDM3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0OTA3MQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425349071", "bodyText": "These are not asserJ assertions.\nThat is better alternative, defenitly improved readability. Steve also reccomands the same.\nthe format is assertThat()\n.describedAs(\"describe what assertion is going tobe made and the same is going to be printed in logs\").<assertion method ex: isEqualTo()>", "author": "bilaharith", "createdAt": "2020-05-14T18:33:10Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelete.java", "diffHunk": "@@ -130,4 +148,50 @@ public Void call() throws Exception {\n     assertPathDoesNotExist(fs, \"deleted\", dir);\n \n   }\n+\n+  @Test\n+  public void testDeleteIdempotency() throws Exception {\n+    org.junit.Assume.assumeTrue(DEFAULT_DELETE_CONSIDERED_IDEMPOTENT);\n+    // Config to reduce the retry and maxBackoff time for test run\n+    AbfsConfiguration abfsConfig = getConfiguration();\n+    abfsConfig.setMaxIoRetries(REDUCED_RETRY_COUNT);\n+    abfsConfig.setMaxBackoffIntervalMilliseconds(REDUCED_MAX_BACKOFF_INTERVALS_MS);\n+\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    AbfsClient abfsClient = fs.getAbfsStore().getClient();\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\",\n+        this.getAccountName(),\n+        abfsConfig);\n+\n+    // Create test AbfsClient\n+    AbfsClient testClient = new AbfsClient(\n+        abfsClient.getBaseUrl(),\n+        new SharedKeyCredentials(abfsConfig.getAccountName().substring(0,\n+            abfsConfig.getAccountName().indexOf(DOT)),\n+            abfsConfig.getStorageAccountKey()),\n+        abfsConfig,\n+        new ExponentialRetryPolicy(REDUCED_RETRY_COUNT),\n+        abfsConfig.getTokenProvider(),\n+        tracker);\n+\n+    // Mock instance of AbfsRestOperation\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    // Set retryCount to non-zero\n+    when(op.isARetriedRequest()).thenReturn(true);\n+\n+    // Mock instance of Http Operation response. This will return HTTP:Not Found\n+    AbfsHttpOperation http404Op = mock(AbfsHttpOperation.class);\n+    when(http404Op.getStatusCode()).thenReturn(HTTP_NOT_FOUND);\n+\n+    // Mock delete response to 404\n+    when(op.getResult()).thenReturn(http404Op);\n+\n+    assertEquals(", "originalCommit": "db5c57826fd39894ad88a2ea0628ebbe84b48091", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzMjg4Ng==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425632886", "bodyText": "Fixed.", "author": "snvijaya", "createdAt": "2020-05-15T08:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0OTA3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM1MDc2Mg==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425350762", "bodyText": "I am not sure if this annotation is reccommanded as the method is anyway public. I personaly find it ok as it adds to readability.", "author": "bilaharith", "createdAt": "2020-05-14T18:36:13Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -764,6 +764,11 @@ public void setMaxIoRetries(int maxIoRetries) {\n     this.maxIoRetries = maxIoRetries;\n   }\n \n+  @VisibleForTesting", "originalCommit": "db5c57826fd39894ad88a2ea0628ebbe84b48091", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYyMTAzOQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425621039", "bodyText": "Yes. As can be seen for other methods  below.", "author": "snvijaya", "createdAt": "2020-05-15T07:38:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM1MDc2Mg=="}], "type": "inlineReview"}, {"oid": "14be440a53dfcd63a6a89548ab8bdc2ca48d2ea1", "url": "https://github.com/apache/hadoop/commit/14be440a53dfcd63a6a89548ab8bdc2ca48d2ea1", "message": "test code refactored", "committedDate": "2020-05-15T07:34:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzNjY2MQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425636661", "bodyText": "Not sure if this class is the right place for this method.", "author": "bilaharith", "createdAt": "2020-05-15T08:09:39Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsConfigurationFieldsValidation.java", "diffHunk": "@@ -182,4 +182,11 @@ public void testSSLSocketFactoryConfiguration()\n     assertEquals(DelegatingSSLSocketFactory.SSLChannelMode.OpenSSL, localAbfsConfiguration.getPreferredSSLFactoryOption());\n   }\n \n+  public static AbfsConfiguration updateRetryConfigs(AbfsConfiguration abfsConfig,", "originalCommit": "14be440a53dfcd63a6a89548ab8bdc2ca48d2ea1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY1MzIwMQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425653201", "bodyText": "This is the unit test class for AbfsConfiguration. I do not want to create separate utility class just to return a test instance of main class.", "author": "snvijaya", "createdAt": "2020-05-15T08:40:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzNjY2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzODMxMg==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425638312", "bodyText": "Same doubt as the above comment.\nTestAbfsClient should have test methods the AbfsClient and supporting methods.\nThis method looks more like a Utility.", "author": "bilaharith", "createdAt": "2020-05-15T08:13:03Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsClient.java", "diffHunk": "@@ -240,4 +242,25 @@ public void verifyUserAgentClusterType() throws Exception {\n       .contains(DEFAULT_VALUE_UNKNOWN);\n   }\n \n+  public static AbfsClient createTestClientFromCurrentContext(", "originalCommit": "14be440a53dfcd63a6a89548ab8bdc2ca48d2ea1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY1Mzk4Mg==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425653982", "bodyText": "Again, dont want to create a separate test utility class for AbfsClient alone to return a test instance and hence have placed it in the unit test class for AbfsClient.\nFor future tests that will need to mock or create new instances, it will be easy to check respective unit test class for any method available.", "author": "snvijaya", "createdAt": "2020-05-15T08:42:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzODMxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzOTE3NQ==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425639175", "bodyText": "This new line can be removed", "author": "bilaharith", "createdAt": "2020-05-15T08:14:45Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelete.java", "diffHunk": "@@ -26,22 +26,40 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n+import org.assertj.core.api.Assertions;\n+import org.junit.Assume;\n import org.junit.Test;\n \n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n+import org.apache.hadoop.fs.azurebfs.services.TestAbfsClient;\n import org.apache.hadoop.fs.FileAlreadyExistsException;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n \n+import static java.net.HttpURLConnection.HTTP_NOT_FOUND;\n+import static java.net.HttpURLConnection.HTTP_OK;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.DEFAULT_DELETE_CONSIDERED_IDEMPOTENT;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertDeleted;\n import static org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist;\n+", "originalCommit": "14be440a53dfcd63a6a89548ab8bdc2ca48d2ea1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTY2MTI5Mw==", "url": "https://github.com/apache/hadoop/pull/2021#discussion_r425661293", "bodyText": "Fixed,", "author": "snvijaya", "createdAt": "2020-05-15T08:55:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYzOTE3NQ=="}], "type": "inlineReview"}, {"oid": "a6e090806bab4cd07928ed895721b32c2633b54e", "url": "https://github.com/apache/hadoop/commit/a6e090806bab4cd07928ed895721b32c2633b54e", "message": "Remove redundant new line", "committedDate": "2020-05-15T08:54:32Z", "type": "commit"}, {"oid": "738af3a2bebe4e515ea23616647e57e453a38761", "url": "https://github.com/apache/hadoop/commit/738af3a2bebe4e515ea23616647e57e453a38761", "message": "Merge to trunk", "committedDate": "2020-05-19T15:30:19Z", "type": "commit"}, {"oid": "639205dbd2d348ca22be9606a632de37af2931ab", "url": "https://github.com/apache/hadoop/commit/639205dbd2d348ca22be9606a632de37af2931ab", "message": "Dummy checkin to Trigger Yetus", "committedDate": "2020-05-19T17:57:27Z", "type": "commit"}, {"oid": "65801090edfe0b0d6fbfea4545d477b76b009d96", "url": "https://github.com/apache/hadoop/commit/65801090edfe0b0d6fbfea4545d477b76b009d96", "message": "Undo earlier dummy checkin to Trigger Yetus", "committedDate": "2020-05-19T18:15:47Z", "type": "commit"}]}