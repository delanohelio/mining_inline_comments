{"pr_number": 2296, "pr_title": "HDFS-15568. namenode start failed to start when dfs.namenode.max.snapshot.limit set.", "pr_createdAt": "2020-09-10T08:19:17Z", "pr_url": "https://github.com/apache/hadoop/pull/2296", "timeline": [{"oid": "c786c270907011ecaef879a98d931d275ab9a15b", "url": "https://github.com/apache/hadoop/commit/c786c270907011ecaef879a98d931d275ab9a15b", "message": "HDFS-15568. namenode start failed to start when dfs.namenode.snapshot.max.limit set.", "committedDate": "2020-09-10T08:17:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTM0OA==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579348", "bodyText": "Add javadoc", "author": "goiri", "createdAt": "2020-09-10T19:19:09Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java", "diffHunk": "@@ -508,6 +508,10 @@ FSNamesystem getFSNamesystem() {\n     return namesystem;\n   }\n \n+  public boolean isImageLoaded() {", "originalCommit": "c786c270907011ecaef879a98d931d275ab9a15b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTQzOA==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579438", "bodyText": "javadoc", "author": "goiri", "createdAt": "2020-09-10T19:19:21Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -368,6 +368,13 @@ void assertFirstSnapshot(INodeDirectory dir,\n     }\n   }\n \n+  boolean captureOpenFiles() {", "originalCommit": "c786c270907011ecaef879a98d931d275ab9a15b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTYxMQ==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579611", "bodyText": "VisibleForTesting", "author": "goiri", "createdAt": "2020-09-10T19:19:37Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -368,6 +368,13 @@ void assertFirstSnapshot(INodeDirectory dir,\n     }\n   }\n \n+  boolean captureOpenFiles() {\n+    return captureOpenFiles;\n+  }\n+\n+  int getMaxSnapshotLimit() {", "originalCommit": "c786c270907011ecaef879a98d931d275ab9a15b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTgxNQ==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579815", "bodyText": "Use LambdaTestUtils#intercept", "author": "goiri", "createdAt": "2020-09-10T19:19:58Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);", "originalCommit": "c786c270907011ecaef879a98d931d275ab9a15b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3OTk0MQ==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486579941", "bodyText": "Extract the getSnapshotManager()", "author": "goiri", "createdAt": "2020-09-10T19:20:15Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(\n+          StringUtils.toLowerCase(se.getMessage()).contains(\n+              \"max snapshot limit\"));\n+    }\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, cluster.getNamesystem().", "originalCommit": "c786c270907011ecaef879a98d931d275ab9a15b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU4MDExOA==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486580118", "bodyText": "LambdaTestUtils", "author": "goiri", "createdAt": "2020-09-10T19:20:36Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,68 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void SnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(\n+          StringUtils.toLowerCase(se.getMessage()).contains(\n+              \"max snapshot limit\"));\n+    }\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, cluster.getNamesystem().\n+        getSnapshotManager().getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2,\n+        cluster.getNamesystem().getSnapshotManager().getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    try {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+      Assert.fail(\"Expected SnapshotException not thrown\");\n+    } catch (SnapshotException se) {\n+      Assert.assertTrue(", "originalCommit": "c786c270907011ecaef879a98d931d275ab9a15b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU5NDQ3OA==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r486594478", "bodyText": "I suggest to add limit type to the error message as below.\ndiff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\nindex 266c0a71241..7a47ab4000d 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java\n@@ -190,8 +190,7 @@ public Snapshot addSnapshot(INodeDirectory snapshotRoot,\n           + n + \" snapshot(s) and the snapshot quota is \"\n           + snapshotQuota);\n     }\n-    snapshotManager.checkSnapshotLimit(snapshotManager.\n-        getMaxSnapshotLimit(), n);\n+    snapshotManager.checkPerDirectorySnapshotLimit(n);\n     final Snapshot s = new Snapshot(id, name, snapshotRoot);\n     final byte[] nameBytes = s.getRoot().getLocalNameBytes();\n     final int i = searchSnapshot(nameBytes);\ndiff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\nindex 0a2e18c3dc3..7c482074486 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n@@ -455,7 +455,7 @@ public String createSnapshot(final LeaseManager leaseManager,\n           \"snapshot IDs and ID rollover is not supported.\");\n     }\n     int n = numSnapshots.get();\n-    checkSnapshotLimit(maxSnapshotFSLimit, n);\n+    checkFileSystemSnapshotLimit(n);\n     srcRoot.addSnapshot(this, snapshotName, leaseManager, mtime);\n       \n     //create success, update id\n@@ -464,12 +464,19 @@ public String createSnapshot(final LeaseManager leaseManager,\n     return Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n   }\n \n-  void checkSnapshotLimit(int limit, int numSnapshots)\n-      throws SnapshotException {\n+  void checkFileSystemSnapshotLimit(int n) throws SnapshotException {\n+    checkSnapshotLimit(maxSnapshotFSLimit, n, \"file system\");\n+  }\n+\n+  void checkPerDirectorySnapshotLimit(int n) throws SnapshotException {\n+    checkSnapshotLimit(maxSnapshotLimit, n, \"per directory\");\n+  }\n+\n+  private void checkSnapshotLimit(int limit, int numSnapshots,\n+      String type) throws SnapshotException {\n     if (numSnapshots >= limit) {\n-      String msg = \"there are already \" + (numSnapshots + 1)\n-          + \" snapshot(s) and the max snapshot limit is \"\n-          + limit;\n+      String msg = \"There are already \" + (numSnapshots + 1)\n+          + \" snapshot(s) and the \" + type + \" snapshot limit is \" + limit;\n       if (fsdir.isImageLoaded()) {\n         // We have reached the maximum snapshot limit\n         throw new SnapshotException(", "author": "szetszwo", "createdAt": "2020-09-10T19:49:15Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -448,22 +455,31 @@ public String createSnapshot(final LeaseManager leaseManager,\n           \"snapshot IDs and ID rollover is not supported.\");\n     }\n     int n = numSnapshots.get();\n-    if (n >= maxSnapshotFSLimit) {\n-      // We have reached the maximum snapshot limit\n-      throw new SnapshotException(\n-          \"Failed to create snapshot: there are already \" + (n + 1)\n-              + \" snapshot(s) and the max snapshot limit is \"\n-              + maxSnapshotFSLimit);\n-    }\n-\n-    srcRoot.addSnapshot(snapshotCounter, snapshotName, leaseManager,\n-        this.captureOpenFiles, maxSnapshotLimit, mtime);\n+    checkSnapshotLimit(maxSnapshotFSLimit, n);\n+    srcRoot.addSnapshot(this, snapshotName, leaseManager, mtime);\n       \n     //create success, update id\n     snapshotCounter++;\n     numSnapshots.getAndIncrement();\n     return Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n   }\n+\n+  void checkSnapshotLimit(int limit, int numSnapshots)", "originalCommit": "c786c270907011ecaef879a98d931d275ab9a15b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "943166ec78a55f558375796ad61d8dd954393ee4", "url": "https://github.com/apache/hadoop/commit/943166ec78a55f558375796ad61d8dd954393ee4", "message": "Addressed review comments.", "committedDate": "2020-09-11T09:09:44Z", "type": "commit"}, {"oid": "ca83922fe5e11535aee53c07495cd03113144669", "url": "https://github.com/apache/hadoop/commit/ca83922fe5e11535aee53c07495cd03113144669", "message": "Addressed Checkstyle issues.", "committedDate": "2020-09-11T13:12:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODA2Mw==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238063", "bodyText": "Who uses this \"i\" later? in for loop def should be good.", "author": "goiri", "createdAt": "2020-09-11T19:12:02Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;", "originalCommit": "ca83922fe5e11535aee53c07495cd03113144669", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODE0NA==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238144", "bodyText": "1 line?", "author": "goiri", "createdAt": "2020-09-11T19:12:12Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    int i = 0;\n+    for (; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    SnapshotManager snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+    // now change max snapshot FS limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    snapshotManager = cluster.getNamesystem().", "originalCommit": "ca83922fe5e11535aee53c07495cd03113144669", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIzODQwNw==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r487238407", "bodyText": "should we clean this cluster?", "author": "goiri", "createdAt": "2020-09-11T19:12:45Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +137,58 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).", "originalCommit": "ca83922fe5e11535aee53c07495cd03113144669", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "66b21abbd9a7d7915e8b12f171d25cd3f3e28c93", "url": "https://github.com/apache/hadoop/commit/66b21abbd9a7d7915e8b12f171d25cd3f3e28c93", "message": "Addressed review comments.", "committedDate": "2020-09-14T16:38:39Z", "type": "commit"}, {"oid": "9db4fbce5f2eb888be5baceb10358cffa20f79bb", "url": "https://github.com/apache/hadoop/commit/9db4fbce5f2eb888be5baceb10358cffa20f79bb", "message": "Addressed TestSnapshotManager test failure.", "committedDate": "2020-09-14T16:48:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEwOTQzMg==", "url": "https://github.com/apache/hadoop/pull/2296#discussion_r488109432", "bodyText": "We may need to do it in a finally to make sure we always clean it. Also check for null.", "author": "goiri", "createdAt": "2020-09-14T17:39:15Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java", "diffHunk": "@@ -133,4 +138,57 @@ public void testValidateSnapshotIDWidth() throws Exception {\n         getMaxSnapshotID() < Snapshot.CURRENT_STATE_ID);\n   }\n \n+  @Test\n+  public void testSnapshotLimitOnRestart() throws Exception {\n+    final Configuration conf = new Configuration();\n+    final Path snapshottableDir\n+        = new Path(\"/\" + getClass().getSimpleName());\n+    int numSnapshots = 5;\n+    conf.setInt(DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, numSnapshots);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT,\n+        numSnapshots * 2);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).\n+        numDataNodes(0).build();\n+    cluster.waitActive();\n+    DistributedFileSystem hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(snapshottableDir);\n+    hdfs.allowSnapshot(snapshottableDir);\n+    for (int i = 0; i < numSnapshots; i++) {\n+      hdfs.createSnapshot(snapshottableDir, \"s\" + i);\n+    }\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\",\n+        () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+\n+    // now change max snapshot directory limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_MAX_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    SnapshotManager snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+\n+    // Any new snapshot creation should still fail\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"snapshot limit\", () -> hdfs.createSnapshot(snapshottableDir, \"s5\"));\n+    // now change max snapshot FS limit to 2 and restart namenode\n+    cluster.getNameNode().getConf().setInt(DFSConfigKeys.\n+        DFS_NAMENODE_SNAPSHOT_FILESYSTEM_LIMIT, 2);\n+    cluster.restartNameNodes();\n+    snapshotManager = cluster.getNamesystem().\n+        getSnapshotManager();\n+    // make sure edits of all previous 5 create snapshots are replayed\n+    Assert.assertEquals(numSnapshots, snapshotManager.getNumSnapshots());\n+\n+    // make sure namenode has the new snapshot limit configured as 2\n+    Assert.assertEquals(2, snapshotManager.getMaxSnapshotLimit());\n+    cluster.shutdown();", "originalCommit": "9db4fbce5f2eb888be5baceb10358cffa20f79bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9f08118c1ff528ecb22a504436dbe53441c9b732", "url": "https://github.com/apache/hadoop/commit/9f08118c1ff528ecb22a504436dbe53441c9b732", "message": "Addressed review comments.", "committedDate": "2020-09-14T18:06:45Z", "type": "commit"}]}