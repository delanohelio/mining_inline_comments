{"pr_number": 1925, "pr_title": "HADOOP-16948. Support single writer dirs.", "pr_createdAt": "2020-03-30T16:43:36Z", "pr_url": "https://github.com/apache/hadoop/pull/1925", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU4MjYyMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r401582621", "bodyText": "this seems to be recurrent merge pain point: too many patches adding more things to every rest call.\nProposed: how about adding a RestOperationContext struct which gets passed down, leaseId would go in there, and later other stuff (statistics, trace context, etc) ?", "author": "steveloughran", "createdAt": "2020-04-01T12:39:36Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -328,13 +406,16 @@ public AbfsRestOperation renamePath(String source, final String destination, fin\n   }\n \n   public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length, boolean flush, boolean isClose)\n+                                  final int length, boolean flush, boolean isClose, final String leaseId)", "originalCommit": "d98c26ca2a785f2523c75b59b8eb1e356af56f4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU4MzQxMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r401583411", "bodyText": "PathIOException with path; make error string a const to use when matching in tests. Consider also a new LeaseRequiredException if that helps testing", "author": "steveloughran", "createdAt": "2020-04-01T12:40:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -168,6 +177,10 @@ public synchronized void write(final byte[] data, final int off, final int lengt\n       throw new IndexOutOfBoundsException();\n     }\n \n+    if (lease != null && lease.isFreed()) {\n+      throw new IOException(\"Attempted to write to file without lease: \" + path);", "originalCommit": "d98c26ca2a785f2523c75b59b8eb1e356af56f4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU4MzQ4MA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r401583480", "bodyText": "will this ever fail?", "author": "steveloughran", "createdAt": "2020-04-01T12:41:00Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -261,6 +274,9 @@ public synchronized void close() throws IOException {\n       // See HADOOP-16785\n       throw wrapException(path, e.getMessage(), e);\n     } finally {\n+      if (lease != null) {\n+        lease.free();", "originalCommit": "d98c26ca2a785f2523c75b59b8eb1e356af56f4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU4MzgzNw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r401583837", "bodyText": "we are on SLF4J now", "author": "steveloughran", "createdAt": "2020-04-01T12:41:34Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.commons.logging.Log;", "originalCommit": "d98c26ca2a785f2523c75b59b8eb1e356af56f4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU4NDQzMA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r401584430", "bodyText": "switch to SLF4J logging style; include full stack @ debug level", "author": "steveloughran", "createdAt": "2020-04-01T12:42:39Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * An Azure blob lease that automatically renews itself indefinitely\n+ * using a background thread. Use it to synchronize distributed processes,\n+ * or to prevent writes to the blob by other processes that don't\n+ * have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is\n+ * acquired.\n+ *\n+ * Call free() to release the Lease.\n+ *\n+ * You can use this Lease like a distributed lock. If the holder process\n+ * dies, the lease will time out since it won't be renewed.\n+ *\n+ * See also {@link org.apache.hadoop.fs.azure.SelfRenewingLease}.\n+ */\n+public class SelfRenewingLease {\n+\n+  private final AzureBlobFileSystemStore store;\n+  private final Path path;\n+  private Thread renewer;\n+  private volatile boolean leaseFreed;\n+  private String leaseID = null;\n+  private static final int LEASE_TIMEOUT = 60;  // Lease timeout in seconds\n+\n+  // Time to wait to renew lease in milliseconds\n+  public static final int LEASE_RENEWAL_PERIOD = 40000;\n+  private static final Log LOG = LogFactory.getLog(SelfRenewingLease.class);\n+\n+  // Used to allocate thread serial numbers in thread name\n+  private static AtomicInteger threadNumber = new AtomicInteger(0);\n+\n+\n+  // Time to wait to retry getting the lease in milliseconds\n+  @VisibleForTesting\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 2000;\n+\n+  public SelfRenewingLease(AzureBlobFileSystemStore store, Path path) {\n+\n+    this.leaseFreed = false;\n+    this.store = store;\n+    this.path = path;\n+\n+    // Keep trying to get the lease until you get it.\n+    while(leaseID == null) {\n+      try {\n+        leaseID = store.acquireLease(this.path, LEASE_TIMEOUT);\n+      } catch (IOException e) {\n+        LOG.info(\"Caught exception when trying to get lease on blob \" + path + \". \" + e.getMessage());\n+      }\n+      if (leaseID == null) {\n+        try {\n+          Thread.sleep(LEASE_ACQUIRE_RETRY_INTERVAL);\n+        } catch (InterruptedException e) {\n+\n+          // Restore the interrupted status\n+          Thread.currentThread().interrupt();\n+        }\n+      }\n+    }\n+    renewer = new Thread(new Renewer());\n+\n+    // A Renewer running should not keep JVM from exiting, so make it a daemon.\n+    renewer.setDaemon(true);\n+    renewer.setName(\"AzureLeaseRenewer-\" + threadNumber.getAndIncrement());\n+    renewer.start();\n+    LOG.debug(\"Acquired lease \" + leaseID + \" on \" + path\n+        + \" managed by thread \" + renewer.getName());\n+  }\n+\n+  /**\n+   * Free the lease and stop the keep-alive thread.\n+   */\n+  public void free() {\n+    try {\n+      store.releaseLease(path, leaseID);\n+    } catch (IOException e) {\n+      LOG.info(\"Exception when trying to free lease \" + leaseID + \" on \" + path + \". \" + e.getMessage());", "originalCommit": "d98c26ca2a785f2523c75b59b8eb1e356af56f4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU4NTE1MA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r401585150", "bodyText": "should be tied in to the FileSystem instance lifecycle too: an FS instance should really have a weak ref to all leases created under it, and fs.close to stop them all", "author": "steveloughran", "createdAt": "2020-04-01T12:43:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * An Azure blob lease that automatically renews itself indefinitely\n+ * using a background thread. Use it to synchronize distributed processes,\n+ * or to prevent writes to the blob by other processes that don't\n+ * have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is\n+ * acquired.\n+ *\n+ * Call free() to release the Lease.\n+ *\n+ * You can use this Lease like a distributed lock. If the holder process\n+ * dies, the lease will time out since it won't be renewed.\n+ *\n+ * See also {@link org.apache.hadoop.fs.azure.SelfRenewingLease}.\n+ */\n+public class SelfRenewingLease {\n+\n+  private final AzureBlobFileSystemStore store;\n+  private final Path path;\n+  private Thread renewer;\n+  private volatile boolean leaseFreed;\n+  private String leaseID = null;\n+  private static final int LEASE_TIMEOUT = 60;  // Lease timeout in seconds\n+\n+  // Time to wait to renew lease in milliseconds\n+  public static final int LEASE_RENEWAL_PERIOD = 40000;\n+  private static final Log LOG = LogFactory.getLog(SelfRenewingLease.class);\n+\n+  // Used to allocate thread serial numbers in thread name\n+  private static AtomicInteger threadNumber = new AtomicInteger(0);\n+\n+\n+  // Time to wait to retry getting the lease in milliseconds\n+  @VisibleForTesting\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 2000;\n+\n+  public SelfRenewingLease(AzureBlobFileSystemStore store, Path path) {\n+\n+    this.leaseFreed = false;\n+    this.store = store;\n+    this.path = path;\n+\n+    // Keep trying to get the lease until you get it.\n+    while(leaseID == null) {\n+      try {\n+        leaseID = store.acquireLease(this.path, LEASE_TIMEOUT);\n+      } catch (IOException e) {\n+        LOG.info(\"Caught exception when trying to get lease on blob \" + path + \". \" + e.getMessage());\n+      }\n+      if (leaseID == null) {\n+        try {\n+          Thread.sleep(LEASE_ACQUIRE_RETRY_INTERVAL);\n+        } catch (InterruptedException e) {\n+\n+          // Restore the interrupted status\n+          Thread.currentThread().interrupt();\n+        }\n+      }\n+    }\n+    renewer = new Thread(new Renewer());\n+\n+    // A Renewer running should not keep JVM from exiting, so make it a daemon.\n+    renewer.setDaemon(true);\n+    renewer.setName(\"AzureLeaseRenewer-\" + threadNumber.getAndIncrement());\n+    renewer.start();\n+    LOG.debug(\"Acquired lease \" + leaseID + \" on \" + path\n+        + \" managed by thread \" + renewer.getName());\n+  }\n+\n+  /**\n+   * Free the lease and stop the keep-alive thread.\n+   */\n+  public void free() {\n+    try {\n+      store.releaseLease(path, leaseID);\n+    } catch (IOException e) {\n+      LOG.info(\"Exception when trying to free lease \" + leaseID + \" on \" + path + \". \" + e.getMessage());\n+    } finally {\n+\n+      // Even if releasing the lease fails (e.g. because the file was deleted),\n+      // make sure to record that we freed the lease, to terminate the\n+      // keep-alive thread.\n+      leaseFreed = true;\n+      LOG.debug(\"Freed lease \" + leaseID + \" on \" + path\n+          + \" managed by thread \" + renewer.getName());\n+    }\n+  }\n+\n+  public boolean isFreed() {\n+    return leaseFreed;\n+  }\n+\n+  public String getLeaseID() {\n+    return leaseID;\n+  }\n+\n+  private class Renewer implements Runnable {\n+\n+    /**\n+     * Start a keep-alive thread that will continue to renew\n+     * the lease until it is freed or the process dies.", "originalCommit": "d98c26ca2a785f2523c75b59b8eb1e356af56f4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d6658798a5140edf4743e47ef0ff1a08f2142551", "url": "https://github.com/apache/hadoop/commit/d6658798a5140edf4743e47ef0ff1a08f2142551", "message": "HADOOP-16948. Support single writer dirs.", "committedDate": "2020-10-27T21:11:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1MDE1Ng==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r514450156", "bodyText": "This likely to take time? I'm worried about what happens if there's network problems and this gets invoked. Ideally this would be done in parallel, but abfs doesnt (yet) have a thread pool", "author": "steveloughran", "createdAt": "2020-10-29T17:45:18Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -243,6 +252,16 @@ public String getPrimaryGroup() {\n \n   @Override\n   public void close() throws IOException {\n+    for (SelfRenewingLease lease : leaseRefs.keySet()) {", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1MDQ5NQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r514450495", "bodyText": "go on, add some javadocs", "author": "steveloughran", "createdAt": "2020-10-29T17:45:47Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -685,10 +712,38 @@ public OutputStream openFileForWrite(final Path path, final FileSystem.Statistic\n           statistics,\n           relativePath,\n           offset,\n-          populateAbfsOutputStreamContext(isAppendBlob));\n+          leaseRefs,\n+          populateAbfsOutputStreamContext(isAppendBlob, enableSingleWriter));\n     }\n   }\n \n+  public String acquireLease(final Path path, final int duration) throws AzureBlobFileSystemException {", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1MjI5MQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r514452291", "bodyText": "as well as the usual import grouping/ordering, we've gone to shaded guava on trunk", "author": "steveloughran", "createdAt": "2020-10-29T17:48:23Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import com.google.common.base.Preconditions;", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1MzQxNg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r514453416", "bodyText": "Prefer you use our normal RetryPolicy if possible", "author": "steveloughran", "createdAt": "2020-10-29T17:49:59Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+\n+/**\n+ * An Azure blob lease that automatically renews itself indefinitely\n+ * using a background thread. Use it to synchronize distributed processes,\n+ * or to prevent writes to the blob by other processes that don't\n+ * have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is\n+ * acquired.\n+ *\n+ * Call free() to release the Lease.\n+ *\n+ * You can use this Lease like a distributed lock. If the holder process\n+ * dies, the lease will time out since it won't be renewed.\n+ *\n+ * See also {@link org.apache.hadoop.fs.azure.SelfRenewingLease}.\n+ */\n+public final class SelfRenewingLease {\n+\n+  private final AbfsClient client;\n+  private final Path path;\n+  private Thread renewer;\n+  private volatile boolean leaseFreed;\n+  private String leaseID = null;\n+  private static final int LEASE_TIMEOUT = 60;  // Lease timeout in seconds\n+\n+  // Time to wait to renew lease in milliseconds\n+  public static final int LEASE_RENEWAL_PERIOD = 40000;\n+  public static final Logger LOG = LoggerFactory.getLogger(SelfRenewingLease.class);\n+\n+  // Used to allocate thread serial numbers in thread name\n+  private static AtomicInteger threadNumber = new AtomicInteger(0);\n+\n+\n+  // Time to wait to retry getting the lease in milliseconds\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 2000;\n+  static final int LEASE_MAX_RETRIES = 5;\n+\n+  public static class LeaseException extends AzureBlobFileSystemException {\n+    public LeaseException(Exception innerException) {\n+      super(ERR_ACQUIRING_LEASE, innerException);\n+    }\n+  }\n+\n+  public SelfRenewingLease(AbfsClient client, Path path) throws AzureBlobFileSystemException {\n+\n+    this.leaseFreed = false;\n+    this.client = client;\n+    this.path = path;\n+\n+    // Try to get the lease a specified number of times, else throw an error\n+    int numRetries = 0;\n+    while (leaseID == null && numRetries < LEASE_MAX_RETRIES) {\n+      numRetries++;\n+      try {\n+        LOG.debug(\"lease path: {}\", path);\n+        final AbfsRestOperation op =\n+            client.acquireLease(getRelativePath(path),\n+                LEASE_TIMEOUT);\n+\n+        leaseID = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_LEASE_ID);\n+      } catch (IOException e) {\n+        if (numRetries < LEASE_MAX_RETRIES) {\n+          LOG.info(\"Caught exception when trying to acquire lease on blob {}, retrying: {}\", path,\n+              e.getMessage());\n+          LOG.debug(\"Exception acquiring lease\", e);\n+        } else {\n+          throw new LeaseException(e);\n+        }\n+      }\n+      if (leaseID == null) {\n+        try {\n+          Thread.sleep(LEASE_ACQUIRE_RETRY_INTERVAL);", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE3NzE5NA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r531177194", "bodyText": "how about using DurationInfo in the try with resources (logging @ debug) to track how long acquire/release took. I can imagine it can take a while to acquire. Indeed, do we have to worry about timeouts, heartbeats, etc?", "author": "steveloughran", "createdAt": "2020-11-26T18:11:38Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -475,6 +475,55 @@ public FileStatus getFileStatus(final Path f) throws IOException {\n     }\n   }\n \n+  public String acquireLease(final Path f, final int duration) throws IOException {\n+    LOG.debug(\"AzureBlobFileSystem.acquireLease path: {}\", f);\n+\n+    Path qualifiedPath = makeQualified(f);\n+\n+    try {\n+      return abfsStore.acquireLease(qualifiedPath, duration);", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE3NzY2MQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r531177661", "bodyText": "o.a.h.utils.IOUtils.close methods do this", "author": "steveloughran", "createdAt": "2020-11-26T18:13:12Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemLease.java", "diffHunk": "@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_EXPIRED;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_PARALLEL_ACCESS_DETECTED;\n+\n+/**\n+ * Test lease operations.\n+ */\n+public class ITestAzureBlobFileSystemLease extends AbstractAbfsIntegrationTest {\n+  private static final int TEST_EXECUTION_TIMEOUT = 30 * 1000;\n+  private static final int LONG_TEST_EXECUTION_TIMEOUT = 90 * 1000;\n+  private static final String TEST_FILE = \"testfile\";\n+\n+  public ITestAzureBlobFileSystemLease() throws Exception {\n+    super();\n+  }\n+\n+  @Test\n+  public void testNoSingleWriter() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      Assert.assertFalse(\"Output stream should not have lease\",\n+          ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testOneWriter() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testSubDir() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(new Path(path(methodName.getMethodName()), \"subdir\"),\n+        TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent().getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testTwoCreate() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.create(testFilePath)) {\n+        Assert.fail(\"Second create succeeded\");\n+      } catch (IOException e) {\n+        Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+            e.getMessage().contains(ERR_PARALLEL_ACCESS_DETECTED));\n+      }\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  private void twoWriters(AzureBlobFileSystem fs, Path testFilePath, boolean expectException) throws Exception {\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+        out2.writeInt(2);\n+        out2.hsync();\n+      } catch (IOException e) {\n+        if (expectException) {\n+          Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+              e.getMessage().contains(ERR_ACQUIRING_LEASE));\n+        } else {\n+          Assert.fail(\"Unexpected exception \" + e.getMessage());\n+        }\n+      }\n+      out.writeInt(1);\n+      out.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendNoSingleWriter() throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    twoWriters(fs, testFilePath, false);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendWithSingleWriterEnabled() throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    twoWriters(fs, testFilePath, true);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedOnClose() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease after close\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testWriteAfterBreakLease() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    out.hsync();\n+\n+    fs.breakLease(testFilePath);\n+    try {\n+      out.write(1);\n+      out.hsync();\n+      Assert.fail(\"Expected exception on write after lease break\");\n+    } catch (IOException e) {\n+      Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+          e.getMessage().contains(ERR_LEASE_EXPIRED));\n+    }\n+    try {\n+      out.close();\n+      Assert.fail(\"Expected exception on close after lease break\");\n+    } catch (IOException e) {\n+      Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+          e.getMessage().contains(ERR_LEASE_EXPIRED));\n+    }\n+\n+    Assert.assertTrue(((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed());\n+\n+    try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+      out2.write(2);\n+      out2.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = LONG_TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedAfterBreak() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = null;\n+    try {\n+      out = fs.create(testFilePath);\n+      out.write(0);\n+\n+      fs.breakLease(testFilePath);\n+      while (!((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed()) {\n+        try {\n+          Thread.sleep(1000);\n+        } catch (InterruptedException e) {\n+        }\n+      }\n+    } finally {\n+      try {\n+        if (out != null) {", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE3NzkyMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r531177921", "bodyText": "GenericTestUtils lets you assert something is in the error message. Its critical to rethrow (maybe wrapped) the exception if it is not the one you were expecting", "author": "steveloughran", "createdAt": "2020-11-26T18:14:02Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemLease.java", "diffHunk": "@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_EXPIRED;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_PARALLEL_ACCESS_DETECTED;\n+\n+/**\n+ * Test lease operations.\n+ */\n+public class ITestAzureBlobFileSystemLease extends AbstractAbfsIntegrationTest {\n+  private static final int TEST_EXECUTION_TIMEOUT = 30 * 1000;\n+  private static final int LONG_TEST_EXECUTION_TIMEOUT = 90 * 1000;\n+  private static final String TEST_FILE = \"testfile\";\n+\n+  public ITestAzureBlobFileSystemLease() throws Exception {\n+    super();\n+  }\n+\n+  @Test\n+  public void testNoSingleWriter() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      Assert.assertFalse(\"Output stream should not have lease\",\n+          ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testOneWriter() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testSubDir() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(new Path(path(methodName.getMethodName()), \"subdir\"),\n+        TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent().getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testTwoCreate() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.create(testFilePath)) {\n+        Assert.fail(\"Second create succeeded\");\n+      } catch (IOException e) {\n+        Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+            e.getMessage().contains(ERR_PARALLEL_ACCESS_DETECTED));\n+      }\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  private void twoWriters(AzureBlobFileSystem fs, Path testFilePath, boolean expectException) throws Exception {\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+        out2.writeInt(2);\n+        out2.hsync();\n+      } catch (IOException e) {\n+        if (expectException) {\n+          Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+              e.getMessage().contains(ERR_ACQUIRING_LEASE));\n+        } else {\n+          Assert.fail(\"Unexpected exception \" + e.getMessage());\n+        }\n+      }\n+      out.writeInt(1);\n+      out.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendNoSingleWriter() throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    twoWriters(fs, testFilePath, false);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendWithSingleWriterEnabled() throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    twoWriters(fs, testFilePath, true);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedOnClose() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease after close\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testWriteAfterBreakLease() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    out.hsync();\n+\n+    fs.breakLease(testFilePath);\n+    try {\n+      out.write(1);\n+      out.hsync();\n+      Assert.fail(\"Expected exception on write after lease break\");\n+    } catch (IOException e) {\n+      Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+          e.getMessage().contains(ERR_LEASE_EXPIRED));\n+    }\n+    try {\n+      out.close();\n+      Assert.fail(\"Expected exception on close after lease break\");\n+    } catch (IOException e) {\n+      Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+          e.getMessage().contains(ERR_LEASE_EXPIRED));\n+    }\n+\n+    Assert.assertTrue(((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed());\n+\n+    try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+      out2.write(2);\n+      out2.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = LONG_TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedAfterBreak() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = null;\n+    try {\n+      out = fs.create(testFilePath);\n+      out.write(0);\n+\n+      fs.breakLease(testFilePath);\n+      while (!((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed()) {\n+        try {\n+          Thread.sleep(1000);\n+        } catch (InterruptedException e) {\n+        }\n+      }\n+    } finally {\n+      try {\n+        if (out != null) {\n+          out.close();\n+        }\n+        // exception might or might not occur\n+      } catch (IOException e) {", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE3ODIzNw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r531178237", "bodyText": "and add a message to raise if the condition is met.\nnote, it's ok to use AssertJ for your asserts, we are adopting it more broadly and enjoying its diagnostics.", "author": "steveloughran", "createdAt": "2020-11-26T18:14:53Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemLease.java", "diffHunk": "@@ -0,0 +1,249 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_EXPIRED;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_PARALLEL_ACCESS_DETECTED;\n+\n+/**\n+ * Test lease operations.\n+ */\n+public class ITestAzureBlobFileSystemLease extends AbstractAbfsIntegrationTest {\n+  private static final int TEST_EXECUTION_TIMEOUT = 30 * 1000;\n+  private static final int LONG_TEST_EXECUTION_TIMEOUT = 90 * 1000;\n+  private static final String TEST_FILE = \"testfile\";\n+\n+  public ITestAzureBlobFileSystemLease() throws Exception {\n+    super();\n+  }\n+\n+  @Test\n+  public void testNoSingleWriter() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      Assert.assertFalse(\"Output stream should not have lease\",\n+          ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testOneWriter() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testSubDir() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(new Path(path(methodName.getMethodName()), \"subdir\"),\n+        TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent().getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testTwoCreate() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.create(testFilePath)) {\n+        Assert.fail(\"Second create succeeded\");\n+      } catch (IOException e) {\n+        Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+            e.getMessage().contains(ERR_PARALLEL_ACCESS_DETECTED));\n+      }\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  private void twoWriters(AzureBlobFileSystem fs, Path testFilePath, boolean expectException) throws Exception {\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+        out2.writeInt(2);\n+        out2.hsync();\n+      } catch (IOException e) {\n+        if (expectException) {\n+          Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+              e.getMessage().contains(ERR_ACQUIRING_LEASE));\n+        } else {\n+          Assert.fail(\"Unexpected exception \" + e.getMessage());\n+        }\n+      }\n+      out.writeInt(1);\n+      out.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendNoSingleWriter() throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    twoWriters(fs, testFilePath, false);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendWithSingleWriterEnabled() throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    twoWriters(fs, testFilePath, true);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedOnClose() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease after close\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testWriteAfterBreakLease() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    out.hsync();\n+\n+    fs.breakLease(testFilePath);\n+    try {\n+      out.write(1);\n+      out.hsync();\n+      Assert.fail(\"Expected exception on write after lease break\");\n+    } catch (IOException e) {\n+      Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+          e.getMessage().contains(ERR_LEASE_EXPIRED));\n+    }\n+    try {\n+      out.close();\n+      Assert.fail(\"Expected exception on close after lease break\");\n+    } catch (IOException e) {\n+      Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+          e.getMessage().contains(ERR_LEASE_EXPIRED));\n+    }\n+\n+    Assert.assertTrue(((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed());\n+\n+    try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+      out2.write(2);\n+      out2.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = LONG_TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedAfterBreak() throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    fs.mkdirs(testFilePath.getParent());\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setAzureSingleWriterDirs(testFilePath.getParent().toString());\n+    fs.getAbfsStore().updateSingleWriterDirs();\n+\n+    FSDataOutputStream out = null;\n+    try {\n+      out = fs.create(testFilePath);\n+      out.write(0);\n+\n+      fs.breakLease(testFilePath);\n+      while (!((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed()) {\n+        try {\n+          Thread.sleep(1000);\n+        } catch (InterruptedException e) {\n+        }\n+      }\n+    } finally {\n+      try {\n+        if (out != null) {\n+          out.close();\n+        }\n+        // exception might or might not occur\n+      } catch (IOException e) {\n+        Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+            e.getMessage().contains(ERR_LEASE_EXPIRED));\n+      }\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());", "originalCommit": "bb65e714888f104676915e8c54a0c2b00b3168e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "43faa3962f8835b5a7b7749f1628d9dd3f8a3fa1", "url": "https://github.com/apache/hadoop/commit/43faa3962f8835b5a7b7749f1628d9dd3f8a3fa1", "message": "HADOOP-16948. Convert ABFS client to use an executor for lease ops", "committedDate": "2021-01-14T15:28:23Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MDgxMw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558290813", "bodyText": "can you pull up to the .thirdparty section and add a newline after. Our guava shading project is confusing IDEs and making backporting/cherrypicking harder", "author": "steveloughran", "createdAt": "2021-01-15T13:00:07Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -29,13 +29,23 @@\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Locale;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n \n import org.apache.hadoop.thirdparty.com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.thirdparty.com.google.common.base.Strings;\n import org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory;\n import org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants;\n import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n import org.apache.hadoop.fs.azurebfs.constants.HttpQueryParams;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.FutureCallback;", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MTc4OQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558291789", "bodyText": "Prefer you use HadoopExecutors here.", "author": "steveloughran", "createdAt": "2021-01-15T13:01:59Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -106,6 +118,9 @@ private AbfsClient(final URL baseUrl, final SharedKeyCredentials sharedKeyCreden\n     this.userAgent = initializeUserAgent(abfsConfiguration, sslProviderName);\n     this.abfsPerfTracker = abfsClientContext.getAbfsPerfTracker();\n     this.abfsCounters = abfsClientContext.getAbfsCounters();\n+\n+    this.executorService = MoreExecutors.listeningDecorator(\n+        Executors.newScheduledThreadPool(this.abfsConfiguration.getNumLeaseThreads()));", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MTk1OQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558291959", "bodyText": "HadoopExecutors.shutdown has some error handling here", "author": "steveloughran", "createdAt": "2021-01-15T13:02:20Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -129,6 +144,7 @@ public void close() throws IOException {\n     if (tokenProvider instanceof Closeable) {\n       IOUtils.cleanupWithLogger(LOG, (Closeable) tokenProvider);\n     }\n+    executorService.shutdownNow();", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MjQxMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558292411", "bodyText": "if the RestOperation doesn't log anything, add something here. Will help debug locking problems", "author": "steveloughran", "createdAt": "2021-01-15T13:03:23Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -306,6 +322,83 @@ public AbfsRestOperation createPath(final String path, final boolean isFile, fin\n     return op;\n   }\n \n+  public AbfsRestOperation acquireLease(final String path, int duration) throws AzureBlobFileSystemException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, ACQUIRE_LEASE_ACTION));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_DURATION, Integer.toString(duration)));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_PROPOSED_LEASE_ID, UUID.randomUUID().toString()));\n+\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+\n+    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\n+    final AbfsRestOperation op = new AbfsRestOperation(", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODQ1OTkwOQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558459909", "bodyText": "There is some good debug logging when the rest operation is executed, so I think we're okay here.", "author": "billierinaldi", "createdAt": "2021-01-15T17:35:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MjQxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MjYzMg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558292632", "bodyText": "add a log if needed", "author": "steveloughran", "createdAt": "2021-01-15T13:03:52Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -306,6 +322,83 @@ public AbfsRestOperation createPath(final String path, final boolean isFile, fin\n     return op;\n   }\n \n+  public AbfsRestOperation acquireLease(final String path, int duration) throws AzureBlobFileSystemException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, ACQUIRE_LEASE_ACTION));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_DURATION, Integer.toString(duration)));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_PROPOSED_LEASE_ID, UUID.randomUUID().toString()));\n+\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+\n+    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\n+    final AbfsRestOperation op = new AbfsRestOperation(\n+        AbfsRestOperationType.LeasePath,\n+        this,\n+        HTTP_METHOD_POST,\n+        url,\n+        requestHeaders);\n+    op.execute();\n+    return op;\n+  }\n+\n+  public AbfsRestOperation renewLease(final String path, final String leaseId) throws AzureBlobFileSystemException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, RENEW_LEASE_ACTION));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n+\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+\n+    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\n+    final AbfsRestOperation op = new AbfsRestOperation(", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MjY5Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558292697", "bodyText": "+log", "author": "steveloughran", "createdAt": "2021-01-15T13:03:59Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -306,6 +322,83 @@ public AbfsRestOperation createPath(final String path, final boolean isFile, fin\n     return op;\n   }\n \n+  public AbfsRestOperation acquireLease(final String path, int duration) throws AzureBlobFileSystemException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, ACQUIRE_LEASE_ACTION));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_DURATION, Integer.toString(duration)));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_PROPOSED_LEASE_ID, UUID.randomUUID().toString()));\n+\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+\n+    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\n+    final AbfsRestOperation op = new AbfsRestOperation(\n+        AbfsRestOperationType.LeasePath,\n+        this,\n+        HTTP_METHOD_POST,\n+        url,\n+        requestHeaders);\n+    op.execute();\n+    return op;\n+  }\n+\n+  public AbfsRestOperation renewLease(final String path, final String leaseId) throws AzureBlobFileSystemException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, RENEW_LEASE_ACTION));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n+\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+\n+    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\n+    final AbfsRestOperation op = new AbfsRestOperation(\n+        AbfsRestOperationType.LeasePath,\n+        this,\n+        HTTP_METHOD_POST,\n+        url,\n+        requestHeaders);\n+    op.execute();\n+    return op;\n+  }\n+\n+  public AbfsRestOperation releaseLease(final String path, final String leaseId) throws AzureBlobFileSystemException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, RELEASE_LEASE_ACTION));\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n+\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+\n+    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\n+    final AbfsRestOperation op = new AbfsRestOperation(", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5MzYzNQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558293635", "bodyText": "this should go into the AbfsOutputStreamContext, its where we are adding more state for a stream constructor", "author": "steveloughran", "createdAt": "2021-01-15T13:05:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -113,7 +120,8 @@ public AbfsOutputStream(\n           final Statistics statistics,\n           final String path,\n           final long position,\n-          AbfsOutputStreamContext abfsOutputStreamContext) {\n+          final Map<SelfRenewingLease, Object> leaseRefs,", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5NDM4OQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558294389", "bodyText": "is isLeaseFreed()", "author": "steveloughran", "createdAt": "2021-01-15T13:07:16Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -209,6 +226,10 @@ public synchronized void write(final byte[] data, final int off, final int lengt\n       throw new IndexOutOfBoundsException();\n     }\n \n+    if (lease != null && lease.isFreed()) {", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5NDQ3MQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558294471", "bodyText": "use hasLease()", "author": "steveloughran", "createdAt": "2021-01-15T13:07:29Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -312,6 +333,10 @@ public synchronized void close() throws IOException {\n       // See HADOOP-16785\n       throw wrapException(path, e.getMessage(), e);\n     } finally {\n+      if (lease != null) {", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5ODM1Mw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558298353", "bodyText": "import ordering. should be java.* javax.* \\n non-asf-and-hadoop.thirdparty \\n o.a.a", "author": "steveloughran", "createdAt": "2021-01-15T13:14:48Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.Path;", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5ODgwMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558298801", "bodyText": "keep t's stack trace by passing up to superclass or in initCause()", "author": "steveloughran", "createdAt": "2021-01-15T13:15:38Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+import org.apache.hadoop.thirdparty.com.google.common.base.Preconditions;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.FutureCallback;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableScheduledFuture;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CancellationException;\n+import java.util.concurrent.TimeUnit;\n+\n+import static java.net.HttpURLConnection.HTTP_INTERNAL_ERROR;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_FUTURE_EXISTS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+\n+/**\n+ * An Azure blob lease that automatically renews itself indefinitely by scheduling lease\n+ * operations through the ABFS client. Use it to prevent writes to the blob by other processes\n+ * that don't have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is acquired. It will\n+ * retry a fixed number of times before failing if there is a problem acquiring the lease.\n+ *\n+ * Call free() to release the Lease. If the holder process dies, the lease will time out since it\n+ * won't be renewed.\n+ */\n+public final class SelfRenewingLease {\n+  private static final Logger LOG = LoggerFactory.getLogger(SelfRenewingLease.class);\n+\n+  static final int LEASE_DURATION = 60; // Lease duration in seconds\n+  static final int LEASE_RENEWAL_PERIOD = 40; // Lease renewal interval in seconds\n+\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 10; // Retry interval for acquiring lease in secs\n+  static final int LEASE_ACQUIRE_MAX_RETRIES = 7; // Number of retries for acquiring lease\n+\n+  private final AbfsClient client;\n+  private final String path;\n+\n+  // Lease status variables\n+  private volatile boolean leaseFreed;\n+  private volatile String leaseID = null;\n+  private volatile Throwable exception = null;\n+  private volatile ListenableScheduledFuture<AbfsRestOperation> future = null;\n+\n+  public static class LeaseException extends AzureBlobFileSystemException {\n+    public LeaseException(Throwable t) {\n+      super(ERR_ACQUIRING_LEASE + \": \" + t.getMessage());", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODI5OTQyMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558299421", "bodyText": "move to lower group", "author": "steveloughran", "createdAt": "2021-01-15T13:16:48Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -39,6 +39,7 @@\n \n import org.apache.hadoop.thirdparty.com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.thirdparty.com.google.common.base.Preconditions;\n+import org.apache.hadoop.util.DurationInfo;", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMDA5NA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558300094", "bodyText": "do we need to worry about running out of workers here, timeouts etc?", "author": "steveloughran", "createdAt": "2021-01-15T13:17:59Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -243,7 +255,24 @@ public String getPrimaryGroup() {\n \n   @Override\n   public void close() throws IOException {\n-    IOUtils.cleanupWithLogger(LOG, client);\n+    List<ListenableFuture<?>> futures = new ArrayList<>();\n+    for (SelfRenewingLease lease : leaseRefs.keySet()) {\n+      if (lease == null) {\n+        continue;\n+      }\n+      ListenableFuture<?> future = client.submit(() -> lease.free());", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTIzMjc5Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r559232797", "bodyText": "I am not sure. This is kind of a failsafe; I would not expect code to close the filesystem without closing output streams first, so ideally this loop will only be double checking that free() has already been called on these lease instances without actually performing REST ops. If the output streams opened for this FS instance (for designated single writer directories) have not been closed before the filesystem is closed, this would perform a REST op for each one.", "author": "billierinaldi", "createdAt": "2021-01-17T20:18:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMDA5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMTAxNA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558301014", "bodyText": "usual comment about import ordering.", "author": "steveloughran", "createdAt": "2021-01-15T13:19:43Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemLease.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.apache.hadoop.conf.Configuration;", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMjkxNA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558302914", "bodyText": "use LambdaTestUtils; return a string with that error message in the closure for it to be used in the exception. Ideally add out.toString() too. eg.\nintercept(ioe, ERR_LEASE_EXPIRED, () -> {\n  out..write(1);\n  out.hsync();\n  return \"expected exception but got \" + out;\n  });", "author": "steveloughran", "createdAt": "2021-01-15T13:23:14Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemLease.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.concurrent.RejectedExecutionException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_LEASE_THREADS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SINGLE_WRITER_KEY;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_EXPIRED;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_NOT_PRESENT;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_ID_SPECIFIED;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_PARALLEL_ACCESS_DETECTED;\n+\n+/**\n+ * Test lease operations.\n+ */\n+public class ITestAzureBlobFileSystemLease extends AbstractAbfsIntegrationTest {\n+  private static final int TEST_EXECUTION_TIMEOUT = 30 * 1000;\n+  private static final int LONG_TEST_EXECUTION_TIMEOUT = 90 * 1000;\n+  private static final String TEST_FILE = \"testfile\";\n+  private final boolean isHNSEnabled;\n+\n+  public ITestAzureBlobFileSystemLease() throws Exception {\n+    super();\n+\n+    this.isHNSEnabled = getConfiguration()\n+        .getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\n+  }\n+\n+  private AzureBlobFileSystem getCustomFileSystem(String singleWriterDirs, int numLeaseThreads)\n+      throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(String.format(\"fs.%s.impl.disable.cache\", getAbfsScheme()), true);\n+    conf.set(FS_AZURE_SINGLE_WRITER_KEY, singleWriterDirs);\n+    conf.setInt(FS_AZURE_LEASE_THREADS, numLeaseThreads);\n+    return getFileSystem(conf);\n+  }\n+\n+  @Test\n+  public void testNoSingleWriter() throws IOException {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.mkdirs(testFilePath.getParent());\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      Assert.assertFalse(\"Output stream should not have lease\",\n+          ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testNoLeaseThreads() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 0);\n+    fs.mkdirs(testFilePath.getParent());\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      Assert.fail(\"No failure when lease requested with 0 lease threads\");\n+    } catch (Exception e) {\n+      GenericTestUtils.assertExceptionContains(ERR_NO_LEASE_THREADS, e);\n+    }\n+  }\n+\n+  @Test\n+  public void testOneWriter() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testSubDir() throws Exception {\n+    final Path testFilePath = new Path(new Path(path(methodName.getMethodName()), \"subdir\"),\n+        TEST_FILE);\n+    final AzureBlobFileSystem fs =\n+        getCustomFileSystem(testFilePath.getParent().getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent().getParent());\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testTwoCreate() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.create(testFilePath)) {\n+        Assert.fail(\"Second create succeeded\");\n+      } catch (IOException e) {\n+        if (isHNSEnabled) {\n+          GenericTestUtils.assertExceptionContains(ERR_PARALLEL_ACCESS_DETECTED, e);\n+        } else {\n+          GenericTestUtils.assertExceptionContains(ERR_NO_LEASE_ID_SPECIFIED, e);\n+        }\n+      }\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  private void twoWriters(AzureBlobFileSystem fs, Path testFilePath, boolean expectException) throws Exception {\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+        out2.writeInt(2);\n+        out2.hsync();\n+      } catch (IOException e) {\n+        if (expectException) {\n+          Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+              e.getMessage().contains(ERR_ACQUIRING_LEASE));\n+        } else {\n+          Assert.fail(\"Unexpected exception \" + e.getMessage());\n+        }\n+      }\n+      out.writeInt(1);\n+      out.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendNoSingleWriter() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    twoWriters(fs, testFilePath, false);\n+  }\n+\n+  @Test(timeout = LONG_TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendWithSingleWriterEnabled() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    twoWriters(fs, testFilePath, true);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedOnClose() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease after close\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testWriteAfterBreakLease() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    out.hsync();\n+\n+    fs.breakLease(testFilePath);\n+    try {", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMzEyNw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558303127", "bodyText": "do we really want a failure in close?", "author": "steveloughran", "createdAt": "2021-01-15T13:23:39Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemLease.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.concurrent.RejectedExecutionException;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_LEASE_THREADS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_SINGLE_WRITER_KEY;\n+import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_EXPIRED;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_NOT_PRESENT;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_ID_SPECIFIED;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_PARALLEL_ACCESS_DETECTED;\n+\n+/**\n+ * Test lease operations.\n+ */\n+public class ITestAzureBlobFileSystemLease extends AbstractAbfsIntegrationTest {\n+  private static final int TEST_EXECUTION_TIMEOUT = 30 * 1000;\n+  private static final int LONG_TEST_EXECUTION_TIMEOUT = 90 * 1000;\n+  private static final String TEST_FILE = \"testfile\";\n+  private final boolean isHNSEnabled;\n+\n+  public ITestAzureBlobFileSystemLease() throws Exception {\n+    super();\n+\n+    this.isHNSEnabled = getConfiguration()\n+        .getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\n+  }\n+\n+  private AzureBlobFileSystem getCustomFileSystem(String singleWriterDirs, int numLeaseThreads)\n+      throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(String.format(\"fs.%s.impl.disable.cache\", getAbfsScheme()), true);\n+    conf.set(FS_AZURE_SINGLE_WRITER_KEY, singleWriterDirs);\n+    conf.setInt(FS_AZURE_LEASE_THREADS, numLeaseThreads);\n+    return getFileSystem(conf);\n+  }\n+\n+  @Test\n+  public void testNoSingleWriter() throws IOException {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.mkdirs(testFilePath.getParent());\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      Assert.assertFalse(\"Output stream should not have lease\",\n+          ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testNoLeaseThreads() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 0);\n+    fs.mkdirs(testFilePath.getParent());\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      Assert.fail(\"No failure when lease requested with 0 lease threads\");\n+    } catch (Exception e) {\n+      GenericTestUtils.assertExceptionContains(ERR_NO_LEASE_THREADS, e);\n+    }\n+  }\n+\n+  @Test\n+  public void testOneWriter() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testSubDir() throws Exception {\n+    final Path testFilePath = new Path(new Path(path(methodName.getMethodName()), \"subdir\"),\n+        TEST_FILE);\n+    final AzureBlobFileSystem fs =\n+        getCustomFileSystem(testFilePath.getParent().getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent().getParent());\n+\n+    FSDataOutputStream out = fs.create(testFilePath);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test\n+  public void testTwoCreate() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.create(testFilePath)) {\n+        Assert.fail(\"Second create succeeded\");\n+      } catch (IOException e) {\n+        if (isHNSEnabled) {\n+          GenericTestUtils.assertExceptionContains(ERR_PARALLEL_ACCESS_DETECTED, e);\n+        } else {\n+          GenericTestUtils.assertExceptionContains(ERR_NO_LEASE_ID_SPECIFIED, e);\n+        }\n+      }\n+    }\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  private void twoWriters(AzureBlobFileSystem fs, Path testFilePath, boolean expectException) throws Exception {\n+    try (FSDataOutputStream out = fs.create(testFilePath)) {\n+      try (FSDataOutputStream out2 = fs.append(testFilePath)) {\n+        out2.writeInt(2);\n+        out2.hsync();\n+      } catch (IOException e) {\n+        if (expectException) {\n+          Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n+              e.getMessage().contains(ERR_ACQUIRING_LEASE));\n+        } else {\n+          Assert.fail(\"Unexpected exception \" + e.getMessage());\n+        }\n+      }\n+      out.writeInt(1);\n+      out.hsync();\n+    }\n+\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendNoSingleWriter() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    twoWriters(fs, testFilePath, false);\n+  }\n+\n+  @Test(timeout = LONG_TEST_EXECUTION_TIMEOUT)\n+  public void testTwoWritersCreateAppendWithSingleWriterEnabled() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    twoWriters(fs, testFilePath, true);\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testLeaseFreedOnClose() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    Assert.assertTrue(\"Output stream should have lease\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    out.close();\n+    Assert.assertFalse(\"Output stream should not have lease after close\",\n+        ((AbfsOutputStream) out.getWrappedStream()).hasLease());\n+    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\n+  }\n+\n+  @Test(timeout = TEST_EXECUTION_TIMEOUT)\n+  public void testWriteAfterBreakLease() throws Exception {\n+    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\n+    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().toString(), 1);\n+    fs.mkdirs(testFilePath.getParent());\n+\n+    FSDataOutputStream out;\n+    out = fs.create(testFilePath);\n+    out.write(0);\n+    out.hsync();\n+\n+    fs.breakLease(testFilePath);\n+    try {\n+      out.write(1);\n+      out.hsync();\n+      Assert.fail(\"Expected exception on write after lease break\");\n+    } catch (IOException e) {\n+      GenericTestUtils.assertExceptionContains(ERR_LEASE_EXPIRED, e);\n+    }\n+    try {\n+      out.close();", "originalCommit": "c4082f019e25b010c1c36f0688cf0cb676b727d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODQ4MTMxMg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558481312", "bodyText": "I think as you mentioned this is due to flushing buffers on close, so I am not sure we can do much about it. I can check what happens when close is called after the lease has expired or is broken and the buffer is empty, to see if we can avoid an exception in that case.", "author": "billierinaldi", "createdAt": "2021-01-15T18:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMzEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU3ODg5Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r558578897", "bodyText": "Looks like there is a flush call to the storage API even when the buffer is empty, so it will always throw an exception if the lease is broken.", "author": "billierinaldi", "createdAt": "2021-01-15T20:35:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMzEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEzNzU1Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r560137557", "bodyText": "should it be doing that flush?", "author": "steveloughran", "createdAt": "2021-01-19T12:21:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMzEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjQzOTEzNQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r566439135", "bodyText": "I looked over the API docs and it seems like it is potentially important to perform that operation.", "author": "billierinaldi", "createdAt": "2021-01-28T22:06:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMzEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MjI4OTAyMA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r572289020", "bodyText": "ok, let's go with the flush", "author": "steveloughran", "createdAt": "2021-02-08T18:53:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwMzEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEzNTQ4OQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r560135489", "bodyText": "can you make sure this executor marks its threads as daemons. Otherwise processes can hang during shutdown. @bgaborg has encountered this elsewhere", "author": "steveloughran", "createdAt": "2021-01-19T12:17:43Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -120,7 +120,7 @@ private AbfsClient(final URL baseUrl, final SharedKeyCredentials sharedKeyCreden\n     this.abfsCounters = abfsClientContext.getAbfsCounters();\n \n     this.executorService = MoreExecutors.listeningDecorator(\n-        Executors.newScheduledThreadPool(this.abfsConfiguration.getNumLeaseThreads()));\n+        HadoopExecutors.newScheduledThreadPool(this.abfsConfiguration.getNumLeaseThreads()));", "originalCommit": "978cedb7b63d6a1b09db291511cc87ae53c7cab0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEzNjUxNA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r560136514", "bodyText": "I think I'd keep that t text in the superclass text, in case a deep tree causes the nested cause not to be listed.\nbut: use toString() (implicitly) rather than getMessage, because some exceptions (NPW) have a null message.", "author": "steveloughran", "createdAt": "2021-01-19T12:19:31Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -71,7 +72,7 @@\n \n   public static class LeaseException extends AzureBlobFileSystemException {\n     public LeaseException(Throwable t) {\n-      super(ERR_ACQUIRING_LEASE + \": \" + t.getMessage());\n+      super(ERR_ACQUIRING_LEASE, t);", "originalCommit": "978cedb7b63d6a1b09db291511cc87ae53c7cab0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEzNzE3Mg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r560137172", "bodyText": "just rethrow it or wrap in an assertion error. we need that full stack trace", "author": "steveloughran", "createdAt": "2021-01-19T12:20:36Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemLease.java", "diffHunk": "@@ -146,8 +150,7 @@ private void twoWriters(AzureBlobFileSystem fs, Path testFilePath, boolean expec\n         out2.hsync();\n       } catch (IOException e) {\n         if (expectException) {\n-          Assert.assertTrue(\"Unexpected error message: \" + e.getMessage(),\n-              e.getMessage().contains(ERR_ACQUIRING_LEASE));\n+          GenericTestUtils.assertExceptionContains(ERR_ACQUIRING_LEASE, e);\n         } else {\n           Assert.fail(\"Unexpected exception \" + e.getMessage());", "originalCommit": "978cedb7b63d6a1b09db291511cc87ae53c7cab0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEzOTA5Mg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r560139092", "bodyText": "little architecture question. Would this be better in the Store than the FS? I don't know, and it is higher level than the Rest API, isn't it? Which implies this is the right place.", "author": "steveloughran", "createdAt": "2021-01-19T12:24:22Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -476,6 +476,20 @@ public FileStatus getFileStatus(final Path f) throws IOException {\n     }\n   }\n \n+  /**\n+   * Acquire a lease on an ABFS file for a specified duration. This requires the file to exist.", "originalCommit": "978cedb7b63d6a1b09db291511cc87ae53c7cab0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a176dc742dcae1efc5a3515f5fba53e4198861ae", "url": "https://github.com/apache/hadoop/commit/a176dc742dcae1efc5a3515f5fba53e4198861ae", "message": "HADOOP-16948. Add error messages to test assertions", "committedDate": "2021-03-03T00:00:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzMxNQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r593783315", "bodyText": "Background threads that will renew lease every 67% of lease i.e. 10 seconds for 15 second lease and 40 seconds for 60 second lease will add extra cost to customers", "author": "snehavarma", "createdAt": "2021-03-13T18:17:21Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.net.HttpURLConnection;\n+import java.util.concurrent.CancellationException;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.FutureCallback;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableScheduledFuture;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.INFINITE_LEASE_DURATION;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_FUTURE_EXISTS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+\n+/**\n+ * An Azure blob lease that automatically renews itself indefinitely by scheduling lease\n+ * operations through the ABFS client. Use it to prevent writes to the blob by other processes\n+ * that don't have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is acquired. It will\n+ * retry a fixed number of times before failing if there is a problem acquiring the lease.\n+ *\n+ * Call free() to release the Lease. If the holder process dies, the lease will time out since it\n+ * won't be renewed.\n+ */\n+public final class SelfRenewingLease {\n+  private static final Logger LOG = LoggerFactory.getLogger(SelfRenewingLease.class);\n+\n+  static final float LEASE_RENEWAL_PERCENT_OF_DURATION = 0.67f; // Lease renewal percent of duration\n+\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 10; // Retry interval for acquiring lease in secs\n+  static final int LEASE_ACQUIRE_MAX_RETRIES = 7; // Number of retries for acquiring lease\n+\n+  private final AbfsClient client;\n+  private final String path;\n+  private final int duration;\n+  private final int renewalPeriod;\n+\n+  // Lease status variables\n+  private volatile boolean leaseFreed;\n+  private volatile String leaseID = null;\n+  private volatile Throwable exception = null;\n+  private volatile ListenableScheduledFuture<AbfsRestOperation> future = null;\n+\n+  public static class LeaseException extends AzureBlobFileSystemException {\n+    public LeaseException(Throwable t) {\n+      super(ERR_ACQUIRING_LEASE + \": \" + t, t);\n+    }\n+\n+    public LeaseException(String s) {\n+      super(s);\n+    }\n+  }\n+\n+  public SelfRenewingLease(AbfsClient client, String path, int duration) throws AzureBlobFileSystemException {", "originalCommit": "e4c7a815ec2f33c9561e2a8446499b46e7004b9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDY3ODgxMg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r594678812", "bodyText": "Thanks Sneha, I didn't realize the lease renewals would be charged as write ops. It might be a poor experience for a user to have unexpected charges related to this lease configuration.", "author": "billierinaldi", "createdAt": "2021-03-15T20:57:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDY5NzQ3Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r594697477", "bodyText": "Hey Billie just wanted to add that it might not be charged as write ops but instead come under other ops or metadata ops. Either way it will be extra cost just not as high as write transaction charges.", "author": "snehavarma", "createdAt": "2021-03-15T21:31:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzcwMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r593783701", "bodyText": "Please check if infinite lease is sufficient for your use case.", "author": "snehavarma", "createdAt": "2021-03-13T18:20:14Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.net.HttpURLConnection;\n+import java.util.concurrent.CancellationException;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.FutureCallback;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableScheduledFuture;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.INFINITE_LEASE_DURATION;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_FUTURE_EXISTS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+\n+/**\n+ * An Azure blob lease that automatically renews itself indefinitely by scheduling lease\n+ * operations through the ABFS client. Use it to prevent writes to the blob by other processes\n+ * that don't have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is acquired. It will\n+ * retry a fixed number of times before failing if there is a problem acquiring the lease.\n+ *\n+ * Call free() to release the Lease. If the holder process dies, the lease will time out since it\n+ * won't be renewed.\n+ */\n+public final class SelfRenewingLease {\n+  private static final Logger LOG = LoggerFactory.getLogger(SelfRenewingLease.class);\n+", "originalCommit": "e4c7a815ec2f33c9561e2a8446499b46e7004b9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDcwNTIzMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r594705231", "bodyText": "I think infinite leases are sufficient for my use case. I would be okay with removing lease renewal from this patch and leaving finite leases for future work in HADOOP-17590, but I am not sure what the best way to handle the configuration properties would be. It sounds like you are proposing a boolean fs.azure.write.enforcelease that would control whether lease ops are applied for all files, and all files would have the same finite lease duration, is that right? I am wondering how to make that work together with the fs.azure.singlewriter.directories property in this patch. Would we want to specify a special set of directories that uses infinite leases? Or do we need to figure out a way to specify lease duration for each directory that supports lease ops?", "author": "billierinaldi", "createdAt": "2021-03-15T21:45:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDcwNzI4Ng==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r594707286", "bodyText": "Yes, We should specify a special set of directories that uses infinite leases. By default we would keep 60 seconds as lease duration for all files.\n\n\nOr do we need to figure out a way to specify lease duration for each directory that supports lease ops?\nThis will not scale.", "author": "snehavarma", "createdAt": "2021-03-15T21:49:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDcxMjkxMQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r594712911", "bodyText": "So, does the property name fs.azure.singlewriter.directories still make sense, or should it be changed to something else such as fs.azure.infinite-lease-directories?", "author": "billierinaldi", "createdAt": "2021-03-15T22:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDcxNDY2MA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r594714660", "bodyText": "Agree,   fs.azure.singlewriter.directories would confuse the users. We can name it something else.", "author": "snehavarma", "createdAt": "2021-03-15T22:03:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MzcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4Mzc5Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r593783797", "bodyText": "Error handling for cases when append may take more time than lease expiry needs to be added incase there is a finite lease.", "author": "snehavarma", "createdAt": "2021-03-13T18:21:36Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SelfRenewingLease.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.net.HttpURLConnection;\n+import java.util.concurrent.CancellationException;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.FutureCallback;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableScheduledFuture;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.INFINITE_LEASE_DURATION;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_FUTURE_EXISTS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+\n+/**\n+ * An Azure blob lease that automatically renews itself indefinitely by scheduling lease\n+ * operations through the ABFS client. Use it to prevent writes to the blob by other processes\n+ * that don't have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is acquired. It will\n+ * retry a fixed number of times before failing if there is a problem acquiring the lease.\n+ *\n+ * Call free() to release the Lease. If the holder process dies, the lease will time out since it\n+ * won't be renewed.\n+ */\n+public final class SelfRenewingLease {\n+  private static final Logger LOG = LoggerFactory.getLogger(SelfRenewingLease.class);\n+\n+  static final float LEASE_RENEWAL_PERCENT_OF_DURATION = 0.67f; // Lease renewal percent of duration\n+\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 10; // Retry interval for acquiring lease in secs\n+  static final int LEASE_ACQUIRE_MAX_RETRIES = 7; // Number of retries for acquiring lease\n+\n+  private final AbfsClient client;\n+  private final String path;\n+  private final int duration;\n+  private final int renewalPeriod;\n+\n+  // Lease status variables\n+  private volatile boolean leaseFreed;\n+  private volatile String leaseID = null;\n+  private volatile Throwable exception = null;\n+  private volatile ListenableScheduledFuture<AbfsRestOperation> future = null;\n+\n+  public static class LeaseException extends AzureBlobFileSystemException {\n+    public LeaseException(Throwable t) {\n+      super(ERR_ACQUIRING_LEASE + \": \" + t, t);\n+    }\n+\n+    public LeaseException(String s) {\n+      super(s);\n+    }\n+  }\n+\n+  public SelfRenewingLease(AbfsClient client, String path, int duration) throws AzureBlobFileSystemException {\n+    this.leaseFreed = false;\n+    this.client = client;\n+    this.path = path;\n+    this.duration = duration;\n+    this.renewalPeriod = (int) (LEASE_RENEWAL_PERCENT_OF_DURATION * this.duration);\n+\n+    if (client.getNumLeaseThreads() < 1) {\n+      throw new LeaseException(ERR_NO_LEASE_THREADS);\n+    }\n+\n+    // Try to get the lease a specified number of times, else throw an error\n+    RetryPolicy retryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n+        LEASE_ACQUIRE_MAX_RETRIES, LEASE_ACQUIRE_RETRY_INTERVAL, TimeUnit.SECONDS);\n+    acquireLease(retryPolicy, 0, 0);\n+\n+    while (leaseID == null && exception == null) {\n+    }\n+    if (exception != null) {\n+      LOG.error(\"Failed to acquire lease on {}\", path);\n+      throw new LeaseException(exception);\n+    }\n+\n+    if (duration != INFINITE_LEASE_DURATION) {\n+      renewLease(renewalPeriod);\n+    }\n+\n+    LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\n+  }\n+\n+  private void acquireLease(RetryPolicy retryPolicy, int numRetries, long delay)\n+      throws LeaseException {\n+    LOG.debug(\"Attempting to acquire lease on {}, retry {}\", path, numRetries);\n+    if (future != null && !future.isDone()) {\n+      throw new LeaseException(ERR_LEASE_FUTURE_EXISTS);\n+    }\n+    future = client.schedule(() -> client.acquireLease(path, duration),\n+        delay, TimeUnit.SECONDS);\n+    client.addCallback(future, new FutureCallback<AbfsRestOperation>() {\n+      @Override\n+      public void onSuccess(@Nullable AbfsRestOperation op) {\n+        leaseID = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_LEASE_ID);\n+        LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\n+      }\n+\n+      @Override\n+      public void onFailure(Throwable throwable) {\n+        try {\n+          if (RetryPolicy.RetryAction.RetryDecision.RETRY\n+              == retryPolicy.shouldRetry(null, numRetries, 0, true).action) {\n+            LOG.debug(\"Failed acquire lease on {}, retrying: {}\", path, throwable);\n+            acquireLease(retryPolicy, numRetries + 1, LEASE_ACQUIRE_RETRY_INTERVAL);\n+          } else {\n+            exception = throwable;\n+          }\n+        } catch (Exception e) {\n+          exception = throwable;\n+        }\n+      }\n+    });\n+  }\n+\n+  private void renewLease(long delay) {\n+    LOG.debug(\"Attempting to renew lease on {}, renew lease id {}, delay {}\", path, leaseID, delay);", "originalCommit": "e4c7a815ec2f33c9561e2a8446499b46e7004b9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDcxMDg5MQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r594710891", "bodyText": "this code should not be required", "author": "snehavarma", "createdAt": "2021-03-15T21:56:17Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -702,15 +735,72 @@ public OutputStream openFileForWrite(final Path path, final FileSystem.Statistic\n         isAppendBlob = true;\n       }\n \n+      SelfRenewingLease lease = maybeCreateLease(relativePath);\n+\n       return new AbfsOutputStream(\n           client,\n           statistics,\n           relativePath,\n           offset,\n-          populateAbfsOutputStreamContext(isAppendBlob));\n+          populateAbfsOutputStreamContext(isAppendBlob, lease));\n     }\n   }\n \n+  /**\n+   * Acquire a lease on an ABFS file for a specified duration. This requires the file to exist.\n+   *\n+   * @param path file name\n+   * @param duration time lease will be held before expiring\n+   * @return the acquired lease ID\n+   * @throws AzureBlobFileSystemException on any exception while acquiring the lease\n+   */\n+  public String acquireLease(final Path path, final int duration) throws AzureBlobFileSystemException {\n+    LOG.debug(\"lease path: {}\", path);\n+\n+    final AbfsRestOperation op =\n+        client.acquireLease(getRelativePath(path), duration);\n+\n+    return op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_LEASE_ID);\n+  }\n+\n+  /**", "originalCommit": "e4c7a815ec2f33c9561e2a8446499b46e7004b9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTI1NTY5Mw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r595255693", "bodyText": "I'm not sure what you mean. Do you mean that we shouldn't have an acquireLease method in the store because leases will be acquired automatically?", "author": "billierinaldi", "createdAt": "2021-03-16T15:01:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDcxMDg5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTMyNzU2Ng==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r595327566", "bodyText": "Yes if the file is being created then infinite lease can be automatically taken, for other cases yes you may need the acquire lease code till we integrate bundling of lease with append.\nRenewLease code might be something you can completely get rid of", "author": "snehavarma", "createdAt": "2021-03-16T16:18:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDcxMDg5MQ=="}], "type": "inlineReview"}, {"oid": "5a2ef896128b2aecf5ff2c9b83281d1d517628db", "url": "https://github.com/apache/hadoop/commit/5a2ef896128b2aecf5ff2c9b83281d1d517628db", "message": "HADOOP-16948. Support single writer dirs.", "committedDate": "2021-03-16T21:48:10Z", "type": "commit"}, {"oid": "86246dc3243ff290766d1a760d128e0194f26a13", "url": "https://github.com/apache/hadoop/commit/86246dc3243ff290766d1a760d128e0194f26a13", "message": "HADOOP-16948. Fix findbugs and checkstyle problems.", "committedDate": "2021-03-16T21:48:10Z", "type": "commit"}, {"oid": "fe3dd135a060400cb761799f8fd1d189deadcd7e", "url": "https://github.com/apache/hadoop/commit/fe3dd135a060400cb761799f8fd1d189deadcd7e", "message": "HADOOP-16948. Fix remaining checkstyle problems.", "committedDate": "2021-03-16T21:48:10Z", "type": "commit"}, {"oid": "1cd1a927f0b63b8c5153a4d8c8c30c10d7e4bb79", "url": "https://github.com/apache/hadoop/commit/1cd1a927f0b63b8c5153a4d8c8c30c10d7e4bb79", "message": "HADOOP-16948. Add DurationInfo, retry policy for acquiring lease, and javadocs", "committedDate": "2021-03-16T21:48:10Z", "type": "commit"}, {"oid": "eeb4ba0f9bc6986d84ea176d8f02ef73cc2fb0e3", "url": "https://github.com/apache/hadoop/commit/eeb4ba0f9bc6986d84ea176d8f02ef73cc2fb0e3", "message": "HADOOP-16948. Convert ABFS client to use an executor for lease ops", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "8e91f6165f8bf599a72bc4e826ef2c19474b1b60", "url": "https://github.com/apache/hadoop/commit/8e91f6165f8bf599a72bc4e826ef2c19474b1b60", "message": "HADOOP-16948. Fix ABFS lease test for non-HNS", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "8eceabb9052daaac60a4fac203ee85073b7ac118", "url": "https://github.com/apache/hadoop/commit/8eceabb9052daaac60a4fac203ee85073b7ac118", "message": "HADOOP-16948. Fix checkstyle and javadoc", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "42f68c2a835ed74fd788e8b87ed19231ce0eded1", "url": "https://github.com/apache/hadoop/commit/42f68c2a835ed74fd788e8b87ed19231ce0eded1", "message": "HADOOP-16948. Address review comments", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "9f3d6897f7cd1cc43061e144abe73c8da18de93d", "url": "https://github.com/apache/hadoop/commit/9f3d6897f7cd1cc43061e144abe73c8da18de93d", "message": "HADOOP-16948. Use daemon threads for ABFS lease ops", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "d67882f80615f8467716a3ed37c10e38249f2afa", "url": "https://github.com/apache/hadoop/commit/d67882f80615f8467716a3ed37c10e38249f2afa", "message": "HADOOP-16948. Make lease duration configurable", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "f00c1459e99b6cdd88ffc55af66fdb44267837bc", "url": "https://github.com/apache/hadoop/commit/f00c1459e99b6cdd88ffc55af66fdb44267837bc", "message": "HADOOP-16948. Add error messages to test assertions", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "f00178355c66810ce7664e21a366cd24fc4c1965", "url": "https://github.com/apache/hadoop/commit/f00178355c66810ce7664e21a366cd24fc4c1965", "message": "HADOOP-16948. Remove extra isSingleWriterKey call", "committedDate": "2021-03-16T21:48:11Z", "type": "commit"}, {"oid": "92e7343b26eb67cf7f2a6ebc885fdd6eb9c9b551", "url": "https://github.com/apache/hadoop/commit/92e7343b26eb67cf7f2a6ebc885fdd6eb9c9b551", "message": "HADOOP-16948. Use only infinite lease duration due to cost of renewal ops", "committedDate": "2021-03-16T21:48:12Z", "type": "commit"}, {"oid": "eca41b4d4ad9aab9ea5caf0df348ddaa824797bc", "url": "https://github.com/apache/hadoop/commit/eca41b4d4ad9aab9ea5caf0df348ddaa824797bc", "message": "HADOOP-16948. Remove acquire/renew/release lease methods", "committedDate": "2021-03-16T21:48:12Z", "type": "commit"}, {"oid": "822615efbf93f3c4056ae62284c631f4bc46554c", "url": "https://github.com/apache/hadoop/commit/822615efbf93f3c4056ae62284c631f4bc46554c", "message": "HADOOP-16948. Rename single writer dirs to infinite lease dirs", "committedDate": "2021-03-16T21:48:12Z", "type": "commit"}, {"oid": "822615efbf93f3c4056ae62284c631f4bc46554c", "url": "https://github.com/apache/hadoop/commit/822615efbf93f3c4056ae62284c631f4bc46554c", "message": "HADOOP-16948. Rename single writer dirs to infinite lease dirs", "committedDate": "2021-03-16T21:48:12Z", "type": "forcePushed"}, {"oid": "9fc4f08bbf794eab83ffaed74a79e0a4ff4aec22", "url": "https://github.com/apache/hadoop/commit/9fc4f08bbf794eab83ffaed74a79e0a4ff4aec22", "message": "HADOOP-16948. Fix checkstyle", "committedDate": "2021-03-17T04:44:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTcyNTYwNA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r595725604", "bodyText": "Do we need these? i.e. Lease threads", "author": "snehavarma", "createdAt": "2021-03-17T05:38:27Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -208,6 +209,15 @@\n       DefaultValue = DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES)\n   private String azureAppendBlobDirs;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_INFINITE_LEASE_KEY,\n+      DefaultValue = DEFAULT_FS_AZURE_INFINITE_LEASE_DIRECTORIES)\n+  private String azureInfiniteLeaseDirs;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_LEASE_THREADS,", "originalCommit": "9fc4f08bbf794eab83ffaed74a79e0a4ff4aec22", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjIwMjU5MA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r596202590", "bodyText": "I think it will still be useful to issue the acquire and release operations in a thread pool for now. Possibly this could be removed if all acquire and release operations are moved into create and flush-with-close in the future.", "author": "billierinaldi", "createdAt": "2021-03-17T16:47:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTcyNTYwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTczNDE4MA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r595734180", "bodyText": "Is the feature for both namespace and flatnamespace enabled accounts?", "author": "snehavarma", "createdAt": "2021-03-17T06:03:35Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -208,6 +209,15 @@\n       DefaultValue = DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES)\n   private String azureAppendBlobDirs;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_INFINITE_LEASE_KEY,", "originalCommit": "9fc4f08bbf794eab83ffaed74a79e0a4ff4aec22", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjIwMzc4NQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r596203785", "bodyText": "I have run the unit test with HNS and flat namespace storage accounts, so I think it will work. I have not done extensive testing with HNS disabled, however.", "author": "billierinaldi", "createdAt": "2021-03-17T16:48:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTczNDE4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODMyMDc0MA==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r598320740", "bodyText": "This looks a CPU-heavy loop. I know it makes for a more responsive app, but it's a busy wait. Any way to replace with some concurrency class?", "author": "steveloughran", "createdAt": "2021-03-21T18:44:54Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.FutureCallback;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableScheduledFuture;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.INFINITE_LEASE_DURATION;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_FUTURE_EXISTS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+\n+/**\n+ * AbfsLease manages an Azure blob lease. It acquires an infinite lease on instantiation and\n+ * releases the lease when free() is called. Use it to prevent writes to the blob by other\n+ * processes that don't have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is acquired. It will\n+ * retry a fixed number of times before failing if there is a problem acquiring the lease.\n+ *\n+ * Call free() to release the Lease. If the holder process dies, AzureBlobFileSystem breakLease\n+ * will need to be called before another client will be able to write to the file.\n+ */\n+public final class AbfsLease {\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsLease.class);\n+\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 10; // Retry interval for acquiring lease in secs\n+  static final int LEASE_ACQUIRE_MAX_RETRIES = 7; // Number of retries for acquiring lease\n+\n+  private final AbfsClient client;\n+  private final String path;\n+\n+  // Lease status variables\n+  private volatile boolean leaseFreed;\n+  private volatile String leaseID = null;\n+  private volatile Throwable exception = null;\n+  private volatile ListenableScheduledFuture<AbfsRestOperation> future = null;\n+\n+  public static class LeaseException extends AzureBlobFileSystemException {\n+    public LeaseException(Throwable t) {\n+      super(ERR_ACQUIRING_LEASE + \": \" + t, t);\n+    }\n+\n+    public LeaseException(String s) {\n+      super(s);\n+    }\n+  }\n+\n+  public AbfsLease(AbfsClient client, String path) throws AzureBlobFileSystemException {\n+    this.leaseFreed = false;\n+    this.client = client;\n+    this.path = path;\n+\n+    if (client.getNumLeaseThreads() < 1) {\n+      throw new LeaseException(ERR_NO_LEASE_THREADS);\n+    }\n+\n+    // Try to get the lease a specified number of times, else throw an error\n+    RetryPolicy retryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n+        LEASE_ACQUIRE_MAX_RETRIES, LEASE_ACQUIRE_RETRY_INTERVAL, TimeUnit.SECONDS);\n+    acquireLease(retryPolicy, 0, 0);\n+\n+    while (leaseID == null && exception == null) {", "originalCommit": "9fc4f08bbf794eab83ffaed74a79e0a4ff4aec22", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODc2NDIxMg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r598764212", "bodyText": "Good point. We will have the Future at that point, so we could wait for it to complete.", "author": "billierinaldi", "createdAt": "2021-03-22T14:27:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODMyMDc0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODc2Nzg2Ng==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r598767866", "bodyText": "I pushed a new change to address this. I am also looking into figuring out if I can mock an acquire lease failure to test this out a bit better.", "author": "billierinaldi", "createdAt": "2021-03-22T14:31:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODMyMDc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5ODMyMDg5OQ==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r598320899", "bodyText": "Failed to", "author": "steveloughran", "createdAt": "2021-03-21T18:45:39Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.FutureCallback;\n+import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableScheduledFuture;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.HttpHeaderConfigurations;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.INFINITE_LEASE_DURATION;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_ACQUIRING_LEASE;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_LEASE_FUTURE_EXISTS;\n+import static org.apache.hadoop.fs.azurebfs.services.AbfsErrors.ERR_NO_LEASE_THREADS;\n+\n+/**\n+ * AbfsLease manages an Azure blob lease. It acquires an infinite lease on instantiation and\n+ * releases the lease when free() is called. Use it to prevent writes to the blob by other\n+ * processes that don't have the lease.\n+ *\n+ * Creating a new Lease object blocks the caller until the Azure blob lease is acquired. It will\n+ * retry a fixed number of times before failing if there is a problem acquiring the lease.\n+ *\n+ * Call free() to release the Lease. If the holder process dies, AzureBlobFileSystem breakLease\n+ * will need to be called before another client will be able to write to the file.\n+ */\n+public final class AbfsLease {\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsLease.class);\n+\n+  static final int LEASE_ACQUIRE_RETRY_INTERVAL = 10; // Retry interval for acquiring lease in secs\n+  static final int LEASE_ACQUIRE_MAX_RETRIES = 7; // Number of retries for acquiring lease\n+\n+  private final AbfsClient client;\n+  private final String path;\n+\n+  // Lease status variables\n+  private volatile boolean leaseFreed;\n+  private volatile String leaseID = null;\n+  private volatile Throwable exception = null;\n+  private volatile ListenableScheduledFuture<AbfsRestOperation> future = null;\n+\n+  public static class LeaseException extends AzureBlobFileSystemException {\n+    public LeaseException(Throwable t) {\n+      super(ERR_ACQUIRING_LEASE + \": \" + t, t);\n+    }\n+\n+    public LeaseException(String s) {\n+      super(s);\n+    }\n+  }\n+\n+  public AbfsLease(AbfsClient client, String path) throws AzureBlobFileSystemException {\n+    this.leaseFreed = false;\n+    this.client = client;\n+    this.path = path;\n+\n+    if (client.getNumLeaseThreads() < 1) {\n+      throw new LeaseException(ERR_NO_LEASE_THREADS);\n+    }\n+\n+    // Try to get the lease a specified number of times, else throw an error\n+    RetryPolicy retryPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n+        LEASE_ACQUIRE_MAX_RETRIES, LEASE_ACQUIRE_RETRY_INTERVAL, TimeUnit.SECONDS);\n+    acquireLease(retryPolicy, 0, 0);\n+\n+    while (leaseID == null && exception == null) {\n+    }\n+    if (exception != null) {\n+      LOG.error(\"Failed to acquire lease on {}\", path);\n+      throw new LeaseException(exception);\n+    }\n+\n+    LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\n+  }\n+\n+  private void acquireLease(RetryPolicy retryPolicy, int numRetries, long delay)\n+      throws LeaseException {\n+    LOG.debug(\"Attempting to acquire lease on {}, retry {}\", path, numRetries);\n+    if (future != null && !future.isDone()) {\n+      throw new LeaseException(ERR_LEASE_FUTURE_EXISTS);\n+    }\n+    future = client.schedule(() -> client.acquireLease(path, INFINITE_LEASE_DURATION),\n+        delay, TimeUnit.SECONDS);\n+    client.addCallback(future, new FutureCallback<AbfsRestOperation>() {\n+      @Override\n+      public void onSuccess(@Nullable AbfsRestOperation op) {\n+        leaseID = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_LEASE_ID);\n+        LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\n+      }\n+\n+      @Override\n+      public void onFailure(Throwable throwable) {\n+        try {\n+          if (RetryPolicy.RetryAction.RetryDecision.RETRY\n+              == retryPolicy.shouldRetry(null, numRetries, 0, true).action) {\n+            LOG.debug(\"Failed acquire lease on {}, retrying: {}\", path, throwable);", "originalCommit": "9fc4f08bbf794eab83ffaed74a79e0a4ff4aec22", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b6803cfa25040895401bc7b6fdeccc2a96e2da7b", "url": "https://github.com/apache/hadoop/commit/b6803cfa25040895401bc7b6fdeccc2a96e2da7b", "message": "HADOOP-16948. Wait for acquire lease future", "committedDate": "2021-03-22T14:26:15Z", "type": "commit"}, {"oid": "4fdfc08089b7a126a8c455ba663637f247ae4d29", "url": "https://github.com/apache/hadoop/commit/4fdfc08089b7a126a8c455ba663637f247ae4d29", "message": "HADOOP-16948. Add unit test for acquire lease failure", "committedDate": "2021-03-23T02:01:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5OTg0MDgwMg==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r599840802", "bodyText": "if this raises an exception, is there any way the while loop will exit?", "author": "steveloughran", "createdAt": "2021-03-23T18:42:59Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java", "diffHunk": "@@ -88,6 +88,12 @@ public AbfsLease(AbfsClient client, String path) throws AzureBlobFileSystemExcep\n     acquireLease(retryPolicy, 0, 0);\n \n     while (leaseID == null && exception == null) {\n+      try {\n+        future.get();\n+      } catch (Exception e) {\n+        LOG.debug(\"Got exception waiting for acquire lease future. Checking if lease ID or \"\n+            + \"exception have been set\", e);\n+      }", "originalCommit": "b6803cfa25040895401bc7b6fdeccc2a96e2da7b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5OTk5NzU2Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r599997567", "bodyText": "Yes, in this section it will retry if it is below the max retries and otherwise set the exception variable. So by max retries we should either have a lease ID or an exception set, and the while loop will exit. In the unit test, I mocked two failures followed by a success as well as persistent failure and verified it had the correct behavior.", "author": "billierinaldi", "createdAt": "2021-03-23T22:22:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5OTg0MDgwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMTg2NzI4Nw==", "url": "https://github.com/apache/hadoop/pull/1925#discussion_r611867287", "bodyText": "understood", "author": "steveloughran", "createdAt": "2021-04-12T18:40:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5OTg0MDgwMg=="}], "type": "inlineReview"}]}