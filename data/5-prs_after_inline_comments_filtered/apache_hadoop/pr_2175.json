{"pr_number": 2175, "pr_title": "HDFS-15497. Make snapshot limit on global as well per snapshot root directory configurable", "pr_createdAt": "2020-07-29T09:57:56Z", "pr_url": "https://github.com/apache/hadoop/pull/2175", "timeline": [{"oid": "dbda6a5cf885b9568f73d956b5bfbe69e4c893d3", "url": "https://github.com/apache/hadoop/commit/dbda6a5cf885b9568f73d956b5bfbe69e4c893d3", "message": "HDFS-15497. Make snapshot limit on global as well per snapshot root directory configurable.", "committedDate": "2020-07-29T09:56:01Z", "type": "commit"}, {"oid": "a2e52ff263fc1f1e3fefd42093cae29c87a458b2", "url": "https://github.com/apache/hadoop/commit/a2e52ff263fc1f1e3fefd42093cae29c87a458b2", "message": "Undo Config name changes.", "committedDate": "2020-07-30T06:04:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4Njk1MQ==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r463786951", "bodyText": "Lets change DFS_NAMENODE_SNAPSHOT_MAX_LIMIT to DFS_NAMENODE_SNAPSHOT_MAX_DIRECTORY_LIMIT. and lets use the same reference of the variable internally;", "author": "mukul1987", "createdAt": "2020-07-31T19:17:38Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java", "diffHunk": "@@ -498,9 +498,14 @@\n   public static final int\n       DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT_DEFAULT = 1000;\n \n-  public static final String DFS_NAMENODE_SNAPSHOT_MAX_LIMIT =\n-      \"dfs.namenode.snapshot.max.limit\";\n+  public static final String\n+      DFS_NAMENODE_SNAPSHOT_MAX_LIMIT = \"dfs.namenode.snapshot.max.limit\";", "originalCommit": "a2e52ff263fc1f1e3fefd42093cae29c87a458b2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzgxOTkyOQ==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r463819929", "bodyText": "It was suggested that the config name should not be changed as it will be an incomatible change. The constant name followes default convention of defining the configs.", "author": "bshashikant", "createdAt": "2020-07-31T20:27:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4Njk1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzgyMDIyMw==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r463820223", "bodyText": "It was changed back to the original name as per Nicholas suggestion.", "author": "bshashikant", "createdAt": "2020-07-31T20:28:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4Njk1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU1MzA2Ng==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464553066", "bodyText": "Yes, we cannot change config names.\nBTW, please revert the white space change so that it will be easier to back port.", "author": "szetszwo", "createdAt": "2020-08-03T17:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4Njk1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MTY5Mw==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464581693", "bodyText": "sure", "author": "bshashikant", "createdAt": "2020-08-03T18:13:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4Njk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4NzMyNA==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r463787324", "bodyText": "maxSnapshotLimit -> maxSnapshotDirectroyLimit", "author": "mukul1987", "createdAt": "2020-07-31T19:18:33Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -117,23 +119,37 @@ public SnapshotManager(final Configuration conf, final FSDirectory fsdir) {\n         DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT,\n         DFSConfigKeys.\n             DFS_NAMENODE_SNAPSHOT_DIFF_ALLOW_SNAP_ROOT_DESCENDANT_DEFAULT);\n+    this.maxSnapshotLimitPerDirectory = conf.getInt(\n+        DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT,\n+        DFSConfigKeys.\n+            DFS_NAMENODE_SNAPSHOT_MAX_LIMIT_DEFAULT);\n     this.maxSnapshotLimit = conf.getInt(\n-        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_MAX_LIMIT,\n-        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_MAX_LIMIT_DEFAULT);\n+        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_GLOBAL_LIMIT,\n+        DFSConfigKeys.DFS_NAMENODE_SNAPSHOT_GLOBAL_LIMIT_DEFAULT);\n     LOG.info(\"Loaded config captureOpenFiles: \" + captureOpenFiles\n         + \", skipCaptureAccessTimeOnlyChange: \"\n         + skipCaptureAccessTimeOnlyChange\n         + \", snapshotDiffAllowSnapRootDescendant: \"\n         + snapshotDiffAllowSnapRootDescendant\n         + \", maxSnapshotLimit: \"\n-        + maxSnapshotLimit);\n+        + maxSnapshotLimit", "originalCommit": "a2e52ff263fc1f1e3fefd42093cae29c87a458b2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQyMDQ2OQ==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464420469", "bodyText": "maxSnapshotLimitPerDirectory is already printed in the line below. I hope this works", "author": "bshashikant", "createdAt": "2020-08-03T13:41:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4NzMyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU1Njc2NA==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464556764", "bodyText": "Let's call this filesystem.limit?  Since there are multiple datacenter settings, \"global\" may be confusing.", "author": "szetszwo", "createdAt": "2020-08-03T17:30:16Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java", "diffHunk": "@@ -498,9 +498,14 @@\n   public static final int\n       DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT_DEFAULT = 1000;\n \n-  public static final String DFS_NAMENODE_SNAPSHOT_MAX_LIMIT =\n-      \"dfs.namenode.snapshot.max.limit\";\n+  public static final String\n+      DFS_NAMENODE_SNAPSHOT_MAX_LIMIT = \"dfs.namenode.snapshot.max.limit\";\n   public static final int DFS_NAMENODE_SNAPSHOT_MAX_LIMIT_DEFAULT = 65536;\n+  public static final String\n+      DFS_NAMENODE_SNAPSHOT_GLOBAL_LIMIT = \"dfs.namenode.snapshot.global.limit\";", "originalCommit": "a2e52ff263fc1f1e3fefd42093cae29c87a458b2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4Nzc4Mw==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464587783", "bodyText": "done", "author": "bshashikant", "createdAt": "2020-08-03T18:26:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU1Njc2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MTM5Ng==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464571396", "bodyText": "Let's keep using maxSnapshotLimit for per directory limit and add a new variable for the new filesystem limit so that it is consistent with the conf.", "author": "szetszwo", "createdAt": "2020-08-03T17:53:28Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -99,13 +99,15 @@\n \n   private boolean allowNestedSnapshots = false;\n   private int snapshotCounter = 0;\n+  private final int maxSnapshotLimitPerDirectory;", "originalCommit": "a2e52ff263fc1f1e3fefd42093cae29c87a458b2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MTc4Mw==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464581783", "bodyText": "sure", "author": "bshashikant", "createdAt": "2020-08-03T18:14:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MTM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4NzcyMA==", "url": "https://github.com/apache/hadoop/pull/2175#discussion_r464587720", "bodyText": "sure", "author": "bshashikant", "createdAt": "2020-08-03T18:25:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MTM5Ng=="}], "type": "inlineReview"}, {"oid": "c4ded1251aee0dfeaf442b36d05ad8412532e96a", "url": "https://github.com/apache/hadoop/commit/c4ded1251aee0dfeaf442b36d05ad8412532e96a", "message": "Addressed review comments.", "committedDate": "2020-08-03T18:24:23Z", "type": "commit"}]}