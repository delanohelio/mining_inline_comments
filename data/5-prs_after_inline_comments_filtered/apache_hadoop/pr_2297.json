{"pr_number": 2297, "pr_title": "HADOOP-17125. Using snappy-java in SnappyCodec", "pr_createdAt": "2020-09-10T20:01:42Z", "pr_url": "https://github.com/apache/hadoop/pull/2297", "timeline": [{"oid": "23da5134a4a14853491d6674b1bfe848efc12b63", "url": "https://github.com/apache/hadoop/commit/23da5134a4a14853491d6674b1bfe848efc12b63", "message": "SnappyCodec with java-snappy", "committedDate": "2020-09-01T17:57:04Z", "type": "commit"}, {"oid": "3aa3f55a770dc1cf736142786dd791ea78960453", "url": "https://github.com/apache/hadoop/commit/3aa3f55a770dc1cf736142786dd791ea78960453", "message": "rebase master", "committedDate": "2020-09-01T17:57:04Z", "type": "commit"}, {"oid": "65fd9f4f2e40da5f64e46afb072fe70ad2ece6ae", "url": "https://github.com/apache/hadoop/commit/65fd9f4f2e40da5f64e46afb072fe70ad2ece6ae", "message": "Reset compressedDirectBuf and uncompressedDirectBuf.", "committedDate": "2020-09-10T19:35:10Z", "type": "commit"}, {"oid": "9c0f08b274261b00436c50f7fe67f7d46486015c", "url": "https://github.com/apache/hadoop/commit/9c0f08b274261b00436c50f7fe67f7d46486015c", "message": "Revert some debugging code.", "committedDate": "2020-09-10T20:00:44Z", "type": "commit"}, {"oid": "f52dd205707259064247b23f7443f5840bace62f", "url": "https://github.com/apache/hadoop/commit/f52dd205707259064247b23f7443f5840bace62f", "message": "Remove snappy native code.", "committedDate": "2020-09-10T22:52:49Z", "type": "commit"}, {"oid": "87903a929342688cc39a512d931441b438b74e72", "url": "https://github.com/apache/hadoop/commit/87903a929342688cc39a512d931441b438b74e72", "message": "Remove snappy compilation.", "committedDate": "2020-09-11T03:40:01Z", "type": "commit"}, {"oid": "5adcf53117d0338910376146766365aa25a13e19", "url": "https://github.com/apache/hadoop/commit/5adcf53117d0338910376146766365aa25a13e19", "message": "Fix limit parameter.", "committedDate": "2020-09-11T17:13:16Z", "type": "commit"}, {"oid": "40cc18a897efd3bbcfbf14bf77babf77890103e8", "url": "https://github.com/apache/hadoop/commit/40cc18a897efd3bbcfbf14bf77babf77890103e8", "message": "Remove require.snappy.", "committedDate": "2020-09-13T22:20:17Z", "type": "commit"}, {"oid": "0e44d4b810fa94253c6eb9b0ad5ea30bc62175ba", "url": "https://github.com/apache/hadoop/commit/0e44d4b810fa94253c6eb9b0ad5ea30bc62175ba", "message": "trigger CI", "committedDate": "2020-09-15T17:55:23Z", "type": "commit"}, {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "url": "https://github.com/apache/hadoop/commit/666a37bc56d6512fe953e438ad4224e935ea6cd2", "message": "Add compatibility test.", "committedDate": "2020-09-15T21:28:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjMyMw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489422323", "bodyText": "use name of config option which users can tun", "author": "steveloughran", "createdAt": "2020-09-16T13:08:03Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");", "originalCommit": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg2MDQ2MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489860460", "bodyText": "I found this check is not needed. Removed.", "author": "viirya", "createdAt": "2020-09-17T01:36:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjY2MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489422661", "bodyText": "please stop the IDE removing trailing whitespace on lines which haven't been edited; complicates life", "author": "steveloughran", "createdAt": "2020-09-16T13:08:33Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");\n+      }\n+      size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);\n+      return size;\n+    }\n+  }\n \n-  private native int decompressBytesDirect();\n-  \n   int decompressDirect(ByteBuffer src, ByteBuffer dst) throws IOException {\n     assert (this instanceof SnappyDirectDecompressor);\n-    \n+", "originalCommit": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg1ODA4OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489858089", "bodyText": "ok, reverted tailing whitespace.", "author": "viirya", "createdAt": "2020-09-17T01:32:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjY2MQ=="}], "type": "inlineReview"}, {"oid": "9de4712af35830e483d37c50261c36812ed047fa", "url": "https://github.com/apache/hadoop/commit/9de4712af35830e483d37c50261c36812ed047fa", "message": "Check snappy library and remove useless code.", "committedDate": "2020-09-17T01:23:24Z", "type": "commit"}, {"oid": "0ed518d238adfee81c821b74434785afb5684710", "url": "https://github.com/apache/hadoop/commit/0ed518d238adfee81c821b74434785afb5684710", "message": "Revert trailing whitespace.", "committedDate": "2020-09-17T01:34:39Z", "type": "commit"}, {"oid": "0ed518d238adfee81c821b74434785afb5684710", "url": "https://github.com/apache/hadoop/commit/0ed518d238adfee81c821b74434785afb5684710", "message": "Revert trailing whitespace.", "committedDate": "2020-09-17T01:34:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMTg5Mw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489921893", "bodyText": "In the original code, we throw a runtime exception if the native snappy is not found. Should we follow?\n      throw new RuntimeException(\"native snappy library not available: \" +\n          \"SnappyCompressor has not been loaded.\");", "author": "dbtsai", "createdAt": "2020-09-17T03:15:02Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,19 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      LOG.warn(\"Error loading snappy libraries: \" + t);", "originalCommit": "0ed518d238adfee81c821b74434785afb5684710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Njg2OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r490596868", "bodyText": "ok, changed to throw RuntimeException.", "author": "viirya", "createdAt": "2020-09-17T22:28:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMTg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMjk4MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489922981", "bodyText": "we need to check if the snappy class is available for SnappyCompressor too.", "author": "dbtsai", "createdAt": "2020-09-17T03:16:50Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,24 +48,6 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;", "originalCommit": "0ed518d238adfee81c821b74434785afb5684710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Njc2MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r490596761", "bodyText": "added.", "author": "viirya", "createdAt": "2020-09-17T22:28:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMjk4MQ=="}], "type": "inlineReview"}, {"oid": "712749c041c012bb8eef41f826d1abc8da937a36", "url": "https://github.com/apache/hadoop/commit/712749c041c012bb8eef41f826d1abc8da937a36", "message": "For review comment.", "committedDate": "2020-09-17T22:22:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDA0OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494054048", "bodyText": "Fix this last sentence if you make a new PR", "author": "saintstack", "createdAt": "2020-09-24T05:51:38Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDgyNg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070826", "bodyText": "Oops, thanks.", "author": "viirya", "createdAt": "2020-09-24T06:37:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDA0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDM4OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494054389", "bodyText": "Is it the 'native snappy library' that is missing or the java-snappy jar?", "author": "saintstack", "createdAt": "2020-09-24T05:52:44Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"native snappy library not available: \" +", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDQ0Mg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070442", "bodyText": "It is java-snappy jar, yeah, I will revise the message.", "author": "viirya", "createdAt": "2020-09-24T06:36:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDM4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTM0NA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494055344", "bodyText": "s/compressBytesDirect/compressBytesDirectBuf/ ? Or.. why the Bytes... I see none referenced in the method so compressDirectBuf?", "author": "saintstack", "createdAt": "2020-09-24T05:55:41Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +282,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressBytesDirect() throws IOException {", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDE4OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070189", "bodyText": "This compressBytesDirect and decompressBytesDirect basically are copied from original method names. compressDirectBuf and decompressDirectBuf looks good to me.", "author": "viirya", "createdAt": "2020-09-24T06:35:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTM0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTkwMQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494055901", "bodyText": "ditto... could this message be more informative: i.e. \"hey, operator... you need to add the snappy-java.jar to your CLASSPATH... its not packaged up for you..\"", "author": "saintstack", "createdAt": "2020-09-24T05:57:15Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,20 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"native snappy library not available: \" +", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NjQyOQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494056429", "bodyText": "ditto", "author": "saintstack", "createdAt": "2020-09-24T05:59:04Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,10 +267,20 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494057480", "bodyText": "hmm... this is a little anemic. Have you considered adding a data file that is a little more interesting than this?", "author": "saintstack", "createdAt": "2020-09-24T06:02:18Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,43 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07050d06050d\";", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA2OTU4Ng==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494069586", "bodyText": "String is to make the test as simple as possible. Maybe further shorten the string?", "author": "viirya", "createdAt": "2020-09-24T06:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI1Njg2Ng==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494256866", "bodyText": "should be split across lines, but otherwise fine inline -simpler for tests", "author": "steveloughran", "createdAt": "2020-09-24T12:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzMjIyNg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494532226", "bodyText": "Ok, I split the long string. Thanks.", "author": "viirya", "createdAt": "2020-09-24T18:36:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}], "type": "inlineReview"}, {"oid": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "url": "https://github.com/apache/hadoop/commit/2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "message": "Address review comments.", "committedDate": "2020-09-24T18:34:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzNzU5Nw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494537597", "bodyText": "nit, uncompressedDirectBuf.limit(uncompressedDirectBuf.capacity()).position(0); for safety.", "author": "dbtsai", "createdAt": "2020-09-24T18:46:06Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(directBufferSize).position(0);", "originalCommit": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU1MDgwOQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494550809", "bodyText": "done. thanks.", "author": "viirya", "createdAt": "2020-09-24T19:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzNzU5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494539014", "bodyText": "Why is this change needed?", "author": "dbtsai", "createdAt": "2020-09-24T18:48:38Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -432,7 +412,11 @@ public void assertCompression(String name, Compressor compressor,\n               joiner.join(name, \"byte arrays not equals error !!!\"),\n               originalRawData, decompressOut.toByteArray());\n         } catch (Exception ex) {\n-          fail(joiner.join(name, ex.getMessage()));\n+          if (ex.getMessage() != null) {\n+            fail(joiner.join(name, ex.getMessage()));\n+          } else {\n+            fail(joiner.join(name, ExceptionUtils.getStackTrace(ex)));", "originalCommit": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU0MjI0OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494542249", "bodyText": "When I first took over this change, the test failed with NPE without any details. It is because the exception thrown returns null from getMessage(). joiner.join(name, null) causes the NPE, so I changed it to print stack trace once getMessage() returns null. It's better for debugging.", "author": "viirya", "createdAt": "2020-09-24T18:54:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTczNjI5NA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r499736294", "bodyText": "NPE is why toString() is what new code should do.\nWhy don't we just throw new AssertionError(name +ex, ex). That way, the stack trace doesn't get lost, which is something we never want to have happen,", "author": "steveloughran", "createdAt": "2020-10-05T16:48:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA=="}], "type": "inlineReview"}, {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "url": "https://github.com/apache/hadoop/commit/1cb398bbbaf702501f558ce32cda07d1ca7917ca", "message": "Take safer approach.", "committedDate": "2020-09-24T19:09:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODM2Mg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495158362", "bodyText": "nit: this seems unnecessary as clear is called shortly after at the call site?", "author": "sunchao", "createdAt": "2020-09-25T18:22:07Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(uncompressedDirectBuf.capacity()).position(0);", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MzE5MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495183191", "bodyText": "Seems so, I remember I added this to fix test failure. It might be SnappyDecompressor, I think, then I copied to SnappyCompressor. Deleted this and see what Jenkins tells.", "author": "viirya", "createdAt": "2020-09-25T19:13:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODM2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODY1NQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495158655", "bodyText": "nit: SnappyLoader is marked as \"internal use-only\" though so not sure if there is better alternative here.", "author": "sunchao", "createdAt": "2020-09-25T18:22:41Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,21 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if it is available.\n+    try {\n+      SnappyLoader.getVersion();", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MjMzMg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495182332", "bodyText": "The \"internal user-only\" of SnappyLoader, based on its comment, seems more related to native library loading stuff.\ngetVersion is static method and it doesn't involve loading of native library described in SnappyLoader, so I guess it is fine? Otherwise, I don't find other proper one to check.", "author": "viirya", "createdAt": "2020-09-25T19:11:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODY1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2Mzg1MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495163850", "bodyText": "nit: can we just call compressedDirectBuf.clear()?", "author": "sunchao", "createdAt": "2020-09-25T18:33:04Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,10 +268,20 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressDirectBuf() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      int size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MzI5MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495183290", "bodyText": "yap", "author": "viirya", "createdAt": "2020-09-25T19:14:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2Mzg1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495165398", "bodyText": "nit: unrelated changes :)", "author": "sunchao", "createdAt": "2020-09-25T18:36:13Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -495,19 +479,16 @@ public String getName() {\n     Compressor compressor = pair.compressor;\n \n     if (compressor.getClass().isAssignableFrom(Lz4Compressor.class)\n-            && (NativeCodeLoader.isNativeCodeLoaded()))", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NTkwOA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495175908", "bodyText": "Oh, this is from @dbtsai's original change. I think adding curly brackets is better? I can revert this if you think it is necessary.", "author": "viirya", "createdAt": "2020-09-25T18:58:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NjkwOA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495176908", "bodyText": "Yeah usually it's not recommended to include unrelated changes in Hadoop patch, we may add another refactoring PR later if this is absolutely necessary.", "author": "sunchao", "createdAt": "2020-09-25T19:00:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5MzMzOA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495193338", "bodyText": "Ok. Reverted the change.", "author": "viirya", "createdAt": "2020-09-25T19:36:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTkxOQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495165919", "bodyText": "nit: long lines (80 chars).", "author": "sunchao", "createdAt": "2020-09-25T18:37:16Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,49 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a08050206\" +\n+            \"0a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a0\" +\n+            \"30a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07\" +\n+            \"050d06050d\";\n+    String compressed = \"8001f07f010a06030a040a0c0109020c0a010204020d02000b010701080605080b0909020\" +\n+            \"60a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d\" +\n+            \"060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b0\" +\n+            \"60e030e0a07050d06050d\";\n+\n+    byte[] rawDataBytes = Hex.decodeHex(rawData);\n+    byte[] compressedBytes = Hex.decodeHex(compressed);\n+\n+    ByteBuffer inBuf = ByteBuffer.allocateDirect(compressedBytes.length);\n+    inBuf.put(compressedBytes, 0, compressedBytes.length);\n+    inBuf.flip();\n+\n+    ByteBuffer outBuf = ByteBuffer.allocateDirect(rawDataBytes.length);\n+    ByteBuffer expected = ByteBuffer.wrap(rawDataBytes);\n+\n+    SnappyDecompressor.SnappyDirectDecompressor decompressor = new SnappyDecompressor.SnappyDirectDecompressor();", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "aa6a6d550d6c154f072622371263287dc1b34f11", "url": "https://github.com/apache/hadoop/commit/aa6a6d550d6c154f072622371263287dc1b34f11", "message": "For review comments.", "committedDate": "2020-09-25T19:25:02Z", "type": "commit"}, {"oid": "19edba29fa0e6949f11926ada6d606bee39dad67", "url": "https://github.com/apache/hadoop/commit/19edba29fa0e6949f11926ada6d606bee39dad67", "message": "Update BUILDING and NativeLibraries.", "committedDate": "2020-09-25T19:33:35Z", "type": "commit"}, {"oid": "beec93160a3dafbc62637672a70c07f53474e3f9", "url": "https://github.com/apache/hadoop/commit/beec93160a3dafbc62637672a70c07f53474e3f9", "message": "Merge remote-tracking branch 'upstream/trunk' into java-snappy", "committedDate": "2020-09-25T19:35:34Z", "type": "commit"}, {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555", "url": "https://github.com/apache/hadoop/commit/fc525faf79b68662f452fc0cd8233194ef9f4555", "message": "Make snappy-java as compile scope.", "committedDate": "2020-09-29T17:53:36Z", "type": "commit"}, {"oid": "562b80d036e43dfe4412219b3887f2e557b2d783", "url": "https://github.com/apache/hadoop/commit/562b80d036e43dfe4412219b3887f2e557b2d783", "message": "For review comment.", "committedDate": "2020-09-29T23:06:44Z", "type": "commit"}, {"oid": "0600169d3fa5d082c6a28defc59872b2f485873f", "url": "https://github.com/apache/hadoop/commit/0600169d3fa5d082c6a28defc59872b2f485873f", "message": "Revert Snappy description in BUILDING.txt.", "committedDate": "2020-09-29T23:15:14Z", "type": "commit"}, {"oid": "0600169d3fa5d082c6a28defc59872b2f485873f", "url": "https://github.com/apache/hadoop/commit/0600169d3fa5d082c6a28defc59872b2f485873f", "message": "Revert Snappy description in BUILDING.txt.", "committedDate": "2020-09-29T23:15:14Z", "type": "forcePushed"}, {"oid": "fd71a20df19543a66f23f384c58ad985f0ee2e67", "url": "https://github.com/apache/hadoop/commit/fd71a20df19543a66f23f384c58ad985f0ee2e67", "message": "Fix style issue.", "committedDate": "2020-10-01T17:10:56Z", "type": "commit"}, {"oid": "7dc320ddddeaeac5a4eee36c703fededbccbf6de", "url": "https://github.com/apache/hadoop/commit/7dc320ddddeaeac5a4eee36c703fededbccbf6de", "message": "Fix another style...", "committedDate": "2020-10-02T18:57:22Z", "type": "commit"}, {"oid": "5685b0b7902e7a6c350d3378dea1159cb968cb24", "url": "https://github.com/apache/hadoop/commit/5685b0b7902e7a6c350d3378dea1159cb968cb24", "message": "Address comments: throwing AssertionError and exclude jobTokenPassword for license check.", "committedDate": "2020-10-05T18:07:50Z", "type": "commit"}]}