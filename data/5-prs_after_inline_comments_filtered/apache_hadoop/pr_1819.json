{"pr_number": 1819, "pr_title": "HDFS-14989. Add a 'swapBlockList' operation to Namenode.", "pr_createdAt": "2020-01-27T06:07:02Z", "pr_url": "https://github.com/apache/hadoop/pull/1819", "timeline": [{"oid": "c728a4dd0ad94a3c1e64a7a51ee97a47ac42a310", "url": "https://github.com/apache/hadoop/commit/c728a4dd0ad94a3c1e64a7a51ee97a47ac42a310", "message": "HDFS-14989. Add a 'swapBlockList' operation to Namenode.", "committedDate": "2020-01-27T06:02:38Z", "type": "commit"}, {"oid": "fb2fda8c97b24b88ab679ada6144a6a939e7aa61", "url": "https://github.com/apache/hadoop/commit/fb2fda8c97b24b88ab679ada6144a6a939e7aa61", "message": "HDFS-14989. Add a 'swapBlockList' operation to Namenode. (Fix checkstyle issues)", "committedDate": "2020-01-27T06:02:46Z", "type": "commit"}, {"oid": "ef331c8120bd898a0ab5a454effc72a867b4f6f8", "url": "https://github.com/apache/hadoop/commit/ef331c8120bd898a0ab5a454effc72a867b4f6f8", "message": "HDFS-14989. Address review comments.", "committedDate": "2020-01-27T06:02:55Z", "type": "commit"}, {"oid": "2ca150ea42fd75ebe11bffbcbac73c94fc1a61e1", "url": "https://github.com/apache/hadoop/commit/2ca150ea42fd75ebe11bffbcbac73c94fc1a61e1", "message": "HDFS-14989. Swap storage policy ID, check destination file genStamp.", "committedDate": "2020-01-27T06:03:03Z", "type": "commit"}, {"oid": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "url": "https://github.com/apache/hadoop/commit/eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "message": "HDFS-14989. Remove unused import.", "committedDate": "2020-01-27T06:03:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMjA1MA==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371322050", "bodyText": "This isn't required, This would be redundant check, You are having the same check in FSNamesystem. Its required only twice, once before taking lock and once after...", "author": "ayushtkn", "createdAt": "2020-01-27T15:52:41Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "diffHunk": "@@ -2667,4 +2667,15 @@ public Long getNextSPSPath() throws IOException {\n     }\n     return namesystem.getBlockManager().getSPSManager().getNextPathId();\n   }\n+\n+  public boolean swapBlockList(String src, String dst, long maxTimestamp)\n+      throws IOException {\n+    checkNNStartup();\n+    if (stateChangeLog.isDebugEnabled()) {\n+      stateChangeLog.debug(\"*DIR* NameNode.swapBlockList: {} and {}\", src, dst);\n+    }\n+    namesystem.checkOperation(OperationCategory.WRITE);", "originalCommit": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjQ1Mw==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236453", "bodyText": "Fixed this.", "author": "avijayanhwx", "createdAt": "2020-01-30T22:50:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMjA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyMzc1Nw==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371323757", "bodyText": "Can use FsDirectory.resolveLastINode()", "author": "ayushtkn", "createdAt": "2020-01-27T15:55:24Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);\n+\n+    long srcHeader = srcINodeFile.getHeaderLong();\n+    long dstHeader = dstINodeFile.getHeaderLong();\n+\n+    byte dstBlockLayoutPolicy =\n+        HeaderFormat.getBlockLayoutPolicy(dstHeader);\n+    byte srcBlockLayoutPolicy =\n+        HeaderFormat.getBlockLayoutPolicy(srcHeader);\n+\n+    byte dstStoragePolicyID = HeaderFormat.getStoragePolicyID(dstHeader);\n+    byte srcStoragePolicyID = HeaderFormat.getStoragePolicyID(srcHeader);\n+\n+    dstINodeFile.updateHeaderWithNewPolicy(srcBlockLayoutPolicy,\n+        srcStoragePolicyID);\n+    dstINodeFile.setModificationTime(mtime);\n+\n+    srcINodeFile.updateHeaderWithNewPolicy(dstBlockLayoutPolicy,\n+        dstStoragePolicyID);\n+    srcINodeFile.setModificationTime(mtime);\n+\n+    return new SwapBlockListResult(true,\n+        fsd.getAuditFileInfo(srcIIP),\n+        fsd.getAuditFileInfo(dstIIP));\n+  }\n+\n+  private static void validateInode(INodesInPath srcIIP)\n+      throws IOException {\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List input \";\n+    final INode srcInode = srcIIP.getLastINode();\n+\n+    // Check if INode is null.\n+    if (srcInode == null) {\n+      error  += srcIIP.getPath() + \" is not found.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new FileNotFoundException(error);\n+    }", "originalCommit": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyNDM4NQ==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371324385", "bodyText": "nit. My IDE complains that the null assignment is redundant. No need to assign null, it is already null.", "author": "ayushtkn", "createdAt": "2020-01-27T15:56:20Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;", "originalCommit": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjQ4MQ==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236481", "bodyText": "Fixed this.", "author": "avijayanhwx", "createdAt": "2020-01-30T22:50:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMyNDM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzMTY2MQ==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371331661", "bodyText": "Can use srcIIP.getLastINode().asFile(). Looks better. :)", "author": "ayushtkn", "createdAt": "2020-01-27T16:07:46Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();", "originalCommit": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjUyOA==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236528", "bodyText": "Fixed this.", "author": "avijayanhwx", "createdAt": "2020-01-30T22:50:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzMTY2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371334237", "bodyText": "nit : Refactored dstInodeFIleBlocks and used only once, but didn't do for srcInodeFile. May be should keep same for both.", "author": "ayushtkn", "createdAt": "2020-01-27T16:11:51Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.FSDirectory.DirOp;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot;\n+import org.apache.hadoop.util.Time;\n+\n+/**\n+ * Class to carry out the operation of swapping blocks from one file to another.\n+ * Along with swapping blocks, we can also optionally swap the block layout\n+ * of a file header, which is useful for client operations like converting\n+ * replicated to EC file.\n+ */\n+public final class SwapBlockListOp {\n+\n+  private SwapBlockListOp() {\n+  }\n+\n+  static SwapBlockListResult swapBlocks(FSDirectory fsd, FSPermissionChecker pc,\n+                          String src, String dst, long genTimestamp)\n+      throws IOException {\n+\n+    final INodesInPath srcIIP = fsd.resolvePath(pc, src, DirOp.WRITE);\n+    final INodesInPath dstIIP = fsd.resolvePath(pc, dst, DirOp.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      fsd.checkAncestorAccess(pc, srcIIP, FsAction.WRITE);\n+      fsd.checkAncestorAccess(pc, dstIIP, FsAction.WRITE);\n+    }\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.swapBlockList: \"\n+          + srcIIP.getPath() + \" and \" + dstIIP.getPath());\n+    }\n+    SwapBlockListResult result = null;\n+    fsd.writeLock();\n+    try {\n+      result = swapBlockList(fsd, srcIIP, dstIIP, genTimestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    return result;\n+  }\n+\n+  private static SwapBlockListResult swapBlockList(FSDirectory fsd,\n+                                    final INodesInPath srcIIP,\n+                                    final INodesInPath dstIIP,\n+                                    long genTimestamp)\n+      throws IOException {\n+\n+    assert fsd.hasWriteLock();\n+    validateInode(srcIIP);\n+    validateInode(dstIIP);\n+    fsd.ezManager.checkMoveValidity(srcIIP, dstIIP);\n+\n+    final String src = srcIIP.getPath();\n+    final String dst = dstIIP.getPath();\n+    if (dst.equals(src)) {\n+      throw new FileAlreadyExistsException(\"The source \" + src +\n+          \" and destination \" + dst + \" are the same\");\n+    }\n+\n+    INodeFile srcINodeFile = (INodeFile) srcIIP.getLastINode();\n+    INodeFile dstINodeFile = (INodeFile) dstIIP.getLastINode();\n+\n+    String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n+    String error = \"Swap Block List destination file \";\n+    BlockInfo lastBlock = dstINodeFile.getLastBlock();\n+    if (lastBlock != null && lastBlock.getGenerationStamp() != genTimestamp) {\n+      error  += dstIIP.getPath() +\n+          \" has last block with different gen timestamp.\";\n+      NameNode.stateChangeLog.warn(errorPrefix + error);\n+      throw new IOException(error);\n+    }\n+\n+    long mtime = Time.now();\n+    BlockInfo[] dstINodeFileBlocks = dstINodeFile.getBlocks();\n+    dstINodeFile.replaceBlocks(srcINodeFile.getBlocks());\n+    srcINodeFile.replaceBlocks(dstINodeFileBlocks);", "originalCommit": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0OTEyNg==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r372649126", "bodyText": "@ayushtkn I don't quite understand the issue here. I implemented a temp variable based swap mechanism. Could you explain it more?", "author": "avijayanhwx", "createdAt": "2020-01-29T21:48:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjk4MDM1MA==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r372980350", "bodyText": "I meant just refactor this too, srcINodeFile.getBlocks() as you did for dstINodeFile.getBlocks() into a variable, to match the code style with  srcINodeFile.replaceBlocks(dstINodeFileBlocks);\nOptional stuff, If you prefer to do. :)", "author": "ayushtkn", "createdAt": "2020-01-30T14:29:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNzE3Nw==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373237177", "bodyText": "@ayushtkn I had to store dstINodeFile.getBlocks() into a variable because without that, after line 101, I would have lost the original dstINodeFile blocks. For scrInodeFile, I don't need that. So I left that out.", "author": "avijayanhwx", "createdAt": "2020-01-30T22:52:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTMzNDIzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM1NDU4NA==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r371354584", "bodyText": "For the validations can use LambdaTestUtils, Like this\n LambdaTestUtils.intercept(FileNotFoundException.class, \"/TestSwapBlockList/dir1/fileXYZ\", () -> fsn .swapBlockList(\"/TestSwapBlockList/dir1/fileXYZ\", \"/TestSwapBlockList/dir1/dir11/file3\", 0L));", "author": "ayushtkn", "createdAt": "2020-01-27T16:45:42Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSwapBlockList.java", "diffHunk": "@@ -0,0 +1,257 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n+import org.apache.hadoop.hdfs.server.namenode.INodeFile.HeaderFormat;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Test SwapBlockListOp working.\n+ */\n+public class TestSwapBlockList {\n+\n+  private static final short REPLICATION = 3;\n+\n+  private static final long SEED = 0;\n+  private final Path rootDir = new Path(\"/\" + getClass().getSimpleName());\n+\n+  private final Path subDir1 = new Path(rootDir, \"dir1\");\n+  private final Path file1 = new Path(subDir1, \"file1\");\n+  private final Path file2 = new Path(subDir1, \"file2\");\n+\n+  private final Path subDir11 = new Path(subDir1, \"dir11\");\n+  private final Path file3 = new Path(subDir11, \"file3\");\n+\n+  private final Path subDir2 = new Path(rootDir, \"dir2\");\n+  private final Path file4 = new Path(subDir2, \"file4\");\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FSNamesystem fsn;\n+  private FSDirectory fsdir;\n+\n+  private DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MAX_XATTRS_PER_INODE_KEY, 2);\n+    cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+\n+    fsn = cluster.getNamesystem();\n+    fsdir = fsn.getFSDirectory();\n+\n+    hdfs = cluster.getFileSystem();\n+\n+    hdfs.mkdirs(subDir2);\n+\n+    DFSTestUtil.createFile(hdfs, file1, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file2, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file3, 1024, REPLICATION, SEED);\n+    DFSTestUtil.createFile(hdfs, file4, 1024, REPLICATION, SEED);\n+\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testInputValidation() throws Exception {\n+\n+    // Source file not found.\n+    try {\n+      fsn.swapBlockList(\"/TestSwapBlockList/dir1/fileXYZ\",\n+          \"/TestSwapBlockList/dir1/dir11/file3\", 0L);\n+      Assert.fail();\n+    } catch (IOException ioEx) {\n+      Assert.assertTrue(ioEx instanceof FileNotFoundException);\n+      Assert.assertTrue(\n+          ioEx.getMessage().contains(\"/TestSwapBlockList/dir1/fileXYZ\"));\n+    }", "originalCommit": "eaee621cd681730b1e4cf1c37c261d5e7292e3fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzIzNjU2Ng==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373236566", "bodyText": "Fixed this.", "author": "avijayanhwx", "createdAt": "2020-01-30T22:50:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM1NDU4NA=="}], "type": "inlineReview"}, {"oid": "5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de", "url": "https://github.com/apache/hadoop/commit/5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de", "message": "HDFS-14989. Address review comment.", "committedDate": "2020-01-30T22:49:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI5NjQyNw==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373296427", "bodyText": "Any reason for not assigning FSDirectory.resolveLastINode(srcIIP); to srcInode?", "author": "ayushtkn", "createdAt": "2020-01-31T02:54:57Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SwapBlockListOp.java", "diffHunk": "@@ -130,15 +129,10 @@ private static void validateInode(INodesInPath srcIIP)\n \n     String errorPrefix = \"DIR* FSDirectory.swapBlockList: \";\n     String error = \"Swap Block List input \";\n-    final INode srcInode = srcIIP.getLastINode();\n \n-    // Check if INode is null.\n-    if (srcInode == null) {\n-      error  += srcIIP.getPath() + \" is not found.\";\n-      NameNode.stateChangeLog.warn(errorPrefix + error);\n-      throw new FileNotFoundException(error);\n-    }\n+    FSDirectory.resolveLastINode(srcIIP);\n \n+    final INode srcInode = srcIIP.getLastINode();", "originalCommit": "5a1e25dcbbc5f45d90d24a53b5daa6c6cdc028de", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY4NTIyMA==", "url": "https://github.com/apache/hadoop/pull/1819#discussion_r373685220", "bodyText": "Thanks, good catch. Missed it.", "author": "avijayanhwx", "createdAt": "2020-01-31T21:06:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI5NjQyNw=="}], "type": "inlineReview"}, {"oid": "b5885b3c7ff0b42dad955133405e4189ad1b3dd3", "url": "https://github.com/apache/hadoop/commit/b5885b3c7ff0b42dad955133405e4189ad1b3dd3", "message": "HDFS-14989. Use FSDirectory.resolveLastINode.", "committedDate": "2020-01-31T21:05:15Z", "type": "commit"}]}