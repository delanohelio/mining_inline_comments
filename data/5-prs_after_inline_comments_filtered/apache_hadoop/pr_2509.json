{"pr_number": 2509, "pr_title": "HADOOP-17404. ABFS: Small write - Merge append and flush", "pr_createdAt": "2020-12-01T19:13:04Z", "pr_url": "https://github.com/apache/hadoop/pull/2509", "timeline": [{"oid": "5b089d7cf8d9d26bd957abf6e1317d8be505dd90", "url": "https://github.com/apache/hadoop/commit/5b089d7cf8d9d26bd957abf6e1317d8be505dd90", "message": "Small write - Merge append and flush", "committedDate": "2020-12-01T19:02:29Z", "type": "commit"}, {"oid": "3301124e58def96e4e7caf9dca388c4d653abeb4", "url": "https://github.com/apache/hadoop/commit/3301124e58def96e4e7caf9dca388c4d653abeb4", "message": "Findbug fix", "committedDate": "2020-12-03T03:30:04Z", "type": "commit"}, {"oid": "843a01510611c4a5a1e4a365f2a5a2e3b43a51f9", "url": "https://github.com/apache/hadoop/commit/843a01510611c4a5a1e4a365f2a5a2e3b43a51f9", "message": "Findbugs and checkstyle fixes", "committedDate": "2020-12-10T04:22:16Z", "type": "commit"}, {"oid": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423", "url": "https://github.com/apache/hadoop/commit/1e7f30877dcc2e3baedf99eeb29c4abff7bf0423", "message": "Review comment incorporation", "committedDate": "2020-12-10T11:25:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDExOTg1Mg==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540119852", "bodyText": "This can be class level constant right, the lieral \"true\" as such does not have anuthing to do with HTTP related operations.", "author": "bilaharith", "createdAt": "2020-12-10T12:13:35Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java", "diffHunk": "@@ -76,6 +76,7 @@\n   public static final String AT = \"@\";\n   public static final String HTTP_HEADER_PREFIX = \"x-ms-\";\n   public static final String HASH = \"#\";\n+  public static final String TRUE = \"true\";", "originalCommit": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY5NTYyNA==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540695624", "bodyText": "Boolean query param value field. Will retain.", "author": "snvijaya", "createdAt": "2020-12-11T05:13:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDExOTg1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyMTMwMw==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540121303", "bodyText": "Can this be 2 separate if blocks", "author": "bilaharith", "createdAt": "2020-12-10T12:15:55Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -395,38 +396,58 @@ public AbfsRestOperation renameIdempotencyCheckOp(\n     return op;\n   }\n \n-  public AbfsRestOperation append(final String path, final long position, final byte[] buffer, final int offset,\n-                                  final int length, final String cachedSasToken, final boolean isAppendBlob) throws AzureBlobFileSystemException {\n+  public AbfsRestOperation append(final String path, final byte[] buffer,\n+      AppendRequestParameters reqParams, final String cachedSasToken)\n+      throws AzureBlobFileSystemException {\n     final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n     // JDK7 does not support PATCH, so to workaround the issue we will use\n     // PUT and specify the real method in the X-Http-Method-Override header.\n     requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE,\n-            HTTP_METHOD_PATCH));\n+        HTTP_METHOD_PATCH));\n \n     final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n     abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, APPEND_ACTION);\n-    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(position));\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(reqParams.getPosition()));\n+\n+    if ((reqParams.getMode() == AppendRequestParameters.Mode.FLUSH_MODE) || (", "originalCommit": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY5NzcwOA==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540697708", "bodyText": "On either condition being true, flush queryParam will be true. and for close alone close queryParam will be true.", "author": "snvijaya", "createdAt": "2020-12-11T05:20:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyMTMwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyODU2MQ==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540128561", "bodyText": "you may remove this new line in between", "author": "bilaharith", "createdAt": "2020-12-10T12:27:08Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestSmallWriteOptimization.java", "diffHunk": "@@ -0,0 +1,524 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.util.Arrays;\n+import java.util.Random;\n+import java.util.UUID;\n+import java.util.Map;\n+import java.io.IOException;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.Assume;\n+import org.junit.runners.Parameterized;\n+import org.junit.runner.RunWith;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+", "originalCommit": "1e7f30877dcc2e3baedf99eeb29c4abff7bf0423", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDcxODgyNg==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540718826", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-12-11T06:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEyODU2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk4MTY3OQ==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r539981679", "bodyText": "ensure number of calls to append is correct", "author": "vinaysbadami", "createdAt": "2020-12-10T08:48:34Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java", "diffHunk": "@@ -108,13 +103,15 @@ public void verifyShortWriteRequest() throws Exception {\n \n     out.hsync();\n \n-    verify(client, times(2)).append(acString.capture(), acLong.capture(), acByteArray.capture(), acBufferOffset.capture(), acBufferLength.capture(),\n-                                    acSASToken.capture(), acAppendBlobAppend.capture());\n-    assertThat(Arrays.asList(PATH, PATH)).describedAs(\"Path of the requests\").isEqualTo(acString.getAllValues());\n-    assertThat(Arrays.asList(Long.valueOf(0), Long.valueOf(WRITE_SIZE))).describedAs(\"Write Position\").isEqualTo(acLong.getAllValues());\n-    assertThat(Arrays.asList(0, 0)).describedAs(\"Buffer Offset\").isEqualTo(acBufferOffset.getAllValues());\n-    assertThat(Arrays.asList(WRITE_SIZE, 2*WRITE_SIZE)).describedAs(\"Buffer length\").isEqualTo(acBufferLength.getAllValues());\n+    AppendRequestParameters firstReqParameters = new AppendRequestParameters(\n+        0, 0, WRITE_SIZE, APPEND_MODE, false);\n+    AppendRequestParameters secondReqParameters = new AppendRequestParameters(\n+        WRITE_SIZE, 0, 2 * WRITE_SIZE, APPEND_MODE, false);\n \n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(firstReqParameters), any());\n+    verify(client, times(1)).append(\n+        eq(PATH), any(byte[].class), refEq(secondReqParameters), any());", "originalCommit": "843a01510611c4a5a1e4a365f2a5a2e3b43a51f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc0NjYxMQ==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r540746611", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-12-11T07:37:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk4MTY3OQ=="}], "type": "inlineReview"}, {"oid": "21b8e6d822f7113ddd2c283a48716ac69813267e", "url": "https://github.com/apache/hadoop/commit/21b8e6d822f7113ddd2c283a48716ac69813267e", "message": "Addressing new line review comment", "committedDate": "2020-12-11T06:24:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjE3MzEzMA==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r552173130", "bodyText": "for newly added config key, a little comment would be very helpful", "author": "DadanielZ", "createdAt": "2021-01-05T20:22:14Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -55,6 +55,7 @@\n   public static final String AZURE_WRITE_MAX_CONCURRENT_REQUESTS = \"fs.azure.write.max.concurrent.requests\";\n   public static final String AZURE_WRITE_MAX_REQUESTS_TO_QUEUE = \"fs.azure.write.max.requests.to.queue\";\n   public static final String AZURE_WRITE_BUFFER_SIZE = \"fs.azure.write.request.size\";\n+  public static final String AZURE_ENABLE_SMALL_WRITE_OPTIMIZATION = \"fs.azure.write.enableappendwithflush\";", "originalCommit": "21b8e6d822f7113ddd2c283a48716ac69813267e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgxMTAyMg==", "url": "https://github.com/apache/hadoop/pull/2509#discussion_r552811022", "bodyText": "Done", "author": "snvijaya", "createdAt": "2021-01-06T17:02:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjE3MzEzMA=="}], "type": "inlineReview"}, {"oid": "a8a9393a3a24e354fcd22225978e63568e7a643f", "url": "https://github.com/apache/hadoop/commit/a8a9393a3a24e354fcd22225978e63568e7a643f", "message": "Merge to trunk", "committedDate": "2021-01-06T11:13:05Z", "type": "commit"}, {"oid": "fa2a50250e60bcd6f5416dc25e6bf37c33fbd4d7", "url": "https://github.com/apache/hadoop/commit/fa2a50250e60bcd6f5416dc25e6bf37c33fbd4d7", "message": "Addressing review comments", "committedDate": "2021-01-06T17:00:22Z", "type": "commit"}]}