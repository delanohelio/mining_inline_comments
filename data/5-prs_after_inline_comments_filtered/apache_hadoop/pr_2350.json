{"pr_number": 2350, "pr_title": " HADOOP-17292. Using lz4-java in Lz4Codec", "pr_createdAt": "2020-09-29T19:23:55Z", "pr_url": "https://github.com/apache/hadoop/pull/2350", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5OTkyNg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r496999926", "bodyText": "compressDirectBuf?", "author": "dbtsai", "createdAt": "2020-09-29T19:43:01Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java", "diffHunk": "@@ -236,7 +237,7 @@ public synchronized int compress(byte[] b, int off, int len)\n     }\n \n     // Compress data\n-    n = useLz4HC ? compressBytesDirectHC() : compressBytesDirect();\n+    n = compressBytesDirect();", "originalCommit": "d2d41750641f6453ab19616ac59d7219a2a9dceb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODYyMjAyOQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r498622029", "bodyText": "fixed.", "author": "viirya", "createdAt": "2020-10-02T05:19:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5OTkyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwMTI3MQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r497001271", "bodyText": "indentation as you remove the { }", "author": "dbtsai", "createdAt": "2020-09-29T19:45:32Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodec.java", "diffHunk": "@@ -143,22 +143,16 @@ public void testSnappyCodec() throws IOException {\n   \n   @Test\n   public void testLz4Codec() throws IOException {\n-    if (NativeCodeLoader.isNativeCodeLoaded()) {\n-      if (Lz4Codec.isNativeCodeLoaded()) {\n-        conf.setBoolean(\n+    conf.setBoolean(\n             CommonConfigurationKeys.IO_COMPRESSION_CODEC_LZ4_USELZ4HC_KEY,", "originalCommit": "d2d41750641f6453ab19616ac59d7219a2a9dceb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODYyMTQ4OQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r498621489", "bodyText": "fixed.", "author": "viirya", "createdAt": "2020-10-02T05:15:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwMTI3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwMTYxNA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r497001614", "bodyText": "Can we add a compatibility test like snappy?", "author": "dbtsai", "createdAt": "2020-09-29T19:46:10Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -494,8 +494,7 @@ public String getName() {\n   private static <T extends Compressor, E extends Decompressor> boolean isAvailable(TesterPair<T, E> pair) {\n     Compressor compressor = pair.compressor;\n \n-    if (compressor.getClass().isAssignableFrom(Lz4Compressor.class)\n-            && (NativeCodeLoader.isNativeCodeLoaded()))\n+    if (compressor.getClass().isAssignableFrom(Lz4Compressor.class))", "originalCommit": "d2d41750641f6453ab19616ac59d7219a2a9dceb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwNDUwMA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r497004500", "bodyText": "Sure. Will add it later.", "author": "viirya", "createdAt": "2020-09-29T19:51:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwMTYxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODYyMTE1MQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r498621151", "bodyText": "Added compatibility test.", "author": "viirya", "createdAt": "2020-10-02T05:13:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwMTYxNA=="}], "type": "inlineReview"}, {"oid": "10b027d05c9bf46aa3432dcd88e9902511387832", "url": "https://github.com/apache/hadoop/commit/10b027d05c9bf46aa3432dcd88e9902511387832", "message": "Add compatibility test and fix style.", "committedDate": "2020-10-02T05:15:13Z", "type": "forcePushed"}, {"oid": "4aefff40c2f0c0a2074644d0de34f6f11bd50bc8", "url": "https://github.com/apache/hadoop/commit/4aefff40c2f0c0a2074644d0de34f6f11bd50bc8", "message": "Add compatibility test and fix style.", "committedDate": "2020-10-02T05:18:43Z", "type": "forcePushed"}, {"oid": "cdb1a68fdf057c753d9d59e5d89af5fba8fa6724", "url": "https://github.com/apache/hadoop/commit/cdb1a68fdf057c753d9d59e5d89af5fba8fa6724", "message": "Add lz4-java to test dependency.", "committedDate": "2020-10-03T23:17:43Z", "type": "forcePushed"}, {"oid": "44662282d18e52f0e8e3c2525fd99b3bc602bcc3", "url": "https://github.com/apache/hadoop/commit/44662282d18e52f0e8e3c2525fd99b3bc602bcc3", "message": "Add lz4-java to test dependency.", "committedDate": "2020-10-07T17:35:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MjY4MQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502552681", "bodyText": "put into the same import block as org.sjf4j", "author": "steveloughran", "createdAt": "2020-10-09T16:45:13Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java", "diffHunk": "@@ -22,9 +22,10 @@\n import java.nio.Buffer;\n import java.nio.ByteBuffer;\n \n+import net.jpountz.lz4.LZ4Factory;\n+import net.jpountz.lz4.LZ4Compressor;", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQyODMyNA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504428324", "bodyText": "They are already in same block, no?\nimport net.jpountz.lz4.LZ4Factory;\nimport net.jpountz.lz4.LZ4Compressor;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.io.compress.Compressor;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;", "author": "viirya", "createdAt": "2020-10-14T06:21:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MjY4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MzEwMQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502553101", "bodyText": "final?", "author": "steveloughran", "createdAt": "2020-10-09T16:46:02Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java", "diffHunk": "@@ -50,20 +51,7 @@\n \n   private final boolean useLz4HC;\n \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded()) {\n-      // Initialize the native library\n-      try {\n-        initIDs();\n-      } catch (Throwable t) {\n-        // Ignore failure to load/initialize lz4\n-        LOG.warn(t.toString());\n-      }\n-    } else {\n-      LOG.error(\"Cannot load \" + Lz4Compressor.class.getName() +\n-          \" without native hadoop library!\");\n-    }\n-  }\n+  private LZ4Compressor lz4Compressor;", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDkwOQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504444909", "bodyText": "thanks. added final.", "author": "viirya", "createdAt": "2020-10-14T07:00:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MzEwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MzMzOA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502553338", "bodyText": "again, not in same import block as org.apache", "author": "steveloughran", "createdAt": "2020-10-09T16:46:25Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java", "diffHunk": "@@ -22,8 +22,9 @@\n import java.nio.Buffer;\n import java.nio.ByteBuffer;\n \n+import net.jpountz.lz4.LZ4Factory;\n+import net.jpountz.lz4.LZ4SafeDecompressor;", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQyODUyMA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504428520", "bodyText": "Already in same block?\nimport net.jpountz.lz4.LZ4Factory;\nimport net.jpountz.lz4.LZ4SafeDecompressor;\nimport org.apache.hadoop.io.compress.Decompressor;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;", "author": "viirya", "createdAt": "2020-10-14T06:22:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MzMzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI0NTc0MQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r520245741", "bodyText": "@viirya I think steveloughran is mentioning about the order of imports and blank line between import blocks.\nhttps://github.com/steveloughran/formality/blob/master/styleguide/styleguide.md#imports\nWhile the original file violates the coding stantdard and we usually avoid large diff just for fixing formatting issue, it would be ok to fix a few lines here.", "author": "iwasakims", "createdAt": "2020-11-10T02:28:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MzMzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM3MDMxMw==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r525370313", "bodyText": "yeah, if things are mixed up, best to leave alone -at least for those files which get lots of changes. For something which rarely sees maintenance, you can make a stronger case for cleanup. I do it sometimes, but as I also get to field cherrypick merge pain, I don't go wild on it", "author": "steveloughran", "createdAt": "2020-11-17T17:57:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1MzMzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1NTA5Mg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502555092", "bodyText": "Shame the constructor can't throw exceptions direct.\nCatch only the exception's known to be raised, and don't convert RTEs or, especially, Errors.", "author": "steveloughran", "createdAt": "2020-10-09T16:49:48Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java", "diffHunk": "@@ -67,6 +55,15 @@\n   public Lz4Decompressor(int directBufferSize) {\n     this.directBufferSize = directBufferSize;\n \n+    try {\n+      LZ4Factory lz4Factory = LZ4Factory.fastestInstance();\n+      lz4Decompressor = lz4Factory.safeDecompressor();\n+    } catch (Throwable t) {", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzNDI5Ng==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504434296", "bodyText": "Hmm, I tried to explicitly catch ClassNotFoundException, but the java compiler complains...\n exception java.lang.ClassNotFoundException is never thrown in body of corresponding try statement", "author": "viirya", "createdAt": "2020-10-14T06:36:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1NTA5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI0OTQ5Mg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r520249492", "bodyText": "LZ4Factory seems to throw java.lang.AssertionError after catching unrecoverable exception.", "author": "iwasakims", "createdAt": "2020-11-10T02:41:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1NTA5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM3MzMyNA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r525373324", "bodyText": "ok, so what's best here?\n\nCatch AssertionError and wrap\nCatch throwable, but with something ahead of it which will catch and rethrow Error without wrapping. Because we shouldn't really be wrapping those high-priority problems that aren't generally things to ignore", "author": "steveloughran", "createdAt": "2020-11-17T18:01:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1NTA5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxODQwNQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r525618405", "bodyText": "Let me just catch AssertionError and wrap it.", "author": "viirya", "createdAt": "2020-11-18T00:37:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1NTA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1NTQ3NQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502555475", "bodyText": "add +t to the end of the string, so the specific error text isn't lost", "author": "steveloughran", "createdAt": "2020-10-09T16:50:29Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java", "diffHunk": "@@ -67,6 +55,15 @@\n   public Lz4Decompressor(int directBufferSize) {\n     this.directBufferSize = directBufferSize;\n \n+    try {\n+      LZ4Factory lz4Factory = LZ4Factory.fastestInstance();\n+      lz4Decompressor = lz4Factory.safeDecompressor();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"lz4-java library is not available: \" +\n+              \"Lz4Decompressor has not been loaded. You need to add \" +\n+              \"lz4-java.jar to your CLASSPATH\", t);", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDc4Mg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504444782", "bodyText": "thanks. added it.", "author": "viirya", "createdAt": "2020-10-14T06:59:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU1NTQ3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2NDE2Ng==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502564166", "bodyText": "The library also allow configuring the compression level, which perhaps we can add a Hadoop option to enable that later. This just use the default compression level.", "author": "sunchao", "createdAt": "2020-10-09T17:07:20Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java", "diffHunk": "@@ -76,6 +64,19 @@ public Lz4Compressor(int directBufferSize, boolean useLz4HC) {\n     this.useLz4HC = useLz4HC;\n     this.directBufferSize = directBufferSize;\n \n+    try {\n+      LZ4Factory lz4Factory = LZ4Factory.fastestInstance();\n+      if (useLz4HC) {\n+        lz4Compressor = lz4Factory.highCompressor();", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzMTYyOA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504431628", "bodyText": "Yeah, sounds good. We can add it later if it is necessary.", "author": "viirya", "createdAt": "2020-10-14T06:30:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2NDE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2NTIwOQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502565209", "bodyText": "nit: seems we no longer need the field useLz4HC with this.", "author": "sunchao", "createdAt": "2020-10-09T17:09:28Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java", "diffHunk": "@@ -76,6 +64,19 @@ public Lz4Compressor(int directBufferSize, boolean useLz4HC) {\n     this.useLz4HC = useLz4HC;\n     this.directBufferSize = directBufferSize;\n \n+    try {\n+      LZ4Factory lz4Factory = LZ4Factory.fastestInstance();\n+      if (useLz4HC) {", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzMTk0NA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504431944", "bodyText": "Oh, yes.", "author": "viirya", "createdAt": "2020-10-14T06:30:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2NTIwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2NjE5MQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502566191", "bodyText": "seems some of the methods in this class look exactly the same as in SnappyCompressor - perhaps we can do some refactoring later.", "author": "sunchao", "createdAt": "2020-10-09T17:11:34Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java", "diffHunk": "@@ -302,11 +303,20 @@ public synchronized long getBytesWritten() {\n   public synchronized void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  private native int compressBytesDirectHC();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() {", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzMjUwNA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504432504", "bodyText": "Ok, currently this PR focuses on turning to use lz4-java. We can refactor compressor/decompressor classes later.", "author": "viirya", "createdAt": "2020-10-14T06:32:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2NjE5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU3MTQyNg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502571426", "bodyText": "I think this isn't necessary since it's called right before the call site?", "author": "sunchao", "createdAt": "2020-10-09T17:22:19Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java", "diffHunk": "@@ -302,11 +303,20 @@ public synchronized long getBytesWritten() {\n   public synchronized void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  private native int compressBytesDirectHC();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      compressedDirectBuf.clear();", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0MTA4MA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504441080", "bodyText": "We can remove the clear and limit calls at the call site.\n// Re-initialize the lz4's output direct-buffer\n    compressedDirectBuf.clear();\n    compressedDirectBuf.limit(0);\nBut we cannot remove clear in compressDirectBuf method.\nFirst, lz4-java cannot accept the compressedDirectBuf if it is called with limit(0).\n[ERROR] Failures:                                                                                                                                                                                                                      \n[ERROR]   TestCompressorDecompressor.testCompressorDecompressor:69  Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: net.jpountz.lz4.LZ4Exception: maxDestLen is too small                        \n        at net.jpountz.lz4.LZ4JNICompressor.compress(LZ4JNICompressor.java:69)                                                                                                                                                         \n        at net.jpountz.lz4.LZ4Compressor.compress(LZ4Compressor.java:158)                                                                                                                                                              \n        at org.apache.hadoop.io.compress.lz4.Lz4Compressor.compressDirectBuf(Lz4Compressor.java:310)                                                                                                                                   \n        at org.apache.hadoop.io.compress.lz4.Lz4Compressor.compress(Lz4Compressor.java:237)                                                                                                                                            \n        at org.apache.hadoop.io.compress.CompressDecompressTester$CompressionTestStrategy$2.assertCompression(CompressDecompressTester.java:286)                                                                                       \n        at org.apache.hadoop.io.compress.CompressDecompressTester.test(CompressDecompressTester.java:114)                                                                                                                              \n        at org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor(TestCompressorDecompressor.java:66)                                                                                                     \n\nSecond, even we remove limit in the call site, one test still failed:\n[ERROR] testCompressorDecompressor(org.apache.hadoop.io.compress.TestCompressorDecompressor)  Time elapsed: 0.539 s  <<< FAILURE!\njava.lang.AssertionError: org.apache.hadoop.io.compress.lz4.Lz4Compressor_org.apache.hadoop.io.compress.lz4.Lz4Decompressor- empty stream compressed output size != 4 expected:<4> but was:<65796>\n        at org.junit.Assert.fail(Assert.java:88)                                                                                                                                                                                       \n        at org.junit.Assert.failNotEquals(Assert.java:834)                                               \n        at org.junit.Assert.assertEquals(Assert.java:645)                                                                                                                                                                              \n        at org.apache.hadoop.io.compress.CompressDecompressTester$CompressionTestStrategy$3.assertCompression(CompressDecompressTester.java:334)\n        at org.apache.hadoop.io.compress.CompressDecompressTester.test(CompressDecompressTester.java:114)      \n        at org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor(TestCompressorDecompressor.java:66)", "author": "viirya", "createdAt": "2020-10-14T06:51:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU3MTQyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5MDM0Ng==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r505690346", "bodyText": "Okay makes sense.", "author": "sunchao", "createdAt": "2020-10-15T16:46:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU3MTQyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU3ODc3MQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502578771", "bodyText": "I don't think this will ever happen but it's not a big deal.", "author": "sunchao", "createdAt": "2020-10-09T17:36:19Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java", "diffHunk": "@@ -272,7 +269,19 @@ public synchronized void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n-\n-  private native int decompressBytesDirect();\n+  private int decompressDirectBuf() {\n+    if (compressedDirectBufLen == 0) {", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU4MDgxMQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502580811", "bodyText": "nit: this comment doesn't add much value - it just state what is exactly being done in the code.", "author": "sunchao", "createdAt": "2020-10-09T17:40:21Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java", "diffHunk": "@@ -272,7 +269,19 @@ public synchronized void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n-\n-  private native int decompressBytesDirect();\n+  private int decompressDirectBuf() {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0MTUyMg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504441522", "bodyText": "ok, removed.", "author": "viirya", "createdAt": "2020-10-14T06:52:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU4MDgxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU4MTU0MQ==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502581541", "bodyText": "you can just call clear?", "author": "sunchao", "createdAt": "2020-10-09T17:41:50Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java", "diffHunk": "@@ -272,7 +269,19 @@ public synchronized void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n-\n-  private native int decompressBytesDirect();\n+  private int decompressDirectBuf() {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      lz4Decompressor.decompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0MTgxNg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504441816", "bodyText": "sure.", "author": "viirya", "createdAt": "2020-10-14T06:53:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU4MTU0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU4NjEyOA==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r502586128", "bodyText": "nit: perhaps some comments on this - not quite sure what it is testing.", "author": "sunchao", "createdAt": "2020-10-09T17:51:40Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/lz4/TestLz4CompressorDecompressor.java", "diffHunk": "@@ -330,4 +328,33 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testLz4Compatibility() throws Exception {\n+    Path filePath = new Path(TestLz4CompressorDecompressor.class", "originalCommit": "1121cf0bf2494af50a6cb58fb1cf0c6246b63ef6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ0NDUyNg==", "url": "https://github.com/apache/hadoop/pull/2350#discussion_r504444526", "bodyText": "ok, added some comment.", "author": "viirya", "createdAt": "2020-10-14T06:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU4NjEyOA=="}], "type": "inlineReview"}, {"oid": "7835a2f2028f2e63ade7c2e3e1f03cdb9469d1ec", "url": "https://github.com/apache/hadoop/commit/7835a2f2028f2e63ade7c2e3e1f03cdb9469d1ec", "message": "Address review comments.", "committedDate": "2020-10-21T21:42:24Z", "type": "forcePushed"}, {"oid": "4fb61dec49f6be2722b53daf4437c6a8f7fad99d", "url": "https://github.com/apache/hadoop/commit/4fb61dec49f6be2722b53daf4437c6a8f7fad99d", "message": "Fix merging issue.", "committedDate": "2020-11-08T06:55:02Z", "type": "forcePushed"}, {"oid": "f40b9658a2d6cd49166ccf20497f9d420625a302", "url": "https://github.com/apache/hadoop/commit/f40b9658a2d6cd49166ccf20497f9d420625a302", "message": "Use lz4-java for Lz4Codec.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "5571c254553931bd15420243a0a0a609ad138467", "url": "https://github.com/apache/hadoop/commit/5571c254553931bd15420243a0a0a609ad138467", "message": "Add compatibility test and fix style.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "9ed789ed415533338473672ef1b3e999afaa2429", "url": "https://github.com/apache/hadoop/commit/9ed789ed415533338473672ef1b3e999afaa2429", "message": "Copy lz4 c file and header.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "1edddbdced3e3279e82cbce200b4044b7446bfa4", "url": "https://github.com/apache/hadoop/commit/1edddbdced3e3279e82cbce200b4044b7446bfa4", "message": "Add lz4-java to test dependency.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "ffda7e8ffa763ba1930a9a2013f9d3eeb75d2395", "url": "https://github.com/apache/hadoop/commit/ffda7e8ffa763ba1930a9a2013f9d3eeb75d2395", "message": "Fix some style issues.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "d1fa9929dc12291daffd5ca7602ef2794b004592", "url": "https://github.com/apache/hadoop/commit/d1fa9929dc12291daffd5ca7602ef2794b004592", "message": "Address review comments.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "c35a8ce80d6400af574c566a9bbe80ef682adb0e", "url": "https://github.com/apache/hadoop/commit/c35a8ce80d6400af574c566a9bbe80ef682adb0e", "message": "Fix merging issue.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "41e59b162b0ce265246361ac58841f1076960dc7", "url": "https://github.com/apache/hadoop/commit/41e59b162b0ce265246361ac58841f1076960dc7", "message": "Separate import blocks.", "committedDate": "2020-11-17T23:50:34Z", "type": "commit"}, {"oid": "41e59b162b0ce265246361ac58841f1076960dc7", "url": "https://github.com/apache/hadoop/commit/41e59b162b0ce265246361ac58841f1076960dc7", "message": "Separate import blocks.", "committedDate": "2020-11-17T23:50:34Z", "type": "forcePushed"}, {"oid": "c02e8ca6ae19fa85b160df7b4dcb740cd7b7ef05", "url": "https://github.com/apache/hadoop/commit/c02e8ca6ae19fa85b160df7b4dcb740cd7b7ef05", "message": "Just catch AssertionError and wrap it.", "committedDate": "2020-11-18T00:38:29Z", "type": "commit"}]}