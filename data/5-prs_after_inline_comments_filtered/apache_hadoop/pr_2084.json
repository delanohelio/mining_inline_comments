{"pr_number": 2084, "pr_title": "HDFS-15418. ViewFileSystemOverloadScheme should represent mount links\u2026", "pr_createdAt": "2020-06-18T20:35:59Z", "pr_url": "https://github.com/apache/hadoop/pull/2084", "timeline": [{"oid": "2d156bcbdda2014cc0d09822aa631ab2c1c8005e", "url": "https://github.com/apache/hadoop/commit/2d156bcbdda2014cc0d09822aa631ab2c1c8005e", "message": "HDFS-15418. ViewFileSystemOverloadScheme should represent mount links as non symlinks", "committedDate": "2020-06-18T20:28:54Z", "type": "commit"}, {"oid": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "url": "https://github.com/apache/hadoop/commit/2c55c9a5cf7f88f3e435a23203504072050d5e43", "message": "HDFS-15418. CHeckstyle and test fixes", "committedDate": "2020-06-19T02:41:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTE4Mw==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442679183", "bodyText": "Should we just fallback to showing link permissions if the directory isn't found? as it was before.\nLs command throwing FNF for a child seems little weird. If the child isn't there the Ls need not to list it. FNF from Ls should be only I guess when the directory on which we are calling getListing() isn't available, For a child if it isn't present in the destination we can just log it and revert back to previous behavior and we can document that too as well, if the actual destination is present then only it shows destination permissions.", "author": "ayushtkn", "createdAt": "2020-06-19T07:31:03Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "diffHunk": "@@ -1216,37 +1222,50 @@ public FileStatus getFileStatus(Path f) throws IOException {\n       for (Entry<String, INode<FileSystem>> iEntry :\n           theInternalDir.getChildren().entrySet()) {\n         INode<FileSystem> inode = iEntry.getValue();\n+        Path path = new Path(inode.fullPath).makeQualified(myUri, null);\n         if (inode.isLink()) {\n           INodeLink<FileSystem> link = (INodeLink<FileSystem>) inode;\n+\n+          if (showMountLinksAsSymlinks) {\n+            // To maintain backward compatibility, with default option(showing\n+            // mount links as symlinks), we will represent target link as\n+            // symlink and rest other properties are belongs to mount link only.\n+            result[i++] =\n+                new FileStatus(0, false, 0, 0, creationTime, creationTime,\n+                    PERMISSION_555, ugi.getShortUserName(),\n+                    ugi.getPrimaryGroupName(), link.getTargetLink(),\n+                    path);\n+            continue;\n+          }\n+\n+          //  We will represent as non-symlinks. Here it will show target\n+          //  directory/file properties like permissions, isDirectory etc on\n+          //  mount path. The path will be a mount link path and isDirectory is\n+          //  true if target is dir, otherwise false.\n+          String linkedPath = link.getTargetFileSystem().getUri().getPath();\n+          if (\"\".equals(linkedPath)) {\n+            linkedPath = \"/\";\n+          }\n           try {\n-            String linkedPath = link.getTargetFileSystem().getUri().getPath();\n-            if(\"\".equals(linkedPath)) {\n-              linkedPath = \"/\";\n-            }\n             FileStatus status =\n                 ((ChRootedFileSystem)link.getTargetFileSystem())\n                 .getMyFs().getFileStatus(new Path(linkedPath));\n-            result[i++] = new FileStatus(status.getLen(), false,\n-              status.getReplication(), status.getBlockSize(),\n-              status.getModificationTime(), status.getAccessTime(),\n-              status.getPermission(), status.getOwner(), status.getGroup(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            result[i++] = new FileStatus(status.getLen(), status.isDirectory(),\n+                status.getReplication(), status.getBlockSize(),\n+                status.getModificationTime(), status.getAccessTime(),\n+                status.getPermission(), status.getOwner(), status.getGroup(),\n+                null, path);\n           } catch (FileNotFoundException ex) {\n-            result[i++] = new FileStatus(0, false, 0, 0,\n-              creationTime, creationTime, PERMISSION_555,\n-              ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            LOG.warn(\"Cannot get one of the children's(\" + path\n+                + \")  target path(\" + link.getTargetFileSystem().getUri()\n+                + \") file status.\", ex);\n+            throw ex;", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk0NTY5MA==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442945690", "bodyText": "This is a non-symlink flow: what we should assume for isDirectory when target not available?\nI feel that users should know that target is not available somehow. In symlink case I am fine as we don't need that isDir info and anyway we will just keep target in symlink.\nIf we simply return successful when one of the child not not available, they should not assume is exist would be successful.\nOr we will simply return is directory as true when target not found and rest all permission are fake. Just logging does not help here.\nDo you know what happens in router? probably we can make it to similar behavior for consistency in this case?", "author": "umamaheswararao", "createdAt": "2020-06-19T16:47:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTE4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk2MjkwMw==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442962903", "bodyText": "In Router if the destination isn't present it will not throw exception, it will show the permissions and owner of the mount entry. In router the mount table entries also have owner/permissions. It will first try to get it from the actual destination, if the actual destination is not available then from the mount table. if Unable to get from Mount Entry(this won't happen general) then it will show owner as super user\nisDir stays true always for mount entries assuming only directories will be mounted in general case.", "author": "ayushtkn", "createdAt": "2020-06-19T17:27:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTE4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2MDM5MQ==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r443060391", "bodyText": "The problem here is, rounter has state, but in viewFS is simply light, state can be build from config only, so no real permissions notion. All mounts will have same perms bit. Just showing that really may not help. Most of the fs what we mount are high available. SO, don't  expect this case should trigger ( even I see this point in you comment \"this won't happen general\"). Even if it triggered, application will anyway starts seeing failures on further ops on that directory. And one more point I am having is, restricting first and relaxing later is safer than relaxing and later restricting is tough. So, I feel Let's throw exception. If this really turning out problems, then we can always relax (that should be safe as we are having general IOException). When coming to symlink notion, you always have an identity other than file/dir, that is link. So, I have no issues. But non-symlinks case, I am not really show some temp values there. With that values anyway app is going to fail if he crawls one more level.", "author": "umamaheswararao", "createdAt": "2020-06-19T22:05:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTE4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTUzMw==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442679533", "bodyText": "assertFalse() ?", "author": "ayushtkn", "createdAt": "2020-06-19T07:31:52Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewfsFileStatus.java", "diffHunk": "@@ -148,9 +148,11 @@ public void testListStatusACL() throws IOException {\n         if (status.getPath().getName().equals(\"file\")) {\n           assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n               status.getPermission());\n+          assertEquals(false, status.isDirectory());", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk0OTMxNg==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442949316", "bodyText": "Oops, thats a copy paste thing :-). Will fix it.", "author": "umamaheswararao", "createdAt": "2020-06-19T16:55:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTUzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTYyNQ==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442679625", "bodyText": "assertTrue() ?", "author": "ayushtkn", "createdAt": "2020-06-19T07:32:04Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewfsFileStatus.java", "diffHunk": "@@ -148,9 +148,11 @@ public void testListStatusACL() throws IOException {\n         if (status.getPath().getName().equals(\"file\")) {\n           assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n               status.getPermission());\n+          assertEquals(false, status.isDirectory());\n         } else {\n           assertEquals(FsPermission.valueOf(\"-r--rwxr--\"),\n               status.getPermission());\n+          assertEquals(true, status.isDirectory());", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3NTI3Mw==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442975273", "bodyText": "done all of this simiar", "author": "umamaheswararao", "createdAt": "2020-06-19T17:56:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3OTYyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4MTUyNQ==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442681525", "bodyText": "A line break after @before should look better.", "author": "ayushtkn", "createdAt": "2020-06-19T07:36:24Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk0NDY5OQ==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442944699", "bodyText": "For some reason, I switched to new intellij setup. somehow Intellij making this change. need to set my formatter rules.", "author": "umamaheswararao", "createdAt": "2020-06-19T16:45:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4MTUyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4NzY3Mg==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442687672", "bodyText": "Line Break.", "author": "ayushtkn", "createdAt": "2020-06-19T07:49:12Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4ODk1OA==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442688958", "bodyText": "assertFalse()", "author": "ayushtkn", "createdAt": "2020-06-19T07:51:52Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {\n+    FileUtil.fullyDelete(TEST_DIR);\n+  }\n+\n+  /**\n+   * Tests the ACL and isDirectory returned from listStatus for directories and\n+   * files.\n+   */\n+  @Test\n+  public void testListStatusACL() throws IOException, URISyntaxException {\n+    String testfilename = \"testFileACL\";\n+    String childDirectoryName = \"testDirectoryACL\";\n+    TEST_DIR.mkdirs();\n+    File infile = new File(TEST_DIR, testfilename);\n+    final byte[] content = \"dingos\".getBytes();\n+\n+    try (FileOutputStream fos = new FileOutputStream(infile)) {\n+      fos.write(content);\n+    }\n+    assertEquals(content.length, infile.length());\n+    File childDir = new File(TEST_DIR, childDirectoryName);\n+    childDir.mkdirs();\n+\n+    Configuration conf = new Configuration();\n+    ConfigUtil.addLink(conf, \"/file\", infile.toURI());\n+    ConfigUtil.addLink(conf, \"/dir\", childDir.toURI());\n+    String fileScheme = \"file\";\n+    conf.set(String.format(\"fs.%s.impl\", fileScheme),\n+        ViewFileSystemOverloadScheme.class.getName());\n+    conf.set(String\n+        .format(FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+            fileScheme), LocalFileSystem.class.getName());\n+    String fileUriStr = \"file:///\";\n+    try (FileSystem vfs = FileSystem.get(new URI(fileUriStr), conf)) {\n+      assertEquals(ViewFileSystemOverloadScheme.class, vfs.getClass());\n+      FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n+\n+      FileSystem localFs = ((ViewFileSystemOverloadScheme) vfs)\n+          .getRawFileSystem(new Path(fileUriStr), conf);\n+      FileStatus fileStat = localFs.getFileStatus(new Path(infile.getPath()));\n+      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n+\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(fileStat.getPermission(), status.getPermission());\n+        } else {\n+          assertEquals(dirStat.getPermission(), status.getPermission());\n+        }\n+      }\n+\n+      localFs.setPermission(new Path(infile.getPath()),\n+          FsPermission.valueOf(\"-rwxr--r--\"));\n+      localFs.setPermission(new Path(childDir.getPath()),\n+          FsPermission.valueOf(\"-r--rwxr--\"));\n+\n+      statuses = vfs.listStatus(new Path(\"/\"));\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n+              status.getPermission());\n+          assertEquals(false, status.isDirectory());", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY4OTA3OQ==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442689079", "bodyText": "assertTrue()?", "author": "ayushtkn", "createdAt": "2020-06-19T07:52:07Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {\n+    FileUtil.fullyDelete(TEST_DIR);\n+  }\n+\n+  /**\n+   * Tests the ACL and isDirectory returned from listStatus for directories and\n+   * files.\n+   */\n+  @Test\n+  public void testListStatusACL() throws IOException, URISyntaxException {\n+    String testfilename = \"testFileACL\";\n+    String childDirectoryName = \"testDirectoryACL\";\n+    TEST_DIR.mkdirs();\n+    File infile = new File(TEST_DIR, testfilename);\n+    final byte[] content = \"dingos\".getBytes();\n+\n+    try (FileOutputStream fos = new FileOutputStream(infile)) {\n+      fos.write(content);\n+    }\n+    assertEquals(content.length, infile.length());\n+    File childDir = new File(TEST_DIR, childDirectoryName);\n+    childDir.mkdirs();\n+\n+    Configuration conf = new Configuration();\n+    ConfigUtil.addLink(conf, \"/file\", infile.toURI());\n+    ConfigUtil.addLink(conf, \"/dir\", childDir.toURI());\n+    String fileScheme = \"file\";\n+    conf.set(String.format(\"fs.%s.impl\", fileScheme),\n+        ViewFileSystemOverloadScheme.class.getName());\n+    conf.set(String\n+        .format(FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+            fileScheme), LocalFileSystem.class.getName());\n+    String fileUriStr = \"file:///\";\n+    try (FileSystem vfs = FileSystem.get(new URI(fileUriStr), conf)) {\n+      assertEquals(ViewFileSystemOverloadScheme.class, vfs.getClass());\n+      FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n+\n+      FileSystem localFs = ((ViewFileSystemOverloadScheme) vfs)\n+          .getRawFileSystem(new Path(fileUriStr), conf);\n+      FileStatus fileStat = localFs.getFileStatus(new Path(infile.getPath()));\n+      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n+\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(fileStat.getPermission(), status.getPermission());\n+        } else {\n+          assertEquals(dirStat.getPermission(), status.getPermission());\n+        }\n+      }\n+\n+      localFs.setPermission(new Path(infile.getPath()),\n+          FsPermission.valueOf(\"-rwxr--r--\"));\n+      localFs.setPermission(new Path(childDir.getPath()),\n+          FsPermission.valueOf(\"-r--rwxr--\"));\n+\n+      statuses = vfs.listStatus(new Path(\"/\"));\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n+              status.getPermission());\n+          assertEquals(false, status.isDirectory());\n+        } else {\n+          assertEquals(FsPermission.valueOf(\"-r--rwxr--\"),\n+              status.getPermission());\n+          assertEquals(true, status.isDirectory());", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY5MTU1NA==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442691554", "bodyText": "Rather than doing this below and re iterating and calling calling listStatus. Can't we add permissions too above and verify in one go like :\n`      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n  localFs.setPermission(new Path(infile.getPath()),\n      FsPermission.valueOf(\"-rwxr--r--\"));\n  localFs.setPermission(new Path(childDir.getPath()),\n      FsPermission.valueOf(\"-r--rwxr--\"));\n\n  FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n  for (FileStatus status : statuses) {\n    if (status.getPath().getName().equals(fileScheme)) {\n      assertEquals(fileStat.getPermission(), status.getPermission());\n      assertEquals(FsPermission.valueOf(\"-rwxr--r--\"),\n          status.getPermission());\n      assertEquals(false, status.isDirectory());\n    } else {\n      assertEquals(dirStat.getPermission(), status.getPermission());\n      assertEquals(FsPermission.valueOf(\"-r--rwxr--\"),\n          status.getPermission());\n      assertEquals(true, status.isDirectory());\n    }\n  }`", "author": "ayushtkn", "createdAt": "2020-06-19T07:57:08Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsOverloadSchemeListStatus.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.viewfs;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * ViewFsOverloadScheme ListStatus.\n+ */\n+public class TestViewFsOverloadSchemeListStatus {\n+\n+  private static final File TEST_DIR =\n+      GenericTestUtils.getTestDir(TestViewfsFileStatus.class.getSimpleName());\n+\n+  @Before public void setUp() {\n+    FileUtil.fullyDelete(TEST_DIR);\n+    assertTrue(TEST_DIR.mkdirs());\n+  }\n+\n+  @After public void tearDown() throws IOException {\n+    FileUtil.fullyDelete(TEST_DIR);\n+  }\n+\n+  /**\n+   * Tests the ACL and isDirectory returned from listStatus for directories and\n+   * files.\n+   */\n+  @Test\n+  public void testListStatusACL() throws IOException, URISyntaxException {\n+    String testfilename = \"testFileACL\";\n+    String childDirectoryName = \"testDirectoryACL\";\n+    TEST_DIR.mkdirs();\n+    File infile = new File(TEST_DIR, testfilename);\n+    final byte[] content = \"dingos\".getBytes();\n+\n+    try (FileOutputStream fos = new FileOutputStream(infile)) {\n+      fos.write(content);\n+    }\n+    assertEquals(content.length, infile.length());\n+    File childDir = new File(TEST_DIR, childDirectoryName);\n+    childDir.mkdirs();\n+\n+    Configuration conf = new Configuration();\n+    ConfigUtil.addLink(conf, \"/file\", infile.toURI());\n+    ConfigUtil.addLink(conf, \"/dir\", childDir.toURI());\n+    String fileScheme = \"file\";\n+    conf.set(String.format(\"fs.%s.impl\", fileScheme),\n+        ViewFileSystemOverloadScheme.class.getName());\n+    conf.set(String\n+        .format(FsConstants.FS_VIEWFS_OVERLOAD_SCHEME_TARGET_FS_IMPL_PATTERN,\n+            fileScheme), LocalFileSystem.class.getName());\n+    String fileUriStr = \"file:///\";\n+    try (FileSystem vfs = FileSystem.get(new URI(fileUriStr), conf)) {\n+      assertEquals(ViewFileSystemOverloadScheme.class, vfs.getClass());\n+      FileStatus[] statuses = vfs.listStatus(new Path(\"/\"));\n+\n+      FileSystem localFs = ((ViewFileSystemOverloadScheme) vfs)\n+          .getRawFileSystem(new Path(fileUriStr), conf);\n+      FileStatus fileStat = localFs.getFileStatus(new Path(infile.getPath()));\n+      FileStatus dirStat = localFs.getFileStatus(new Path(childDir.getPath()));\n+\n+      for (FileStatus status : statuses) {\n+        if (status.getPath().getName().equals(fileScheme)) {\n+          assertEquals(fileStat.getPermission(), status.getPermission());\n+        } else {\n+          assertEquals(dirStat.getPermission(), status.getPermission());\n+        }\n+      }\n+\n+      localFs.setPermission(new Path(infile.getPath()),\n+          FsPermission.valueOf(\"-rwxr--r--\"));\n+      localFs.setPermission(new Path(childDir.getPath()),\n+          FsPermission.valueOf(\"-r--rwxr--\"));\n+", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk0Nzc5MA==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442947790", "bodyText": "No, here the test is explicitly change permissions on fs explicitly and get listStatus then assert again whether it's respecting ur change. That was intended.", "author": "umamaheswararao", "createdAt": "2020-06-19T16:52:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY5MTU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY5MjE3Ng==", "url": "https://github.com/apache/hadoop/pull/2084#discussion_r442692176", "bodyText": "Ditto. Can be a fallback to original", "author": "ayushtkn", "createdAt": "2020-06-19T07:58:25Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java", "diffHunk": "@@ -999,39 +1007,53 @@ public int getUriDefaultPort() {\n      * will be listed in the returned result.\n      */\n     @Override\n-    public FileStatus[] listStatus(final Path f) throws AccessControlException,\n-        IOException {\n+    public FileStatus[] listStatus(final Path f) throws IOException {\n       checkPathIsSlash(f);\n       FileStatus[] fallbackStatuses = listStatusForFallbackLink();\n       FileStatus[] result = new FileStatus[theInternalDir.getChildren().size()];\n       int i = 0;\n       for (Entry<String, INode<AbstractFileSystem>> iEntry :\n           theInternalDir.getChildren().entrySet()) {\n         INode<AbstractFileSystem> inode = iEntry.getValue();\n-\n-        \n+        Path path = new Path(inode.fullPath).makeQualified(myUri, null);\n         if (inode.isLink()) {\n           INodeLink<AbstractFileSystem> link = \n             (INodeLink<AbstractFileSystem>) inode;\n \n+          if (showMountLinksAsSymlinks) {\n+            // To maintain backward compatibility, with default option(showing\n+            // mount links as symlinks), we will represent target link as\n+            // symlink and rest other properties are belongs to mount link only.\n+            result[i++] =\n+                new FileStatus(0, false, 0, 0, creationTime, creationTime,\n+                    PERMISSION_555, ugi.getShortUserName(),\n+                    ugi.getPrimaryGroupName(), link.getTargetLink(),\n+                    path);\n+            continue;\n+          }\n+\n+          //  We will represent as non-symlinks. Here it will show target\n+          //  directory/file properties like permissions, isDirectory etc on\n+          //  mount path. The path will be a mount link path and isDirectory is\n+          //  true if target is dir, otherwise false.\n+          String linkedPath = link.getTargetFileSystem().getUri().getPath();\n+          if (\"\".equals(linkedPath)) {\n+            linkedPath = \"/\";\n+          }\n           try {\n-            String linkedPath = link.getTargetFileSystem().getUri().getPath();\n-            FileStatus status = ((ChRootedFs)link.getTargetFileSystem())\n-                .getMyFs().getFileStatus(new Path(linkedPath));\n-            result[i++] = new FileStatus(status.getLen(), false,\n-              status.getReplication(), status.getBlockSize(),\n-              status.getModificationTime(), status.getAccessTime(),\n-              status.getPermission(), status.getOwner(), status.getGroup(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            FileStatus status =\n+                ((ChRootedFs) link.getTargetFileSystem()).getMyFs()\n+                    .getFileStatus(new Path(linkedPath));\n+            result[i++] = new FileStatus(status.getLen(), status.isDirectory(),\n+                status.getReplication(), status.getBlockSize(),\n+                status.getModificationTime(), status.getAccessTime(),\n+                status.getPermission(), status.getOwner(), status.getGroup(),\n+                null, path);\n           } catch (FileNotFoundException ex) {\n-            result[i++] = new FileStatus(0, false, 0, 0,\n-              creationTime, creationTime, PERMISSION_555,\n-              ugi.getShortUserName(), ugi.getPrimaryGroupName(),\n-              link.getTargetLink(),\n-              new Path(inode.fullPath).makeQualified(\n-                  myUri, null));\n+            LOG.warn(\"Cannot get one of the children's(\" + path\n+                + \")  target path(\" + link.getTargetFileSystem().getUri()\n+                + \") file status.\", ex);\n+            throw ex;", "originalCommit": "2c55c9a5cf7f88f3e435a23203504072050d5e43", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2d2ec00ca3c7d4f5d5706153b3eab162262fddfc", "url": "https://github.com/apache/hadoop/commit/2d2ec00ca3c7d4f5d5706153b3eab162262fddfc", "message": "Fixed a checkstyle and few review comments", "committedDate": "2020-06-19T17:55:31Z", "type": "commit"}, {"oid": "14577639dcc74a2b0ec0ff67191b9739d8b3a2d1", "url": "https://github.com/apache/hadoop/commit/14577639dcc74a2b0ec0ff67191b9739d8b3a2d1", "message": "Fixed few doc issues", "committedDate": "2020-06-20T00:27:45Z", "type": "commit"}]}