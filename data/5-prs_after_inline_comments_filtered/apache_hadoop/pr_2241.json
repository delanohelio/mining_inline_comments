{"pr_number": 2241, "pr_title": "HADOOP-17222. Create socket address combined with URI cache", "pr_createdAt": "2020-08-24T17:42:34Z", "pr_url": "https://github.com/apache/hadoop/pull/2241", "timeline": [{"oid": "987820528a7754ce83ffa963d10c5a47aa9a5779", "url": "https://github.com/apache/hadoop/commit/987820528a7754ce83ffa963d10c5a47aa9a5779", "message": "HADOOP-17222. Create socket address combined with cache", "committedDate": "2020-08-25T03:00:34Z", "type": "forcePushed"}, {"oid": "f08b5120268a57b5a439d452ddc7cd87c0bbba8f", "url": "https://github.com/apache/hadoop/commit/f08b5120268a57b5a439d452ddc7cd87c0bbba8f", "message": "HADOOP-17222. Create socket address combined with cache", "committedDate": "2020-08-25T03:19:23Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476127785", "bodyText": "Thanks @1996fanrui for your proposal here.\nA. Not sure if this improvement could impact some other modules because {{NetUtil}} is very common util for different modules rather than DFSClient only.\nB. It is better to make the scope to DFSClient only if we could based on the original description HADOOP-17222.\nC. add evict policy for {{URI_CACHE}}?", "author": "Hexiaoqiao", "createdAt": "2020-08-25T03:47:05Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java", "diffHunk": "@@ -218,6 +211,28 @@ public static InetSocketAddress createSocketAddr(String target,\n     return createSocketAddrForHost(host, port);\n   }\n \n+  private static final Map<String, URI> URI_CACHE = new ConcurrentHashMap<>();", "originalCommit": "f08b5120268a57b5a439d452ddc7cd87c0bbba8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjE0MDM3OA==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476140378", "bodyText": "Hi @Hexiaoqiao , thank you very much for your suggestions.\nAB. Now Cache is just URI, URI object does not contain ip, only some fixed information. The same target corresponds to the same URI. If the URI contains some variable attributes, I think the current Cache may impact other modules, and even the hdfs client will also be affected. According to my understanding, cache URI should be safe.\nC. Is evict policy considering that the URI will change? Or consider it may take up more memory?", "author": "1996fanrui", "createdAt": "2020-08-25T04:05:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjE4MDUwNg==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476180506", "bodyText": "Thanks @1996fanrui for your proposal here.\nA. Not sure if this improvement could impact some other modules because {{NetUtil}} is very common util for different modules rather than DFSClient only.\nB. It is better to make the scope to DFSClient only if we could based on the original description HADOOP-17222.\nC. add evict policy for {{URI_CACHE}}?\n\nAnother idea: add a parameter isUseURICache to the createSocketAddr() method.\nisUseURICache default is false.", "author": "1996fanrui", "createdAt": "2020-08-25T05:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjM5MTM0Mw==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476391343", "bodyText": "Hi @Hexiaoqiao\nDFSUtilClient.isLocalAddress also uses Cache for acceleration. It is basically similar to the maximum data size of the current patch Cache, that is, the number of DNs. The Cache of DFSUtilClient.isLocalAddress does not use the evict policy, so the current patch should not worry about taking up too much memory.\nOf course, if the evict policy is added, the code will be more robust.\nIf necessary, I can use guava Cache instead of Map. What do you think?\n\n  \n    \n      hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java\n    \n    \n         Line 634\n      in\n      82a7505\n    \n    \n    \n    \n\n        \n          \n           private static final Map<String, Boolean> localAddrMap = Collections", "author": "1996fanrui", "createdAt": "2020-08-25T11:56:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEyNzc4NQ=="}], "type": "inlineReview"}, {"oid": "5be64e594431b57a0f5f9bae76eb818c9ca60145", "url": "https://github.com/apache/hadoop/commit/5be64e594431b57a0f5f9bae76eb818c9ca60145", "message": "HADOOP-17222. Create socket address combined with cache", "committedDate": "2020-08-25T06:40:28Z", "type": "commit"}, {"oid": "5be64e594431b57a0f5f9bae76eb818c9ca60145", "url": "https://github.com/apache/hadoop/commit/5be64e594431b57a0f5f9bae76eb818c9ca60145", "message": "HADOOP-17222. Create socket address combined with cache", "committedDate": "2020-08-25T06:40:28Z", "type": "forcePushed"}, {"oid": "2221aa0ecd5ab37032ec53021117f3af32425129", "url": "https://github.com/apache/hadoop/commit/2221aa0ecd5ab37032ec53021117f3af32425129", "message": "add conf of dfs.client.read.uri.cache.enable", "committedDate": "2020-08-25T11:39:19Z", "type": "commit"}, {"oid": "266831f1e1b658435d0f542c52749df34852eb59", "url": "https://github.com/apache/hadoop/commit/266831f1e1b658435d0f542c52749df34852eb59", "message": "add parameter of uri cache expire time", "committedDate": "2020-08-26T14:11:55Z", "type": "commit"}, {"oid": "f7a46d4987a9a2fed7f447aef10aae1e5c12c1f5", "url": "https://github.com/apache/hadoop/commit/f7a46d4987a9a2fed7f447aef10aae1e5c12c1f5", "message": "refactor to fix checkstyle", "committedDate": "2020-08-27T03:28:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjYxODY5Mg==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r476618692", "bodyText": "nit: name this ...uri.cache.enabled by replacing enable with enabled?\nAlso change other variables related to uriCacheEnable", "author": "liuml07", "createdAt": "2020-08-25T17:27:21Z", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +414,9 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";", "originalCommit": "2221aa0ecd5ab37032ec53021117f3af32425129", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE0ODg4OA==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478148888", "bodyText": "ok, thanks for your review", "author": "1996fanrui", "createdAt": "2020-08-27T05:45:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjYxODY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478114810", "bodyText": "I think it's better not to have this config as I suggested in the JIRA. It's for two reasons:\n\nThis seems not critical parameter and Hadoop already has too many configurations. I think the default value 12hours is good enough. It's not meant to be tuned by per client since the cache is static and shared. So removing this config seems preferred to me.\nThe URI cache is initialized only once. So passing the uriCacheExpireMs parameter every time does not really carry useful information. And it may be confusing if people mistakenly interpret this as a per-URI setting when they call NetUtils.createSocketAddr() which is not.\n\nThoughts?", "author": "liuml07", "createdAt": "2020-08-27T04:51:55Z", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java", "diffHunk": "@@ -414,6 +415,11 @@\n \n     String  PREFETCH_SIZE_KEY = PREFIX + \"prefetch.size\";\n \n+    String URI_CACHE_KEY = PREFIX + \"uri.cache.enable\";\n+    boolean URI_CACHE_DEFAULT = false;\n+    String URI_CACHE_EXPIRE_MS_KEY = PREFIX + \"uri.cache.expire.ms\";", "originalCommit": "f7a46d4987a9a2fed7f447aef10aae1e5c12c1f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE0ODE5MA==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478148190", "bodyText": "I'm so sorry, I misunderstood the meaning of jira. I thought the two parameters cache size and expire time seem a bit overkill, so the cache size was removed and the expire time was retained.\nNow I will remove expire time and use 12 hours as the default value.", "author": "1996fanrui", "createdAt": "2020-08-27T05:44:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE2NDU5NA==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478164594", "bodyText": "Thanks, it's my bad. I could have made it clearer.", "author": "liuml07", "createdAt": "2020-08-27T06:09:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODE2NjQyMg==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478166422", "bodyText": "Thanks for your review, I have fixed all the comments.", "author": "1996fanrui", "createdAt": "2020-08-27T06:11:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODExNDgxMA=="}], "type": "inlineReview"}, {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9", "url": "https://github.com/apache/hadoop/commit/e0a0103b7887d612217800c4f7d7647fe4d29dd9", "message": "remove parameter of uri cache expire time", "committedDate": "2020-08-27T06:14:41Z", "type": "commit"}, {"oid": "e0a0103b7887d612217800c4f7d7647fe4d29dd9", "url": "https://github.com/apache/hadoop/commit/e0a0103b7887d612217800c4f7d7647fe4d29dd9", "message": "remove parameter of uri cache expire time", "committedDate": "2020-08-27T06:14:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4NzYxMQ==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478387611", "bodyText": "nit: delete no meaningless annotation?", "author": "Hexiaoqiao", "createdAt": "2020-08-27T12:40:58Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "diffHunk": "@@ -360,6 +360,49 @@ public void testCreateSocketAddress() throws Throwable {\n     }\n   }\n \n+  @Test\n+  public void testCreateSocketAddressWithURICache() throws Throwable {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    // ----------------------------------------------------", "originalCommit": "e0a0103b7887d612217800c4f7d7647fe4d29dd9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5Nzg1OQ==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478397859", "bodyText": "@Hexiaoqiao , thanks for your review.\nTo test the Cache, the same case is executed twice. There are a total of three cases. In order to divide the three cases, these comments are added.", "author": "1996fanrui", "createdAt": "2020-08-27T12:57:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4NzYxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4ODMxNA==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478388314", "bodyText": "nit: addr seems useless? no need to assign to addr anymore?", "author": "Hexiaoqiao", "createdAt": "2020-08-27T12:42:03Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetUtils.java", "diffHunk": "@@ -360,6 +360,49 @@ public void testCreateSocketAddress() throws Throwable {\n     }\n   }\n \n+  @Test\n+  public void testCreateSocketAddressWithURICache() throws Throwable {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1:12345\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(12345, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    addr = NetUtils.createSocketAddr(\n+        \"127.0.0.1\", 1000, \"myconfig\", true);\n+    assertEquals(\"127.0.0.1\", addr.getAddress().getHostAddress());\n+    assertEquals(1000, addr.getPort());\n+\n+    // ----------------------------------------------------\n+\n+    try {\n+      addr = NetUtils.createSocketAddr(", "originalCommit": "e0a0103b7887d612217800c4f7d7647fe4d29dd9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMjAwNQ==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478422005", "bodyText": "ok, I will do it", "author": "1996fanrui", "createdAt": "2020-08-27T13:32:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4ODMxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ2MzczMw==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478463733", "bodyText": "@Hexiaoqiao I have done it", "author": "1996fanrui", "createdAt": "2020-08-27T14:30:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM4ODMxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5MTY0NQ==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478391645", "bodyText": "nit: the changes seems no related with this ticket,  Should we update it in another single one?", "author": "Hexiaoqiao", "createdAt": "2020-08-27T12:47:07Z", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java", "diffHunk": "@@ -211,24 +212,7 @@ public DfsClientConf(Configuration conf) {\n         Write.MAX_PACKETS_IN_FLIGHT_KEY,\n         Write.MAX_PACKETS_IN_FLIGHT_DEFAULT);\n \n-    final boolean byteArrayManagerEnabled = conf.getBoolean(\n-        Write.ByteArrayManager.ENABLED_KEY,\n-        Write.ByteArrayManager.ENABLED_DEFAULT);\n-    if (!byteArrayManagerEnabled) {\n-      writeByteArrayManagerConf = null;\n-    } else {\n-      final int countThreshold = conf.getInt(\n-          Write.ByteArrayManager.COUNT_THRESHOLD_KEY,\n-          Write.ByteArrayManager.COUNT_THRESHOLD_DEFAULT);\n-      final int countLimit = conf.getInt(\n-          Write.ByteArrayManager.COUNT_LIMIT_KEY,\n-          Write.ByteArrayManager.COUNT_LIMIT_DEFAULT);\n-      final long countResetTimePeriodMs = conf.getLong(\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_KEY,\n-          Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_DEFAULT);\n-      writeByteArrayManagerConf = new ByteArrayManager.Conf(\n-          countThreshold, countLimit, countResetTimePeriodMs);\n-    }\n+    writeByteArrayManagerConf = loadWriteByteArrayManagerConf(conf);", "originalCommit": "e0a0103b7887d612217800c4f7d7647fe4d29dd9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5OTk4OQ==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478399989", "bodyText": "The constructor of DfsClientConf exceeds 150 lines, and the checkstyle is not passed. So a simple refactoring was made.", "author": "1996fanrui", "createdAt": "2020-08-27T13:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5MTY0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQwMjMwNw==", "url": "https://github.com/apache/hadoop/pull/2241#discussion_r478402307", "bodyText": "Thanks for your quick response, this is reasonable.", "author": "Hexiaoqiao", "createdAt": "2020-08-27T13:03:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM5MTY0NQ=="}], "type": "inlineReview"}, {"oid": "008d865ca93009b38638b1347df11295dc71790e", "url": "https://github.com/apache/hadoop/commit/008d865ca93009b38638b1347df11295dc71790e", "message": "remove some useless variables", "committedDate": "2020-08-27T14:29:14Z", "type": "commit"}, {"oid": "9baba782f9ce295cb321380a257e90d8badf3ee0", "url": "https://github.com/apache/hadoop/commit/9baba782f9ce295cb321380a257e90d8badf3ee0", "message": "trigger new CI check", "committedDate": "2020-09-09T02:21:50Z", "type": "commit"}]}