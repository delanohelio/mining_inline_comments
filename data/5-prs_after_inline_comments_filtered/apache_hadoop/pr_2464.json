{"pr_number": 2464, "pr_title": "HADOOP-17347. ABFS: Read optimizations", "pr_createdAt": "2020-11-16T05:03:52Z", "pr_url": "https://github.com/apache/hadoop/pull/2464", "timeline": [{"oid": "00ff5c2db2a15c07570b2b581620a3430344fb06", "url": "https://github.com/apache/hadoop/commit/00ff5c2db2a15c07570b2b581620a3430344fb06", "message": "ABFS: Read small files completely", "committedDate": "2020-11-14T06:49:35Z", "type": "commit"}, {"oid": "dce042af52715ff0b9c8cea7bfbd99248fa4afb5", "url": "https://github.com/apache/hadoop/commit/dce042af52715ff0b9c8cea7bfbd99248fa4afb5", "message": "Footer readoptimisation", "committedDate": "2020-11-14T06:49:35Z", "type": "commit"}, {"oid": "eda2b7684a89600ecacae3c96b88ffb48906fc82", "url": "https://github.com/apache/hadoop/commit/eda2b7684a89600ecacae3c96b88ffb48906fc82", "message": "Splitting the method readOneBlock to 3 different methods", "committedDate": "2020-11-14T06:49:35Z", "type": "commit"}, {"oid": "04acdac63bbd648269f9346d97eafd3653e8d9a5", "url": "https://github.com/apache/hadoop/commit/04acdac63bbd648269f9346d97eafd3653e8d9a5", "message": "Test case fixes", "committedDate": "2020-11-14T06:49:35Z", "type": "commit"}, {"oid": "147b5a73e8e6c98084024c0502f3549ad842331b", "url": "https://github.com/apache/hadoop/commit/147b5a73e8e6c98084024c0502f3549ad842331b", "message": "Checkstyle fixes", "committedDate": "2020-11-14T11:13:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkwODc4NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523908784", "bodyText": "smallfilescompletely\noptimizefooterread", "author": "vinaysbadami", "createdAt": "2020-11-16T05:39:00Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -56,6 +56,8 @@\n   public static final String AZURE_WRITE_MAX_REQUESTS_TO_QUEUE = \"fs.azure.write.max.requests.to.queue\";\n   public static final String AZURE_WRITE_BUFFER_SIZE = \"fs.azure.write.request.size\";\n   public static final String AZURE_READ_BUFFER_SIZE = \"fs.azure.read.request.size\";", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYzMTUwMQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r528631501", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-11-23T11:20:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkwODc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxMTE1Ng==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523911156", "bodyText": "why the change? Please revert if not require", "author": "vinaysbadami", "createdAt": "2020-11-16T05:49:25Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -128,7 +138,7 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     }\n     incrementReadOps();\n     do {\n-      lastReadBytes = readOneBlock(b, currentOff, currentLen);\n+      lastReadBytes = readToUserBuffer(b, currentOff, currentLen);", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4Mjk5OA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r526582998", "bodyText": "I have forked 3 different methods from the existing ethod readOneBlock, readFileCompletely. readLastBlock and readOneBlock. There are common parts from the existing method readOneBlock is used from for all the 3 forked methods. In that case renaming the existing method to readToUserBuffer would make sense.", "author": "bilaharith", "createdAt": "2020-11-19T04:14:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxMTE1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxNjQ2Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523916467", "bodyText": "default false", "author": "vinaysbadami", "createdAt": "2020-11-16T06:11:23Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -50,13 +50,15 @@\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL = SIXTY_SECONDS;\n   public static final int DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF = 2;\n \n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n+  public static final int ONE_KB = 1024;\n+  public static final int ONE_MB = ONE_KB * ONE_KB;\n \n   // Default upload and download buffer size\n   public static final int DEFAULT_WRITE_BUFFER_SIZE = 8 * ONE_MB;  // 8 MB\n   public static final int APPENDBLOB_MAX_WRITE_BUFFER_SIZE = 4 * ONE_MB;  // 4 MB\n   public static final int DEFAULT_READ_BUFFER_SIZE = 4 * ONE_MB;  // 4 MB\n+  public static final boolean DEFAULT_READ_SMALL_FILES_COMPLETELY = true;\n+  public static final boolean DEFAULT_OPTIMIZE_FOOTER_READ = true;", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4Mzc4NQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r526583785", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-11-19T04:16:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxNjQ2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxNzA3MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523917070", "bodyText": "we are passed in an AbfsInputStreamContext. We should hold onto that instance instead of creating member variables for each of the members of AbfsInputStreamContext.", "author": "vinaysbadami", "createdAt": "2020-11-16T06:13:45Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -92,6 +99,9 @@ public AbfsInputStream(\n     this.cachedSasToken = new CachedSASToken(\n         abfsInputStreamContext.getSasTokenRenewPeriodForStreamsInSeconds());\n     this.streamStatistics = abfsInputStreamContext.getStreamStatistics();\n+    this.readSmallFilesCompletely = abfsInputStreamContext\n+        .readSmallFilesCompletely();", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4NjYzOQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r526586639", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-11-19T04:28:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxNzA3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxODE5MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523918190", "bodyText": "&& len ==  FOOTER_DELTA", "author": "vinaysbadami", "createdAt": "2020-11-16T06:18:06Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4ODE3OQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r526588179", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-11-19T04:34:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxODE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxOTA3MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523919070", "bodyText": "if (firstRead) {\nif (smallfilecase)\n{\nres = readfilecompletelty;\nfirstread = false;\n}\nelse if (readfootercase)\n{\nres = readfooter\nfirstread = false\n}\n}", "author": "vinaysbadami", "createdAt": "2020-11-16T06:21:25Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxOTgyMA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523919820", "bodyText": "limit = bytesread", "author": "vinaysbadami", "createdAt": "2020-11-16T06:24:15Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the\n+      // read is for the footer\n+      if (readLastBlock() == -1) {\n+        return -1;\n+      }\n+    } else {\n+      if (readOneBlock(b) == -1) {\n+        return -1;\n+      }\n+    }\n+    fCursorAfterLastRead = fCursor;\n+\n+    //If there is anything in the buffer, then return lesser of (requested bytes) and (bytes in buffer)\n+    //(bytes returned may be less than requested)\n+    int bytesRemaining = limit - bCursor;\n+    int bytesToRead = Math.min(len, bytesRemaining);\n+    System.arraycopy(buffer, bCursor, b, off, bytesToRead);\n+    bCursor += bytesToRead;\n+    if (statistics != null) {\n+      statistics.incrementBytesRead(bytesToRead);\n+    }\n+    if (streamStatistics != null) {\n+      // Bytes read from the local buffer.\n+      streamStatistics.bytesReadFromBuffer(bytesToRead);\n+      streamStatistics.bytesRead(bytesToRead);\n+    }\n+    return bytesToRead;\n+  }\n+\n+  private long readFileCompletely() throws IOException {\n+    bCursor = (int) getPos();\n+    //  Read full file in case the file size <= buffer size\n+    buffer = new byte[bufferSize];\n+    long bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+\n+    limit = (int) bytesRead;\n+    fCursor = bytesRead;\n+    return bytesRead;\n+  }\n+\n+  private long readLastBlock() throws IOException {\n+    bCursor = (int) (\n+        ((contentLength < bufferSize) ? contentLength : bufferSize)\n+            - FOOTER_DELTA);\n+    buffer = new byte[bufferSize];\n+    long startPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    long bytesRead = readInternal(startPos, buffer, 0, bufferSize, true);\n+    firstRead = false;\n+\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+\n+    limit += (int) bytesRead;", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4ODE1Mw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r526588153", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-11-19T04:34:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkxOTgyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyMTc2MQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523921761", "bodyText": "check this - should be the offset into buffer passed into read", "author": "vinaysbadami", "createdAt": "2020-11-16T06:31:25Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the\n+      // read is for the footer\n+      if (readLastBlock() == -1) {\n+        return -1;\n+      }\n+    } else {\n+      if (readOneBlock(b) == -1) {\n+        return -1;\n+      }\n+    }\n+    fCursorAfterLastRead = fCursor;\n+\n+    //If there is anything in the buffer, then return lesser of (requested bytes) and (bytes in buffer)\n+    //(bytes returned may be less than requested)\n+    int bytesRemaining = limit - bCursor;\n+    int bytesToRead = Math.min(len, bytesRemaining);\n+    System.arraycopy(buffer, bCursor, b, off, bytesToRead);\n+    bCursor += bytesToRead;\n+    if (statistics != null) {\n+      statistics.incrementBytesRead(bytesToRead);\n+    }\n+    if (streamStatistics != null) {\n+      // Bytes read from the local buffer.\n+      streamStatistics.bytesReadFromBuffer(bytesToRead);\n+      streamStatistics.bytesRead(bytesToRead);\n+    }\n+    return bytesToRead;\n+  }\n+\n+  private long readFileCompletely() throws IOException {\n+    bCursor = (int) getPos();", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4ODI1MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r526588250", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-11-19T04:34:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyMTc2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyMjIwOA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r523922208", "bodyText": "is this correct?", "author": "vinaysbadami", "createdAt": "2020-11-16T06:33:21Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +172,86 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    if (firstRead && this.readSmallFilesCompletely\n+        && contentLength <= bufferSize) { //  Read small files completely\n+      if (readFileCompletely() == -1) {\n+        return -1;\n+      }\n+    } else if (firstRead && this.optimizeFooterRead && fCursor\n+        == contentLength - FOOTER_DELTA) {  //  Read the last one block if the\n+      // read is for the footer\n+      if (readLastBlock() == -1) {\n+        return -1;\n+      }\n+    } else {\n+      if (readOneBlock(b) == -1) {\n+        return -1;\n+      }\n+    }\n+    fCursorAfterLastRead = fCursor;\n+\n+    //If there is anything in the buffer, then return lesser of (requested bytes) and (bytes in buffer)\n+    //(bytes returned may be less than requested)\n+    int bytesRemaining = limit - bCursor;\n+    int bytesToRead = Math.min(len, bytesRemaining);\n+    System.arraycopy(buffer, bCursor, b, off, bytesToRead);\n+    bCursor += bytesToRead;\n+    if (statistics != null) {\n+      statistics.incrementBytesRead(bytesToRead);\n+    }\n+    if (streamStatistics != null) {\n+      // Bytes read from the local buffer.\n+      streamStatistics.bytesReadFromBuffer(bytesToRead);\n+      streamStatistics.bytesRead(bytesToRead);\n+    }\n+    return bytesToRead;\n+  }\n+\n+  private long readFileCompletely() throws IOException {\n+    bCursor = (int) getPos();\n+    //  Read full file in case the file size <= buffer size\n+    buffer = new byte[bufferSize];\n+    long bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+\n+    limit = (int) bytesRead;\n+    fCursor = bytesRead;", "originalCommit": "147b5a73e8e6c98084024c0502f3549ad842331b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODY0MjcwMA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r528642700", "bodyText": "corrected", "author": "bilaharith", "createdAt": "2020-11-23T11:41:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyMjIwOA=="}], "type": "inlineReview"}, {"oid": "78f5a1b57bb2611c9682ac8f7349dc67465d15fc", "url": "https://github.com/apache/hadoop/commit/78f5a1b57bb2611c9682ac8f7349dc67465d15fc", "message": "Addressing review comments", "committedDate": "2020-11-23T04:54:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTIxNjA3Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r529216077", "bodyText": "move above the if check", "author": "bilaharith", "createdAt": "2020-11-24T05:43:16Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -162,14 +169,87 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       throw new IndexOutOfBoundsException();\n     }\n \n-    //If buffer is empty, then fill the buffer.\n-    if (bCursor == limit) {\n+    int bytesRead = 0;\n+    if (shouldReadFully()) {\n+      bytesRead = readFileCompletely();\n+    } else if (shouldReadLastBlock(len)) {\n+      bytesRead = readLastBlock();\n+    } else {\n+      bytesRead = readOneBlock(b);\n+    }\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;", "originalCommit": "78f5a1b57bb2611c9682ac8f7349dc67465d15fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f3a88a395ddd561a483220cdcf0661a62973a5c5", "url": "https://github.com/apache/hadoop/commit/f3a88a395ddd561a483220cdcf0661a62973a5c5", "message": "Moving fCursorAfterLastRead to the place where backend calls made", "committedDate": "2020-11-24T10:06:31Z", "type": "commit"}, {"oid": "4d446994dae4ef6566c93c40bc7f95c07b9565b3", "url": "https://github.com/apache/hadoop/commit/4d446994dae4ef6566c93c40bc7f95c07b9565b3", "message": "Moving common code around the readOneBlock to separate methods and the same is used across all the 3 read forks from the start itself", "committedDate": "2020-11-25T04:03:51Z", "type": "commit"}, {"oid": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "url": "https://github.com/apache/hadoop/commit/00d1328cb85fcb5ebb1778319bc36e7797ae1272", "message": "Rearranging methods", "committedDate": "2020-11-25T04:08:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDA0NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531364044", "bodyText": "Making methods public for test purposes is not a good idea, esp. for AzureBlobFileSystem class. Find alternative.", "author": "snvijaya", "createdAt": "2020-11-27T03:23:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1218,7 +1218,7 @@ public boolean failed() {\n   }\n \n   @VisibleForTesting\n-  AzureBlobFileSystemStore getAbfsStore() {\n+  public AzureBlobFileSystemStore getAbfsStore() {", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2MDIyMQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533560221", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-01T16:43:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDA0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDA3NQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531364075", "bodyText": "Same as above.", "author": "snvijaya", "createdAt": "2020-11-27T03:24:06Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1242,7 +1242,7 @@ boolean getIsNamespaceEnabled() throws AzureBlobFileSystemException {\n   }\n \n   @VisibleForTesting\n-  Map<String, Long> getInstrumentationMap() {\n+  public Map<String, Long> getInstrumentationMap() {", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2MDI4OQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533560289", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-01T16:43:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDA3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDQwMQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531364401", "bodyText": "static finals together, non-static together .. no need for new lines above and below for every new field", "author": "snvijaya", "createdAt": "2020-11-27T03:25:42Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -47,6 +47,10 @@\n         StreamCapabilities {\n   private static final Logger LOG = LoggerFactory.getLogger(AbfsInputStream.class);\n \n+  public static final int FOOTER_SIZE = 8;\n+", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2MDMzMw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533560333", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-01T16:43:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NDQwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTA0NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531365044", "bodyText": "Why does validate return an int ? There is a return from what validate does in below method, even if its for error case, its a wrong case.\nreadOneBlock is supposed to written size of the data read, validate as such does no data read.\nChange to return boolean true for success here.", "author": "snvijaya", "createdAt": "2020-11-27T03:29:02Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -141,7 +154,7 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     return totalReadBytes > 0 ? totalReadBytes : lastReadBytes;\n   }\n \n-  private int readOneBlock(final byte[] b, final int off, final int len) throws IOException {\n+  private int validate(byte[] b, int off, int len) throws IOException {", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU1MTU4OA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533551588", "bodyText": "That would alter the existing flow. In certain cases it has to return 0 and -1 in some other cases, exceptions in case validations failed for len, off values.", "author": "bilaharith", "createdAt": "2020-12-01T16:32:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEyOTU3OA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r535129578", "bodyText": "Returning int from validate which is used by caller as bytes read is wrong. Any failure in validate should return Boolean false and the caller in turn can convert it to -1 for bytes read.", "author": "snvijaya", "createdAt": "2020-12-03T11:24:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODMzOTMxMA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r538339310", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-08T12:59:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTA0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTUzMg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531365532", "bodyText": "validate should return a boolean , true if validation is successful and false if not.\nFor cases it needs to throw exception is already present, so the if check wont be hit. Change to:\nif (!validate(b, off, len)) { return -1; }", "author": "snvijaya", "createdAt": "2020-11-27T03:31:15Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -161,6 +174,14 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     if (off < 0 || len < 0 || len > b.length - off) {\n       throw new IndexOutOfBoundsException();\n     }\n+    return 1; // 1 indicate success\n+  }\n+\n+  private int readOneBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU1MTc2OQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533551769", "bodyText": "That would alter the existing flow. In certain cases it has to return 0 and -1 in some other cases, exceptions in case validations failed for len, off values.", "author": "bilaharith", "createdAt": "2020-12-01T16:32:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEzMDY4MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r535130680", "bodyText": "see comment above", "author": "snvijaya", "createdAt": "2020-12-03T11:25:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODMzOTQ2Mg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r538339462", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-08T12:59:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NTUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NjQxNg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531366416", "bodyText": "Refer to validate method comments", "author": "snvijaya", "createdAt": "2020-11-27T03:35:34Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODMzOTU5OQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r538339599", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-08T12:59:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NjQxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzIzOA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531367238", "bodyText": "If read failed due to some reason, say throttling. Will this line still be hit ? Is there a scenario to handle if it doesnt ?", "author": "snvijaya", "createdAt": "2020-11-27T03:39:34Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEwMTc5Ng==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r539101796", "bodyText": "in case of 429, exception is thrown and  the line ( firstRead = false;) won't get executed.", "author": "bilaharith", "createdAt": "2020-12-09T08:25:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzIzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDgyNzY0NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544827644", "bodyText": "Testcases needed for fault handling.", "author": "snvijaya", "createdAt": "2020-12-17T05:44:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzIzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTMwMjI3Mg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r549302272", "bodyText": "Test cases added for the same", "author": "bilaharith", "createdAt": "2020-12-28T10:47:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzIzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzQwOQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531367409", "bodyText": "Refer to comments in validate.", "author": "snvijaya", "createdAt": "2020-11-27T03:40:26Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODMzOTc1Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r538339757", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-08T12:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2NzQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2OTk4Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531369987", "bodyText": "Validation for read beyond EOF -  fCursor > contentLength - can also be added here", "author": "snvijaya", "createdAt": "2020-11-27T03:53:39Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -161,6 +174,14 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     if (off < 0 || len < 0 || len > b.length - off) {", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE0NTk2Ng==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r535145966", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-03T11:41:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM2OTk4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MTQxOQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531371419", "bodyText": "In both the new handling, we are bypassing readAheads.\n(Unless readAhead is disabled) if read.request.size is configured to 100MB, data read from store would still be in 4MB chunks as that is the readAhead buffer size.\nIf any of these logics determines that more data from an earlier offset needs to be read, it is triggering reads by bypassing readAheads. So wont it end up in a 100MB direct read to store ?", "author": "snvijaya", "createdAt": "2020-11-27T04:00:54Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) (((contentLength < bufferSize) ? contentLength : bufferSize)\n+        - FOOTER_SIZE);\n+    long lastBlockStartPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    int bytesRead = readInternal(lastBlockStartPos, buffer, 0, bufferSize,\n+        true);", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM0MDEwMQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r538340101", "bodyText": "Yes, it will read 100MB.", "author": "bilaharith", "createdAt": "2020-12-08T13:00:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MTQxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MTcxMA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531371710", "bodyText": "Shouldnt fCursorAfterLastRead be set after this line ?", "author": "snvijaya", "createdAt": "2020-11-27T04:02:31Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) (((contentLength < bufferSize) ? contentLength : bufferSize)\n+        - FOOTER_SIZE);\n+    long lastBlockStartPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    int bytesRead = readInternal(lastBlockStartPos, buffer, 0, bufferSize,\n+        true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = lastBlockStartPos + bytesRead;", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM0MDY5MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r538340690", "bodyText": "fCursor is moved after the read is complete and the current fCursor is set to fCursorAfterLastRead. SO this is correct.", "author": "bilaharith", "createdAt": "2020-12-08T13:00:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MTcxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTI5NDI2NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r549294264", "bodyText": "Dont recall the context here. But the code is completely different now and this doesnt hold ?", "author": "snvijaya", "createdAt": "2020-12-28T10:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MTcxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MzE0Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531373147", "bodyText": "So readInternal will go directly to server to fetch the data and server returning partial data is a valid case. Expectation is that the client will loop over and continue calling read until it has fetched all the data it needs.\nTake a case of client requesting last 8 bytes of a file which here will translate to a read of last 4 MB. ReadInternal returned 3MB data, and so we havent got the last 8 bytes that needs to be returned to client app. bCursor on the other hand has already been set to 4MB - 8 bytes location. This will end up returning what ever is present in the allocated buffer which is corrupt data.\nbytesRead has to be of bufferSize else the whole logic falls apart. As server can return partial data, respective handling is needed.", "author": "snvijaya", "createdAt": "2020-11-27T04:10:31Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -211,6 +235,59 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) fCursor;\n+    int bytesRead = readInternal(0, buffer, 0, (int) contentLength, true);\n+    firstRead = false;\n+    if (bytesRead == -1) {\n+      return -1;\n+    }\n+    fCursorAfterLastRead = fCursor;\n+    limit = bytesRead;\n+    fCursor = bytesRead;\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len) throws IOException {\n+    int validation = validate(b, off, len);\n+    if (validation < 1) {\n+      return validation;\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    bCursor = (int) (((contentLength < bufferSize) ? contentLength : bufferSize)\n+        - FOOTER_SIZE);\n+    long lastBlockStartPos = (contentLength < bufferSize)\n+        ? 0\n+        : contentLength - bufferSize;\n+    int bytesRead = readInternal(lastBlockStartPos, buffer, 0, bufferSize,", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTAyNzQwMg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r539027402", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-09T05:50:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MzE0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM1MzA0OA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540353048", "bodyText": "Fixes are done for partial read scenarios", "author": "bilaharith", "createdAt": "2020-12-10T17:20:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MzE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MzU0MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531373540", "bodyText": "For testing of footer read optimized, better to have a file that is atleast 3 or 4 times buffer size. For testing purposes, filesize of 1 mb is good, set the buffer to a much smaller size over config.", "author": "snvijaya", "createdAt": "2020-11-27T04:12:53Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.AbfsStatistic.CONNECTIONS_MADE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_MB;\n+\n+public class ITestAbfsInputStreamReadFooter\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int TEN = 10;\n+  private static final int TWENTY = 20;\n+\n+  public ITestAbfsInputStreamReadFooter() throws Exception {\n+  }\n+\n+  @Test\n+  public void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception {\n+    testNumBackendCalls(true);\n+  }\n+\n+  @Test\n+  public void testMultipleServerCallsAreMadeWhenTheConfIsFalse()\n+      throws Exception {\n+    testNumBackendCalls(false);\n+  }\n+\n+  private void testNumBackendCalls(boolean optimizeFooterRead)\n+      throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 1; i <= 4; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU1MjgyNQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533552825", "bodyText": "File sizes are now 2 MB to 6MB", "author": "bilaharith", "createdAt": "2020-12-01T16:33:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3MzU0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NDE1Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531374157", "bodyText": "This shouldnt be enforced by config. As mentioned in earlier comments, align buffer size and file size for it to become eligible for footer read optimization.\nWe need the test to validate that it goes to footer checks even when the other config is on.", "author": "snvijaya", "createdAt": "2020-11-27T04:15:59Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.AbfsStatistic.CONNECTIONS_MADE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_MB;\n+\n+public class ITestAbfsInputStreamReadFooter\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int TEN = 10;\n+  private static final int TWENTY = 20;\n+\n+  public ITestAbfsInputStreamReadFooter() throws Exception {\n+  }\n+\n+  @Test\n+  public void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception {\n+    testNumBackendCalls(true);\n+  }\n+\n+  @Test\n+  public void testMultipleServerCallsAreMadeWhenTheConfIsFalse()\n+      throws Exception {\n+    testNumBackendCalls(false);\n+  }\n+\n+  private void testNumBackendCalls(boolean optimizeFooterRead)\n+      throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 1; i <= 4; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      int length = AbfsInputStream.FOOTER_SIZE;\n+      try (FSDataInputStream iStream = fs.open(testFilePath)) {\n+        byte[] buffer = new byte[length];\n+\n+        Map<String, Long> metricMap = fs.getInstrumentationMap();\n+        long requestsMadeBeforeTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        iStream.seek(fileSize - 8);\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TEN * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TWENTY * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        metricMap = fs.getInstrumentationMap();\n+        long requestsMadeAfterTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        if (optimizeFooterRead) {\n+          assertEquals(1, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        } else {\n+          assertEquals(3, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfTrue() throws Exception {\n+    testSeekToEndAndReadWithConf(true);\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfFalse() throws Exception {\n+    testSeekToEndAndReadWithConf(false);\n+  }\n+\n+  private void testSeekToEndAndReadWithConf(boolean optimizeFooterRead) throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 5; i <= 10; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      seekReadAndTest(fs, testFilePath, fileSize - AbfsInputStream.FOOTER_SIZE,\n+          AbfsInputStream.FOOTER_SIZE, fileContent);\n+    }\n+  }\n+\n+  private AzureBlobFileSystem getFileSystem(boolean optimizeFooterRead)\n+      throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setOptimizeFooterRead(optimizeFooterRead);\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setReadSmallFilesCompletely(false);", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2MDQxOQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533560419", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-01T16:44:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NDE1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTA1MDEyOQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r541050129", "bodyText": "For small files this setting is required for testing the footer read.\nFor big files this is not set and expected to skip the smallfile read if condition.", "author": "bilaharith", "createdAt": "2020-12-11T15:59:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NDE1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NDU5Mw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531374593", "bodyText": "File content asserts are needed on the data that is returned through the client provided buffer as well.", "author": "snvijaya", "createdAt": "2020-11-27T04:18:19Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+\n+import static org.apache.hadoop.fs.azurebfs.AbfsStatistic.CONNECTIONS_MADE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_MB;\n+\n+public class ITestAbfsInputStreamReadFooter\n+    extends AbstractAbfsIntegrationTest {\n+\n+  private static final int TEN = 10;\n+  private static final int TWENTY = 20;\n+\n+  public ITestAbfsInputStreamReadFooter() throws Exception {\n+  }\n+\n+  @Test\n+  public void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception {\n+    testNumBackendCalls(true);\n+  }\n+\n+  @Test\n+  public void testMultipleServerCallsAreMadeWhenTheConfIsFalse()\n+      throws Exception {\n+    testNumBackendCalls(false);\n+  }\n+\n+  private void testNumBackendCalls(boolean optimizeFooterRead)\n+      throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 1; i <= 4; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      int length = AbfsInputStream.FOOTER_SIZE;\n+      try (FSDataInputStream iStream = fs.open(testFilePath)) {\n+        byte[] buffer = new byte[length];\n+\n+        Map<String, Long> metricMap = fs.getInstrumentationMap();\n+        long requestsMadeBeforeTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        iStream.seek(fileSize - 8);\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TEN * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        iStream.seek(fileSize - (TWENTY * ONE_KB));\n+        iStream.read(buffer, 0, length);\n+\n+        metricMap = fs.getInstrumentationMap();\n+        long requestsMadeAfterTest = metricMap\n+            .get(CONNECTIONS_MADE.getStatName());\n+\n+        if (optimizeFooterRead) {\n+          assertEquals(1, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        } else {\n+          assertEquals(3, requestsMadeAfterTest - requestsMadeBeforeTest);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfTrue() throws Exception {\n+    testSeekToEndAndReadWithConf(true);\n+  }\n+\n+  @Test\n+  public void testSeekToEndAndReadWithConfFalse() throws Exception {\n+    testSeekToEndAndReadWithConf(false);\n+  }\n+\n+  private void testSeekToEndAndReadWithConf(boolean optimizeFooterRead) throws Exception {\n+    final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead);\n+    for (int i = 5; i <= 10; i++) {\n+      String fileName = methodName.getMethodName() + i;\n+      int fileSize = i * ONE_MB;\n+      byte[] fileContent = getRandomBytesArray(fileSize);\n+      Path testFilePath = createFileWithContent(fs, fileName, fileContent);\n+      seekReadAndTest(fs, testFilePath, fileSize - AbfsInputStream.FOOTER_SIZE,\n+          AbfsInputStream.FOOTER_SIZE, fileContent);\n+    }\n+  }\n+\n+  private AzureBlobFileSystem getFileSystem(boolean optimizeFooterRead)\n+      throws IOException {\n+    final AzureBlobFileSystem fs = getFileSystem();\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setOptimizeFooterRead(optimizeFooterRead);\n+    fs.getAbfsStore().getAbfsConfiguration()\n+        .setReadSmallFilesCompletely(false);\n+    return fs;\n+  }\n+\n+  private Path createFileWithContent(FileSystem fs, String fileName,\n+      byte[] fileContent) throws IOException {\n+    Path testFilePath = path(fileName);\n+    try (FSDataOutputStream oStream = fs.create(testFilePath)) {\n+      oStream.write(fileContent);\n+      oStream.flush();\n+    }\n+    return testFilePath;\n+  }\n+\n+  private void seekReadAndTest(final FileSystem fs, final Path testFilePath,\n+      final int seekPos, final int length, final byte[] fileContent) throws IOException {\n+    try (FSDataInputStream iStream = fs.open(testFilePath)) {\n+      iStream.seek(seekPos);\n+      byte[] buffer = new byte[length];\n+      iStream.read(buffer, 0, length);\n+      assertSuccessfulRead(fileContent, seekPos, length, buffer);\n+      AbfsInputStream abfsInputStream = (AbfsInputStream) iStream\n+          .getWrappedStream();\n+\n+      AzureBlobFileSystem abfs = (AzureBlobFileSystem) fs;\n+      AbfsConfiguration conf = abfs.getAbfsStore().getAbfsConfiguration();\n+\n+      int expectedFCursor = fileContent.length;\n+      int expectedLimit;\n+      int expectedBCursor;\n+      if (conf.optimizeFooterRead()) {\n+        expectedBCursor = ((conf.getReadBufferSize() < fileContent.length)\n+            ? conf.getReadBufferSize()\n+            : fileContent.length);\n+        expectedLimit = (conf.getReadBufferSize() < fileContent.length)\n+            ? conf.getReadBufferSize()\n+            : fileContent.length;\n+      } else {\n+        expectedBCursor = length;\n+        expectedLimit = length;\n+      }\n+      assertSuccessfulRead(fileContent, abfsInputStream.getBuffer(),\n+          conf, length);\n+      assertEquals(expectedFCursor, abfsInputStream.getFCursor());\n+      assertEquals(expectedBCursor, abfsInputStream.getBCursor());\n+      assertEquals(expectedLimit, abfsInputStream.getLimit());\n+    }\n+  }\n+\n+  private void assertSuccessfulRead(byte[] actualFileContent,\n+      byte[] contentRead, AbfsConfiguration conf, int len) {\n+    int buffersize = conf.getReadBufferSize();\n+    int actualContentSize = actualFileContent.length;\n+    if (conf.optimizeFooterRead()) {\n+    len = (actualContentSize < buffersize)\n+        ? actualContentSize\n+        : buffersize;\n+    }\n+    assertSuccessfulRead(actualFileContent, actualContentSize - len, len,", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2MDUzNw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533560537", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-01T16:44:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NDU5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NTIxNA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531375214", "bodyText": "I recall it was discussed that small file criteria will be capped at 4Mb file size. I dont see that in the review. Was it scratched back to small file being one less than buffer size ?", "author": "snvijaya", "createdAt": "2020-11-27T04:21:13Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -517,6 +527,14 @@ public int getWriteBufferSize() {\n     return this.writeBufferSize;\n   }\n \n+  public boolean readSmallFilesCompletely() {", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2MTA1OQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r533561059", "bodyText": "It was finalised as the criteria to consider a file small as the size smaller than the buffer size.", "author": "bilaharith", "createdAt": "2020-12-01T16:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NTIxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDc2OTA1Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r534769057", "bodyText": "In that case, skipping readAhead wont be ideal when reads are done by means of this optimizations. if 100 MB is set as buffer size, current logic is forcing a single server read request for 100 MB.", "author": "snvijaya", "createdAt": "2020-12-03T07:03:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NTIxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTAyNTI5Mw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r539025293", "bodyText": "Yes, if 100 MB is set as buffer size, current logic is forcing a single server read request for 100 MB.", "author": "bilaharith", "createdAt": "2020-12-09T05:46:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NTIxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NjIzOQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r531376239", "bodyText": "Run read tests in particular and contract test on a whole multiple times to check for any random failures.", "author": "snvijaya", "createdAt": "2020-11-27T04:26:48Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsInputStreamReadFooter.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/**", "originalCommit": "00d1328cb85fcb5ebb1778319bc36e7797ae1272", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA2MjE0NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r539062144", "bodyText": "Sure", "author": "bilaharith", "createdAt": "2020-12-09T07:14:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTM3NjIzOQ=="}], "type": "inlineReview"}, {"oid": "ce1777e33efe34c6af70ec7a4a49fa1b82330610", "url": "https://github.com/apache/hadoop/commit/ce1777e33efe34c6af70ec7a4a49fa1b82330610", "message": "Cleaning the test cases", "committedDate": "2020-12-01T04:29:16Z", "type": "forcePushed"}, {"oid": "ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "url": "https://github.com/apache/hadoop/commit/ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "message": "Cleaning the test cases", "committedDate": "2020-12-01T06:24:32Z", "type": "commit"}, {"oid": "ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "url": "https://github.com/apache/hadoop/commit/ad90fb96a6e58a1180c981b6ba4977a93355b1c7", "message": "Cleaning the test cases", "committedDate": "2020-12-01T06:24:32Z", "type": "forcePushed"}, {"oid": "b92a52701746e5ca09d02ce73fa691c04aa89bf8", "url": "https://github.com/apache/hadoop/commit/b92a52701746e5ca09d02ce73fa691c04aa89bf8", "message": "Merge branch 'trunk' into readoptimizations", "committedDate": "2020-12-01T15:35:06Z", "type": "commit"}, {"oid": "08c504b57712536485462c8ff0cbd853f9748bd6", "url": "https://github.com/apache/hadoop/commit/08c504b57712536485462c8ff0cbd853f9748bd6", "message": "Addressing eview comments", "committedDate": "2020-12-01T16:42:15Z", "type": "commit"}, {"oid": "319a79f0046102168f51f113238443e1aeca8f02", "url": "https://github.com/apache/hadoop/commit/319a79f0046102168f51f113238443e1aeca8f02", "message": "Addressing review comments", "committedDate": "2020-12-09T03:58:50Z", "type": "commit"}, {"oid": "3239a8c95203adae381db68b6a9fb0bcbdcd79db", "url": "https://github.com/apache/hadoop/commit/3239a8c95203adae381db68b6a9fb0bcbdcd79db", "message": "Setting firstRead to false after the existing flow", "committedDate": "2020-12-10T05:45:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA1NTEzNg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540055136", "bodyText": "validate logic should be grouped into a method and be called from here so that code duplication can be avoided in the 3 flows below.", "author": "snvijaya", "createdAt": "2020-12-10T10:31:06Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -137,7 +142,13 @@ public synchronized int read(final byte[] b, final int off, final int len) throw\n     }\n     incrementReadOps();\n     do {\n-      lastReadBytes = readOneBlock(b, currentOff, currentLen);\n+      if (shouldReadFully()) {", "originalCommit": "3239a8c95203adae381db68b6a9fb0bcbdcd79db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM1MzM5Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540353397", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-10T17:21:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA1NTEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA4MjcwMQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540082701", "bodyText": "As discussed, this logic needs to be relooked for\n\nWhat if the requested data is already available in the partial read done ?\nReduce the loopCount as the retry logic on ABFS driver can make the client read overall expensive. Fail faster with just 1 or 2 tries.\nNever fail the read request because optimization code failed to read full file. Fail fast and send read for client requested position.\nAll pointer fields need to be in valid state (bCursor, fCursor, fCursorAfterLastRead). In failure case currently fCursor could be in a different position that last seek done.", "author": "snvijaya", "createdAt": "2020-12-10T11:12:32Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -224,6 +240,123 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (closed) {\n+      throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\n+    }\n+\n+    Preconditions.checkNotNull(b);\n+    LOG.debug(\"read one block requested b.length = {} off {} len {}\", b.length,\n+        off, len);\n+\n+    if (len == 0) {\n+      return 0;\n+    }\n+\n+    if (this.available() == 0) {\n+      return -1;\n+    }\n+\n+    if (off < 0 || len < 0 || len > b.length - off) {\n+      throw new IndexOutOfBoundsException();\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    fCursorAfterLastRead = fCursor;\n+    int totalBytesRead = 0;\n+    int loopCount = 0;\n+    // Read from begining\n+    fCursor = 0;\n+    while (fCursor < contentLength) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,\n+          (int) contentLength - limit, true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+      if (loopCount++ >= 10) {", "originalCommit": "3239a8c95203adae381db68b6a9fb0bcbdcd79db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM1NDAxOA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540354018", "bodyText": "Made changes the following way\n\nThe available data is returned\nDone\nDone\nDone", "author": "bilaharith", "createdAt": "2020-12-10T17:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA4MjcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA4Mzc3Ng==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540083776", "bodyText": "Comments added in readSmallFileCompletely will apply here too.\nAlso, failure state recovery might lead to throwing away the data that was retried till then. Using readAhead threads to read will help to save the partial data read and be of use for next read call.", "author": "snvijaya", "createdAt": "2020-12-10T11:14:24Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -224,6 +240,123 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n     return bytesToRead;\n   }\n \n+  private boolean shouldReadFully() {\n+    return this.firstRead && this.context.readSmallFilesCompletely()\n+        && this.contentLength <= this.bufferSize;\n+  }\n+\n+  private boolean shouldReadLastBlock(int len) {\n+    return this.firstRead && this.context.optimizeFooterRead()\n+        && len == FOOTER_SIZE\n+        && this.fCursor == this.contentLength - FOOTER_SIZE;\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (closed) {\n+      throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\n+    }\n+\n+    Preconditions.checkNotNull(b);\n+    LOG.debug(\"read one block requested b.length = {} off {} len {}\", b.length,\n+        off, len);\n+\n+    if (len == 0) {\n+      return 0;\n+    }\n+\n+    if (this.available() == 0) {\n+      return -1;\n+    }\n+\n+    if (off < 0 || len < 0 || len > b.length - off) {\n+      throw new IndexOutOfBoundsException();\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    fCursorAfterLastRead = fCursor;\n+    int totalBytesRead = 0;\n+    int loopCount = 0;\n+    // Read from begining\n+    fCursor = 0;\n+    while (fCursor < contentLength) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,\n+          (int) contentLength - limit, true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+      if (loopCount++ >= 10) {\n+        throw new IOException(\n+            \"Too many attempts in reading whole file \" + path);\n+      }\n+    }\n+    firstRead = false;\n+    if (totalBytesRead == -1) {\n+      return -1;\n+    }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (closed) {\n+      throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\n+    }\n+\n+    Preconditions.checkNotNull(b);\n+    LOG.debug(\"read one block requested b.length = {} off {} len {}\", b.length,\n+        off, len);\n+\n+    if (len == 0) {\n+      return 0;\n+    }\n+\n+    if (this.available() == 0) {\n+      return -1;\n+    }\n+\n+    if (off < 0 || len < 0 || len > b.length - off) {\n+      throw new IndexOutOfBoundsException();\n+    }\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, for small\n+    // files the bCursor will be contentlength - footer size,\n+    // otherwise buffersize - footer size\n+    bCursor = (int) (Math.min(contentLength, bufferSize) - FOOTER_SIZE);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    fCursor = Math.max(0, contentLength - bufferSize);\n+    int totalBytesRead = 0;\n+    int loopCount = 0;\n+    while (fCursor < contentLength) {\n+      int bytesRead = readInternal(fCursor, buffer, limit, bufferSize - limit,\n+          true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+      if (loopCount++ >= 10) {", "originalCommit": "3239a8c95203adae381db68b6a9fb0bcbdcd79db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM1NDE0NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r540354144", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-10T17:22:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA4Mzc3Ng=="}], "type": "inlineReview"}, {"oid": "b223ce5f9e6aa140b3681a42957aa654e2215997", "url": "https://github.com/apache/hadoop/commit/b223ce5f9e6aa140b3681a42957aa654e2215997", "message": "Improved validate method", "committedDate": "2020-12-10T13:25:35Z", "type": "commit"}, {"oid": "a75bb1065463fd44d9846c898c83d63cf6d74c2f", "url": "https://github.com/apache/hadoop/commit/a75bb1065463fd44d9846c898c83d63cf6d74c2f", "message": "Improved method for optimized reads", "committedDate": "2020-12-10T13:52:50Z", "type": "commit"}, {"oid": "a268eaecda8a49f1cc2bba78fc1f47ad30b9c0e5", "url": "https://github.com/apache/hadoop/commit/a268eaecda8a49f1cc2bba78fc1f47ad30b9c0e5", "message": "Saving current state of pointers for better resilience", "committedDate": "2020-12-10T17:18:33Z", "type": "commit"}, {"oid": "5daee36b17c86a3c8637773b07b334be6dda7ce9", "url": "https://github.com/apache/hadoop/commit/5daee36b17c86a3c8637773b07b334be6dda7ce9", "message": "Partial read scenarios", "committedDate": "2020-12-11T13:39:05Z", "type": "commit"}, {"oid": "baddc1d7bfb70513e348fe08659411547d848c64", "url": "https://github.com/apache/hadoop/commit/baddc1d7bfb70513e348fe08659411547d848c64", "message": "Merge branch 'trunk' into readoptimizations", "committedDate": "2020-12-11T13:45:31Z", "type": "commit"}, {"oid": "59682d79753d3e09016276b1d4d24f345aa42daa", "url": "https://github.com/apache/hadoop/commit/59682d79753d3e09016276b1d4d24f345aa42daa", "message": "Checkstye fixes", "committedDate": "2020-12-11T15:17:10Z", "type": "commit"}, {"oid": "a61983f749b25a416ca19006318a7e34901b694a", "url": "https://github.com/apache/hadoop/commit/a61983f749b25a416ca19006318a7e34901b694a", "message": "Making changes to the footer read logic", "committedDate": "2020-12-14T10:29:13Z", "type": "commit"}, {"oid": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "url": "https://github.com/apache/hadoop/commit/1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "message": "Adding more test cases", "committedDate": "2020-12-14T16:07:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwMTU2Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543101567", "bodyText": "Add a comment that footer size is set to qualify for both ORC and parquet files.", "author": "snvijaya", "createdAt": "2020-12-15T07:17:27Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -46,6 +50,7 @@\n public class AbfsInputStream extends FSInputStream implements CanUnbuffer,\n         StreamCapabilities {\n   private static final Logger LOG = LoggerFactory.getLogger(AbfsInputStream.class);\n+  public static final int FOOTER_SIZE = 16 * ONE_KB;", "originalCommit": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE1NjExMQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543156111", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-15T08:51:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwMTU2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODMwNg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543108306", "bodyText": "2 => field name", "author": "snvijaya", "createdAt": "2020-12-15T07:30:41Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +229,120 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    for (int i = 0; i < 2 && fCursor < contentLength; i++) {", "originalCommit": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE1NjUzOA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543156538", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-15T08:52:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODMwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODM1Mw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543108353", "bodyText": "re-write the comment", "author": "snvijaya", "createdAt": "2020-12-15T07:30:48Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +229,120 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    for (int i = 0; i < 2 && fCursor < contentLength; i++) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,\n+          (int) actualLen - limit, true);\n+      if (bytesRead > 0) {\n+        totalBytesRead += bytesRead;\n+        limit += bytesRead;\n+        fCursor += bytesRead;\n+      }\n+    }\n+    //  if the read was not success and the user requested part of data has", "originalCommit": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE1NzU2MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543157560", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-15T08:53:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODM1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODcxNA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543108714", "bodyText": "In failure cases, file pointers can be in incorrect position. Need proper restore.", "author": "snvijaya", "createdAt": "2020-12-15T07:31:29Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +229,120 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    for (int i = 0; i < 2 && fCursor < contentLength; i++) {\n+      int bytesRead = readInternal(fCursor, buffer, limit,", "originalCommit": "1a358d068bd4e93e99b6dc0f6aa234aa2d585975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE1NzYzNA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543157634", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-15T08:53:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEwODcxNA=="}], "type": "inlineReview"}, {"oid": "2c647c58ef2598549c38b74e665116e76d4d6985", "url": "https://github.com/apache/hadoop/commit/2c647c58ef2598549c38b74e665116e76d4d6985", "message": "Addressing review comments", "committedDate": "2020-12-15T08:56:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg4MTU2OA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543881568", "bodyText": "Fix comments", "author": "snvijaya", "createdAt": "2020-12-16T03:22:43Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE3NzY4Nw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544177687", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-16T10:18:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg4MTU2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg4Nzg2Mw==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543887863", "bodyText": "Rename OPTIMIZATION_ATTEMPTS to MAX_OPTIMIZED_READ_ATTEMPTS", "author": "snvijaya", "createdAt": "2020-12-16T03:30:43Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE3NjU0MQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544176541", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-16T10:16:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg4Nzg2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg5OTM5MA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543899390", "bodyText": "\"Optimized read failed. Defaulting to readOneBlock\"", "author": "snvijaya", "createdAt": "2020-12-16T03:46:41Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {\n+        int bytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (bytesRead > 0) {\n+          totalBytesRead += bytesRead;\n+          limit += bytesRead;\n+          fCursor += bytesRead;\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Optimized read failed. alling back to readOneBlock {}\", e);", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE3NjEyOA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544176128", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-16T10:16:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg5OTM5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkwMDM1OA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543900358", "bodyText": "Initialize buffer here", "author": "snvijaya", "createdAt": "2020-12-16T03:47:44Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE3NTc2NQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544175765", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-16T10:15:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkwMDM1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkwMzY2Ng==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543903666", "bodyText": "Shouldnt this happen before the partial read check block ?", "author": "snvijaya", "createdAt": "2020-12-16T03:52:11Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {\n+        int bytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (bytesRead > 0) {\n+          totalBytesRead += bytesRead;\n+          limit += bytesRead;\n+          fCursor += bytesRead;\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Optimized read failed. alling back to readOneBlock {}\", e);\n+      restorePointerState();\n+      return readOneBlock(b, off, len);\n+    }\n+    //  If the read was partial and the user requested part of data has\n+    //  not read then fallback to readoneblock. When limit is smaller than\n+    //  bCursor that means the user requested data has not been read.\n+    if (fCursor < contentLength && bCursor > limit) {\n+      restorePointerState();\n+      return readOneBlock(b, off, len);\n+    }\n+    firstRead = false;\n+    if (totalBytesRead == -1) {", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE1MTAzMA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544151030", "bodyText": "Yes, done.", "author": "bilaharith", "createdAt": "2020-12-16T09:40:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkwMzY2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkxNjY1Ng==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543916656", "bodyText": "What about fCursorAfterLastRead in readSmallFileCompletely case ? Other case too not clear why this should be set anywhere but here.", "author": "snvijaya", "createdAt": "2020-12-16T04:09:13Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    int totalBytesRead = 0;\n+    fCursor = readFrom;\n+    try {\n+      for (int i = 0;\n+           i < OPTIMIZATION_ATTEMPTS && fCursor < contentLength; i++) {\n+        int bytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (bytesRead > 0) {\n+          totalBytesRead += bytesRead;\n+          limit += bytesRead;\n+          fCursor += bytesRead;\n+        }", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE0ODAwNQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544148005", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-16T09:36:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkxNjY1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkxOTUyNA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r543919524", "bodyText": "why is this done here ? Why cant this be updated as the read progresses ?", "author": "snvijaya", "createdAt": "2020-12-16T04:12:39Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,127 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n \n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+\n+    buffer = new byte[bufferSize];\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going p contain dta from footer start. In\n+    // that case bCursor will be set to footerStart - fCursor\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // read API call is considered 1 single operation in reality server could\n+    // return partial data and client has to retry untill the last full block\n+    // is read. So setting the fCursorAfterLastRead before the possible\n+    // multiple server calls\n+    fCursorAfterLastRead = fCursor;", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE0NzU0MQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544147541", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-16T09:35:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzkxOTUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDEzMzk4NQ==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544133985", "bodyText": "any reason for not impoting java.lang.math?", "author": "vinaysbadami", "createdAt": "2020-12-16T09:15:56Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -38,6 +38,10 @@\n import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n import org.apache.hadoop.fs.azurebfs.utils.CachedSASToken;\n \n+import static java.lang.Math.max;", "originalCommit": "2c647c58ef2598549c38b74e665116e76d4d6985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDE4Mjc4NA==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544182784", "bodyText": "When static import is used, I can simply write min(a,b) instead of Math.min(a, b)\nAlso I don't want to import the whole class statically for just these 2 methods.", "author": "bilaharith", "createdAt": "2020-12-16T10:26:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDEzMzk4NQ=="}], "type": "inlineReview"}, {"oid": "9a5edd15333877440862ebfe3b2c59f2450a5658", "url": "https://github.com/apache/hadoop/commit/9a5edd15333877440862ebfe3b2c59f2450a5658", "message": "Addressing review comments", "committedDate": "2020-12-16T10:30:08Z", "type": "commit"}, {"oid": "80bcd6161f28e9f016b932c235c2774453c0d88f", "url": "https://github.com/apache/hadoop/commit/80bcd6161f28e9f016b932c235c2774453c0d88f", "message": "Addressing review comments", "committedDate": "2020-12-16T11:08:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDgyNjQwNg==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r544826406", "bodyText": "Restore needed in case of no read to get the file pointer to where app had set it earlier. Also return explicit -1 as it is a no data read case.", "author": "snvijaya", "createdAt": "2020-12-17T05:41:03Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -206,11 +231,121 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n       fCursor += bytesRead;\n       fCursorAfterLastRead = fCursor;\n     }\n+    return copyToUserBuffer(b, off, len);\n+  }\n+\n+  private int readFileCompletely(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+    // data need to be copied to user buffer from index bCursor, bCursor has\n+    // to be the current fCusor\n+    bCursor = (int) fCursor;\n+    return optimisedRead(b, off, len, 0, contentLength);\n+  }\n+\n+  private int readLastBlock(final byte[] b, final int off, final int len)\n+      throws IOException {\n+    if (len == 0) {\n+      return 0;\n+    }\n+    if (!validate(b, off, len)) {\n+      return -1;\n+    }\n+    savePointerState();\n+    // data need to be copied to user buffer from index bCursor,\n+    // AbfsInutStream buffer is going to contain data from last block start. In\n+    // that case bCursor will be set to fCursor - lastBlockStart\n+    long lastBlockStart = max(0, contentLength - bufferSize);\n+    bCursor = (int) (fCursor - lastBlockStart);\n+    // 0 if contentlength is < buffersize\n+    long actualLenToRead = min(bufferSize, contentLength);\n+    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\n+  }\n+\n+  private int optimisedRead(final byte[] b, final int off, final int len,\n+      final long readFrom, final long actualLen) throws IOException {\n+    fCursor = readFrom;\n+    int totalBytesRead = 0;\n+    int lastBytesRead = 0;\n+    try {\n+      buffer = new byte[bufferSize];\n+      for (int i = 0;\n+           i < MAX_OPTIMIZED_READ_ATTEMPTS && fCursor < contentLength; i++) {\n+        lastBytesRead = readInternal(fCursor, buffer, limit,\n+            (int) actualLen - limit, true);\n+        if (lastBytesRead > 0) {\n+          totalBytesRead += lastBytesRead;\n+          limit += lastBytesRead;\n+          fCursor += lastBytesRead;\n+          fCursorAfterLastRead = fCursor;\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Optimized read failed. Defaulting to readOneBlock {}\", e);\n+      restorePointerState();\n+      return readOneBlock(b, off, len);\n+    }\n+    firstRead = false;\n+    if (totalBytesRead < 1) {\n+      return lastBytesRead;", "originalCommit": "80bcd6161f28e9f016b932c235c2774453c0d88f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODYwNDY0Ng==", "url": "https://github.com/apache/hadoop/pull/2464#discussion_r548604646", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-12-24T16:36:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDgyNjQwNg=="}], "type": "inlineReview"}, {"oid": "dfbba71b239ac06c02dbb0820099d5f461e93448", "url": "https://github.com/apache/hadoop/commit/dfbba71b239ac06c02dbb0820099d5f461e93448", "message": "Adding more test cases", "committedDate": "2020-12-23T17:18:14Z", "type": "commit"}, {"oid": "55ac64197a5eaef9400181a96d9cad66fef4c4e2", "url": "https://github.com/apache/hadoop/commit/55ac64197a5eaef9400181a96d9cad66fef4c4e2", "message": "Merge branch 'trunk' into readoptimizations", "committedDate": "2020-12-23T17:29:50Z", "type": "commit"}, {"oid": "3d199634949381e307a88108fb558092ca7c0fe6", "url": "https://github.com/apache/hadoop/commit/3d199634949381e307a88108fb558092ca7c0fe6", "message": "Checkstyle fixes", "committedDate": "2020-12-24T03:26:24Z", "type": "commit"}, {"oid": "8631510c38de2e50954fdd099df0b6364f859baf", "url": "https://github.com/apache/hadoop/commit/8631510c38de2e50954fdd099df0b6364f859baf", "message": "Adding more test cases", "committedDate": "2020-12-24T16:32:34Z", "type": "commit"}]}