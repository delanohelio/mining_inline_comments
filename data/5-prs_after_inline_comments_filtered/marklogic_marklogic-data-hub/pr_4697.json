{"pr_number": 4697, "pr_title": "DHFPROD-6095: - Run connector against DHF 5.x", "pr_createdAt": "2020-10-09T21:50:49Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4697", "timeline": [{"oid": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "url": "https://github.com/marklogic/marklogic-data-hub/commit/e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "message": "DHFPROD-6095 - Run connector against DHF 5.x", "committedDate": "2020-10-09T21:48:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDM1NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502984354", "bodyText": "These imports should all be removed as they're not used", "author": "rjrudin", "createdAt": "2020-10-12T00:00:45Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java", "diffHunk": "@@ -15,15 +15,18 @@\n  */\n package com.marklogic.hub.spark.sql.sources.v2.writer;\n \n+import com.fasterxml.jackson.databind.node.ObjectNode;", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDY3MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502984670", "bodyText": "This PR should including deleting the two files from /data-hub/5/data-services/ingestion from git, as they're being moved to /marklogic-data-hub-spark-connector/", "author": "rjrudin", "createdAt": "2020-10-12T00:03:19Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -132,6 +132,20 @@ void invalidPermissionsString() {\n         assertTrue(ex.getCause().getMessage().contains(\"Unable to parse permissions: rest-reader,read,rest-writer\"), \"Unexpected error message: \" + ex.getCause().getMessage());\n     }\n \n+    @Test\n+    public void testEndpointsAreLoaded() throws Exception {\n+        TextDocumentManager modMgr = getHubClient().getModulesClient().newTextDocumentManager();\n+        modMgr.delete(\"/data-hub/5/data-services/ingestion/bulkIngester.api\", \"/data-hub/5/data-services/ingestion/bulkIngester.sjs\",", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA5NTQ2MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r504095460", "bodyText": "Should we keep these in datahub modules too? Just in case if installer is installing these modules?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-13T16:32:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDY3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE1NTAyNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r504155027", "bodyText": "The modules should still be in DHF core ,but under /marklogic-data-hub-spark-connector/", "author": "rjrudin", "createdAt": "2020-10-13T18:05:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDY3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTQwNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502985405", "bodyText": "Pinging @SameeraPriyathamTadikonda and @ehennum  about this - I believe we should move the initialization to HubDataSourceWriter. This is based on my assumption that @SameeraPriyathamTadikonda  will be adding the initialization and finalization logic there for the job document.\nI think it makes sense though to not write this module if Ernie specifies his own ingestion path. Thus, it seems that all of the logic in this constructor should move into HubDataWriteSource. Does that seem right?", "author": "rjrudin", "createdAt": "2020-10-12T00:09:20Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -142,7 +146,20 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> options)\n         }\n \n         if (doesNotHaveApiPath) {\n-            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTUxNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502985516", "bodyText": "We don't want to lose the original exception here - e.g:\nthrow new RuntimeException(\"Unable to write default ingestion endpoint at path: \" + path + \"; cause: \" + e.getMessage(), e);", "author": "rjrudin", "createdAt": "2020-10-12T00:10:19Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -142,7 +146,20 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> options)\n         }\n \n         if (doesNotHaveApiPath) {\n-            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+\n+            if(hubClient.getModulesClient().newJSONDocumentManager().exists(apiPath) == null) {\n+                IOUtil ioUtil = new IOUtil(this.hubClient.getModulesClient());\n+                try {\n+                    ioUtil.load(\"bulkIngester.api\");\n+                } catch (IOException e) {\n+                    throw new RuntimeException(\"Error occurred while loading default endpoints.\");", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTgxNA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502985814", "bodyText": "We'll need to toss these files into the shadowJar, which is part of #4695 .\nOnce it's in the jar, the Spring class ClassPathResource can be used to retrieve it, no need for IOUtil:\nnew ClassPathResource(\"marklogic-data-hub-spark-connector/bulkIngester.api\").getInputStream()", "author": "rjrudin", "createdAt": "2020-10-12T00:12:51Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -142,7 +146,20 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> options)\n         }\n \n         if (doesNotHaveApiPath) {\n-            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+\n+            if(hubClient.getModulesClient().newJSONDocumentManager().exists(apiPath) == null) {\n+                IOUtil ioUtil = new IOUtil(this.hubClient.getModulesClient());\n+                try {\n+                    ioUtil.load(\"bulkIngester.api\");", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcwNDc5NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r503704795", "bodyText": "@rjrudin . I am guessing -\nnew ClassPathResource(\"marklogic-data-hub-spark-connector/bulkIngester.api\").getInputStream()\nis a replacement for IOUtil.testFileToString().\nThere are other operations in IOUtil as well like - adding permissions and writing to data-hub-MODULES database. Please let me know if there is a method in data-hub that handles the permissions part. Will go ahead and replace them.", "author": "anu3990", "createdAt": "2020-10-13T06:49:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzg3MDk4NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r503870985", "bodyText": "@anu3990 @SameeraPriyathamTadikonda I think you'll want a separate method for building a DocumentWriteOperation for each endpoint module that needs to be written (it would be unusual, but it's possible that the connector only needs to write 1 or 2 of the 3 endpoint modules). And then another method will handle adding each DWO to a DocumentWriteSet, which can then be written to the modules database. I think all of that logic should be in HubDataWriterSource for now - there's no need for a separate utility class, the Java Client makes it pretty simple. We'd just want to avoid any duplication within HDWS.", "author": "rjrudin", "createdAt": "2020-10-13T11:22:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTgxNA=="}], "type": "inlineReview"}, {"oid": "772e8eb137126e7cc01eaab1c873ae3f0261e757", "url": "https://github.com/marklogic/marklogic-data-hub/commit/772e8eb137126e7cc01eaab1c873ae3f0261e757", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-13T06:44:04Z", "type": "forcePushed"}, {"oid": "a7e6528112d6d687cdd22a56453643d9e2404b71", "url": "https://github.com/marklogic/marklogic-data-hub/commit/a7e6528112d6d687cdd22a56453643d9e2404b71", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-13T21:35:51Z", "type": "forcePushed"}, {"oid": "27dc710892e1cf3932e893b38aa95fb638fa6385", "url": "https://github.com/marklogic/marklogic-data-hub/commit/27dc710892e1cf3932e893b38aa95fb638fa6385", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-13T21:41:07Z", "type": "forcePushed"}, {"oid": "32e8b00e7e782027a9c851a48c035f87fe4b5077", "url": "https://github.com/marklogic/marklogic-data-hub/commit/32e8b00e7e782027a9c851a48c035f87fe4b5077", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-14T18:20:43Z", "type": "forcePushed"}, {"oid": "d27333597f30fde6888612ff8cddec40a20a9c5f", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d27333597f30fde6888612ff8cddec40a20a9c5f", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-14T21:00:14Z", "type": "forcePushed"}, {"oid": "60ff9d21af78718355afc02d44e11d839b55b1e3", "url": "https://github.com/marklogic/marklogic-data-hub/commit/60ff9d21af78718355afc02d44e11d839b55b1e3", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-15T20:51:58Z", "type": "forcePushed"}, {"oid": "f552ca6da03eb6b99ef0223da3477c930346c942", "url": "https://github.com/marklogic/marklogic-data-hub/commit/f552ca6da03eb6b99ef0223da3477c930346c942", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-16T15:47:04Z", "type": "forcePushed"}, {"oid": "18b9fc6cae177a213eaa57ee5f596a8afa58c3e5", "url": "https://github.com/marklogic/marklogic-data-hub/commit/18b9fc6cae177a213eaa57ee5f596a8afa58c3e5", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-16T15:50:20Z", "type": "forcePushed"}, {"oid": "a4264ba2a69f7b8451c68590698396580ab68073", "url": "https://github.com/marklogic/marklogic-data-hub/commit/a4264ba2a69f7b8451c68590698396580ab68073", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-19T22:42:21Z", "type": "forcePushed"}, {"oid": "d0705843a43bf1bda0fe498733258f6c6a0d8419", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d0705843a43bf1bda0fe498733258f6c6a0d8419", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-20T06:30:42Z", "type": "forcePushed"}, {"oid": "edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "url": "https://github.com/marklogic/marklogic-data-hub/commit/edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-20T16:47:53Z", "type": "commit"}, {"oid": "edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "url": "https://github.com/marklogic/marklogic-data-hub/commit/edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-20T16:47:53Z", "type": "forcePushed"}]}