{"pr_number": 4633, "pr_title": "DHFPROD-5944: Configure Document Ingestion via connector", "pr_createdAt": "2020-09-28T20:10:33Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4633", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNTg2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496205865", "bodyText": "I don't think it's safe to write all of the params as a JSON object, as that could contain the user password and other sensitive information.\nWe should be much more precise about what goes into workUnit. So we should have a method of e.g. \"ObjectNode buildWorkUnitFromParams(Map)\" that builds an ObjectNode based on the params we know may exist in the Map, and nothing more than that.", "author": "rjrudin", "createdAt": "2020-09-28T20:14:12Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -61,7 +62,15 @@ public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<St\n             hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n         ).bulkCaller();\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n+        String workUnit = \"{\\\"taskId\\\":\" + taskId + \"}\";\n+        ObjectMapper objectMapper = new ObjectMapper();\n+        try {\n+            workUnit = workUnit.concat(objectMapper.writeValueAsString(params));", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNjM3OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496206378", "bodyText": "See my comment to @anu3990  about an Options class that will simplify writing tests, as opposed to building a Map. We will likely have many tests that populate different sets of options. The Options class will handle that for us.", "author": "rjrudin", "createdAt": "2020-09-28T20:15:14Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -86,9 +86,13 @@ public void testBulkIngestWithoutUriPrefix() throws IOException {\n         Map<String, String> params = getHubPropertiesAsMap();\n         params.put(\"batchsize\", batchSize);\n \n-        if(uriPrefix!=null && uriPrefix.length()!=0) {\n+        if (uriPrefix != null && uriPrefix.length() != 0) {\n             params.put(\"uriprefix\", uriPrefix);\n         }\n+        params.put(\"collections\", \"fruits,gluefruits\");", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "url": "https://github.com/marklogic/marklogic-data-hub/commit/6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "message": "DHFPROD-5944: Configure Document Ingestion via connector", "committedDate": "2020-10-02T18:50:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjI4NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499016284", "bodyText": "uriprefix should go into buildWorkUnitFromParams. Also, I'd just call that \"buildDefaultWorkUnit\" - it's clear that it's built from params because that's the sole argument.", "author": "rjrudin", "createdAt": "2020-10-02T19:40:58Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,28 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMjk3Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499032976", "bodyText": "Added uriprefix to buildDefaultWorkUnit", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:21:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjk3Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499016973", "bodyText": "A more succinct approach with Jackson is to use \"has\":\nif (params.has(\"collections\"))\n\nAlso, all 4 conditionals are the same, so you could simplify this like so:\nStream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n  if (params.containsKey(key)) {\n    defaultWorkUnit.put(key, params.get(key));\n  }\n}\n\nThe one downside is we have to use the lower-casing that Spark requires. I think we can live with that for now though.", "author": "rjrudin", "createdAt": "2020-10-02T19:42:41Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,28 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");\n+            buildWorkUnitFromParams(params);\n             endpointParams.set(\"workUnit\", defaultWorkUnit);\n         }\n \n         return endpointParams;\n     }\n+\n+    protected void buildWorkUnitFromParams(Map<String, String> params) {\n+        if (params.get(\"collections\") != null) {", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMzA1NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499033054", "bodyText": "Updated the changes", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:21:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjk3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxODEyNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499018126", "bodyText": "Put \"Test\" as a suffix on this class name, that's our convention for every test class.", "author": "rjrudin", "createdAt": "2020-10-02T19:45:31Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptions.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptions extends AbstractSparkConnectorTest {", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMzE3NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499033175", "bodyText": "Renamed the class", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:22:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxODEyNg=="}], "type": "inlineReview"}, {"oid": "410088ee47dbd98710c389893448d1f1b40e33bb", "url": "https://github.com/marklogic/marklogic-data-hub/commit/410088ee47dbd98710c389893448d1f1b40e33bb", "message": "DHFPROD-5944: Configure Document Ingestion via connector", "committedDate": "2020-10-02T20:19:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzQxNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499637417", "bodyText": "The endpoint should do this, not the connector. The endpoint should say - if uriprefix is in the workUnit, then I'll use that, even if it's null. It's up to the client to pass in the correct value for uriprefix - if the client passes in \"uriprefix\": null, then the endpoint says - Well I guess you want null as the prefix, so I'll use that.", "author": "rjrudin", "createdAt": "2020-10-05T14:24:00Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,22 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            if (params.get(\"uriprefix\") == null) {", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDM5MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804390", "bodyText": "Made the changes", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzQxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzgzNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499637835", "bodyText": "This test should verify that two collections work, so e.g. \"fruits,stuff\".", "author": "rjrudin", "createdAt": "2020-10-05T14:24:32Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDU5Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804592", "bodyText": "Addressed via https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzgzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0MzcxNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499643716", "bodyText": "Let's not specify a uriprefix here, as the scope of this test is to verify the collections.\nSo instead of doing a uriMatch query, do a cts.uris(null, null, cts.collectionQuery('fruits')). And I recommend getting the value back as a string, and then splitting that into an array based on the newline symbol:\nString[] uris = getHubClient().getStagingClient().newServerEval().javascript(\"cts.uris(null, null, cts.collectionQuery('fruits'))\").evalAs(String.class).split(\"\\n\");\n        assertEquals(1, uris.length);\n        DocumentMetadataHandle metadata = getHubClient().getStagingClient().newDocumentManager().readMetadata(uris[0], new DocumentMetadataHandle());\n        assertEquals(2, metadata.getCollections().size());\n        assertTrue(metadata.getCollections().contains(\"fruits\"));\n        assertTrue(metadata.getCollections().contains(\"stuff\"));", "author": "rjrudin", "createdAt": "2020-10-05T14:32:12Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDY5MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804690", "bodyText": "Same https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0MzcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NDc0MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499654740", "bodyText": "There's a lot of duplication across these 4 tests - I'm going to submit a PR to yours to resolve that.", "author": "rjrudin", "createdAt": "2020-10-05T14:46:45Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));\n+        dataWriter.write(buildRow(\"pineapple\", \"green\"));\n+        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n+\n+        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n+        String uri = uriQueryResult.next().getString();\n+\n+        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n+        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n+\n+        assertEquals(collections, docMetaData.getCollections().toArray()[0].toString());\n+    }\n+\n+    @Test\n+    void ingestDocsWithSourceName() throws IOException {\n+        String sourceName = \"spark\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceName(sourceName));\n+        dataWriter.write(buildRow(\"pineapple\", \"green\"));\n+        String uriQuery = \"cts.uriMatch('/testFruit**')\";", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1OTI0Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499659242", "bodyText": "See https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "rjrudin", "createdAt": "2020-10-05T14:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NDc0MA=="}], "type": "inlineReview"}, {"oid": "8b72cb309d0c8138f8669d70f3328b8775453a26", "url": "https://github.com/marklogic/marklogic-data-hub/commit/8b72cb309d0c8138f8669d70f3328b8775453a26", "message": "DHFPROD-5944: Configure Document Ingestion via connector\n\nDHFPROD-5944: Adding helper methods for test\n\nuriprefix", "committedDate": "2020-10-05T18:43:08Z", "type": "commit"}, {"oid": "8b72cb309d0c8138f8669d70f3328b8775453a26", "url": "https://github.com/marklogic/marklogic-data-hub/commit/8b72cb309d0c8138f8669d70f3328b8775453a26", "message": "DHFPROD-5944: Configure Document Ingestion via connector\n\nDHFPROD-5944: Adding helper methods for test\n\nuriprefix", "committedDate": "2020-10-05T18:43:08Z", "type": "forcePushed"}]}