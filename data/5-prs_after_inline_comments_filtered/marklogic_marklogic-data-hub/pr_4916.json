{"pr_number": 4916, "pr_title": "DHFPROD-6172: Customize Spark connector endpoints for reading rows", "pr_createdAt": "2020-11-30T06:00:33Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4916", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MzYxNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532643617", "bodyText": "I don't think most of these should be class fields, which complicates the class. See next note below.", "author": "rjrudin", "createdAt": "2020-11-30T14:38:23Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -29,6 +33,11 @@\n     private final Map<String, String> options;\n     private final JsonNode initializationResponse;\n     private final StructType sparkSchema;\n+    private final HubClient hubClient;", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0NDg2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532644865", "bodyText": "I think this is all work to be done in initializeRead, which is already instantiating a HubClient. That can become a local variable in initializeRead, which can then say - if you've specified an initializereadapipath option, then I'll read that using the HubClient and pass it in as the second arg to the SparkService.on call.", "author": "rjrudin", "createdAt": "2020-11-30T14:40:00Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -41,7 +50,12 @@ public HubDataSourceReader(DataSourceOptions dataSourceOptions) {\n \n         this.options = dataSourceOptions.asMap();\n         validateOptions(this.options);\n+        this.hubClientConfig = Util.buildHubClientConfig(options);\n+        this.hubClient = HubClient.withHubClientConfig(hubClientConfig);\n+        this.objectMapper = new ObjectMapper();\n+        this.customReadApiDefinitions = readCustomJobApiDefinitions(options);\n \n+        this.endpointParams = determineReadRecordsEndpointParams(options);", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0NzY2Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532647666", "bodyText": "Because there's only one API endpoint, there's no need for a CustomReadApiDefinition class. CustomWriteApiDefinitions was created as a holder for two API definitions. We don't have that need here, so this method should just return a single InputStreamHandle.\nAlso, should name this method \"readCustomInitializeApiDefinition\" - \"Job\" is not an appropriate name here.", "author": "rjrudin", "createdAt": "2020-11-30T14:43:42Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -126,6 +145,20 @@ private int determineNumPartitions(Map<String, String> options) {\n         return numPartitions;\n     }\n \n+    private CustomReadApiDefinitions readCustomJobApiDefinitions(Map<String, String> options) {", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0NzkyMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532647920", "bodyText": "Should name this \"ReadRows\" instead of \"ReadRecords\" for consistency.", "author": "rjrudin", "createdAt": "2020-11-30T14:44:02Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -141,5 +174,49 @@ private int determineNumPartitions(Map<String, String> options) {\n     public StructType readSchema() {\n         return sparkSchema;\n     }\n+\n+    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1MDE0OQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532650149", "bodyText": "Let's give the same context as the error message above this, and I think some guidance is useful here - e.g. \"Cannot set endpointConstants in readrowsendpointparams option; can only set apiPath and endpointState\".", "author": "rjrudin", "createdAt": "2020-11-30T14:46:57Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -141,5 +174,49 @@ private int determineNumPartitions(Map<String, String> options) {\n     public StructType readSchema() {\n         return sparkSchema;\n     }\n+\n+    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {\n+        ObjectNode endpointParams;\n+        if (options.containsKey(\"readrowsendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"readrowsendpointparams\"));\n+            } catch (IOException e) {\n+                throw new IllegalArgumentException(\"Unable to parse readrowsendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasEndpointConstants = endpointParams.hasNonNull(\"endpointConstants\") && !StringUtils.isEmpty(endpointParams.get(\"endpointConstants\").asText());\n+        boolean hasEndpointState = endpointParams.hasNonNull(\"endpointState\") && !StringUtils.isEmpty(endpointParams.get(\"endpointState\").asText());\n+        if (doesNotHaveApiPath && hasEndpointState) {\n+            throw new IllegalArgumentException(\"Cannot set endpointState in readrowsendpointparams unless apiPath is defined as well.\");\n+        }\n+        if(hasEndpointConstants)\n+            throw new IllegalArgumentException(\"Cannot override endpointConstants.\");", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1MDkyNA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532650924", "bodyText": "Good error here, but add \"option\" after \"readrowsendpointparams\" to give it a little more context", "author": "rjrudin", "createdAt": "2020-11-30T14:47:56Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -141,5 +174,49 @@ private int determineNumPartitions(Map<String, String> options) {\n     public StructType readSchema() {\n         return sparkSchema;\n     }\n+\n+    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {\n+        ObjectNode endpointParams;\n+        if (options.containsKey(\"readrowsendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"readrowsendpointparams\"));\n+            } catch (IOException e) {\n+                throw new IllegalArgumentException(\"Unable to parse readrowsendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasEndpointConstants = endpointParams.hasNonNull(\"endpointConstants\") && !StringUtils.isEmpty(endpointParams.get(\"endpointConstants\").asText());\n+        boolean hasEndpointState = endpointParams.hasNonNull(\"endpointState\") && !StringUtils.isEmpty(endpointParams.get(\"endpointState\").asText());\n+        if (doesNotHaveApiPath && hasEndpointState) {\n+            throw new IllegalArgumentException(\"Cannot set endpointState in readrowsendpointparams unless apiPath is defined as well.\");", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1Mzg5Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532653893", "bodyText": "\"readRowsEndpointParams\" should be here as well, and it needs to be tested too. A good way to test that, along with the custom initialize endpoint, is the following:\n\nDon't load any customers, nor the TDE\nHave the initialize endpoint return a single static partition, regardless of what the other inputs are\nHave the initialize endpoint return a static Spark schema as well, regardless of what the other inputs are\nFor readrowsendpointparams/endpointState, define a single JSON row that can be returned by the readRows endpoint\nThen have the readRows endpoint return whatever value is in endpointState (it'll also need to use something in the endpointState object to know whether it's been called, since you only want to call it once)\n\nThat will allow you to verify that both your custom initialize and read endpoints are being used.", "author": "rjrudin", "createdAt": "2020-11-30T14:51:48Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/Options.java", "diffHunk": "@@ -27,6 +27,7 @@\n     private String initializeWriteApiPath;\n     private String finalizeWriteApiPath;\n     private JsonNode additionalExternalMetadata;\n+    private String initializeReadApiPath;", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1NDE5Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532654192", "bodyText": "Replace \"Job\" with \"Read\"", "author": "rjrudin", "createdAt": "2020-11-30T14:52:13Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.document.GenericDocumentManager;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+\n+public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void installCustomJobEndpoints() {", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1NDQ3OQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532654479", "bodyText": "Let's put these in a folder called \"/custom-read-endpoints\" so it's clear that they're for testing the read endpoint.", "author": "rjrudin", "createdAt": "2020-11-30T14:52:33Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.document.GenericDocumentManager;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+\n+public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void installCustomJobEndpoints() {\n+        GenericDocumentManager mgr = getHubClient().getModulesClient().newDocumentManager();\n+        DocumentMetadataHandle metadata = new DocumentMetadataHandle()\n+            .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE, DocumentMetadataHandle.Capability.EXECUTE);\n+\n+        String apiPath = \"/custom-job-endpoints/initializeRead.api\";", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b2af72bf64782998315b17456152959b3272b7f0", "url": "https://github.com/marklogic/marklogic-data-hub/commit/b2af72bf64782998315b17456152959b3272b7f0", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-11-30T05:59:09Z", "type": "forcePushed"}, {"oid": "3abd77b345a84efab105e403c0c2eb21c4165c96", "url": "https://github.com/marklogic/marklogic-data-hub/commit/3abd77b345a84efab105e403c0c2eb21c4165c96", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-01T01:07:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY5MDIzNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r533690237", "bodyText": "This test isn't verifying that the customReadRows.api endpoint is being used, because it's asserting that rows is empty. Rows is empty because no customers have been loaded.\nThe test class I added - ReadWithCustomEndpointsTest - is failing because HubInputPartitionReader needs to look at the options to determine if it should use a custom API path or not. That test should suffice too for the happy path (you'll still want test methods for when endpointState is provided by not apiPath, and also for when endpointConstants is erroneously provided; I recommend moving both of those into ReadWithCustomEndpointsTest).", "author": "rjrudin", "createdAt": "2020-12-01T20:09:29Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,78 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.*;\n+\n+public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void setUp() {\n+        installCustomEndpoint(\"custom-read-endpoints/initializeRead.api\", \"custom-read-endpoints/initializeRead.sjs\");\n+        installCustomEndpoint(\"custom-read-endpoints/customReadRows.api\", \"custom-read-endpoints/customReadRows.sjs\");\n+    }\n+\n+    @Test\n+    public void testCustomSchema() {\n+        setupCustomers();\n+        Options options = newOptions().withView(\"Customer\")\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        StructType testStructType = dataSourceReader.readSchema();\n+\n+        assertTrue(testStructType.toList().length() == 2, \"Custom schema not as expected.\");\n+        for(int i=0; i<testStructType.toList().length(); i++) {\n+            String name = testStructType.toList().apply(0).name();\n+            assertTrue(name.equals(\"myName\") || name.equals(\"myId\"));\n+        }\n+    }\n+\n+    @Test\n+    public void testHasEndpointStateWithoutApiPath() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointState\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointState in readrowsendpointparams option unless apiPath is defined as well.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testHasEndpointConstants() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointConstants\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointConstants in readrowsendpointparams option; can only set apiPath and endpointState.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testReadRowsEndpointparams() {", "originalCommit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY5MDQ4OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r533690488", "bodyText": "Once HubInputPartitionReader is updated to use the custom endpoint, this test should pass.", "author": "rjrudin", "createdAt": "2020-12-01T20:09:51Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class ReadWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void setup() {\n+        installCustomEndpoint(\"custom-read-endpoints/initializeRead.api\", \"custom-read-endpoints/initializeRead.sjs\");\n+        installCustomEndpoint(\"custom-read-endpoints/customReadRows.api\", \"custom-read-endpoints/customReadRows.sjs\");\n+    }\n+\n+    @Test\n+    void test() {\n+        ObjectNode endpointState = objectMapper.createObjectNode();\n+        ObjectNode testRow = endpointState.putObject(\"testRow\");\n+        testRow.put(\"myId\", \"12345\");\n+        testRow.put(\"myName\", \"This is a test row\");\n+\n+        Options options = newOptions().withView(\"doesnt-matter\")\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\")\n+            .withReadRowsApiPath(\"/custom-read-endpoints/customReadRows.api\")\n+            .withReadRowsEndpointState(endpointState);\n+\n+        List<InternalRow> rows = readRows(new HubDataSourceReader(options.toDataSourceOptions()));", "originalCommit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d62c66cb746c126345a5e401668fdb8d0181d372", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d62c66cb746c126345a5e401668fdb8d0181d372", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-02T22:09:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3MzIzNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r535373237", "bodyText": "I like to minimize the number of class fields as much as possible to simplify a class; this one can be a local variable in the initializeRead method, it doesn't need to be a class field.", "author": "rjrudin", "createdAt": "2020-12-03T16:12:55Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -29,6 +33,9 @@\n     private final Map<String, String> options;\n     private final JsonNode initializationResponse;\n     private final StructType sparkSchema;\n+    private InputStreamHandle initializeDefinition;", "originalCommit": "d62c66cb746c126345a5e401668fdb8d0181d372", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3MzU5OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r535373598", "bodyText": "Let's make this consistent with the HubInputPartitionReader constructor, where endpointParams is the last argument out of 4.", "author": "rjrudin", "createdAt": "2020-12-03T16:13:22Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartition.java", "diffHunk": "@@ -16,15 +16,17 @@\n     private Map<String, String> options;\n     private JsonNode initializeResponse;\n     private int partitionNumber;\n+    private JsonNode endpointParams;\n \n-    public HubInputPartition(Map<String, String> options, JsonNode initializeResponse, int partitionNumber) {\n+    public HubInputPartition(JsonNode endpointParams, Map<String, String> options, JsonNode initializeResponse, int partitionNumber) {", "originalCommit": "d62c66cb746c126345a5e401668fdb8d0181d372", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "732bbdd1049bf20b7849d1b2295f3531f13019ca", "url": "https://github.com/marklogic/marklogic-data-hub/commit/732bbdd1049bf20b7849d1b2295f3531f13019ca", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-03T19:53:04Z", "type": "forcePushed"}, {"oid": "c3314a133efccc235d871f5aa85d15303ef9b611", "url": "https://github.com/marklogic/marklogic-data-hub/commit/c3314a133efccc235d871f5aa85d15303ef9b611", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-04T18:23:41Z", "type": "forcePushed"}, {"oid": "675038c5ceff4eb1580c9fd26190f3b49f4b693d", "url": "https://github.com/marklogic/marklogic-data-hub/commit/675038c5ceff4eb1580c9fd26190f3b49f4b693d", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-04T21:12:25Z", "type": "commit"}, {"oid": "675038c5ceff4eb1580c9fd26190f3b49f4b693d", "url": "https://github.com/marklogic/marklogic-data-hub/commit/675038c5ceff4eb1580c9fd26190f3b49f4b693d", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-04T21:12:25Z", "type": "forcePushed"}]}