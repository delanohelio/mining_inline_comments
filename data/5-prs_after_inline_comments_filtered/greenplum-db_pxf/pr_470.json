{"pr_number": 470, "pr_title": "Add Support for reading ORC without Hive", "pr_createdAt": "2020-10-22T17:03:39Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/470", "timeline": [{"oid": "9b76a44c678099cb874cb6bda3caccfa4f0b81ef", "url": "https://github.com/greenplum-db/pxf/commit/9b76a44c678099cb874cb6bda3caccfa4f0b81ef", "message": "Refactor, add tests", "committedDate": "2020-10-24T10:39:10Z", "type": "forcePushed"}, {"oid": "13fb13a9f65fe104c4b124b7b634e3ab8011ecc4", "url": "https://github.com/greenplum-db/pxf/commit/13fb13a9f65fe104c4b124b7b634e3ab8011ecc4", "message": "Fix automation tests", "committedDate": "2020-10-27T17:52:41Z", "type": "forcePushed"}, {"oid": "f85a6770417d6f9e1010557a94051459770c9293", "url": "https://github.com/greenplum-db/pxf/commit/f85a6770417d6f9e1010557a94051459770c9293", "message": "Minor update", "committedDate": "2020-11-09T23:08:11Z", "type": "forcePushed"}, {"oid": "968245c02fc3555b8c8923e4d08ebca95205a82d", "url": "https://github.com/greenplum-db/pxf/commit/968245c02fc3555b8c8923e4d08ebca95205a82d", "message": "Minor update", "committedDate": "2020-11-19T16:02:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMDgwNA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528930804", "bodyText": "Could we convert these to guard clauses with early returns?\nif (!(node instandeof OperatorNode)) {\n    return node\n}\n\nif (operatorNode.getOperator() != Operator.IN || !(operatorNode.getLeft() instanceof ColumnIndexOperationNode) || !(operatorNode.getRight() instanceof CollectionOperandNode) {\n    return node;\n}\nThat way the bulk of the method isn't doubly indented.", "author": "bradfordb-vmware", "createdAt": "2020-11-23T19:03:26Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+\n+import java.util.List;\n+\n+/**\n+ * Transforms IN operator into a chain of OR operators. This transformer is\n+ * useful for predicate builders that do not support the IN operator.\n+ */\n+public class InOperatorTransformer implements TreeVisitor {\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {", "originalCommit": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk2NDQxMw==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528964413", "bodyText": "I originally had the code like that, but then other implementations of TreeVisitor follow this pattern. So, I preferred to keep it like that for consistency across the code. I also prefer early returns.", "author": "frankgh", "createdAt": "2020-11-23T20:04:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMDgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzNTA2OA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528935068", "bodyText": "is data.size() guaranteed to have size 2? If not, doesn't the transform tree look more like\n      (or)\n      /  \\\n   (or)  (eq)\n   /  \\\n(eq)  (eq)", "author": "bradfordb-vmware", "createdAt": "2020-11-23T19:10:54Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+\n+import java.util.List;\n+\n+/**\n+ * Transforms IN operator into a chain of OR operators. This transformer is\n+ * useful for predicate builders that do not support the IN operator.\n+ */\n+public class InOperatorTransformer implements TreeVisitor {\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+\n+            if (operatorNode.getOperator() == Operator.IN\n+                    && operatorNode.getLeft() instanceof ColumnIndexOperandNode\n+                    && operatorNode.getRight() instanceof CollectionOperandNode) {\n+\n+                ColumnIndexOperandNode columnNode = (ColumnIndexOperandNode) operatorNode.getLeft();\n+                CollectionOperandNode collectionOperandNode = (CollectionOperandNode) operatorNode.getRight();\n+                List<String> data = collectionOperandNode.getData();\n+                DataType type = collectionOperandNode.getDataType().getTypeElem() != null\n+                        ? collectionOperandNode.getDataType().getTypeElem()\n+                        : collectionOperandNode.getDataType();\n+\n+                // Transform the IN operator into a chain of ORs", "originalCommit": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk2NTMyOQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528965329", "bodyText": "data size can be at least 1 up to n. When the size is 1 then we just transform in to eq, otherwise we return a chain of ORs", "author": "frankgh", "createdAt": "2020-11-23T20:06:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzNTA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzNzc2OQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528937769", "bodyText": "Same comment as above about converting to guard clauses with early returns.", "author": "bradfordb-vmware", "createdAt": "2020-11-23T19:15:57Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/filter/BPCharOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.plugins.hdfs.filter;\n+\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.ScalarOperandNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.Utilities;\n+\n+/**\n+ * Transforms non-logical operator nodes that have scalar operand nodes as its\n+ * children of BPCHAR type and which values have whitespace at the end of the\n+ * string.\n+ */\n+public class BPCharOperatorTransformer implements TreeVisitor {\n+    @Override\n+    public Node before(Node node, final int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {", "originalCommit": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "39587e272832b9d1c32a657a3173b14612a355ce", "url": "https://github.com/greenplum-db/pxf/commit/39587e272832b9d1c32a657a3173b14612a355ce", "message": "Add the InOperatorTransformer TreeVisitor (#500)\n\nAdd the InOperatorTransformer that will take the IN operator and\r\ntransform it into a chain of OR operators. This transformer can be used\r\nby filter builders that do not support the IN operator.", "committedDate": "2020-12-03T13:21:48Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2NzU0Ng==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537767546", "bodyText": "speed is usually measured in units/units of time, like records/second, so you will need to divide the other way and update the unit label", "author": "denalex", "createdAt": "2020-12-07T19:24:12Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/model/BasePlugin.java", "diffHunk": "@@ -31,4 +33,27 @@ public void setRequestContext(RequestContext context) {\n     @Override\n     public void afterPropertiesSet() {\n     }\n+\n+    /**\n+     * When DEBUG mode is enabled, logs the total number of rows read, the\n+     * amount of time it took to read the file, and the average read speed\n+     * in nanoseconds\n+     *\n+     * @param totalRowsRead        the total number of rows read\n+     * @param totalReadTimeInNanos the total nanoseconds it took to read the file\n+     */\n+    protected void logReadStats(long totalRowsRead, long totalReadTimeInNanos) {\n+        if (LOG.isDebugEnabled()) {\n+            final long millis = TimeUnit.NANOSECONDS.toMillis(totalReadTimeInNanos);\n+            long average = totalRowsRead == 0 ? 0 : totalReadTimeInNanos / totalRowsRead;\n+            LOG.debug(\"{}-{}: Read TOTAL of {} rows from file {} on server {} in {} ms. Average speed: {} nanoseconds\",", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2OTA1MA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537769050", "bodyText": "ideally we should not be touching Parquet in ORC PR.", "author": "denalex", "createdAt": "2020-12-07T19:26:39Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -322,16 +315,17 @@ public void closeForWrite() throws IOException, InterruptedException {\n         List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n         ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n                 tupleDescription, originalFieldsMap);\n-        TreeVisitor pruner = new ParquetOperatorPrunerAndTransformer(\n+        TreeVisitor pruner = new ParquetOperatorPruner(", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5NDk1MA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537894950", "bodyText": "I will open a separate PR for the changes here", "author": "frankgh", "createdAt": "2020-12-07T22:53:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2OTA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc3MDE4OA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537770188", "bodyText": "why adding BPCHAR transformer is needed here ?", "author": "denalex", "createdAt": "2020-12-07T19:28:21Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -322,16 +315,17 @@ public void closeForWrite() throws IOException, InterruptedException {\n         List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n         ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n                 tupleDescription, originalFieldsMap);\n-        TreeVisitor pruner = new ParquetOperatorPrunerAndTransformer(\n+        TreeVisitor pruner = new ParquetOperatorPruner(\n                 tupleDescription, originalFieldsMap, SUPPORTED_OPERATORS);\n \n         try {\n             // Parse the filter string into a expression tree Node\n             Node root = new FilterParser().parse(filterString);\n-            // Prune the parsed tree with valid supported operators and then\n+            // Transform IN operators into a chain of ORs, then\n+            // prune the parsed tree with valid supported operators and then\n             // traverse the pruned tree with the ParquetRecordFilterBuilder to\n             // produce a record filter for parquet\n-            TRAVERSER.traverse(root, pruner, filterBuilder);\n+            TRAVERSER.traverse(root, IN_OPERATOR_TRANSFORMER, pruner, BPCHAR_TRANSFORMER, filterBuilder);", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQxMjg0MA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r538412840", "bodyText": "The logic in ParquetOperatorPrunerAndTransformer was split into ParquetOperatorPruner and BPCharOperatorTransformer. This effectively preserves the same logic as before. We need the BPCHAR transformer here because, depending on the implementation, the char fields in parquet files will either be padded to the width of the field or the second case is when the char field is trimmed. So for that reason we need BPCHAR_TRANSFORMER", "author": "frankgh", "createdAt": "2020-12-08T14:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc3MDE4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NTEwNQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537795105", "bodyText": "don't we need to do this multiple times if there are multiple blank space paddings ?", "author": "denalex", "createdAt": "2020-12-07T20:04:37Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/filter/BPCharOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.plugins.hdfs.filter;\n+\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.ScalarOperandNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.Utilities;\n+\n+/**\n+ * Transforms non-logical operator nodes that have scalar operand nodes as its\n+ * children of BPCHAR type and which values have whitespace at the end of the\n+ * string.\n+ */\n+public class BPCharOperatorTransformer implements TreeVisitor {\n+    @Override\n+    public Node before(Node node, final int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical()\n+                    && operatorNode.getLeft() instanceof ColumnIndexOperandNode\n+                    && operatorNode.getRight() instanceof ScalarOperandNode) {\n+                ScalarOperandNode scalarOperandNode = (ScalarOperandNode) operatorNode.getRight();\n+\n+                if (scalarOperandNode.getDataType() == DataType.BPCHAR) {\n+                    String value = scalarOperandNode.getValue();\n+\n+                    /* Determine whether the string has whitespace at the end */\n+                    if (value.length() > 0 && value.charAt(value.length() - 1) == ' ') {", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY4MzUyNQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r538683525", "bodyText": "I have addressed this in a separate PR.", "author": "frankgh", "createdAt": "2020-12-08T18:14:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NTEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NjMwOQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537796309", "bodyText": "so I wonder why HiveVectorizedORC is not using filters, just omission ?", "author": "denalex", "createdAt": "2020-12-07T20:06:40Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3MjQ0NA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537872444", "bodyText": "most likely, good question. I guess we can support it.", "author": "frankgh", "createdAt": "2020-12-07T22:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NjMwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5ODQzOQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537798439", "bodyText": "not sure how valuable timekeeping is here, as the reader might not actually read the whole batch in memory in nextBatch() or will it ?", "author": "denalex", "createdAt": "2020-12-07T20:10:03Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});\n+\n+        // Read the row data\n+        recordReader = fileReader.rows(options);\n+        batch = readSchema.createRowBatch();\n+        context.setMetadata(readSchema);\n+        return true;\n+    }\n+\n+    /**\n+     * Reads the next batch for the current fragment\n+     *\n+     * @return the next batch in OneRow format, the key is the batch number, and data is the batch\n+     * @throws IOException when reading of the next batch occurs\n+     */\n+    @Override\n+    public OneRow readNextObject() throws IOException {\n+        final Instant start = Instant.now();\n+        final boolean hasNextBatch = recordReader.nextBatch(batch);\n+        totalReadTimeInNanos += Duration.between(start, Instant.now()).toNanos();", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5ODIzMQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537898231", "bodyText": "there are two cases, when the stripe needs to be read, and when the stripe is in memory. Depending on the case, timing will vary, but I think it's still worth keeping track. What would an alternative be for this?", "author": "frankgh", "createdAt": "2020-12-07T23:00:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5ODQzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMTEyNQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537801125", "bodyText": "so, will we support the cases where GP has more columns than ORC and will fill with NULL and the other one where ORC has more columns and we will ignore them ? What about when diff ORC files have a bit different schemas ?", "author": "denalex", "createdAt": "2020-12-07T20:14:32Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});\n+\n+        // Read the row data\n+        recordReader = fileReader.rows(options);\n+        batch = readSchema.createRowBatch();\n+        context.setMetadata(readSchema);\n+        return true;\n+    }\n+\n+    /**\n+     * Reads the next batch for the current fragment\n+     *\n+     * @return the next batch in OneRow format, the key is the batch number, and data is the batch\n+     * @throws IOException when reading of the next batch occurs\n+     */\n+    @Override\n+    public OneRow readNextObject() throws IOException {\n+        final Instant start = Instant.now();\n+        final boolean hasNextBatch = recordReader.nextBatch(batch);\n+        totalReadTimeInNanos += Duration.between(start, Instant.now()).toNanos();\n+        if (hasNextBatch) {\n+            totalRowsRead += batch.size;\n+            return new OneRow(new LongWritable(batchIndex++), batch);\n+        }\n+        return null; // all batches are exhausted\n+    }\n+\n+    @Override\n+    public void closeForRead() throws IOException {\n+        logReadStats(totalRowsRead, totalReadTimeInNanos);\n+        if (recordReader != null) {\n+            recordReader.close();\n+        }\n+        if (fileReader != null) {\n+            fileReader.close();\n+        }\n+    }\n+\n+    @Override\n+    public boolean openForWrite() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public boolean writeNextObject(OneRow onerow) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void closeForWrite() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    /**\n+     * Given a filter string, builds the SearchArgument object to perform\n+     * predicated pushdown for ORC\n+     *\n+     * @param filterString   the serialized filter string from the query predicate\n+     * @param originalSchema the original schema for the ORC file\n+     * @return null if filter string is null, the built SearchArgument otherwise\n+     * @throws IOException when a filter parsing error occurs\n+     */\n+    private SearchArgument getSearchArgument(String filterString, TypeDescription originalSchema) throws IOException {\n+        if (StringUtils.isBlank(filterString)) {\n+            return null;\n+        }\n+\n+        List<ColumnDescriptor> descriptors = columnDescriptors;\n+\n+        if (positionalAccess) {\n+            // We need to adjust the descriptors to match the column names\n+            // in the ORC schema to support predicate push down\n+            descriptors = new ArrayList<>();\n+            for (int i = 0; i < columnDescriptors.size() && i < originalSchema.getFieldNames().size(); i++) {\n+                ColumnDescriptor columnDescriptor = columnDescriptors.get(i);\n+                String columnName = originalSchema.getFieldNames().get(i);\n+                ColumnDescriptor copyDescriptor = new ColumnDescriptor(\n+                        columnName, // the name of the column in the ORC schema\n+                        columnDescriptor.columnTypeCode(),\n+                        columnDescriptor.columnIndex(),\n+                        columnDescriptor.columnTypeName(),\n+                        columnDescriptor.columnTypeModifiers(),\n+                        columnDescriptor.isProjected());\n+                descriptors.add(copyDescriptor);\n+            }\n+        }\n+\n+        SearchArgumentBuilder searchArgumentBuilder =\n+                new SearchArgumentBuilder(descriptors, configuration);\n+\n+        // Parse the filter string into a expression tree Node\n+        Node root = new FilterParser().parse(filterString);\n+        // Prune the parsed tree with valid supported operators and then\n+        // traverse the pruned tree with the searchArgumentBuilder to produce a\n+        // SearchArgument for ORC\n+        TRAVERSER.traverse(root, PRUNER, BPCHAR_TRANSFORMER, searchArgumentBuilder);\n+\n+        // Build the SearchArgument object\n+        return searchArgumentBuilder.getFilterBuilder().build();\n+    }\n+\n+    /**\n+     * Given the column descriptors that we receive from Greenplum, builds\n+     * the read schema that will perform column projection\n+     *\n+     * @param originalSchema the original schema for the ORC file\n+     * @return the read schema\n+     */\n+    private TypeDescription buildReadSchema(TypeDescription originalSchema) {", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3NTU3Nw==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537875577", "bodyText": "we have tests where ORC have different schemas, so we support both GP has more columns than ORC and we fill with null: https://github.com/greenplum-db/pxf/pull/470/files#diff-f4945312d8b51ad817a8d7882795a2e32575ac346d2d8567a6dda9f4a4334a1dR74\nWe also support reading a subset of columns from an ORC file (column projection).", "author": "frankgh", "createdAt": "2020-12-07T22:18:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMTEyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNDQ3OA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537804478", "bodyText": "repeating means all the values of this column in a batch are the same ?", "author": "denalex", "createdAt": "2020-12-07T20:19:59Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3NzE2Nw==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537877167", "bodyText": "correct, this is from java docs\n\nTrue if same value repeats for whole column vector.\nIf so, vector[0] holds the repeating value.", "author": "frankgh", "createdAt": "2020-12-07T22:21:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNDQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNTkzOQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537805939", "bodyText": "the body of these methods is all the same, other than types, would generics work here: Mapper<T>, DoubleMapper extends Mapper<Double>, etc ?", "author": "denalex", "createdAt": "2020-12-07T20:22:02Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Float value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? (float) dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] doubleMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Double value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3NzgwOA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537877808", "bodyText": "I didn't want to address this here, I wanted to solve this at the refactor level, where we determine what type we need for the wire format.", "author": "frankgh", "createdAt": "2020-12-07T22:22:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNTkzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNzg5MA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537807890", "bodyText": "we can pre-define OneField(oid, null) as singletons for a given oid and then use Arrays.fill to fill the array. That will save a lot of object creation on null values.", "author": "denalex", "createdAt": "2020-12-07T20:25:15Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Float value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? (float) dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] doubleMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Double value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] textMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        BytesColumnVector bcv = (BytesColumnVector) columnVector;\n+        if (bcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = bcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        String value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+\n+            value = bcv.noNulls || !bcv.isNull[rowId] ?\n+                    new String(bcv.vector[rowIndex], bcv.start[rowIndex],\n+                            bcv.length[rowIndex], StandardCharsets.UTF_8) : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] decimalMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        DecimalColumnVector dcv = (DecimalColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        HiveDecimalWritable value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] binaryMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        BytesColumnVector bcv = (BytesColumnVector) columnVector;\n+        if (bcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = bcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        byte[] value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            if (bcv.noNulls || !bcv.isNull[rowId]) {\n+                value = new byte[bcv.length[rowId]];\n+                System.arraycopy(bcv.vector[rowId], bcv.start[rowId], value, 0, bcv.length[rowId]);\n+            } else {\n+                value = null;\n+            }\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    // DateWritable is no longer deprecated in newer versions of storage api \u00af\\_(\u30c4)_/\u00af\n+    @SuppressWarnings(\"deprecation\")\n+    public static OneField[] dateMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Date value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? Date.valueOf(LocalDate.ofEpochDay(lcv.vector[rowIndex]))\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] timestampMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        TimestampColumnVector tcv = (TimestampColumnVector) columnVector;\n+        if (tcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = tcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        String value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (tcv.noNulls || !tcv.isNull[rowId])\n+                    ? timestampToString(tcv.asScratchTimestamp(rowId))\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] getNullResultSet(int oid, int size) {\n+        OneField[] result = new OneField[size];\n+        for (int i = 0; i < result.length; i++)\n+            result[i] = new OneField(oid, null);", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3OTUwNA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537879504", "bodyText": "good idea!", "author": "frankgh", "createdAt": "2020-12-07T22:25:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNzg5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMTUyNg==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537811526", "bodyText": "wonder if we can reuse these between batches", "author": "denalex", "createdAt": "2020-12-07T20:31:10Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "diffHunk": "@@ -0,0 +1,272 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.ReadVectorizedResolver;\n+import org.greenplum.pxf.api.error.PxfRuntimeException;\n+import org.greenplum.pxf.api.error.UnsupportedTypeException;\n+import org.greenplum.pxf.api.function.TriFunction;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+import static org.greenplum.pxf.api.io.DataType.BIGINT;\n+import static org.greenplum.pxf.api.io.DataType.BOOLEAN;\n+import static org.greenplum.pxf.api.io.DataType.BPCHAR;\n+import static org.greenplum.pxf.api.io.DataType.BYTEA;\n+import static org.greenplum.pxf.api.io.DataType.DATE;\n+import static org.greenplum.pxf.api.io.DataType.FLOAT8;\n+import static org.greenplum.pxf.api.io.DataType.INTEGER;\n+import static org.greenplum.pxf.api.io.DataType.NUMERIC;\n+import static org.greenplum.pxf.api.io.DataType.REAL;\n+import static org.greenplum.pxf.api.io.DataType.SMALLINT;\n+import static org.greenplum.pxf.api.io.DataType.TEXT;\n+import static org.greenplum.pxf.api.io.DataType.TIMESTAMP;\n+import static org.greenplum.pxf.api.io.DataType.VARCHAR;\n+import static org.greenplum.pxf.plugins.hdfs.orc.ORCVectorizedAccessor.MAP_BY_POSITION_OPTION;\n+\n+/**\n+ * Resolves ORC VectorizedRowBatch into lists of List<OneField>. Only primitive\n+ * types are supported. Currently, Timestamp and Timestamp with TimeZone are\n+ * not supported. The supported mapping is as follows:\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Long             |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Long             |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Long             |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Long             |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Long             |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Double           |  float             |  REAL          |  700          |\n+ * |  Double           |  double            |  FLOAT8        |  701          |\n+ * |  byte[]           |  string            |  TEXT          |  25           |\n+ * |  byte[]           |  char              |  BPCHAR        |  1042         |\n+ * |  byte[]           |  varchar           |  VARCHAR       |  1043         |\n+ * |  byte[]           |  binary            |  BYTEA         |  17           |\n+ * |  Long             |  date              |  DATE          |  1082         |\n+ * |  binary           |  decimal           |  NUMERIC       |  1700         |\n+ * |  binary           |  timestamp         |  TIMESTAMP     |  1114         |\n+ * ---------------------------------------------------------------------------\n+ */\n+public class ORCVectorizedResolver extends BasePlugin implements ReadVectorizedResolver, Resolver {\n+\n+    /**\n+     * The schema used to read the ORC file.\n+     */\n+    private TypeDescription readSchema;\n+\n+    /**\n+     * An array of functions that resolve ColumnVectors into Lists of OneFields\n+     * The array has the same size as the readSchema, and the functions depend\n+     * on the type of the elements in the schema.\n+     */\n+    private TriFunction<VectorizedRowBatch, ColumnVector, Integer, OneField[]>[] functions;\n+\n+    /**\n+     * An array of types that map from the readSchema types to Greenplum OIDs.\n+     */\n+    private int[] typeOidMappings;\n+\n+    private Map<String, TypeDescription> readFields;\n+\n+    /**\n+     * True if the resolver resolves the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+\n+    /**\n+     * A local copy of the column descriptors coming from the RequestContext.\n+     * We make this variable local to improve performance while accessing the\n+     * descriptors.\n+     */\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    /**\n+     * Returns the resolved list of list of OneFields given a\n+     * VectorizedRowBatch\n+     *\n+     * @param batch unresolved batch\n+     * @return the resolved batch mapped to the Greenplum type\n+     */\n+    @Override\n+    public List<List<OneField>> getFieldsForBatch(OneRow batch) {\n+        ensureFunctionsAreInitialized();\n+        VectorizedRowBatch vectorizedBatch = (VectorizedRowBatch) batch.getData();\n+        int batchSize = vectorizedBatch.size;\n+\n+        // The resolved batch returns a list of the list of OneField that\n+        // matches the size of the batch. Every internal list, has a list of\n+        // OneFields with size the number of columns\n+        List<List<OneField>> resolvedBatch = new ArrayList<>(batchSize);\n+\n+        // Initialize the internal lists\n+        for (int i = 0; i < batchSize; i++) {\n+            resolvedBatch.add(new ArrayList<>(columnDescriptors.size()));", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5MjM0NQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537892345", "bodyText": "good point", "author": "frankgh", "createdAt": "2020-12-07T22:48:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMTUyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMzAzMA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537813030", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        for (int i = 0; i < batchSize; i++) {\n          \n          \n            \n                        for (int row = 0; row < batchSize; row++) {", "author": "denalex", "createdAt": "2020-12-07T20:33:40Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "diffHunk": "@@ -0,0 +1,272 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.ReadVectorizedResolver;\n+import org.greenplum.pxf.api.error.PxfRuntimeException;\n+import org.greenplum.pxf.api.error.UnsupportedTypeException;\n+import org.greenplum.pxf.api.function.TriFunction;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+import static org.greenplum.pxf.api.io.DataType.BIGINT;\n+import static org.greenplum.pxf.api.io.DataType.BOOLEAN;\n+import static org.greenplum.pxf.api.io.DataType.BPCHAR;\n+import static org.greenplum.pxf.api.io.DataType.BYTEA;\n+import static org.greenplum.pxf.api.io.DataType.DATE;\n+import static org.greenplum.pxf.api.io.DataType.FLOAT8;\n+import static org.greenplum.pxf.api.io.DataType.INTEGER;\n+import static org.greenplum.pxf.api.io.DataType.NUMERIC;\n+import static org.greenplum.pxf.api.io.DataType.REAL;\n+import static org.greenplum.pxf.api.io.DataType.SMALLINT;\n+import static org.greenplum.pxf.api.io.DataType.TEXT;\n+import static org.greenplum.pxf.api.io.DataType.TIMESTAMP;\n+import static org.greenplum.pxf.api.io.DataType.VARCHAR;\n+import static org.greenplum.pxf.plugins.hdfs.orc.ORCVectorizedAccessor.MAP_BY_POSITION_OPTION;\n+\n+/**\n+ * Resolves ORC VectorizedRowBatch into lists of List<OneField>. Only primitive\n+ * types are supported. Currently, Timestamp and Timestamp with TimeZone are\n+ * not supported. The supported mapping is as follows:\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Long             |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Long             |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Long             |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Long             |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Long             |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Double           |  float             |  REAL          |  700          |\n+ * |  Double           |  double            |  FLOAT8        |  701          |\n+ * |  byte[]           |  string            |  TEXT          |  25           |\n+ * |  byte[]           |  char              |  BPCHAR        |  1042         |\n+ * |  byte[]           |  varchar           |  VARCHAR       |  1043         |\n+ * |  byte[]           |  binary            |  BYTEA         |  17           |\n+ * |  Long             |  date              |  DATE          |  1082         |\n+ * |  binary           |  decimal           |  NUMERIC       |  1700         |\n+ * |  binary           |  timestamp         |  TIMESTAMP     |  1114         |\n+ * ---------------------------------------------------------------------------\n+ */\n+public class ORCVectorizedResolver extends BasePlugin implements ReadVectorizedResolver, Resolver {\n+\n+    /**\n+     * The schema used to read the ORC file.\n+     */\n+    private TypeDescription readSchema;\n+\n+    /**\n+     * An array of functions that resolve ColumnVectors into Lists of OneFields\n+     * The array has the same size as the readSchema, and the functions depend\n+     * on the type of the elements in the schema.\n+     */\n+    private TriFunction<VectorizedRowBatch, ColumnVector, Integer, OneField[]>[] functions;\n+\n+    /**\n+     * An array of types that map from the readSchema types to Greenplum OIDs.\n+     */\n+    private int[] typeOidMappings;\n+\n+    private Map<String, TypeDescription> readFields;\n+\n+    /**\n+     * True if the resolver resolves the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+\n+    /**\n+     * A local copy of the column descriptors coming from the RequestContext.\n+     * We make this variable local to improve performance while accessing the\n+     * descriptors.\n+     */\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    /**\n+     * Returns the resolved list of list of OneFields given a\n+     * VectorizedRowBatch\n+     *\n+     * @param batch unresolved batch\n+     * @return the resolved batch mapped to the Greenplum type\n+     */\n+    @Override\n+    public List<List<OneField>> getFieldsForBatch(OneRow batch) {\n+        ensureFunctionsAreInitialized();\n+        VectorizedRowBatch vectorizedBatch = (VectorizedRowBatch) batch.getData();\n+        int batchSize = vectorizedBatch.size;\n+\n+        // The resolved batch returns a list of the list of OneField that\n+        // matches the size of the batch. Every internal list, has a list of\n+        // OneFields with size the number of columns\n+        List<List<OneField>> resolvedBatch = new ArrayList<>(batchSize);\n+\n+        // Initialize the internal lists\n+        for (int i = 0; i < batchSize; i++) {\n+            resolvedBatch.add(new ArrayList<>(columnDescriptors.size()));\n+        }\n+\n+        // index to the projected columns\n+        int columnIndex = 0;\n+        OneField[] oneFields;\n+        for (ColumnDescriptor columnDescriptor : columnDescriptors) {\n+            if (!columnDescriptor.isProjected()) {\n+                oneFields = ORCVectorizedMappingFunctions\n+                        .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+            } else {\n+                TypeDescription orcColumn = positionalAccess\n+                        ? columnIndex < readSchema.getChildren().size() ? readSchema.getChildren().get(columnIndex) : null\n+                        : readFields.get(columnDescriptor.columnName());\n+                if (orcColumn == null) {\n+                    // this column is missing in the underlying ORC file, but\n+                    // it is defined in the Greenplum table. This can happen\n+                    // when a schema evolves, for example the original\n+                    // ORC-backed table had 4 columns, and at a later point in\n+                    // time a fifth column was added. Files written before the\n+                    // column was added will have 4 columns, and new files\n+                    // will have 5 columns\n+                    oneFields = ORCVectorizedMappingFunctions\n+                            .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+                } else if (orcColumn.getCategory().isPrimitive()) {\n+                    oneFields = functions[columnIndex]\n+                            .apply(vectorizedBatch, vectorizedBatch.cols[columnIndex], typeOidMappings[columnIndex]);\n+                    columnIndex++;\n+                } else {\n+                    throw new UnsupportedTypeException(\n+                            String.format(\"Unable to resolve column '%s' with category '%s'. Only primitive types are supported.\",\n+                                    readSchema.getFieldNames().get(columnIndex), orcColumn.getCategory()));\n+                }\n+            }\n+\n+            // oneFields is the array of fields for the current column we are\n+            // processing. We need to add it to the corresponding list\n+            for (int i = 0; i < batchSize; i++) {", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxNDg0Nw==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537814847", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            resolvedBatch.get(i).add(oneFields[i]);\n          \n          \n            \n                            resolvedBatch.get(row).add(oneFieldsForRow[row]);", "author": "denalex", "createdAt": "2020-12-07T20:36:39Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "diffHunk": "@@ -0,0 +1,272 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.ReadVectorizedResolver;\n+import org.greenplum.pxf.api.error.PxfRuntimeException;\n+import org.greenplum.pxf.api.error.UnsupportedTypeException;\n+import org.greenplum.pxf.api.function.TriFunction;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+import static org.greenplum.pxf.api.io.DataType.BIGINT;\n+import static org.greenplum.pxf.api.io.DataType.BOOLEAN;\n+import static org.greenplum.pxf.api.io.DataType.BPCHAR;\n+import static org.greenplum.pxf.api.io.DataType.BYTEA;\n+import static org.greenplum.pxf.api.io.DataType.DATE;\n+import static org.greenplum.pxf.api.io.DataType.FLOAT8;\n+import static org.greenplum.pxf.api.io.DataType.INTEGER;\n+import static org.greenplum.pxf.api.io.DataType.NUMERIC;\n+import static org.greenplum.pxf.api.io.DataType.REAL;\n+import static org.greenplum.pxf.api.io.DataType.SMALLINT;\n+import static org.greenplum.pxf.api.io.DataType.TEXT;\n+import static org.greenplum.pxf.api.io.DataType.TIMESTAMP;\n+import static org.greenplum.pxf.api.io.DataType.VARCHAR;\n+import static org.greenplum.pxf.plugins.hdfs.orc.ORCVectorizedAccessor.MAP_BY_POSITION_OPTION;\n+\n+/**\n+ * Resolves ORC VectorizedRowBatch into lists of List<OneField>. Only primitive\n+ * types are supported. Currently, Timestamp and Timestamp with TimeZone are\n+ * not supported. The supported mapping is as follows:\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Long             |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Long             |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Long             |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Long             |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Long             |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Double           |  float             |  REAL          |  700          |\n+ * |  Double           |  double            |  FLOAT8        |  701          |\n+ * |  byte[]           |  string            |  TEXT          |  25           |\n+ * |  byte[]           |  char              |  BPCHAR        |  1042         |\n+ * |  byte[]           |  varchar           |  VARCHAR       |  1043         |\n+ * |  byte[]           |  binary            |  BYTEA         |  17           |\n+ * |  Long             |  date              |  DATE          |  1082         |\n+ * |  binary           |  decimal           |  NUMERIC       |  1700         |\n+ * |  binary           |  timestamp         |  TIMESTAMP     |  1114         |\n+ * ---------------------------------------------------------------------------\n+ */\n+public class ORCVectorizedResolver extends BasePlugin implements ReadVectorizedResolver, Resolver {\n+\n+    /**\n+     * The schema used to read the ORC file.\n+     */\n+    private TypeDescription readSchema;\n+\n+    /**\n+     * An array of functions that resolve ColumnVectors into Lists of OneFields\n+     * The array has the same size as the readSchema, and the functions depend\n+     * on the type of the elements in the schema.\n+     */\n+    private TriFunction<VectorizedRowBatch, ColumnVector, Integer, OneField[]>[] functions;\n+\n+    /**\n+     * An array of types that map from the readSchema types to Greenplum OIDs.\n+     */\n+    private int[] typeOidMappings;\n+\n+    private Map<String, TypeDescription> readFields;\n+\n+    /**\n+     * True if the resolver resolves the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+\n+    /**\n+     * A local copy of the column descriptors coming from the RequestContext.\n+     * We make this variable local to improve performance while accessing the\n+     * descriptors.\n+     */\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    /**\n+     * Returns the resolved list of list of OneFields given a\n+     * VectorizedRowBatch\n+     *\n+     * @param batch unresolved batch\n+     * @return the resolved batch mapped to the Greenplum type\n+     */\n+    @Override\n+    public List<List<OneField>> getFieldsForBatch(OneRow batch) {\n+        ensureFunctionsAreInitialized();\n+        VectorizedRowBatch vectorizedBatch = (VectorizedRowBatch) batch.getData();\n+        int batchSize = vectorizedBatch.size;\n+\n+        // The resolved batch returns a list of the list of OneField that\n+        // matches the size of the batch. Every internal list, has a list of\n+        // OneFields with size the number of columns\n+        List<List<OneField>> resolvedBatch = new ArrayList<>(batchSize);\n+\n+        // Initialize the internal lists\n+        for (int i = 0; i < batchSize; i++) {\n+            resolvedBatch.add(new ArrayList<>(columnDescriptors.size()));\n+        }\n+\n+        // index to the projected columns\n+        int columnIndex = 0;\n+        OneField[] oneFields;\n+        for (ColumnDescriptor columnDescriptor : columnDescriptors) {\n+            if (!columnDescriptor.isProjected()) {\n+                oneFields = ORCVectorizedMappingFunctions\n+                        .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+            } else {\n+                TypeDescription orcColumn = positionalAccess\n+                        ? columnIndex < readSchema.getChildren().size() ? readSchema.getChildren().get(columnIndex) : null\n+                        : readFields.get(columnDescriptor.columnName());\n+                if (orcColumn == null) {\n+                    // this column is missing in the underlying ORC file, but\n+                    // it is defined in the Greenplum table. This can happen\n+                    // when a schema evolves, for example the original\n+                    // ORC-backed table had 4 columns, and at a later point in\n+                    // time a fifth column was added. Files written before the\n+                    // column was added will have 4 columns, and new files\n+                    // will have 5 columns\n+                    oneFields = ORCVectorizedMappingFunctions\n+                            .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+                } else if (orcColumn.getCategory().isPrimitive()) {\n+                    oneFields = functions[columnIndex]\n+                            .apply(vectorizedBatch, vectorizedBatch.cols[columnIndex], typeOidMappings[columnIndex]);\n+                    columnIndex++;\n+                } else {\n+                    throw new UnsupportedTypeException(\n+                            String.format(\"Unable to resolve column '%s' with category '%s'. Only primitive types are supported.\",\n+                                    readSchema.getFieldNames().get(columnIndex), orcColumn.getCategory()));\n+                }\n+            }\n+\n+            // oneFields is the array of fields for the current column we are\n+            // processing. We need to add it to the corresponding list\n+            for (int i = 0; i < batchSize; i++) {\n+                resolvedBatch.get(i).add(oneFields[i]);", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxODc3MQ==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537818771", "bodyText": "will this be fixed in ORC, is there a JIRA to refer to ?", "author": "denalex", "createdAt": "2020-12-07T20:43:31Z", "path": "server/pxf-hive/src/main/java/org/apache/hadoop/hive/ql/io/orc/PxfRecordReaderImpl.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DateColumnVector;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+\n+import java.io.IOException;\n+\n+/**\n+ * This class fixes an issue introduced in ORC core library version 1.5.9", "originalCommit": "39587e272832b9d1c32a657a3173b14612a355ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5NDcxOA==", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537894718", "bodyText": "as the time of this fix, there's no jira. But I assume that this will be addressed at some point since newer ORC libraries break Hive functionality", "author": "frankgh", "createdAt": "2020-12-07T22:53:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxODc3MQ=="}], "type": "inlineReview"}, {"oid": "487ed12b0480e5ba84d84c04e2df4aaada875a2b", "url": "https://github.com/greenplum-db/pxf/commit/487ed12b0480e5ba84d84c04e2df4aaada875a2b", "message": "Address PR feedback", "committedDate": "2020-12-08T14:55:27Z", "type": "forcePushed"}, {"oid": "622e1b35bd90762a2cc9486a7eb039cdf55e272b", "url": "https://github.com/greenplum-db/pxf/commit/622e1b35bd90762a2cc9486a7eb039cdf55e272b", "message": "Add the InOperatorTransformer TreeVisitor (#500)\n\nAdd the InOperatorTransformer that will take the IN operator and\ntransform it into a chain of OR operators. This transformer can be used\nby filter builders that do not support the IN operator.", "committedDate": "2020-12-08T18:13:32Z", "type": "forcePushed"}, {"oid": "34e59b7490d424d3397b5c9bd8bfb4e4d721a546", "url": "https://github.com/greenplum-db/pxf/commit/34e59b7490d424d3397b5c9bd8bfb4e4d721a546", "message": "Add Support for reading ORC without Hive\n\nSupport reading ORC files directly from blob storage without accessing\nthe Hive metastore.\n\nSupport reading ORC files by creating the schema by index\n\nRead ORC files by creating a positional column matching as opposed to\nmatching columns by name. This allows users create a PXF external table\nin Greenplum that does not match the column names in the ORC schema.\nThsi is helpful for ORC files that were created by a writer earlier than\nHIVE-4243, where the column names would be stored as \"_col{index}\". This\nis noticeable in ORC files written by Hive 1.2 for example.\n\nTo use positional access, we introduce the MAP_BY_POSITION boolean\noption to be specified in the LOCATION of the table definition, for\nexample:\n\n```\nCREATE EXTERNAL TABLE pxf_orc_r (id int, name text)\nLOCATION\n('pxf://path/to/orc?PROFILE=hdfs:orc&MAP_BY_POSITION=true&SERVER=cdh')\nFORMAT 'CUSTOM' (formatter='pxfwritable_import');\n```\n\nSupport predicate pushdown with positional access\n\nWhen using positional access in ORC, the ORC schema might not match the\nGreenplum schema. This results in predicate pushdown being ignored\nbecause the pushdown information relies on matching the column names in\nthe Greenplum and ORC schemas. This commit fixes the pushdown when the\ncolumn names do not match.\n\nAdd ORC automation tests\n\nTest querying tables, subset of the ORC file.\nTest querying multiple ORC files that are not in the same order and have\na subset of the columns.\nTest MAP_BY_INDEX property.\nTest column projection and predicate pushdown.", "committedDate": "2020-12-08T18:20:21Z", "type": "commit"}, {"oid": "343369032fd35ce0706301bb4e137c6a920fe1c7", "url": "https://github.com/greenplum-db/pxf/commit/343369032fd35ce0706301bb4e137c6a920fe1c7", "message": "Add the InOperatorTransformer TreeVisitor (#500)\n\nAdd the InOperatorTransformer that will take the IN operator and\ntransform it into a chain of OR operators. This transformer can be used\nby filter builders that do not support the IN operator.", "committedDate": "2020-12-08T18:20:29Z", "type": "commit"}, {"oid": "343369032fd35ce0706301bb4e137c6a920fe1c7", "url": "https://github.com/greenplum-db/pxf/commit/343369032fd35ce0706301bb4e137c6a920fe1c7", "message": "Add the InOperatorTransformer TreeVisitor (#500)\n\nAdd the InOperatorTransformer that will take the IN operator and\ntransform it into a chain of OR operators. This transformer can be used\nby filter builders that do not support the IN operator.", "committedDate": "2020-12-08T18:20:29Z", "type": "forcePushed"}]}