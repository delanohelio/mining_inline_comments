{"pr_number": 881, "pr_title": "HDDS-3081. Replication manager should detect and correct containers which don't meet the replication policy", "pr_createdAt": "2020-04-28T17:35:23Z", "pr_url": "https://github.com/apache/ozone/pull/881", "timeline": [{"oid": "6204d0afbcecc5f01b32fee33ba55c312892d8d5", "url": "https://github.com/apache/ozone/commit/6204d0afbcecc5f01b32fee33ba55c312892d8d5", "message": "Extend interface to allow for checking container placement status", "committedDate": "2020-04-28T16:58:46Z", "type": "commit"}, {"oid": "cd482fda23d5f0d43fc2243da71343d6ed75b739", "url": "https://github.com/apache/ozone/commit/cd482fda23d5f0d43fc2243da71343d6ed75b739", "message": "Handle under replicated containers caused by mis-replication", "committedDate": "2020-04-28T17:28:53Z", "type": "commit"}, {"oid": "dbdca02d45d80ce82359ef2f3ccdff042e4b3402", "url": "https://github.com/apache/ozone/commit/dbdca02d45d80ce82359ef2f3ccdff042e4b3402", "message": "Fixed failing unit test", "committedDate": "2020-04-28T19:55:47Z", "type": "commit"}, {"oid": "03c70fd069e8c7ff8aaa315eb90f53118d5797f5", "url": "https://github.com/apache/ozone/commit/03c70fd069e8c7ff8aaa315eb90f53118d5797f5", "message": "retrigger build with empty commit", "committedDate": "2020-04-29T09:27:59Z", "type": "commit"}, {"oid": "f11797daf91c60436147eb82bd014839a923e5a3", "url": "https://github.com/apache/ozone/commit/f11797daf91c60436147eb82bd014839a923e5a3", "message": "Ensure handleOverReplicatedContainer correctly deals with mis-replicated blocks", "committedDate": "2020-04-29T12:30:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NTcxOQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418295719", "bodyText": "nit. toplogy -> topology", "author": "avijayanhwx", "createdAt": "2020-04-30T21:19:55Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should", "originalCommit": "f11797daf91c60436147eb82bd014839a923e5a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODUwNzMwNA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418507304", "bodyText": "fixed.", "author": "sodonnel", "createdAt": "2020-05-01T11:28:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NTcxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NTk5NQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418295995", "bodyText": "nit. space after 'maxLevel'.", "author": "avijayanhwx", "createdAt": "2020-04-30T21:20:31Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      // placement is always satisfied if there is at least one DN.\n+      return new ContainerPlacementStatusDefault(dns.size(), 1, 1);\n+    }\n+    // We have a network topology so calculate if it is satisfied or not.\n+    int numRacks = 1;\n+    final int maxLevel = topology.getMaxLevel();\n+    // The leaf nodes are all at max level, so the number of nodes at\n+    // leafLevel - 1 is the rack count\n+    numRacks = topology.getNumOfNodes(maxLevel- 1);", "originalCommit": "f11797daf91c60436147eb82bd014839a923e5a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODUwODQxNg==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418508416", "bodyText": "fixed.", "author": "sodonnel", "createdAt": "2020-05-01T11:33:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NTk5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NzEwMQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418297101", "bodyText": "At this point if we know that the placement is satisfied, do we need to create a class that has to \"compute\" that again? Can we use an inline object here?", "author": "avijayanhwx", "createdAt": "2020-04-30T21:23:03Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      // placement is always satisfied if there is at least one DN.\n+      return new ContainerPlacementStatusDefault(dns.size(), 1, 1);", "originalCommit": "f11797daf91c60436147eb82bd014839a923e5a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODUwODA4Ng==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418508086", "bodyText": "Are you suggesting that I should have a pre-created \"policy is met object\" and just return that same object here, rather than a new one each time? If so, that is a good idea. I have made that change assuming that is what you meant.", "author": "sodonnel", "createdAt": "2020-05-01T11:31:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NzEwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5ODcwMg==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418298702", "bodyText": "Should we be doing this getNumOfNodes logN operation for every container in a run of the Replication Manager? Doesn't this stay fairly static?", "author": "avijayanhwx", "createdAt": "2020-04-30T21:26:28Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      // placement is always satisfied if there is at least one DN.\n+      return new ContainerPlacementStatusDefault(dns.size(), 1, 1);\n+    }\n+    // We have a network topology so calculate if it is satisfied or not.\n+    int numRacks = 1;\n+    final int maxLevel = topology.getMaxLevel();\n+    // The leaf nodes are all at max level, so the number of nodes at\n+    // leafLevel - 1 is the rack count\n+    numRacks = topology.getNumOfNodes(maxLevel- 1);", "originalCommit": "f11797daf91c60436147eb82bd014839a923e5a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODUxMjUxOQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418512519", "bodyText": "That is a good point. This should be static except when nodes register or are removed, which is rare.\nI don't really want to pass \"rack count\" into this method, as in the future, we may have other implementations of this (eg node groups, upgrade domain etc), and putting that into the interface would not be ideal.\nI wonder if it would make sense to cache the count for a given level in NetworkTopologyImpl, rather than passing around the count here? The only things that change it, are add() and remove(), so we could easily have a hash map or list to store the count for a given level, and upon any add or remove wipe the cached values. This would be fairly simple to do and would benefit any other parts of the code which happen to call this existing method.\nDo you think that makes sense?", "author": "sodonnel", "createdAt": "2020-05-01T11:52:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5ODcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxMTAwNA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418311004", "bodyText": "nit. Can be new ArrayList<>(source).", "author": "avijayanhwx", "createdAt": "2020-04-30T21:54:26Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,61 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>();", "originalCommit": "f11797daf91c60436147eb82bd014839a923e5a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODUxNTcwOA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418515708", "bodyText": "Fixed.", "author": "sodonnel", "createdAt": "2020-05-01T12:05:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxMTAwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxNTg1MQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418315851", "bodyText": "If delta > 0, do we still need to validate the placement status? Cannot we just choose a DN in accordance to policy and issue the replication command? In that case, this logic can be executed only if we are \"misreplicated\" and not \"underreplicated\".", "author": "avijayanhwx", "createdAt": "2020-04-30T22:05:59Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,61 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>();\n+        targetReplicas.addAll(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final int additionalRacks", "originalCommit": "f11797daf91c60436147eb82bd014839a923e5a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODUxNjg4MQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418516881", "bodyText": "For now, mis-replicated count can only return 0 or 1, as replicas must be on at most 2 racks, and if there is an existing replica, it must be on a rack already.\nHowever, if we introduced a new policy \"must be on 3 racks\", then it would be possible to be under replicated with a delta of 1 and mis-replicated count could return 2, meaning we need to schedule more than 1 replica now.\nYou suggestion is valid if we only consider \"two racks\" as the only option, but I am hoping to cover off future enhancements here too.\nDo you think that makes sense?", "author": "sodonnel", "createdAt": "2020-05-01T12:11:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxNTg1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDk1NjIxNw==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420956217", "bodyText": "Yes, thanks for the explanation.", "author": "avijayanhwx", "createdAt": "2020-05-06T17:14:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxNTg1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxODI3OA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418318278", "bodyText": "Can we use something like findAny() here instead of breaking from the loop after first iteration? 'excess' is always > 0 here.", "author": "avijayanhwx", "createdAt": "2020-04-30T22:11:53Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -582,17 +629,71 @@ private void handleOverReplicatedContainer(final ContainerInfo container,\n           .filter(r -> !compareState(container.getState(), r.getState()))\n           .collect(Collectors.toList());\n \n-      //Move the unhealthy replicas to the front of eligible replicas to delete\n-      eligibleReplicas.removeAll(unhealthyReplicas);\n-      eligibleReplicas.addAll(0, unhealthyReplicas);\n-\n-      for (int i = 0; i < excess; i++) {\n-        sendDeleteCommand(container,\n-            eligibleReplicas.get(i).getDatanodeDetails(), true);\n+      // If there are unhealthy replicas, then we should remove them even if it\n+      // makes the container violate the placement policy, as excess unhealthy\n+      // containers are not really useful. It will be corrected later as a\n+      // mis-replicated container will be seen as under-replicated.\n+      for (ContainerReplica r : unhealthyReplicas) {", "originalCommit": "f11797daf91c60436147eb82bd014839a923e5a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODUxODc0Nw==", "url": "https://github.com/apache/ozone/pull/881#discussion_r418518747", "bodyText": "There could be more unhealthyReplicas than excess, and in that case, we want to break if we have removed the excess count. The loop will always make at least one pass.\nI am not sure how we can use findAny() to make this cleaner, while also decrementing excess.\nRemember that it is possible to have an excess of, say 2, with one unhealthy. So we would use the logic here to remove that unhealthy one unconditionally. Then fall into the other logic to remove 1 more.\nIf you have a cleaner idea about how to do this please post a short code snippet.", "author": "sodonnel", "createdAt": "2020-05-01T12:18:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxODI3OA=="}], "type": "inlineReview"}, {"oid": "a05ab1bdfd661b8a703c43faf629637e3e2e1fe2", "url": "https://github.com/apache/ozone/commit/a05ab1bdfd661b8a703c43faf629637e3e2e1fe2", "message": "Address review comments from Aravindan", "committedDate": "2020-05-01T12:29:19Z", "type": "commit"}, {"oid": "59489f5ea6ddb973be77afd19f079b4ac118b6e0", "url": "https://github.com/apache/ozone/commit/59489f5ea6ddb973be77afd19f079b4ac118b6e0", "message": "Fixed failing unit and checkstyle", "committedDate": "2020-05-01T14:21:37Z", "type": "commit"}, {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd", "url": "https://github.com/apache/ozone/commit/db29a6f2f90d8835623373c692d530da33ae17cd", "message": "Remove rack references inside replication manager", "committedDate": "2020-05-04T16:53:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDkwNQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420070905", "bodyText": "Considering that we have the NetworkTopology inside NodeManager, do we need this method?\nIt is used only in the validateContainerPlacement method where we can use NodeManager also.", "author": "fapifta", "createdAt": "2020-05-05T12:30:07Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +211,69 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network topology this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {", "originalCommit": "db29a6f2f90d8835623373c692d530da33ae17cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDEzNjMzOA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420136338", "bodyText": "The constructor for PipelinePlacementPolicy only receives nodeManager:\n      final PipelineStateManager stateManager, final ConfigurationSource conf) {\n\nBut the container one receives a networkTopology as part of its constructor:\n public SCMContainerPlacementRackAware(final NodeManager nodeManager,\n      final ConfigurationSource conf, final NetworkTopology networkTopology,\n      final boolean fallback, final SCMContainerPlacementMetrics metrics) {\n\nThe pipline Policy always asks the nodeManager for the policy, but the Container one uses the one passed. Ideally they should be the same, but I have a feeling some tests may make use of this \"feature\".\nThe other old placement policies (SCMContainerPlacementRandom, SCMContainerPlacementCapacity) have the same constructors, but they don't even use the topology at all.\nSo, I think we can get rid of this new method and then we should create another Jira to do the small refactor on all those older constructors to remove network topology from the parameter list and ensure they pull the topology from the node manager - what do you think?", "author": "sodonnel", "createdAt": "2020-05-05T14:06:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDkwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUwMzgyNA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420503824", "bodyText": "Sounds right, let's do so. \ud83d\udc4d", "author": "fapifta", "createdAt": "2020-05-06T01:46:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDk4NQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420070985", "bodyText": "This part of the method is clearly dealing with non-topology-aware stuff, and returns, while the rest is dealing with topology-aware stuff. Why we don't use polymorphism here to separate the two? I understand that this requires copying the topology-aware stuff to two places, but that is an other problem we have to deal with at one point.\nSo I believe this if should be the default implementation in this class, and the rest should be inside the rack aware and pipeline placement policy.\nThis would render the getRequiredRackCount to be unnecessary as well, as here it would be possible to just remove the expression yielding into true always for non-topology-aware policies, while the topology aware ones can have and use the constant in their implementation.", "author": "fapifta", "createdAt": "2020-05-05T12:30:15Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +211,69 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network topology this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      if (dns.size() > 0) {\n+        // placement is always satisfied if there is at least one DN.\n+        return validPlacement;\n+      } else {\n+        return invalidPlacement;\n+      }\n+    }", "originalCommit": "db29a6f2f90d8835623373c692d530da33ae17cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDEzOTI0Mg==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420139242", "bodyText": "I kept it here, to ensure the logic is not duplicated. Ideally, we move all the rack aware logic into a common parent and have PipelinePlacementPolicy and ContainerPlacementRackAware inherit from that. However there was some push back on that earlier, and its probably a non-trivial task.\nRather than duplicating the logic, I think it is better where it is and then we have HDDS-3079 (which probably needs renamed) to extract the rack aware stuff into one common parent.", "author": "sodonnel", "createdAt": "2020-05-05T14:10:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDk4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUwNDQxNA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420504414", "bodyText": "I am afraid that anyone who gets to HDDS-3079 will not find this out, maybe don't even check... It would be better to duplicate, as we feel the pain more and more of this, or at least to mention this method as well as the target for the JIRA ;)", "author": "fapifta", "createdAt": "2020-05-06T01:48:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDk4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY4MzE2NA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420683164", "bodyText": "I have reworded HDDS-3079 and added a specific note:\n\nNote that as part of this move, the rack specific logic in SCMCommonPlacementPolicy.validateContainerPlacement() should be extracted and moved into this common parent too.\n\nI really don't like the idea of duplicating more code into both classes and I think the above Jira now makes it clear what should happen when the rack specific logic moved.\nI don't think it is a trivial task to extract the rack specific part of both placement policies either, so I think it is best left to that other change rather than doing it with this change. It will make this change too big.", "author": "sodonnel", "createdAt": "2020-05-06T10:16:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDk4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcwMzc3OQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420703779", "bodyText": "\ud83d\udc4d", "author": "fapifta", "createdAt": "2020-05-06T10:56:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDk4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTA1MA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071050", "bodyText": "I understand that this is here to check if we have less than 2 racks with replicas, and if so, consider the container to be under replicated.\nI am unsure though if we want to ensure that a container is replicated to exactly 2 racks, it does not seems to be the case in the rack aware placement policy, but if so, then mis-replication should be checked somewhere else also to handle the case when a container is replicated to 3 racks due to for example removing a rack from config and place the nodes in a rack to other racks. (Probably this can go to isContainerOverReplicated(), if we want to check for this case.)", "author": "fapifta", "createdAt": "2020-05-05T12:30:22Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -393,8 +396,11 @@ private boolean isContainerHealthy(final ContainerInfo container,\n    */\n   private boolean isContainerUnderReplicated(final ContainerInfo container,\n       final Set<ContainerReplica> replicas) {\n+    boolean misReplicated = !getPlacementStatus(", "originalCommit": "db29a6f2f90d8835623373c692d530da33ae17cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE0MTA1Ng==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420141056", "bodyText": "My thinking on this, is that the policy states the replica \"must be on AT LEAST two racks\". I don't believe we should consider it over-replicated if it turns out to be on 3 somehow. That logic is consistent with HDFS, and probably provides better redundancy and locality (more racks is likely better in almost all cases).", "author": "sodonnel", "createdAt": "2020-05-05T14:12:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUwNDc2NA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420504764", "bodyText": "Sounds reasonable for me, let's leave it this way, I wanted to be sure we considered this when it came to my minds as a possible starting state.", "author": "fapifta", "createdAt": "2020-05-06T01:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071127", "bodyText": "This method in reality gives back the additional rack count required, not the additional replicas required.\nThis is why we need to check against the replication factor and the replica count difference also, while we provide the replication factor to the validation method, so it would be able to return the number of replicas really required.\nLet see the different cases:\n\nnon-topology aware placement policies: if replica count == replication factor we are good, otherwise the required additional replica count is replication factor - replica count, and we need that many extra replicas later from chooseDataNodes.\na topology aware placement policy with replication factor of 1 in case it is under replicated we already miss the block\na topology aware placement policy with replication factor of 2 in case it is under replicated either we miss the block, or we have it in one rack, and chooseDatanodes will give us the required one in a different rack.\ntopology aware placement policies with replication factor of 3 (or more):\n\nwhen we have 1 healthy replica, we need 2 additional replicas anyways, it is not really interesting in which rack.\nwhen we have 2 healthy replica, we need 1 additional replica from either one of the racks in which we already have a replica or in an other rack instead if we have the two replica in the same rack, this the policy already handles as it uses the excluded node list to anchor to proper racks in chooseDatanodes().\nwhen we have 3 healthy replica, but all three is in the same rack, then additionalReplicaRequired should yield to 1 still, and chooseDataNode should select a new set of DataNodes correctly, consisting of the nodes already have the replica, and one additional in a different rack. Which makes the container over replicated, so one excess replica will be removed later, after replication finished.\n\n\n\nSo if the additionalReplicaRequired works as the name suggests, we should not need to check replication factor and replica count here, but just rely on the value given back by the method.", "author": "fapifta", "createdAt": "2020-05-05T12:30:30Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,60 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final ContainerPlacementStatus placementStatus =\n+            containerPlacement.validateContainerPlacement(\n+                targetReplicas, replicationFactor);\n+        final int misRepDelta = placementStatus.additionalReplicaRequired();", "originalCommit": "db29a6f2f90d8835623373c692d530da33ae17cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE0NzE2MQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420147161", "bodyText": "The PlacementStatus.additionalReplicaRequired() method is intended to state how many replicas are need to meet the placement policy and not the replication factor. In other places (eg Recon) it might be useful to be able to get the \"additional racks required\" separately from under replication.\nI've added extra details in the PlacementStatus interface JavaDoc for additionalReplicaRequired to make that more clear.", "author": "sodonnel", "createdAt": "2020-05-05T14:20:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUxNzIxMg==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420517212", "bodyText": "Where we have a difference is two cases with replication factor of 3:\n\nwe have only 1 replica, we need two replica in one rack (delta = 2, misRepDelta = 1)\nwe have 2 replica in different racks, we need one replica and zero rack (delta = 1, misRepDelta = 0)\n\nI see this as something that still implicitly brings in rack awareness into ReplicationManager via an arbitrary int value, that comes from a method which has a name, that - for me at least - does not suggest this subtle but fundamental difference at all, that is why I started to check into this and suggested to get rid of it. I don't see the \"it might be good to report this for Recon\" as a good reason to increase the complexity, if we want this in Recon, we can add a new status implementation with this value and its accessor added for rack aware policies and we can grab the value from that, if it will be needed.\nFor the same reason (implicit rack awareness) I am unhappy about the check in lines from L562, and suggested a way there if we can achieve it.\nI understand the statement \"a container has a valid placement\", as \"we have the replication factor amount of replicas placed as we want them to be placed\". And my suggestion reflects this understanding. Isn't this the good way to look at it? If not, why, what is the gain if we look it differently?", "author": "fapifta", "createdAt": "2020-05-06T02:45:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDcwNzQwMA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420707400", "bodyText": "I understand the statement \"a container has a valid placement\", as \"we have the replication factor amount of replicas placed as we want them to be placed\"\n\nPlacement and replication are two different things, and I think it is good to keep them that way.\nIf you consider HDFS fsck - if a container is under replicated, it will report that. If it is mis-replicated it will report that. If it is both, it reports them both separately, along with their \"missing count\" for each. In HDFS for racks, there can only ever be 1 additional rack needed. But with upgrade domain, there can be possibly two additional domains needed.\n\nI see this as something that still implicitly brings in rack awareness into ReplicationManager via an arbitrary int value\n\nIt brings mis-replication into the RM, not rack awareness, and this is OK. A container needs to meet several goals to be properly replicated - the correct number of healthy replicas and the correct placement and this is what the replication manager checks.\nIn Recon, we want to be able to give an indicator about mis-replication and under replication separately in a similar way and ideally with Recon we just load the exact same placement policy as the cluster is using and call the same methods.\n\nI am unhappy about the check in lines from L562, and suggested a way there if we can achieve it.\n\nThis check is still useful. If we combined the two rules as you are suggesting so \"addtitionalReplicaRequired\" return a single number representing delta and misRep, then in the case where you have delta=1, misRep=1 (ie two replicas on one rack), then if the newly selected node, due to fallback, gives us 3 replicas on one rack. This is OK, but the next time this container is checked, we have 3 replicas on one rack, and the logic will return 1, telling us to create another. We need to be sure this improves the mis-replication before creating a 4th replica, as that will make the container over-replicated.\nOf course, we can check:\n\nIf we test 2 replicas with 1 rack, the logic will return 1.\nIf we test 3 replicas with 1 rack, the logic will return 1.\nIf we test 4 replicas with 1 rack, the logic will return 1.\n\nSo if we push the under and mis-replication numbers together, we cannot really see what is going on, and we will need to test the replication count too I think anyway.\nI can give these methods new names or improve the java docs a bit more to make it clear that placement != replication and the number returned from the ContainerPlacementStatus object does not reflect replicas. I've made a small change to the doc now and will push it up:\n  /**\n   * Returns a boolean indicating if the container replicas meet the desired\n   * placement policy. That is, they are placed on a sufficient number of\n   * racks, or node groups etc. This does not check if the container is over\n   * or under replicated, as it is possible for a container to have enough\n   * replicas and still not meet the placement rules.\n   * @return True if the containers meet the policy. False otherwise.\n   */\n  boolean isPolicySatisfied();\n\n  /**\n   * Returns an String describing why a container does not meet the placement\n   * policy.\n   * @return String indicating the reason the policy is not satisfied or null if\n   *         the policy is satisfied.\n   */\n  String misReplicatedReason();\n\n  /**\n   * If the container does not meet the placement policy, return an integer\n   * indicating how many additional replicas are required so the container\n   * meets the placement policy. Otherwise return zero.\n   * Note the count returned are the number of replicas needed to meet the\n   * placement policy. The container may need additional replicas if it is\n   * under replicated. The container could also have sufficient replicas but\n   * require more to make it meet the policy, as the existing replicas are not\n   * placed correctly.\n   * @return The number of additional replicas required, or zero.\n   */\n  int additionalReplicaRequired();\n\nI could change the name of additionalReplicaRequired() to be something like replicasRequiredForPlacement(), if yo think it makes it more clear.", "author": "sodonnel", "createdAt": "2020-05-06T11:04:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc2ODU4NQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420768585", "bodyText": "Ah ok, finally I understood :) Sorry for being a bit hard headed on this one, but it seems I haven't thought of the ambiguity in the mentioned case which is needed to be handled unless we will run into an infinite loop with the current fallback logic. I still feel that this we might revise when/if we touch fallback logic, but unsure :)\nI would be glad if the name can be changed though, I don't have a better idea then the suggested one though... maybe misReplicationCount()?\nAlso in the javadoc, in the last sentence before @return do you mean \"placed incorrectly\"?", "author": "fapifta", "createdAt": "2020-05-06T12:57:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg0NTk1MA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420845950", "bodyText": "Do you mean this line:\n\nThe container could also have sufficient replicas but require more to make it meet the policy, as the existing replicas are not placed correctly.\n\nDid you miss the \"not\" in there? :)\nI will change this method to be called misReplicationCount() which hopefully makes it more clear.", "author": "sodonnel", "createdAt": "2020-05-06T14:41:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDk0OTk3Mw==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420949973", "bodyText": "oh... I did, it was hiding behind a scrollbar I haven't scrolled here in the comment stream :)\nLooks good this way ;)", "author": "fapifta", "createdAt": "2020-05-06T17:04:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTE2Nw==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071167", "bodyText": "Do we add the in-flight replications twice here this way, or I am missing something?", "author": "fapifta", "createdAt": "2020-05-05T12:30:34Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,60 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final ContainerPlacementStatus placementStatus =\n+            containerPlacement.validateContainerPlacement(\n+                targetReplicas, replicationFactor);\n+        final int misRepDelta = placementStatus.additionalReplicaRequired();\n+        final int replicasNeeded\n+            = delta < misRepDelta ? misRepDelta : delta;\n+\n         final List<DatanodeDetails> excludeList = replicas.stream()\n             .map(ContainerReplica::getDatanodeDetails)\n             .collect(Collectors.toList());\n+        excludeList.addAll(replicationInFlight);", "originalCommit": "db29a6f2f90d8835623373c692d530da33ae17cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE1MjMyMA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420152320", "bodyText": "Well spotted. I think I can remove the logic after this line, as I already have created that list earlier in the method now:\n        if (actionList != null) {\n          actionList.stream().map(r -> r.datanode)\n              .forEach(excludeList::add);\n        }", "author": "sodonnel", "createdAt": "2020-05-05T14:27:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTE2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUwOTY3MQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420509671", "bodyText": "\ud83d\udc4d", "author": "fapifta", "createdAt": "2020-05-06T02:12:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTE2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTIxMQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071211", "bodyText": "The rack-aware policy's chooseDatanodes() method we use ensures that based on the excluded nodes list we get back a proper list, I believe we do not need to check it twice Are you aware of any cases where this is not true? Based on my understanding of the logic there I would expect all future placement policies to do so, as other existing ones implicitly do so now. If necessary we can add this to the contract of the interface, though this is an interesting property of the rack-aware policy that would worth some re-thinking but that is out of scope for now.", "author": "fapifta", "createdAt": "2020-05-05T12:30:38Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,60 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final ContainerPlacementStatus placementStatus =\n+            containerPlacement.validateContainerPlacement(\n+                targetReplicas, replicationFactor);\n+        final int misRepDelta = placementStatus.additionalReplicaRequired();\n+        final int replicasNeeded\n+            = delta < misRepDelta ? misRepDelta : delta;\n+\n         final List<DatanodeDetails> excludeList = replicas.stream()\n             .map(ContainerReplica::getDatanodeDetails)\n             .collect(Collectors.toList());\n+        excludeList.addAll(replicationInFlight);\n         List<InflightAction> actionList = inflightReplication.get(id);\n         if (actionList != null) {\n           actionList.stream().map(r -> r.datanode)\n               .forEach(excludeList::add);\n         }\n         final List<DatanodeDetails> selectedDatanodes = containerPlacement\n-            .chooseDatanodes(excludeList, null, delta,\n+            .chooseDatanodes(excludeList, null, replicasNeeded,\n                 container.getUsedBytes());\n-\n-        LOG.info(\"Container {} is under replicated. Expected replica count\" +\n-                \" is {}, but found {}.\", id, replicationFactor,\n-            replicationFactor - delta);\n-\n-        for (DatanodeDetails datanode : selectedDatanodes) {\n-          sendReplicateCommand(container, datanode, source);\n+        if (delta > 0) {\n+          LOG.info(\"Container {} is under replicated. Expected replica count\" +\n+                  \" is {}, but found {}.\", id, replicationFactor,\n+              replicationFactor - delta);\n+        }\n+        int newMisRepDelta = misRepDelta;\n+        if (misRepDelta > 0) {\n+          LOG.info(\"Container: {}. {}\",\n+              id, placementStatus.misReplicatedReason());\n+          // Check if the new target nodes (original plus newly selected nodes)\n+          // makes the placement policy valid.\n+          targetReplicas.addAll(selectedDatanodes);\n+          newMisRepDelta = containerPlacement.validateContainerPlacement(\n+              targetReplicas, replicationFactor).additionalReplicaRequired();", "originalCommit": "db29a6f2f90d8835623373c692d530da33ae17cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDIwMzAwOA==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420203008", "bodyText": "The rack-aware policy's chooseDatanodes() method we use ensures that based on the excluded nodes list we get back a proper list\n\nThis is where it gets tricky. There is a fallback (certainly in the pipeline placement policy, but I have not checked the ContainerPlacementRackAware in complete detail) where if the placement policy cannot select enough racks, it will give back nodes which do not meet the placement.\nThis is to ensure (in the case of pipelinePlacementPolicy) that the cluster can keep on operating if only 1 rack is available for some reason. For the closed container placement, this should be the case too, as if you are down to 1 replica, it is better it gets replicated onto more nodes on the same rack, rather than risk losing the data.\nWe probably need a future change to scrub the bad pipelines if this fall back happens, while replication manager should deal with closed containers.", "author": "sodonnel", "createdAt": "2020-05-05T15:33:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUxMjk3Nw==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420512977", "bodyText": "Right, I see your point... I am not sure, haven't checked, but I guess based on NetworkTopology, and maybe NodeManager we can tell how many healthy racks we have, and if we have one, and the fallback is enabled, then we can report the number of required replicas to be zero in the case we got down to 1 live rack only, while if fallback is not enabled, then we should report 1 additional replica to be required, and give a warning in the logs as the policy can not be met.\nI think it would be more clear to handle that in the validation of rack aware policies if we can. What do you think?\nNote that we are hardcoding fallback = true at the moment at the only place where we call the policy factory in non-test code.", "author": "fapifta", "createdAt": "2020-05-06T02:26:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY5MTM0Mw==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420691343", "bodyText": "The nodemanager + topology can easily tell us the total racks every registered since SCM was started. But if you have a case where the cluster had 2 racks and than a rack has stopped, that is not so easy to see. You would need to filter all the non-healthy nodes, which is basically what the placement policy does. Then it falls back to give you a non-placement valid node if it needs to. It is important it does this, as it needs to handle under-replication too via the same chooseNodes methods.\nThe whole area around the fallback needs further thought in general. If we fall to 1 rack, on a cluster that had 2 racks, then its safe to assume the other rack should come back, and until it does the containers are mis-replicated (and probably under-replicated too) and should be reported as such.\nIn both cases a warning would be written to the logs stating:\nLOG.warn(\"Container {} is mis-replicated, requiring {} additional \" +\n              \"replicas. After selecting new nodes, mis-replication has not \" +\n              \"improved. No additional replicas will be scheduled\",\n              id, misRepDelta);\n\nThis is really an edge case (2 rack cluster going into 1 rack) and I think we should leave this as it is for now as there are surely other things need to be thought about here in the wider network topology logic.", "author": "sodonnel", "createdAt": "2020-05-06T10:32:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3NTY1NQ==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420775655", "bodyText": "Ok, let's do so for now, but if we can, try to keep this in mind as a possible improvement, as the current check feels a bit hacky still :)", "author": "fapifta", "createdAt": "2020-05-06T13:08:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg0NjY5Mg==", "url": "https://github.com/apache/ozone/pull/881#discussion_r420846692", "bodyText": "Yea, there are improvements needed in this area for sure. I'd like to see how it works in practice before we jump in with too many changes, but its certain on my radar for improvements.", "author": "sodonnel", "createdAt": "2020-05-06T14:42:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTIxMQ=="}], "type": "inlineReview"}, {"oid": "8ca5723cd4e6e72cb18a8125d7be243cd623936c", "url": "https://github.com/apache/ozone/commit/8ca5723cd4e6e72cb18a8125d7be243cd623936c", "message": "Address comments from Pifta", "committedDate": "2020-05-05T15:39:10Z", "type": "commit"}, {"oid": "332939031984154c0c54bf5cded748aa3723893e", "url": "https://github.com/apache/ozone/commit/332939031984154c0c54bf5cded748aa3723893e", "message": "Fixed failing unit test", "committedDate": "2020-05-05T17:17:11Z", "type": "commit"}, {"oid": "b44f67b314b8216fa32c3816e129de6ec9b48491", "url": "https://github.com/apache/ozone/commit/b44f67b314b8216fa32c3816e129de6ec9b48491", "message": "Further javadoc changes", "committedDate": "2020-05-06T11:04:48Z", "type": "commit"}, {"oid": "f69b724bdc84295cbf7b8faba038229dd27da948", "url": "https://github.com/apache/ozone/commit/f69b724bdc84295cbf7b8faba038229dd27da948", "message": "Change method additionalReplicaRequired to misReplicationCount", "committedDate": "2020-05-06T15:07:28Z", "type": "commit"}, {"oid": "9cee52e6e421b5aa408cf0c1cd5b88b13906ef9a", "url": "https://github.com/apache/ozone/commit/9cee52e6e421b5aa408cf0c1cd5b88b13906ef9a", "message": "Trigger build", "committedDate": "2020-05-07T10:57:52Z", "type": "commit"}, {"oid": "adf8bcd1f1ee7fe763d8cb4924482aaf860f61c0", "url": "https://github.com/apache/ozone/commit/adf8bcd1f1ee7fe763d8cb4924482aaf860f61c0", "message": "Trigger build", "committedDate": "2020-05-07T13:00:54Z", "type": "commit"}]}