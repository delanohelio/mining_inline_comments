{"pr_number": 742, "pr_title": "HDDS-3217. Datanode startup is slow due to iterating container DB 2-3 times.", "pr_createdAt": "2020-03-31T19:46:33Z", "pr_url": "https://github.com/apache/ozone/pull/742", "timeline": [{"oid": "6848e5f3220150fc0a1c84b76f533ba3470cfbc7", "url": "https://github.com/apache/ozone/commit/6848e5f3220150fc0a1c84b76f533ba3470cfbc7", "message": "remove unused import.", "committedDate": "2020-04-07T04:42:16Z", "type": "forcePushed"}, {"oid": "32792479d6bbf3cdbea3c8141833846bf7650160", "url": "https://github.com/apache/ozone/commit/32792479d6bbf3cdbea3c8141833846bf7650160", "message": "remove unused import.", "committedDate": "2020-04-16T01:19:17Z", "type": "forcePushed"}, {"oid": "d2140125022148c2f508b345dfbe4e04cb1bd01b", "url": "https://github.com/apache/ozone/commit/d2140125022148c2f508b345dfbe4e04cb1bd01b", "message": "add changes missing during handling layout version", "committedDate": "2020-04-16T21:22:15Z", "type": "forcePushed"}, {"oid": "1a45c09e23ed7bd56d35fefd206a2cc580c43bd4", "url": "https://github.com/apache/ozone/commit/1a45c09e23ed7bd56d35fefd206a2cc580c43bd4", "message": "explain not to use newer version automatically", "committedDate": "2020-04-17T19:31:23Z", "type": "forcePushed"}, {"oid": "3b7ada28fa19992945a81fe7a16d76db43f69bb9", "url": "https://github.com/apache/ozone/commit/3b7ada28fa19992945a81fe7a16d76db43f69bb9", "message": "revert unneeded", "committedDate": "2020-04-20T23:20:59Z", "type": "forcePushed"}, {"oid": "2190396738190e2bf7b92ad93a4e15625fa441d0", "url": "https://github.com/apache/ozone/commit/2190396738190e2bf7b92ad93a4e15625fa441d0", "message": "revert unneeded", "committedDate": "2020-04-23T00:44:51Z", "type": "forcePushed"}, {"oid": "7e69e9b230a8911902e8e9d171e0fbcafe51d707", "url": "https://github.com/apache/ozone/commit/7e69e9b230a8911902e8e9d171e0fbcafe51d707", "message": "Add test for container reader", "committedDate": "2020-05-05T19:06:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE0NTc5MA==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421145790", "bodyText": "updateDeleteTransactionId() updates the deleteTransactionId only if it more than the current deleteTransactionId. This check is not done before updating the DB. This could lead to different states in DB and in memory, right? I am not sure when it could be possible that new deleteTransactionId is less than the current one.\nWe could probably combine containerData.updateDeleteTransactionId and containerData.incrPendingDeletionBlocks with the batch put operation by making these functions return the updated values.", "author": "hanishakoneru", "createdAt": "2020-05-06T23:14:47Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteBlocksCommandHandler.java", "diffHunk": "@@ -251,12 +253,23 @@ private void deleteKeyValueContainerBlocks(\n         }\n       }\n \n-      containerDB.getStore()\n-          .put(DFSUtil.string2Bytes(OzoneConsts.DELETE_TRANSACTION_KEY_PREFIX),\n-              Longs.toByteArray(delTX.getTxID()));\n-      containerData\n-          .updateDeleteTransactionId(delTX.getTxID());\n-      // update pending deletion blocks count in in-memory container status\n+      // Finally commit the DB counters.\n+      BatchOperation batchOperation = new BatchOperation();\n+\n+      // Update in DB pending delete key count and delete transaction ID.\n+      batchOperation.put(DB_CONTAINER_DELETE_TRANSACTION_KEY,\n+          Longs.toByteArray(delTX.getTxID()));\n+\n+      batchOperation.put(DB_PENDING_DELETE_BLOCK_COUNT_KEY, Longs.toByteArray(\n+          containerData.getNumPendingDeletionBlocks() + newDeletionBlocks));\n+\n+      containerDB.getStore().writeBatch(batchOperation);\n+\n+\n+      // update pending deletion blocks count and delete transaction ID in\n+      // in-memory container status\n+      containerData.updateDeleteTransactionId(delTX.getTxID());\n+", "originalCommit": "7e69e9b230a8911902e8e9d171e0fbcafe51d707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTY5NzI1NA==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421697254", "bodyText": "Good catch. Added code to update DB only when delete TxID > current Tx ID", "author": "bharatviswa504", "createdAt": "2020-05-07T18:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE0NTc5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1MzM0NQ==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421153345", "bodyText": "Just a NIT - We can optimize the code by using blockIter.hasNext() as the while condition check instead of instantiating another variable.", "author": "hanishakoneru", "createdAt": "2020-05-06T23:38:17Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -150,29 +160,109 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n-    try(ReferenceCountedDB metadata =\n-            BlockUtils.getDB(kvContainerData, config)) {\n-      long bytesUsed = 0;\n-      List<Map.Entry<byte[], byte[]>> liveKeys = metadata.getStore()\n-          .getRangeKVs(null, Integer.MAX_VALUE,\n-              MetadataKeyFilters.getNormalKeyFilter());\n \n-      bytesUsed = liveKeys.parallelStream().mapToLong(e-> {\n-        BlockData blockData;\n+    boolean isBlockMetadataSet = false;\n+\n+    try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n+        config)) {\n+\n+      // Set pending deleted block count.\n+      byte[] pendingDeleteBlockCount =\n+          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      if (pendingDeleteBlockCount != null) {\n+        kvContainerData.incrPendingDeletionBlocks(\n+            Ints.fromByteArray(pendingDeleteBlockCount));\n+      } else {\n+        // Set pending deleted block count.\n+        MetadataKeyFilters.KeyPrefixFilter filter =\n+            new MetadataKeyFilters.KeyPrefixFilter()\n+                .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\n+        int numPendingDeletionBlocks =\n+            containerDB.getStore().getSequentialRangeKVs(null,\n+                Integer.MAX_VALUE, filter)\n+                .size();\n+        kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\n+      }\n+\n+      // Set delete transaction id.\n+      byte[] delTxnId =\n+          containerDB.getStore().get(DB_CONTAINER_DELETE_TRANSACTION_KEY);\n+      if (delTxnId != null) {\n+        kvContainerData\n+            .updateDeleteTransactionId(Longs.fromByteArray(delTxnId));\n+      }\n+\n+      // Set BlockCommitSequenceId.\n+      byte[] bcsId = containerDB.getStore().get(\n+          DB_BLOCK_COMMIT_SEQUENCE_ID_KEY);\n+      if (bcsId != null) {\n+        kvContainerData\n+            .updateBlockCommitSequenceId(Longs.fromByteArray(bcsId));\n+      }\n+\n+      // Set bytes used.\n+      // commitSpace for Open Containers relies on usedBytes\n+      byte[] bytesUsed =\n+          containerDB.getStore().get(DB_CONTAINER_BYTES_USED_KEY);\n+      if (bytesUsed != null) {\n+        isBlockMetadataSet = true;\n+        kvContainerData.setBytesUsed(Longs.fromByteArray(bytesUsed));\n+      }\n+\n+      // Set block count.\n+      byte[] blockCount = containerDB.getStore().get(DB_BLOCK_COUNT_KEY);\n+      if (blockCount != null) {\n+        isBlockMetadataSet = true;\n+        kvContainerData.setKeyCount(Longs.fromByteArray(blockCount));\n+      }\n+    }\n+\n+    if (!isBlockMetadataSet) {\n+      initializeUsedBytesAndBlockCount(kvContainerData);\n+    }\n+  }\n+\n+\n+  /**\n+   * Initialize bytes used and block count.\n+   * @param kvContainerData\n+   * @throws IOException\n+   */\n+  private static void initializeUsedBytesAndBlockCount(\n+      KeyValueContainerData kvContainerData) throws IOException {\n+\n+    long blockCount = 0;\n+    try (KeyValueBlockIterator blockIter = new KeyValueBlockIterator(\n+        kvContainerData.getContainerID(),\n+        new File(kvContainerData.getContainerPath()))) {\n+      long usedBytes = 0;\n+\n+\n+      boolean success = true;\n+      while (success) {", "originalCommit": "7e69e9b230a8911902e8e9d171e0fbcafe51d707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTY5MDUxOQ==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421690519", "bodyText": "This is done like this because hasNext will return StorageContainerException when unable to parse block data. So, that we will not bail out and still continue the iteration.", "author": "bharatviswa504", "createdAt": "2020-05-07T17:59:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1MzM0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1NTQ3NQ==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421155475", "bodyText": "If there was an error in parsing the block data, we might end up updating incorrect values for usedBytes and blockCount. Previously, we would fail to parse that container and not add it to containerSet. With this change, we would add wrongly parsed containers also to containerSet.", "author": "hanishakoneru", "createdAt": "2020-05-06T23:44:59Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -150,29 +160,109 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n-    try(ReferenceCountedDB metadata =\n-            BlockUtils.getDB(kvContainerData, config)) {\n-      long bytesUsed = 0;\n-      List<Map.Entry<byte[], byte[]>> liveKeys = metadata.getStore()\n-          .getRangeKVs(null, Integer.MAX_VALUE,\n-              MetadataKeyFilters.getNormalKeyFilter());\n \n-      bytesUsed = liveKeys.parallelStream().mapToLong(e-> {\n-        BlockData blockData;\n+    boolean isBlockMetadataSet = false;\n+\n+    try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n+        config)) {\n+\n+      // Set pending deleted block count.\n+      byte[] pendingDeleteBlockCount =\n+          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      if (pendingDeleteBlockCount != null) {\n+        kvContainerData.incrPendingDeletionBlocks(\n+            Ints.fromByteArray(pendingDeleteBlockCount));\n+      } else {\n+        // Set pending deleted block count.\n+        MetadataKeyFilters.KeyPrefixFilter filter =\n+            new MetadataKeyFilters.KeyPrefixFilter()\n+                .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\n+        int numPendingDeletionBlocks =\n+            containerDB.getStore().getSequentialRangeKVs(null,\n+                Integer.MAX_VALUE, filter)\n+                .size();\n+        kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\n+      }\n+\n+      // Set delete transaction id.\n+      byte[] delTxnId =\n+          containerDB.getStore().get(DB_CONTAINER_DELETE_TRANSACTION_KEY);\n+      if (delTxnId != null) {\n+        kvContainerData\n+            .updateDeleteTransactionId(Longs.fromByteArray(delTxnId));\n+      }\n+\n+      // Set BlockCommitSequenceId.\n+      byte[] bcsId = containerDB.getStore().get(\n+          DB_BLOCK_COMMIT_SEQUENCE_ID_KEY);\n+      if (bcsId != null) {\n+        kvContainerData\n+            .updateBlockCommitSequenceId(Longs.fromByteArray(bcsId));\n+      }\n+\n+      // Set bytes used.\n+      // commitSpace for Open Containers relies on usedBytes\n+      byte[] bytesUsed =\n+          containerDB.getStore().get(DB_CONTAINER_BYTES_USED_KEY);\n+      if (bytesUsed != null) {\n+        isBlockMetadataSet = true;\n+        kvContainerData.setBytesUsed(Longs.fromByteArray(bytesUsed));\n+      }\n+\n+      // Set block count.\n+      byte[] blockCount = containerDB.getStore().get(DB_BLOCK_COUNT_KEY);\n+      if (blockCount != null) {\n+        isBlockMetadataSet = true;\n+        kvContainerData.setKeyCount(Longs.fromByteArray(blockCount));\n+      }\n+    }\n+\n+    if (!isBlockMetadataSet) {\n+      initializeUsedBytesAndBlockCount(kvContainerData);\n+    }\n+  }\n+\n+\n+  /**\n+   * Initialize bytes used and block count.\n+   * @param kvContainerData\n+   * @throws IOException\n+   */\n+  private static void initializeUsedBytesAndBlockCount(\n+      KeyValueContainerData kvContainerData) throws IOException {\n+\n+    long blockCount = 0;\n+    try (KeyValueBlockIterator blockIter = new KeyValueBlockIterator(\n+        kvContainerData.getContainerID(),\n+        new File(kvContainerData.getContainerPath()))) {\n+      long usedBytes = 0;\n+\n+\n+      boolean success = true;\n+      while (success) {\n         try {\n-          blockData = BlockUtils.getBlockData(e.getValue());\n-          return blockData.getSize();\n+          if (blockIter.hasNext()) {\n+            BlockData block = blockIter.nextBlock();\n+            long blockLen = 0;\n+\n+            List< ContainerProtos.ChunkInfo > chunkInfoList = block.getChunks();\n+            for (ContainerProtos.ChunkInfo chunk : chunkInfoList) {\n+              ChunkInfo info = ChunkInfo.getFromProtoBuf(chunk);\n+              blockLen += info.getLen();\n+            }\n+\n+            usedBytes += blockLen;\n+            blockCount++;\n+          } else {\n+            success = false;\n+          }\n         } catch (IOException ex) {\n-          return 0L;\n+          LOG.error(\"Failed to parse block data for Container {}\",\n+              kvContainerData.getContainerID());", "originalCommit": "7e69e9b230a8911902e8e9d171e0fbcafe51d707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTY5MzMxMQ==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421693311", "bodyText": "Previously also we used to do this. Slightly modified the code. And I think for one block corruption, if we don't add it to container set, then BackGround Container Scanner will never know about this container, and it will never be fixed. So, I think adding to container set is right thing to do here. Let me know your thoughts?\n     bytesUsed = liveKeys.parallelStream().mapToLong(e-> {\n        BlockData blockData;\n        try {\n          blockData = BlockUtils.getBlockData(e.getValue());\n          return blockData.getSize();\n        } catch (IOException ex) {\n          return 0L;\n        }\n      }).sum();", "author": "bharatviswa504", "createdAt": "2020-05-07T18:03:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1NTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzY3Mg==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421837672", "bodyText": "I see that previously initializeUsedBytes() was done only for OPEN containers. And for others, we would just get the BlockData and update usedBytes.\nWhat I mean to say is that the behavior has changed with this patch. I might have missed some background here. Can you please explain why the change.", "author": "hanishakoneru", "createdAt": "2020-05-07T22:47:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1NTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MDU0Mw==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421840543", "bodyText": "Previously we used to compute twice, one using the  logic above and for open containers using different logic which is initializeUsedBytes. Now we are doing only once by using initializeUsedBytesAndBlockCount logic.\nSo, previously if a container is open, we used to compute bytesUsed twice and now with this only once, and for all containers computing using initializeUsedBytesAndBlockCount. And reason for not using the above logic is we get all blockData to in-memory at once and compute it. Now with current logic using iterators and computing size block by block.", "author": "bharatviswa504", "createdAt": "2020-05-07T22:56:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1NTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MjQ5Mw==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421842493", "bodyText": "And also at the end, even new logic which is using iterator finally uses BlockUtils.getBlockData to getNext Block. I feel the current way (PR proposed) of iterating and computing the size is cleaner way, instead of getting all blocks into in-memory at once.", "author": "bharatviswa504", "createdAt": "2020-05-07T23:02:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1NTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0ODA2MA==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421848060", "bodyText": "Thanks for the explanation Bharat. Makes sense.", "author": "hanishakoneru", "createdAt": "2020-05-07T23:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1NTQ3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1OTUzMw==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421159533", "bodyText": "There are two places where blocks are deleted - BlockManagerImpl#deleteBlock and BlockDeletingService.\nI am trying to understand when which is used.\nWhen is BlockManagerImpl#deleteBlock called? Is it always called after the chunk is deleted? Otherwise there would not be any change in bytesUsed.", "author": "hanishakoneru", "createdAt": "2020-05-06T23:58:01Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -224,11 +239,20 @@ public void deleteBlock(Container container, BlockID blockID) throws\n       // are not atomic. Leaving it here since the impact is refusing\n       // to delete a Block which might have just gotten inserted after\n       // the get check.\n-      byte[] kKey = Longs.toByteArray(blockID.getLocalID());\n+      byte[] blockKey = Longs.toByteArray(blockID.getLocalID());\n \n       getBlockByID(db, blockID);\n-      db.getStore().delete(kKey);\n-      // Decrement blockcount here\n+\n+      // Update DB to delete block and set block count and bytes used.\n+      BatchOperation batch = new BatchOperation();\n+      batch.delete(blockKey);\n+      batch.put(DB_CONTAINER_BYTES_USED_KEY,\n+          Longs.toByteArray(container.getContainerData().getBytesUsed()));", "originalCommit": "7e69e9b230a8911902e8e9d171e0fbcafe51d707", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTY5NjE1NQ==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421696155", "bodyText": "Right now deleteBlock in BlockManagerImpl is not used, BlockDeletingService delete Chunks and then delete the blocks which are marked for deleted from container DB.\nAnd also updating bytes used is not correct here, so only updated block count here. As anyway bytes used is taken care during delete chunk.\n\n        // Once files are deleted... replace deleting entries with deleted\n        // entries\n        BatchOperation batch = new BatchOperation();\n        succeedBlocks.forEach(entry -> {\n          String blockId =\n              entry.substring(OzoneConsts.DELETING_KEY_PREFIX.length());\n          String deletedEntry = OzoneConsts.DELETED_KEY_PREFIX + blockId;\n          batch.put(DFSUtil.string2Bytes(deletedEntry),\n              DFSUtil.string2Bytes(blockId));\n          batch.delete(DFSUtil.string2Bytes(entry));\n        });", "author": "bharatviswa504", "createdAt": "2020-05-07T18:08:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE1OTUzMw=="}], "type": "inlineReview"}, {"oid": "6c2807cb3b9a1df829fa3eac5e6f4db3c248c374", "url": "https://github.com/apache/ozone/commit/6c2807cb3b9a1df829fa3eac5e6f4db3c248c374", "message": "review comments", "committedDate": "2020-05-07T18:09:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTUyMw==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421819523", "bodyText": "Should be if (delTx.getTxID > containerData.getDeleteTransactionId())", "author": "hanishakoneru", "createdAt": "2020-05-07T21:59:28Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteBlocksCommandHandler.java", "diffHunk": "@@ -256,9 +256,14 @@ private void deleteKeyValueContainerBlocks(\n       // Finally commit the DB counters.\n       BatchOperation batchOperation = new BatchOperation();\n \n-      // Update in DB pending delete key count and delete transaction ID.\n-      batchOperation.put(DB_CONTAINER_DELETE_TRANSACTION_KEY,\n-          Longs.toByteArray(delTX.getTxID()));\n+\n+      // In memory is updated only when existing delete transactionID is\n+      // greater.\n+      if (containerData.getDeleteTransactionId() > delTX.getTxID()) {\n+        // Update in DB pending delete key count and delete transaction ID.", "originalCommit": "6c2807cb3b9a1df829fa3eac5e6f4db3c248c374", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzMTU4NA==", "url": "https://github.com/apache/ozone/pull/742#discussion_r421831584", "bodyText": "Done", "author": "bharatviswa504", "createdAt": "2020-05-07T22:30:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTUyMw=="}], "type": "inlineReview"}, {"oid": "697b08f0ef753cd152e5727b5a7465e237a3b310", "url": "https://github.com/apache/ozone/commit/697b08f0ef753cd152e5727b5a7465e237a3b310", "message": "HDDS-3217. Datanode startup is slow due to iterating container DB 2-3 times.", "committedDate": "2020-05-07T22:28:05Z", "type": "commit"}, {"oid": "66f33262d508717210d389b137b022eda952c40d", "url": "https://github.com/apache/ozone/commit/66f33262d508717210d389b137b022eda952c40d", "message": "review comments", "committedDate": "2020-05-07T22:29:56Z", "type": "commit"}, {"oid": "66f33262d508717210d389b137b022eda952c40d", "url": "https://github.com/apache/ozone/commit/66f33262d508717210d389b137b022eda952c40d", "message": "review comments", "committedDate": "2020-05-07T22:29:56Z", "type": "forcePushed"}]}