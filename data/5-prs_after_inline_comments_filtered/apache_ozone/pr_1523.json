{"pr_number": 1523, "pr_title": "HDDS-4320. Let Ozone input streams implement CanUnbuffer", "pr_createdAt": "2020-10-26T17:03:38Z", "pr_url": "https://github.com/apache/ozone/pull/1523", "timeline": [{"oid": "726def81c9a1d6ff42193c5ef21beea9459c9abb", "url": "https://github.com/apache/ozone/commit/726def81c9a1d6ff42193c5ef21beea9459c9abb", "message": "HDDS-4320. Let Ozone input streams implement CanUnbuffer", "committedDate": "2020-10-26T13:12:03Z", "type": "commit"}, {"oid": "5d3b1514a3783d5c48f02a8cbfffddabbdaac212", "url": "https://github.com/apache/ozone/commit/5d3b1514a3783d5c48f02a8cbfffddabbdaac212", "message": "Add unit test", "committedDate": "2020-10-26T15:44:26Z", "type": "commit"}, {"oid": "f1db1b13a2c85b142bdf82d9c0c401c8b28eba3a", "url": "https://github.com/apache/ozone/commit/f1db1b13a2c85b142bdf82d9c0c401c8b28eba3a", "message": "trigger new CI check", "committedDate": "2020-10-26T20:31:15Z", "type": "commit"}, {"oid": "00f0c4eecd3053c108c0621e8f29a83d07288015", "url": "https://github.com/apache/ozone/commit/00f0c4eecd3053c108c0621e8f29a83d07288015", "message": "trigger new CI check", "committedDate": "2020-10-27T05:43:24Z", "type": "commit"}, {"oid": "13e274f47d077022adcb12a09df1a333cfe64d86", "url": "https://github.com/apache/ozone/commit/13e274f47d077022adcb12a09df1a333cfe64d86", "message": "Merge remote-tracking branch 'origin/master' into HDDS-4320", "committedDate": "2020-11-17T16:45:24Z", "type": "commit"}, {"oid": "263ce7888903530656baf596b9f827636ec2ed28", "url": "https://github.com/apache/ozone/commit/263ce7888903530656baf596b9f827636ec2ed28", "message": "Merge remote-tracking branch 'origin/master' into HDDS-4320", "committedDate": "2020-11-19T14:47:16Z", "type": "commit"}, {"oid": "aa910488eb4f64eed7f2c3c3b6c15122e1f83cd7", "url": "https://github.com/apache/ozone/commit/aa910488eb4f64eed7f2c3c3b6c15122e1f83cd7", "message": "Refresh pipeline if read fails with CONTAINER_NOT_FOUND", "committedDate": "2020-11-20T13:32:39Z", "type": "commit"}, {"oid": "658a16b9c0483da99bda30250de67ed19e9f39f8", "url": "https://github.com/apache/ozone/commit/658a16b9c0483da99bda30250de67ed19e9f39f8", "message": "Fix findbugs", "committedDate": "2020-11-20T15:00:03Z", "type": "commit"}, {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "url": "https://github.com/apache/ozone/commit/96394e72bb6f6d7544c8812f08f490f6002b1a30", "message": "Add integration test; Refresh pipeline on any read error", "committedDate": "2020-11-23T14:09:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgwMTE2NQ==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532801165", "bodyText": "This error message might be confusing if the refresh pipeline function exists. In the else case under refreshPipelineFunction != null, can we change the log level to info maybe?", "author": "hanishakoneru", "createdAt": "2020-11-30T18:14:45Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "diffHunk": "@@ -171,6 +162,23 @@ public synchronized void initialize() throws IOException {\n     }\n   }\n \n+  private void refreshPipeline(IOException cause) throws IOException {\n+    LOG.error(\"Unable to read information for block {} from pipeline {}: {}\",\n+        blockID, pipeline.getId(), cause.getMessage());", "originalCommit": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE3MTkwOA==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533171908", "bodyText": "It was an error previously, I only changed the message a bit from:\nLOG.error(\"Unable to read block information from pipeline.\");\n\nbut I agree, it should be logged at lower level.", "author": "adoroszlai", "createdAt": "2020-12-01T08:54:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgwMTE2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg0MjAxMQ==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532842011", "bodyText": "Can we also please update the javadoc for chunkPosition?", "author": "hanishakoneru", "createdAt": "2020-11-30T19:23:33Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -292,6 +305,11 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = bufferOffset + bufferLength;\n     }\n \n+    // bufferOffset and bufferLength are updated below, but if read fails\n+    // and is retried, we need the previous position.  Position is reset after\n+    // successful read in adjustBufferPosition()\n+    storePosition();", "originalCommit": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3OTUyMg==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532979522", "bodyText": "waitForReplicaCount=3 will encompass waitForReplicaCount=2.", "author": "hanishakoneru", "createdAt": "2020-11-30T23:51:08Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -397,4 +410,118 @@ public void testSkip() throws Exception {\n       Assert.assertEquals(inputData[chunkSize + 50 + i], readData[i]);\n     }\n   }\n+\n+  @Test\n+  public void readAfterReplication() throws Exception {\n+    testReadAfterReplication(false);\n+  }\n+\n+  @Test\n+  public void unbuffer() throws Exception {\n+    testReadAfterReplication(true);\n+  }\n+\n+  private void testReadAfterReplication(boolean doUnbuffer) throws Exception {\n+    Assume.assumeTrue(cluster.getHddsDatanodes().size() > 3);\n+\n+    int dataLength = 2 * chunkSize;\n+    String keyName = getKeyName();\n+    OzoneOutputStream key = TestHelper.createKey(keyName,\n+        ReplicationType.RATIS, dataLength, objectStore, volumeName, bucketName);\n+\n+    byte[] data = writeRandomBytes(key, dataLength);\n+\n+    OmKeyArgs keyArgs = new OmKeyArgs.Builder().setVolumeName(volumeName)\n+        .setBucketName(bucketName)\n+        .setKeyName(keyName)\n+        .setType(HddsProtos.ReplicationType.RATIS)\n+        .setFactor(HddsProtos.ReplicationFactor.THREE)\n+        .build();\n+    OmKeyInfo keyInfo = cluster.getOzoneManager().lookupKey(keyArgs);\n+\n+    OmKeyLocationInfoGroup locations = keyInfo.getLatestVersionLocations();\n+    Assert.assertNotNull(locations);\n+    List<OmKeyLocationInfo> locationInfoList = locations.getLocationList();\n+    Assert.assertEquals(1, locationInfoList.size());\n+    OmKeyLocationInfo loc = locationInfoList.get(0);\n+    long containerID = loc.getContainerID();\n+    Assert.assertEquals(3, countReplicas(containerID, cluster));\n+\n+    TestHelper.waitForContainerClose(cluster, containerID);\n+\n+    List<DatanodeDetails> pipelineNodes = loc.getPipeline().getNodes();\n+\n+    // read chunk data\n+    try (KeyInputStream keyInputStream = (KeyInputStream) objectStore\n+        .getVolume(volumeName).getBucket(bucketName)\n+        .readKey(keyName).getInputStream()) {\n+\n+      int b = keyInputStream.read();\n+      Assert.assertNotEquals(-1, b);\n+\n+      if (doUnbuffer) {\n+        keyInputStream.unbuffer();\n+      }\n+\n+      // stop one node, wait for container to be replicated to another one\n+      cluster.shutdownHddsDatanode(pipelineNodes.get(0));\n+      waitForNodeToBecomeDead(pipelineNodes.get(0));\n+      waitForReplicaCount(containerID, 2, cluster);\n+      waitForReplicaCount(containerID, 3, cluster);", "originalCommit": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE4MTIwMw==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533181203", "bodyText": "We need to waitForReplicaCount=2 first to ensure one of the original replicas is lost.", "author": "adoroszlai", "createdAt": "2020-12-01T09:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3OTUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MDQzMQ==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532980431", "bodyText": "How about naming this method something like readAfterReplicationWithUnbuffering or something to represent what the test verifies?", "author": "hanishakoneru", "createdAt": "2020-11-30T23:53:45Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -397,4 +410,118 @@ public void testSkip() throws Exception {\n       Assert.assertEquals(inputData[chunkSize + 50 + i], readData[i]);\n     }\n   }\n+\n+  @Test\n+  public void readAfterReplication() throws Exception {\n+    testReadAfterReplication(false);\n+  }\n+\n+  @Test\n+  public void unbuffer() throws Exception {", "originalCommit": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MTk2Ng==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532981966", "bodyText": "What are the transient cases in which this might fail?\nShould there be a retry in those cases? Otherwise these tests might fail intermittently.", "author": "hanishakoneru", "createdAt": "2020-11-30T23:57:48Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());\n+  }\n+\n+  protected void validateFullFileContents(FSDataInputStream stream)\n+          throws IOException {\n+    validateFileContents(stream, TEST_FILE_LEN, 0);\n+  }\n+\n+  protected void validateFileContents(FSDataInputStream stream, int length,\n+                                      int startIndex)\n+          throws IOException {\n+    byte[] streamData = new byte[length];\n+    assertEquals(\"failed to read expected number of bytes from \"\n+            + \"stream. This may be transient\",\n+        length, stream.read(streamData));", "originalCommit": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5MTc3MQ==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533191771", "bodyText": "I don't know.  AbstractContractUnbufferTest is copied as is from Hadoop 3.3 source, since we cannot yet upgrade dependency to that version.  The only changes I had to make were for checkstyle (too long line) and the new constant SUPPORTS_UNBUFFER introduced in ContractOptions (which the test implements).  I'd rather keep it close to the original version (and post any improvements/fixes to apache/hadoop first).", "author": "adoroszlai", "createdAt": "2020-12-01T09:10:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MTk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4Mjk4NA==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532982984", "bodyText": "typo in description", "author": "hanishakoneru", "createdAt": "2020-12-01T00:00:50Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");", "originalCommit": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MzM3Nw==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532983377", "bodyText": "unbuffer() here just checks that the position is maintained. Would it be possible to also verify that the buffers are actually released?", "author": "hanishakoneru", "createdAt": "2020-12-01T00:01:57Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());", "originalCommit": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5MTcxMg==", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533191712", "bodyText": "I don't think it can, as it does not know about implementation internals.  This is verified in TestChunkInputStream instead.  Hadoop also has additional implementation-specific test for this reason.", "author": "adoroszlai", "createdAt": "2020-12-01T09:10:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MzM3Nw=="}], "type": "inlineReview"}, {"oid": "6550545d1669ffe54d9ce0032cf54bb04dd6db6c", "url": "https://github.com/apache/ozone/commit/6550545d1669ffe54d9ce0032cf54bb04dd6db6c", "message": "Merge remote-tracking branch 'origin/master' into HDDS-4320", "committedDate": "2020-12-01T08:22:29Z", "type": "commit"}, {"oid": "ec191ebf80dcc7ffadc4ef01793192c57586252d", "url": "https://github.com/apache/ozone/commit/ec191ebf80dcc7ffadc4ef01793192c57586252d", "message": "Address review comments", "committedDate": "2020-12-01T09:13:24Z", "type": "commit"}]}