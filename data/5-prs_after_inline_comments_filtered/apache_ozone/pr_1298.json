{"pr_number": 1298, "pr_title": "HDDS-3869. Use different column families for datanode block and metadata", "pr_createdAt": "2020-08-06T18:26:09Z", "pr_url": "https://github.com/apache/ozone/pull/1298", "timeline": [{"oid": "8aab8f529f96562b5f1ab201e694fdb7dcc74788", "url": "https://github.com/apache/ozone/commit/8aab8f529f96562b5f1ab201e694fdb7dcc74788", "message": "Switch TestContainerCache to use new DatanodeStore interface", "committedDate": "2020-08-05T19:20:56Z", "type": "commit"}, {"oid": "eff1ad5c3b01ee7b5c8af1d09b5dd4bda9f28c1d", "url": "https://github.com/apache/ozone/commit/eff1ad5c3b01ee7b5c8af1d09b5dd4bda9f28c1d", "message": "Fix issues highlighted by GitHub pull request diffs\n\nMostly removing wildcard imports automatically added by Intellij.\nDelete the now unused NoData and NoDataCodec classes.\nMinor documentation and readability fixes.", "committedDate": "2020-08-06T14:37:05Z", "type": "commit"}, {"oid": "f08767eebf1044cee52a2c83468ad1613ded563c", "url": "https://github.com/apache/ozone/commit/f08767eebf1044cee52a2c83468ad1613ded563c", "message": "Remove unused ChunkInfoCodec class", "committedDate": "2020-08-06T15:46:42Z", "type": "commit"}, {"oid": "38de96efa8912290dea3643e17eea456271daa51", "url": "https://github.com/apache/ozone/commit/38de96efa8912290dea3643e17eea456271daa51", "message": "Remove old log backup files from test database", "committedDate": "2020-08-06T16:03:12Z", "type": "commit"}, {"oid": "dd09d5b7f9229e20fc4b6aaa5b797ee4c444e5a2", "url": "https://github.com/apache/ozone/commit/dd09d5b7f9229e20fc4b6aaa5b797ee4c444e5a2", "message": "Remove old TODO message", "committedDate": "2020-08-06T16:37:07Z", "type": "commit"}, {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39", "url": "https://github.com/apache/ozone/commit/a3d0fa1f8da98c535999e46d56fe41c65a195d39", "message": "Fix checkstyle violation", "committedDate": "2020-08-07T14:46:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgxODk2Ng==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468818966", "bodyText": "Throw UnsupportedOperation for this implementation and schema 2 implementation.", "author": "errose28", "createdAt": "2020-08-11T19:36:30Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs},\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#iterator} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */\n+public class SchemaOneDeletedBlocksTable implements Table<String,\n+        ChunkInfoList> {\n+  public static final String DELETED_KEY_PREFIX = \"#deleted#\";\n+\n+  private final Table<String, ChunkInfoList> table;\n+\n+  public SchemaOneDeletedBlocksTable(Table<String, ChunkInfoList> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(String key, ChunkInfoList value) throws IOException {\n+    table.put(prefix(key), value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, String key,\n+                           ChunkInfoList value)\n+          throws IOException {\n+    table.putWithBatch(batch, prefix(key), value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(String key) throws IOException {\n+    table.delete(prefix(key));\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, String key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, prefix(key));\n+  }\n+\n+  /**\n+   * Because the actual underlying table in this schema version is the\n+   * default table where all keys are stored, this method will iterate\n+   * through all keys in the database.\n+   */\n+  @Override\n+  public TableIterator<String, ? extends KeyValue<String, ChunkInfoList>>\n+      iterator() {\n+    return table.iterator();\n+  }", "originalCommit": "a3d0fa1f8da98c535999e46d56fe41c65a195d39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIxMDA3NA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r470210074", "bodyText": "In order to not support this method, the KeyValueBlockIterator will need a different way to gain access to the iterator it uses internally when filtering blocks by prefix. The updated version of the code will make this class internal to the AbstractDatanodeStore, and callers can get it using getters in the DatanodeStore interface that return the KeyValueBlockIterator's interface: BlockIterator. This way, the AbstractDatanodeStore can initialize it with an iterator retrieved from the block data table before wrapping it in a class to disable access to this method.", "author": "errose28", "createdAt": "2020-08-13T19:54:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgxODk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzMDIyNw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468830227", "bodyText": "Rename, because this is not actually a prefix, but a piece of metadata. Also make sure it is placed in the metadata table when used.", "author": "errose28", "createdAt": "2020-08-11T19:58:47Z", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "diffHunk": "@@ -140,7 +139,6 @@ public static Versioning getVersioning(boolean versioning) {\n   }\n \n   public static final String DELETING_KEY_PREFIX = \"#deleting#\";\n-  public static final String DELETED_KEY_PREFIX = \"#deleted#\";\n   public static final String DELETE_TRANSACTION_KEY_PREFIX = \"#delTX#\";\n   public static final String BLOCK_COMMIT_SEQUENCE_ID_PREFIX = \"#BCSID\";", "originalCommit": "a3d0fa1f8da98c535999e46d56fe41c65a195d39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzNTAwMw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468835003", "bodyText": "Do we need to check schema version here, or will it always be the latest version?", "author": "errose28", "createdAt": "2020-08-11T20:08:32Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -91,8 +83,16 @@ public static void createContainerMetaData(File containerMetaDataPath, File\n           \" Path: \" + chunksPath);\n     }\n \n-    MetadataStore store = MetadataStoreBuilder.newBuilder().setConf(conf)\n-        .setCreateIfMissing(true).setDbFile(dbFile).build();\n+    DatanodeStore store;\n+    if (schemaVersion.equals(OzoneConsts.SCHEMA_V1)) {\n+      store = new DatanodeStoreSchemaOneImpl(conf, dbFile.getAbsolutePath());\n+    } else if (schemaVersion.equals(OzoneConsts.SCHEMA_V2)) {\n+      store = new DatanodeStoreSchemaTwoImpl(conf, dbFile.getAbsolutePath());\n+    } else {\n+      throw new IllegalArgumentException(\n+              \"Unrecognized schema version for container: \" + schemaVersion);\n+    }", "originalCommit": "a3d0fa1f8da98c535999e46d56fe41c65a195d39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNTU2Ng==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r469425566", "bodyText": "The latest version was always being passed in by KeyValueContainer#create (the only place this method is called), but it uses the OzoneConsts.SCHEMA_LATEST variable to make sure that the latest version is always used for new containers. If this parameter is omitted, we will need to hardcode the current latest DatanodeStore implementation here and remember to update it on change. Probably best to leave as is so that if a new version is added and this method is not updated, an IllegalArgumentException is thrown.", "author": "errose28", "createdAt": "2020-08-12T17:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzNTAwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMjA0NA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r469432044", "bodyText": "Update documentation to clarify this.", "author": "errose28", "createdAt": "2020-08-12T17:43:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzNTAwMw=="}], "type": "inlineReview"}, {"oid": "2de4e706995652fa63c93200e35ee936ad3fa2eb", "url": "https://github.com/apache/ozone/commit/2de4e706995652fa63c93200e35ee936ad3fa2eb", "message": "Merge branch 'master' into HDDS-3869\n\n* master: (28 commits)\n  HDDS-4037. Incorrect container numberOfKeys and usedBytes in SCM after key deletion (#1295)\n  HDDS-3232. Include the byteman scripts in the distribution tar file (#1309)\n  HDDS-4095. Byteman script to debug HCFS performance (#1311)\n  HDDS-4057. Failed acceptance test missing from bundle (#1283)\n  HDDS-4040. [OFS] BasicRootedOzoneFileSystem to support batchDelete (#1286)\n  HDDS-4061. Pending delete blocks are not always included in #BLOCKCOUNT metadata (#1288)\n  HDDS-4067. Implement toString for OMTransactionInfo (#1300)\n  HDDS-3878. Make OMHA serviceID optional if one (but only one) is defined in the config (#1149)\n  HDDS-3833. Use Pipeline choose policy to choose pipeline from exist pipeline list (#1096)\n  HDDS-3979. Make bufferSize configurable for stream copy (#1212)\n  HDDS-4048. Show more information while SCM version info mismatch (#1278)\n  HDDS-4078. Use HDDS InterfaceAudience/Stability annotations (#1302)\n  HDDS-4034. Add Unit Test for HadoopNestedDirGenerator. (#1266)\n  HDDS-4076. Translate CSI.md into Chinese (#1299)\n  HDDS-4046. Extensible subcommands for CLI applications (#1276)\n  HDDS-4051. Remove whitelist/blacklist terminology from Ozone (#1306)\n  HDDS-4055. Cleanup GitHub workflow (#1282)\n  HDDS-4042. Update documentation for the GA release (#1269)\n  HDDS-4066. Add core-site.xml to intellij configuration (#1292)\n  HDDS-4073. Remove leftover robot.robot (#1297)\n  ...", "committedDate": "2020-08-12T13:50:47Z", "type": "commit"}, {"oid": "e4c6f86f77a1dd7f5726d91bf151ccdc2ca40299", "url": "https://github.com/apache/ozone/commit/e4c6f86f77a1dd7f5726d91bf151ccdc2ca40299", "message": "Remove \"PREFIX\" from the names of metadata keys that are not actually prefixes", "committedDate": "2020-08-12T14:16:29Z", "type": "commit"}, {"oid": "20b4a1289f44d44622ba8b0caac91e6a489d927a", "url": "https://github.com/apache/ozone/commit/20b4a1289f44d44622ba8b0caac91e6a489d927a", "message": "Remove BCSID from metadata key filters, and update TestKeyValueBlockIterator to not use it as a block data prefix\n\nTestKeyValueBlockIterator does not care about the values of the prefixes it uses, it\nonly needs two different types of block prefixes, along with unprefixed blocks,\nto run its tests. The existing tests were refactored to allow specifying an\narbitrary prefix to be created for testing, and make it easier to validate that\nthe iterator returned the correct blocks for each prefix.", "committedDate": "2020-08-12T17:15:20Z", "type": "commit"}, {"oid": "81d0333838bd66696623ab800409b4a7e19022a2", "url": "https://github.com/apache/ozone/commit/81d0333838bd66696623ab800409b4a7e19022a2", "message": "Update docs on KeyValueContainerUtil.createContainerMetadata to reflect shcema version usage", "committedDate": "2020-08-12T17:43:21Z", "type": "commit"}, {"oid": "e614fb352f1a9f19af7dfc555f49a655fbe0c671", "url": "https://github.com/apache/ozone/commit/e614fb352f1a9f19af7dfc555f49a655fbe0c671", "message": "Add DatanodeTable that disallows iterating tables directly without key prefix filters", "committedDate": "2020-08-12T19:06:35Z", "type": "commit"}, {"oid": "7a70e3471df379c2157c3e31d8a31042bebeb7e9", "url": "https://github.com/apache/ozone/commit/7a70e3471df379c2157c3e31d8a31042bebeb7e9", "message": "Add test to ensure direct table iterating is disabled for schema version one", "committedDate": "2020-08-12T20:42:16Z", "type": "commit"}, {"oid": "d362c3d3aa8b88f183b09f047da46c9ed752fca1", "url": "https://github.com/apache/ozone/commit/d362c3d3aa8b88f183b09f047da46c9ed752fca1", "message": "Add internal implementation of BlockIterator to AbstractDatanodeStore", "committedDate": "2020-08-13T14:27:26Z", "type": "commit"}, {"oid": "2a0285b266eefa0b1fcd1faf01f552b2df61023c", "url": "https://github.com/apache/ozone/commit/2a0285b266eefa0b1fcd1faf01f552b2df61023c", "message": "Remove Container#blockIterator method that was only called in a unit test\n\nAll block iterator implementations will now be retrieved from DatanodeStore.", "committedDate": "2020-08-13T14:38:14Z", "type": "commit"}, {"oid": "f46fc83c0497f5556fcaacf1f7fc2743201f6578", "url": "https://github.com/apache/ozone/commit/f46fc83c0497f5556fcaacf1f7fc2743201f6578", "message": "Move KeyValueBlockIterator implementation inside AbstractDatanodeStore\n\nStill need to get the container ID passed in to it for log messages.", "committedDate": "2020-08-13T16:38:53Z", "type": "commit"}, {"oid": "2211daeec0967f62ee0088386837fae0159bc687", "url": "https://github.com/apache/ozone/commit/2211daeec0967f62ee0088386837fae0159bc687", "message": "Add container ID to log messages for new KeyValueBlockIterator", "committedDate": "2020-08-13T17:05:22Z", "type": "commit"}, {"oid": "2785fe82463a78e80b3a1e1786c68bea37124578", "url": "https://github.com/apache/ozone/commit/2785fe82463a78e80b3a1e1786c68bea37124578", "message": "Fix mizzed places where containerID was required", "committedDate": "2020-08-13T17:20:37Z", "type": "commit"}, {"oid": "e6a9ffee7d23697818da81dbf9916e2ea1f069ca", "url": "https://github.com/apache/ozone/commit/e6a9ffee7d23697818da81dbf9916e2ea1f069ca", "message": "Fix null pointer exceptions in unit tests", "committedDate": "2020-08-13T17:37:49Z", "type": "commit"}, {"oid": "3a3b73d8339fed724a5b734bf304714fb8af98d9", "url": "https://github.com/apache/ozone/commit/3a3b73d8339fed724a5b734bf304714fb8af98d9", "message": "Merge branch 'table-iterator-impl' into HDDS-3869\n\n* table-iterator-impl:\n  Fix null pointer exceptions in unit tests\n  Fix mizzed places where containerID was required\n  Add container ID to log messages for new KeyValueBlockIterator\n  Move KeyValueBlockIterator implementation inside AbstractDatanodeStore\n  Remove Container#blockIterator method that was only called in a unit test\n  Add internal implementation of BlockIterator to AbstractDatanodeStore", "committedDate": "2020-08-13T19:17:46Z", "type": "commit"}, {"oid": "a295c541ae6a3437470c65dd0e4537d44fbfbb30", "url": "https://github.com/apache/ozone/commit/a295c541ae6a3437470c65dd0e4537d44fbfbb30", "message": "Fix checkstyle violations", "committedDate": "2020-08-13T20:26:14Z", "type": "commit"}, {"oid": "e62e815c20e684f80c4a42c78c239ee2dd9dbc7c", "url": "https://github.com/apache/ozone/commit/e62e815c20e684f80c4a42c78c239ee2dd9dbc7c", "message": "Merge branch 'master' into HDDS-3869\n\n* master:\n  HDDS-4099. No Log4j 2 configuration file found error appears in CLI (#1318)\n  HDDS-4108. ozone debug ldb scan without arguments results in core dump (#1317)\n  HDDS-4009. Recon Overview page: The volume, bucket and key counts are not accurate (#1305)", "committedDate": "2020-08-13T20:28:04Z", "type": "commit"}, {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "url": "https://github.com/apache/ozone/commit/5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "message": "Fix integratino test issues where ReferenceCountedDB was not being properly cleaned up", "committedDate": "2020-08-14T13:33:42Z", "type": "commit"}, {"oid": "352faf65b92d001259da6c67d5e463815b43fc89", "url": "https://github.com/apache/ozone/commit/352faf65b92d001259da6c67d5e463815b43fc89", "message": "Add documentation explaining why schema version is tracked per container\n\nSince containers in older schema versions are currently not reformatted to\nnewer schema versions, a datanode may have containers with a mix of schema\nversions, requiring this property to be tracked on a per container basis.", "committedDate": "2020-09-01T14:16:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAzNjU2NQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476036565", "bodyText": "Is the BlockData table loaded in memory when the store is initialized?", "author": "hanishakoneru", "createdAt": "2020-08-25T01:24:53Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteBlocksCommandHandler.java", "diffHunk": "@@ -209,30 +208,35 @@ private void deleteKeyValueContainerBlocks(\n     int newDeletionBlocks = 0;\n     try(ReferenceCountedDB containerDB =\n             BlockUtils.getDB(containerData, conf)) {\n-      for (Long blk : delTX.getLocalIDList()) {\n-        BatchOperation batch = new BatchOperation();\n-        byte[] blkBytes = Longs.toByteArray(blk);\n-        byte[] blkInfo = containerDB.getStore().get(blkBytes);\n+      Table<String, BlockData> blockDataTable =\n+              containerDB.getStore().getBlockDataTable();", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA0Mjg0MQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476042841", "bodyText": "When reading old containerDataYaml which does not container the Schema version field, what value would be returned? IIRC and it returns null, then we should set it to version V1.", "author": "hanishakoneru", "createdAt": "2020-08-25T01:34:34Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerDataYaml.java", "diffHunk": "@@ -280,6 +280,9 @@ public Object construct(Node node) {\n         String state = (String) nodes.get(OzoneConsts.STATE);\n         kvData\n             .setState(ContainerProtos.ContainerDataProto.State.valueOf(state));\n+        String schemaVersion = (String) nodes.get(OzoneConsts.SCHEMA_VERSION);\n+        kvData.setSchemaVersion(schemaVersion);", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIyMzQ4Mw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485223483", "bodyText": "See #1298 (comment)", "author": "errose28", "createdAt": "2020-09-08T22:10:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA0Mjg0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA1OTI1Nw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476059257", "bodyText": "@arp7, is it ok to remove LevelDB support?\nThe option to configure DNs to use LevelDB was removed in 0.6.0. And since upgrade from previous versions is not supported, I think it is safe to remove LevelDB support.\nIf yes, we can open a new Jira to clean up the LevelDB code path. If no, we need to take care of that here.\nThe containerDBType parameter is redundant here if old LevelDB containers will not be supported.", "author": "hanishakoneru", "createdAt": "2020-08-25T01:59:04Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/ContainerCache.java", "diffHunk": "@@ -115,11 +116,14 @@ protected boolean removeLRU(LinkEntry entry) {\n    * @param containerID - ID of the container.\n    * @param containerDBType - DB type of the container.\n    * @param containerDBPath - DB path of the container.\n+   * @param schemaVersion - Schema version of the container.\n    * @param conf - Hadoop Configuration.\n    * @return ReferenceCountedDB.\n    */\n   public ReferenceCountedDB getDB(long containerID, String containerDBType,\n-                             String containerDBPath, ConfigurationSource conf)\n+                                  String containerDBPath,", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476689381", "bodyText": "importContainerData() is called when replicating containers. If an old container needs to be replicated, it would not have the schema version. This can be fixed by setting a default value for schema version if it does not exist in ContainerDataYaml Constructor.", "author": "hanishakoneru", "createdAt": "2020-08-25T19:33:48Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java", "diffHunk": "@@ -487,6 +486,7 @@ public void importContainerData(InputStream input,\n       containerData.setState(originalContainerData.getState());\n       containerData\n           .setContainerDBType(originalContainerData.getContainerDBType());\n+      containerData.setSchemaVersion(originalContainerData.getSchemaVersion());", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjcyOTg4NA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476729884", "bodyText": "I see that schema version is being set in KeyValueContainerUtil#parseKVContainerData.\nWe can explore the option of setting the default schema version (V1) while reading the Yaml itself so that it is never missed.", "author": "hanishakoneru", "createdAt": "2020-08-25T20:52:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIyMzQwMQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485223401", "bodyText": "After discussion, decided to move the null check to KeyValueContainerData#setSchemaVersion. Placing it in the parser is more confusing and possibly error prone in the future if the code is modified. ContainerDataYaml calls this setter in KeyValueContainerData anyways.", "author": "errose28", "createdAt": "2020-09-08T22:10:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc2ODk0OA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476768948", "bodyText": "Any reason for using intValue here instead of the long value as incrPendingDeletionBlocks takes in a long parameter?", "author": "hanishakoneru", "createdAt": "2020-08-25T21:48:01Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI2MDg2NQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482260865", "bodyText": "Nope. Actually even a .longValue() call is redundant with Java auto unboxing, so I'll just pass the pendingDeleteBlockCount as is.", "author": "errose28", "createdAt": "2020-09-02T17:57:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc2ODk0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc4MTM3OQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476781379", "bodyText": "The way usedBytes is calculated has been changed. I am not sure if there will be any implications if this calculation us wrong. Need to dig deeper.\nWe should probably separate this optimization into a separate Jira. What do you think?", "author": "hanishakoneru", "createdAt": "2020-08-25T22:03:15Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());\n       } else {\n         // Set pending deleted block count.\n         MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter()\n-                .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\n+                MetadataKeyFilters.getDeletingKeyFilter();\n         int numPendingDeletionBlocks =\n-            containerDB.getStore().getSequentialRangeKVs(null,\n-                Integer.MAX_VALUE, filter)\n-                .size();\n+            containerDB.getStore().getBlockDataTable()\n+            .getSequentialRangeKVs(null, Integer.MAX_VALUE, filter)\n+            .size();\n         kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\n       }\n \n       // Set delete transaction id.\n-      byte[] delTxnId =\n-          containerDB.getStore().get(DB_CONTAINER_DELETE_TRANSACTION_KEY);\n+      Long delTxnId =\n+          metadataTable.get(OzoneConsts.DELETE_TRANSACTION_KEY);\n       if (delTxnId != null) {\n         kvContainerData\n-            .updateDeleteTransactionId(Longs.fromByteArray(delTxnId));\n+            .updateDeleteTransactionId(delTxnId);\n       }\n \n       // Set BlockCommitSequenceId.\n-      byte[] bcsId = containerDB.getStore().get(\n-          DB_BLOCK_COMMIT_SEQUENCE_ID_KEY);\n+      Long bcsId = metadataTable.get(\n+          OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID);\n       if (bcsId != null) {\n         kvContainerData\n-            .updateBlockCommitSequenceId(Longs.fromByteArray(bcsId));\n+            .updateBlockCommitSequenceId(bcsId);\n       }\n \n       // Set bytes used.\n       // commitSpace for Open Containers relies on usedBytes\n-      byte[] bytesUsed =\n-          containerDB.getStore().get(DB_CONTAINER_BYTES_USED_KEY);\n+      Long bytesUsed =\n+          metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED);\n       if (bytesUsed != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setBytesUsed(Longs.fromByteArray(bytesUsed));\n+        kvContainerData.setBytesUsed(bytesUsed);\n       }\n \n       // Set block count.\n-      byte[] blockCount = containerDB.getStore().get(DB_BLOCK_COUNT_KEY);\n+      Long blockCount = metadataTable.get(OzoneConsts.BLOCK_COUNT);\n       if (blockCount != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setKeyCount(Longs.fromByteArray(blockCount));\n+        kvContainerData.setKeyCount(blockCount);\n       }\n     }\n \n     if (!isBlockMetadataSet) {\n-      initializeUsedBytesAndBlockCount(kvContainerData);\n+      initializeUsedBytesAndBlockCount(kvContainerData, config);\n     }\n   }\n \n \n   /**\n    * Initialize bytes used and block count.\n-   * @param kvContainerData\n+   * @param kvData\n    * @throws IOException\n    */\n   private static void initializeUsedBytesAndBlockCount(\n-      KeyValueContainerData kvContainerData) throws IOException {\n-\n-    MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter();\n+      KeyValueContainerData kvData, ConfigurationSource config)\n+          throws IOException {\n \n-    // Ignore all blocks except those with no prefix, or those with\n-    // #deleting# prefix.\n-    filter.addFilter(OzoneConsts.DELETED_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.DELETE_TRANSACTION_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COUNT, true)\n-          .addFilter(OzoneConsts.CONTAINER_BYTES_USED, true)\n-          .addFilter(OzoneConsts.PENDING_DELETE_BLOCK_COUNT, true);\n+    final String errorMessage = \"Failed to parse block data for\" +\n+            \" Container \" + kvData.getContainerID();\n \n     long blockCount = 0;\n-    try (KeyValueBlockIterator blockIter = new KeyValueBlockIterator(\n-        kvContainerData.getContainerID(),\n-        new File(kvContainerData.getContainerPath()), filter)) {\n-      long usedBytes = 0;\n-\n-\n-      boolean success = true;\n-      while (success) {\n-        try {\n-          if (blockIter.hasNext()) {\n-            BlockData block = blockIter.nextBlock();\n-            long blockLen = 0;\n-\n-            List< ContainerProtos.ChunkInfo > chunkInfoList = block.getChunks();\n-            for (ContainerProtos.ChunkInfo chunk : chunkInfoList) {\n-              ChunkInfo info = ChunkInfo.getFromProtoBuf(chunk);\n-              blockLen += info.getLen();\n-            }\n-\n-            usedBytes += blockLen;\n-            blockCount++;\n-          } else {\n-            success = false;\n+    long usedBytes = 0;\n+\n+    try(ReferenceCountedDB db = BlockUtils.getDB(kvData, config)) {\n+      // Count all regular blocks.\n+      try (BlockIterator<BlockData> blockIter =\n+                   db.getStore().getBlockIterator(\n+                           MetadataKeyFilters.getUnprefixedKeyFilter())) {\n+\n+        while (blockIter.hasNext()) {\n+          blockCount++;\n+          try {\n+            usedBytes += blockIter.nextBlock().getSize();", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5ODU3NQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485198575", "bodyText": "I will switch it back to the old calculation and we can do it in a separate jira later if we want.", "author": "errose28", "createdAt": "2020-09-08T21:11:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc4MTM3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg0NTk5NA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476845994", "bodyText": "blockKey variable is redundant now and can be removed.", "author": "hanishakoneru", "createdAt": "2020-08-25T23:18:59Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -262,14 +264,17 @@ public void deleteBlock(Container container, BlockID blockID) throws\n       getBlockByID(db, blockID);\n \n       // Update DB to delete block and set block count and bytes used.\n-      BatchOperation batch = new BatchOperation();\n-      batch.delete(blockKey);", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI2NjEyMw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482266123", "bodyText": "Will do", "author": "errose28", "createdAt": "2020-09-02T18:06:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg0NTk5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NzI3MQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476857271", "bodyText": "All the usages of getBlockByID convert the returned byte array back to ContainerProtos.BlockData. We can avoid this serialization-deserialization.\nNoting it down here so that we can open a new Jira to optimize this.", "author": "hanishakoneru", "createdAt": "2020-08-25T23:29:38Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -324,14 +328,14 @@ public void shutdown() {\n \n   private byte[] getBlockByID(ReferenceCountedDB db, BlockID blockID)\n       throws IOException {\n-    byte[] blockKey = Longs.toByteArray(blockID.getLocalID());\n+    String blockKey = Long.toString(blockID.getLocalID());\n \n-    byte[] blockData = db.getStore().get(blockKey);\n+    BlockData blockData = db.getStore().getBlockDataTable().get(blockKey);\n     if (blockData == null) {\n       throw new StorageContainerException(NO_SUCH_BLOCK_ERR_MSG,\n           NO_SUCH_BLOCK);\n     }\n \n-    return blockData;\n+    return blockData.getProtoBufMessage().toByteArray();", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI2NzU3OA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482267578", "bodyText": "Good catch. We could fix it here since this pull request moves the coding/decoding into the tables and away from the caller, or do it in a new Jira.", "author": "errose28", "createdAt": "2020-09-02T18:09:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NzI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE4OTU2Mg==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485189562", "bodyText": "When this patch is merged, I will change this in this jira: https://issues.apache.org/jira/browse/HDDS-4220", "author": "errose28", "createdAt": "2020-09-08T20:52:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NzI3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2NjA2MA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477666060", "bodyText": "Can we define the table names as static final fields either here or in OzoneConsts?", "author": "hanishakoneru", "createdAt": "2020-08-26T23:23:27Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeSchemaTwoDBDefinition.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.db.DBColumnFamilyDefinition;\n+import org.apache.hadoop.hdds.utils.db.LongCodec;\n+import org.apache.hadoop.hdds.utils.db.StringCodec;\n+import org.apache.hadoop.ozone.container.common.helpers.BlockData;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+/**\n+ * This class defines the RocksDB structure for datanodes following schema\n+ * version 2, where the block data, metadata, and deleted block ids are put in\n+ * their own separate column families.\n+ */\n+public class DatanodeSchemaTwoDBDefinition extends\n+        AbstractDatanodeDBDefinition {\n+\n+  public static final DBColumnFamilyDefinition<String, BlockData>\n+          BLOCK_DATA =\n+          new DBColumnFamilyDefinition<>(\n+                  \"block_data\",", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI3MTkyMQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482271921", "bodyText": "Placing the table names here is consistent with the existing DBDefinitions, there are currently no table names defined in OzoneConsts. The names are basically static final already, since the DBColumnFamilyDefinieions are static final, and the name property of a DBColumnFamilyDefinition cannot be changed after object creation. The call to read a table name would be DatanodeSchemaTwoDBDefinition.BLOCK_DATA.getTableName(), and there is no setter for a caller to change this value.", "author": "errose28", "createdAt": "2020-09-02T18:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2NjA2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcxMzc2Mg==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477713762", "bodyText": "I think we don't need cache functionality for DatanodeTable? I see it being used only in OM.", "author": "hanishakoneru", "createdAt": "2020-08-26T23:56:22Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+\n+  public DatanodeTable(Table<KEY, VALUE> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(KEY key, VALUE value) throws IOException {\n+    table.put(key, value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, KEY key,\n+                           VALUE value) throws IOException {\n+    table.putWithBatch(batch, key, value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(KEY key) throws IOException {\n+    table.delete(key);\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, KEY key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, key);\n+  }\n+\n+  @Override\n+  public final TableIterator<KEY, ? extends KeyValue<KEY, VALUE>> iterator() {\n+    throw new UnsupportedOperationException(\"Iterating tables directly is not\" +\n+            \" supported for datanode containers due to differing schema \" +\n+            \"version.\");\n+  }\n+\n+  @Override\n+  public String getName() throws IOException {\n+    return table.getName();\n+  }\n+\n+  @Override\n+  public long getEstimatedKeyCount() throws IOException {\n+    return table.getEstimatedKeyCount();\n+  }\n+\n+  @Override\n+  public void addCacheEntry(CacheKey<KEY> cacheKey,\n+                            CacheValue<VALUE> cacheValue) {", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI3NjIxOA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482276218", "bodyText": "I believe table cache is only used in OM. I will remove this to default to Not Implemented as defined in the Table interface.", "author": "errose28", "createdAt": "2020-09-02T18:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcxMzc2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcyODQwMA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477728400", "bodyText": "From what I could understand, the underlying table structure is TypedTabled. Is that correct?\nIf yes, should DatanodeTable extend TypedTable instead of Table<KEY, VALUE> so that it does not have to override all the methods?", "author": "hanishakoneru", "createdAt": "2020-08-27T00:05:23Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIwNDY4OQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485204689", "bodyText": "This would be nice, but unfortunately the AbstractDatanodeStore that is creating the DatanodeTables to return is getting pre-constructed instances of the Table interface from the DBColumnFamilyDefinition class. AbstractDatanodeStore must then convert these to DatanodeTables.\nLets say we make DatanodeTable extend TypedTable. Since AbstractDatanodeStore is given an existing instance that is not a DatanodeTable, creating a DatanodeTable from this requires assuming the original table is a TypedTable (which is currently true but may not hold in the future), casting it, and somehow copying all its internal state over to a new DatanodeTable.\nAlthough using composition here requires a lot of wrapper methods that just call into the composed object, it is ultimately a cleaner approach because it allows creating a DatanodeTable from any existing Table implementation.", "author": "errose28", "createdAt": "2020-09-08T21:25:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcyODQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNjEwOA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479506108", "bodyText": "Shouldn't Schema 1 DBs store the deleted blocks in the old format?\nIIUC, if SchemaOneChunkInfoListCodec implements the Block ID codec, then the InvalidProtocolBufferException can be avoided.\nPlease let me know if there is any other reason for implementing it this way.", "author": "hanishakoneru", "createdAt": "2020-08-28T19:53:42Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneChunkInfoListCodec.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n+import org.apache.hadoop.hdds.utils.db.Codec;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.ratis.thirdparty.com.google.protobuf.InvalidProtocolBufferException;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Codec for parsing {@link ContainerProtos.ChunkInfoList} objects from data\n+ * that may have been written using schema version one. Before upgrading\n+ * schema versions, deleted block IDs were stored with a duplicate copy of\n+ * their ID as the value in the database. After upgrading the code, any\n+ * deletes that happen on the DB will save the chunk information with the\n+ * deleted blocks instead, even if those deletes are performed on a database\n+ * created with schema version one.\n+ * <p>", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4MTY1Mw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482281653", "bodyText": "Because tables are now strongly typed, reads and writes to a table must have the same type, regardless of schema version. Having a different type for schema v1 and v2 tables will force callers to know which schema version they are working with, which we do not want. Therefore, since the value type of deleted block data was changed, all new reads and writes must use this type. This means the data written with the old schema version can no longer be read, but since it was just a duplicate of the key value (effectively a placeholder), this is not an issue.", "author": "errose28", "createdAt": "2020-09-02T18:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNjEwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwODQwNA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479508404", "bodyText": "This would mean someone iterating through all the deleted blocks in a DN might get a mixture of keys with and without the deleted key prefix.\nIs there such a scenario in the code currently?", "author": "hanishakoneru", "createdAt": "2020-08-28T19:59:13Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5MjIzNA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485192234", "bodyText": "No such scenario that I am aware of, but I agree that this implementation is not ideal. I will change these methods to get the list from the parent class, and then make a pass through it to remove the prefixes before returning the list to the caller. This implementation would be similar to how TypedTable currently applies typing to iterator results returned from its raw table.", "author": "errose28", "createdAt": "2020-09-08T20:58:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwODQwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMzM2NA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479523364", "bodyText": "This test is dependent on the order of deleted blocks. Does RocksDB iterator ensure that the deleting and deleted block keys are returned in the same sorted order?", "author": "hanishakoneru", "createdAt": "2020-08-28T20:36:56Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java", "diffHunk": "@@ -471,4 +470,59 @@ public void testBlockThrottle() throws Exception {\n       service.shutdown();\n     }\n   }\n+\n+  @Test\n+  public void testDeletedChunkInfo() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    conf.setInt(OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL, 10);\n+    conf.setInt(OZONE_BLOCK_DELETING_LIMIT_PER_CONTAINER, 2);\n+    ContainerSet containerSet = new ContainerSet();\n+    createToDeleteBlocks(containerSet, conf, 1, 2, 3);\n+\n+    List<ContainerData> containerData = Lists.newArrayList();\n+    containerSet.listContainer(0L, 1, containerData);\n+\n+    try(ReferenceCountedDB meta = BlockUtils.getDB(\n+            (KeyValueContainerData) containerData.get(0), conf)) {\n+\n+      // Collect all ChunkInfo from blocks marked for deletion.\n+      List<? extends Table.KeyValue<String, BlockData>> deletingBlocks =\n+              meta.getStore().getBlockDataTable()\n+              .getRangeKVs(null, 100,\n+                      MetadataKeyFilters.getDeletingKeyFilter());\n+\n+      // Delete all blocks marked for deletion.\n+      BlockDeletingServiceTestImpl svc =\n+              getBlockDeletingService(containerSet, conf);\n+      svc.start();\n+      GenericTestUtils.waitFor(svc::isStarted, 100, 3000);\n+      deleteAndWait(svc, 1);\n+      svc.shutdown();\n+\n+      // Get deleted blocks from their table, and check their ChunkInfo lists\n+      // against those we saved for them before deletion.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              meta.getStore().getDeletedBlocksTable()\n+              .getRangeKVs(null, 100);\n+\n+      Assert.assertEquals(deletingBlocks.size(), deletedBlocks.size());\n+\n+      Iterator<? extends Table.KeyValue<String, BlockData>>\n+              deletingBlocksIter = deletingBlocks.iterator();\n+      Iterator<? extends Table.KeyValue<String, ChunkInfoList>>\n+              deletedBlocksIter = deletedBlocks.iterator();\n+\n+      while(deletingBlocksIter.hasNext() && deletedBlocksIter.hasNext())  {\n+        List<ContainerProtos.ChunkInfo> deletingChunks =\n+                deletingBlocksIter.next().getValue().getChunks();\n+        List<ContainerProtos.ChunkInfo> deletedChunks =\n+                deletedBlocksIter.next().getValue().asList();\n+", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4MzIxMQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482283211", "bodyText": "Yes, the iterator will return blocks in sorted order. The existing code already depends on this property in the existing test cases found in TestKeyValueBlockIterator.", "author": "errose28", "createdAt": "2020-09-02T18:29:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMzM2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzODM3NQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479538375", "bodyText": "Nitpick: It would be good to have a method in ContainerCache which calls the current getDB with Schema_Latest. We can avoid specifying the schema version every time then.\npublic ReferenceCountedDB getDB(long containerID, String containerDBType, String containerDBPath,\n    ConfigurationSource conf) {\n    getDB(containerID, containerDBType, OzoneConsts.SCHEMA_LATEST, conf);\n}", "author": "hanishakoneru", "createdAt": "2020-08-28T21:17:00Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestContainerCache.java", "diffHunk": "@@ -85,17 +86,17 @@ public void testContainerCacheEviction() throws Exception {\n \n     // Get 2 references out of the same db and verify the objects are same.\n     ReferenceCountedDB db1 = cache.getDB(1, \"RocksDB\",\n-        containerDir1.getPath(), conf);\n+            containerDir1.getPath(), OzoneConsts.SCHEMA_LATEST, conf);", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5NTg4MQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485195881", "bodyText": "This could be done, but it may not be clear to the caller that the latest schema version would be applied. Since there is no mention of it in the method name, the caller may assume that the schema version would be automatically determined based on some property of the DB. Aside from this series of calls in one test, the method is only called once in the code, so I think it is OK to leave it the way it is for clarity.", "author": "errose28", "createdAt": "2020-09-08T21:05:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzODM3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzI3Mg==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479547272", "bodyText": "Shouldn't Bytes used be decreased by the deleted blocks size?", "author": "hanishakoneru", "createdAt": "2020-08-28T21:43:21Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4NTYyNw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482285627", "bodyText": "Yes, I think it should. I will investigate why this passes because it does look incorrect.", "author": "errose28", "createdAt": "2020-09-02T18:32:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5ODEzOA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485198138", "bodyText": "It turns out the bytes used is not updated due to the mock objects used to create the block deleting service for tests. This is actually consistent with the way the existing TestBlockDeletingService unit tests work, and there is a comment in those tests noting that the bytes used will not be updated. I will follow the lead of TestBlockDeletingService and replace this assert with a comment explaining this.", "author": "errose28", "createdAt": "2020-09-08T21:10:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzI3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzQ3NQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479547475", "bodyText": "Unused code block.", "author": "hanishakoneru", "createdAt": "2020-08-28T21:44:03Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading the chunk info saved from a block that was deleted from a\n+   * database in schema version one. Blocks deleted from schema version one\n+   * before the upgrade will have the block ID saved as their value. Trying\n+   * to retrieve this value as a {@link ChunkInfoList} should fail. Blocks\n+   * deleted from schema version one after the upgrade should have their\n+   * {@link ChunkInfoList} saved as the corresponding value in the deleted\n+   * blocks table. Reading these values should succeed.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadDeletedBlockChunkInfo() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Read blocks that were already deleted before the upgrade.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      Set<String> preUpgradeBlocks = new HashSet<>();\n+\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        preUpgradeBlocks.add(chunkListKV.getKey());\n+        try {\n+          chunkListKV.getValue();\n+          Assert.fail(\"No exception thrown when trying to retrieve old \" +\n+                  \"deleted blocks values as chunk lists.\");\n+        } catch(IOException ex) {\n+          // Exception thrown as expected.\n+        }\n+      }\n+\n+      Assert.assertEquals(TestDB.NUM_DELETED_BLOCKS, preUpgradeBlocks.size());\n+\n+      runBlockDeletingService();\n+\n+      // After the block deleting service runs, get the updated list of\n+      // deleted blocks.\n+      deletedBlocks = refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      int numPostUpgradeDeletesFound = 0;\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        if (!preUpgradeBlocks.contains(chunkListKV.getKey())) {\n+          numPostUpgradeDeletesFound++;\n+          Assert.assertNotNull(chunkListKV.getValue());\n+        }\n+      }\n+\n+      // The blocks that were originally marked for deletion should now be\n+      // deleted.\n+      Assert.assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              numPostUpgradeDeletesFound);\n+    }\n+\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore().getDeletedBlocksTable()\n+                      .getRangeKVs(null, 100);", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4NjE5OQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482286199", "bodyText": "Yes, looks like a copy/paste leftover, will remove.", "author": "errose28", "createdAt": "2020-09-02T18:32:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzQ3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTk2MA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479555960", "bodyText": "The general continuation indent used throughout the code is 4.\n@elek, any idea if continuation indent should be 4 or does it not matter? Checkstyle CI seems to not care about it.", "author": "hanishakoneru", "createdAt": "2020-08-28T22:13:26Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4NzA5Mw==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482287093", "bodyText": "Not sure why lines 115-116 are split up, I will combine them. Looks like I had my IntelliJ line continuation set for 8 instead of 4. I will see if I can change that.", "author": "errose28", "createdAt": "2020-09-02T18:33:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTk2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU1NTk2NA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r483555964", "bodyText": "@elek, any idea if continuation indent should be 4 or does it not matter? Checkstyle CI seems to not care about it.\n\nI don't think there is any rule. It can be reasonable to introduce new one (to reduce noise during merges) but in that case we should provide good auto-formatting rules to avoid any overhead.\n(Similar to the import format, we can improve checkstyle, but we also need autoformatter rules).\nI think @maobaolong had a few ideas, but the effort is blocked by #921", "author": "elek", "createdAt": "2020-09-04T11:24:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTk2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NDgzMg==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479574832", "bodyText": "Nitpick: Expected value comes before Actual value in assert. It might be confusing in case there is an exception.", "author": "hanishakoneru", "createdAt": "2020-08-28T23:36:54Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4Nzk5Mg==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482287992", "bodyText": "Yes, I will swap the values.", "author": "errose28", "createdAt": "2020-09-02T18:34:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NDgzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NzE4NA==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479577184", "bodyText": "Javadoc is not clear.", "author": "hanishakoneru", "createdAt": "2020-08-28T23:49:26Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));\n+        counter++;\n       }\n-      assertEquals(10, counter);\n+\n+      assertEquals(deletingBlocks, counter);\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithOnlyDeletedBlocks() throws\n       Exception {\n-    long containerId = 104L;\n-    createContainerWithBlocks(containerId, 0, 5);\n+    createContainerWithBlocks(CONTAINER_ID, 0, 5);\n     String containerPath = new File(containerData.getMetadataPath())\n         .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath))) {\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n       //As all blocks are deleted blocks, blocks does not match with normal key\n       // filter.\n       assertFalse(keyValueBlockIterator.hasNext());\n     }\n   }\n \n+  /**\n+   * Due to RocksDB internals, prefixed keys may be grouped all at the\n+   * beginning or end of the key iteration, depending on the serialization\n+   * used. Keys of the same prefix are grouped\n+   * together. This method runs the same set of tests on the iterator first\n+   * positively filtering one prefix, and then positively filtering\n+   * a second prefix. If the sets of keys with prefix one, prefix\n+   * two, and no prefixes are not empty, it follows that the filter will\n+   * encounter both of the following cases:\n+   *\n+   * 1. A failing key followed by a passing key.\n+   * 2. A passing key followed by a failing key.\n+   *\n+   * Note that with the current block data table implementation, there is\n+   * only ever one type of prefix. This test adds a dummy second prefix type\n+   * to ensure that the iterator will continue to work if more prefixes are\n+   * added in the future.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testKeyValueBlockIteratorWithAdvancedFilter() throws\n+          Exception {\n+    // Block data table currently only uses one prefix type.\n+    // Introduce a second prefix type to make sure the iterator functions\n+    // correctly if more prefixes were to be added in the future.\n+    final String secondPrefix = \"#FOOBAR#\";\n+    Map<String, Integer> prefixCounts = new HashMap<>();\n+    prefixCounts.put(OzoneConsts.DELETING_KEY_PREFIX, 3);\n+    prefixCounts.put(\"\", 3);\n+    prefixCounts.put(secondPrefix, 3);\n+\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            prefixCounts);\n+    // Test deleting filter.\n+    testWithFilter(MetadataKeyFilters.getDeletingKeyFilter(),\n+            blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX));\n+\n+    // Test arbitrary filter.\n+    MetadataKeyFilters.KeyPrefixFilter secondFilter =\n+            new MetadataKeyFilters.KeyPrefixFilter()\n+            .addFilter(secondPrefix);\n+    testWithFilter(secondFilter, blockIDs.get(secondPrefix));\n+  }\n+\n+  /**\n+   * Helper method to run some iterator tests with a provided filter.\n+   */\n+  private void testWithFilter(MetadataKeyFilters.KeyPrefixFilter filter,\n+                              List<Long> expectedIDs) throws Exception {\n+    try(BlockIterator<BlockData> iterator =\n+                db.getStore().getBlockIterator(filter)) {\n+      // Test seek.\n+      iterator.seekToFirst();\n+      long firstID = iterator.nextBlock().getLocalID();\n+      assertEquals(expectedIDs.get(0).longValue(), firstID);\n+      assertTrue(iterator.hasNext());\n+\n+      // Test atypical iteration use.\n+      iterator.seekToFirst();\n+      int numIDsSeen = 0;\n+      for (long id: expectedIDs) {\n+        assertEquals(iterator.nextBlock().getLocalID(), id);\n+        numIDsSeen++;\n+\n+        // Test that iterator can handle sporadic hasNext() calls.\n+        if (id % 2 == 0 && numIDsSeen < expectedIDs.size()) {\n+          assertTrue(iterator.hasNext());\n+        }\n+      }\n+\n+      assertFalse(iterator.hasNext());\n+    }\n+  }\n+\n+  /**\n+   * Creates a container with specified number of unprefixed (normal) blocks.\n+   * @param containerId\n+   * @param normalBlocks\n+   * @return The list of block IDs of normal blocks that were created.\n+   * @throws Exception\n+   */\n+  private List<Long> createContainerWithBlocks(long containerId,\n+            int normalBlocks) throws Exception {\n+    return createContainerWithBlocks(containerId, normalBlocks, 0).get(\"\");\n+  }\n+\n   /**\n    * Creates a container with specified number of normal blocks and deleted\n-   * blocks. First it will insert normal blocks, and then it will insert\n-   * deleted blocks.\n+   * blocks.\n+   * deleting blocks, then it will insert deleted blocks.", "originalCommit": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4ODc4NQ==", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482288785", "bodyText": "Looks like I made some bizarre copy/paste typo, will fix.", "author": "errose28", "createdAt": "2020-09-02T18:35:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NzE4NA=="}], "type": "inlineReview"}, {"oid": "c00f46a37fba58ac9c9036c2d3c262f75fd264b2", "url": "https://github.com/apache/ozone/commit/c00f46a37fba58ac9c9036c2d3c262f75fd264b2", "message": "Fix comments, dead code, and formatting issues raised in code review", "committedDate": "2020-09-02T21:16:36Z", "type": "commit"}, {"oid": "1501acbad98861fce113f9e68a3900d127276e77", "url": "https://github.com/apache/ozone/commit/1501acbad98861fce113f9e68a3900d127276e77", "message": "Fix checkstyle error", "committedDate": "2020-09-03T16:12:12Z", "type": "commit"}, {"oid": "de0cae3f070598a2f6150da940e1a4a3b78c9805", "url": "https://github.com/apache/ozone/commit/de0cae3f070598a2f6150da940e1a4a3b78c9805", "message": "Remove unused implementation of table cache method", "committedDate": "2020-09-03T17:08:19Z", "type": "commit"}, {"oid": "d68a63b1ec623564ffa58fa3df421fd88cfb946c", "url": "https://github.com/apache/ozone/commit/d68a63b1ec623564ffa58fa3df421fd88cfb946c", "message": "Remove prefixes from keys in returned lists\n\nRequires doing an extra pass through the generated results.", "committedDate": "2020-09-03T19:22:14Z", "type": "commit"}, {"oid": "f29ea9d791f1fe392ef77ca48b731f35e0b725ce", "url": "https://github.com/apache/ozone/commit/f29ea9d791f1fe392ef77ca48b731f35e0b725ce", "message": "Add unit test to check that deleted block table iterators have no prefixes", "committedDate": "2020-09-03T19:38:17Z", "type": "commit"}, {"oid": "2b11bc631f958184f0a41291a75e94bf66690247", "url": "https://github.com/apache/ozone/commit/2b11bc631f958184f0a41291a75e94bf66690247", "message": "Merge branch 'preprocess-schema1-iterators' into HDDS-3869\n\n* preprocess-schema1-iterators:\n  Add unit test to check that deleted block table iterators have no prefixes\n  Remove prefixes from keys in returned lists", "committedDate": "2020-09-08T20:39:25Z", "type": "commit"}, {"oid": "43cac801e344f64e8000d82f9cd0b9cd17c0df3d", "url": "https://github.com/apache/ozone/commit/43cac801e344f64e8000d82f9cd0b9cd17c0df3d", "message": "Switch block length calculation back to original method", "committedDate": "2020-09-08T21:40:47Z", "type": "commit"}, {"oid": "b2910162610a16e35513b5835c988fea642bd58f", "url": "https://github.com/apache/ozone/commit/b2910162610a16e35513b5835c988fea642bd58f", "message": "Remove check of bytes used after test block delete\n\nA comment was added to explain why the value is not updated due to mock classes used.", "committedDate": "2020-09-08T21:48:48Z", "type": "commit"}, {"oid": "899c4acff7cfb7f3f97c28257ae72b20fe86feee", "url": "https://github.com/apache/ozone/commit/899c4acff7cfb7f3f97c28257ae72b20fe86feee", "message": "Move schema version null check to container data's schema version setter\n\nA null value for schema version indicates it has schema version 1.", "committedDate": "2020-09-09T14:11:59Z", "type": "commit"}, {"oid": "2d4146686034cb63a78b7f1a47d634c9881636f9", "url": "https://github.com/apache/ozone/commit/2d4146686034cb63a78b7f1a47d634c9881636f9", "message": "All unit tests pass with new method of reading in container\n\nNew container reading method gets the container file the same way the actual volume reading code does.", "committedDate": "2020-09-09T15:15:05Z", "type": "commit"}, {"oid": "43a71791b716fc36b4f13a55e81781d843eaba87", "url": "https://github.com/apache/ozone/commit/43a71791b716fc36b4f13a55e81781d843eaba87", "message": "Remove fileds from .container file that must be calculated at run time", "committedDate": "2020-09-09T15:38:57Z", "type": "commit"}, {"oid": "ce1466364ade66afaf3a2288f219def12af85d38", "url": "https://github.com/apache/ozone/commit/ce1466364ade66afaf3a2288f219def12af85d38", "message": "Merge branch 'HDDS-3869-refactor-schema-tests' into HDDS-3869\n\n* HDDS-3869-refactor-schema-tests:\n  Remove fileds from .container file that must be calculated at run time\n  All unit tests pass with new method of reading in container", "committedDate": "2020-09-09T15:42:54Z", "type": "commit"}, {"oid": "30b1fcd346f3eb4dd3a611ec7ce738f3b27139b5", "url": "https://github.com/apache/ozone/commit/30b1fcd346f3eb4dd3a611ec7ce738f3b27139b5", "message": "Add more comments on usage of DatanodeTable", "committedDate": "2020-09-09T15:56:16Z", "type": "commit"}, {"oid": "467d71a8db813d74f3bdae937d8aea3ff184ea9c", "url": "https://github.com/apache/ozone/commit/467d71a8db813d74f3bdae937d8aea3ff184ea9c", "message": "Fix checkstyle errors", "committedDate": "2020-09-09T16:13:56Z", "type": "commit"}, {"oid": "9662789ce404df655bc90826ad72654673afe6d0", "url": "https://github.com/apache/ozone/commit/9662789ce404df655bc90826ad72654673afe6d0", "message": "Add options cache optimization from HDDS-2283 to new API\n\nThis optimization saves RocksDB options objects based on the configuration\nused, so that the expensive JNI call to create a new object is not performed\nevery time a new container is created. In HDDS-2283 and this commit,\nthis feature was not done for WriteOptions.", "committedDate": "2020-09-21T20:16:54Z", "type": "commit"}, {"oid": "22182bb7fd608a47efc48e5a448b7b5853ffc912", "url": "https://github.com/apache/ozone/commit/22182bb7fd608a47efc48e5a448b7b5853ffc912", "message": "Merge branch 'HDDS-3869-cache-db-options' into HDDS-3869\n\n* HDDS-3869-cache-db-options:\n  Add options cache optimization from HDDS-2283 to new API", "committedDate": "2020-09-21T20:20:08Z", "type": "commit"}, {"oid": "dd5bbd928c17453373e3d1de1ce87d07fc956cea", "url": "https://github.com/apache/ozone/commit/dd5bbd928c17453373e3d1de1ce87d07fc956cea", "message": "Move schema version null check back to the container data parser\n\nThe default schema version on null value cannot be set when reading in the .container file, because it will fail checksum validation.", "committedDate": "2020-09-24T20:49:39Z", "type": "commit"}, {"oid": "3effacda78c2748c0408516fc41662fdc5d0260f", "url": "https://github.com/apache/ozone/commit/3effacda78c2748c0408516fc41662fdc5d0260f", "message": "Remove options cache implementation from new database utils\n\nThis implementation causes seg faults at the C++ level in RocksDB.", "committedDate": "2020-09-24T21:51:27Z", "type": "commit"}, {"oid": "711017e5effa66bd22f7ee0994de2ec42d96e8bf", "url": "https://github.com/apache/ozone/commit/711017e5effa66bd22f7ee0994de2ec42d96e8bf", "message": "Add unit tests to expose issues with encoding/decoding block data keys", "committedDate": "2020-09-30T15:45:47Z", "type": "commit"}, {"oid": "c34291be1122667edc16c4b4a75e920f80e792a6", "url": "https://github.com/apache/ozone/commit/c34291be1122667edc16c4b4a75e920f80e792a6", "message": "Add new codec for keys that passes unit tests", "committedDate": "2020-09-30T18:33:14Z", "type": "commit"}, {"oid": "471092d8648338956f209d25546c5c95906674f6", "url": "https://github.com/apache/ozone/commit/471092d8648338956f209d25546c5c95906674f6", "message": "Add comments and clearer class name for new codec", "committedDate": "2020-09-30T21:00:55Z", "type": "commit"}, {"oid": "620c9a74a5e6839e3849bc87f70243e0b3fdc940", "url": "https://github.com/apache/ozone/commit/620c9a74a5e6839e3849bc87f70243e0b3fdc940", "message": "Fix checkstyle errors", "committedDate": "2020-09-30T21:05:49Z", "type": "commit"}, {"oid": "c6ff9e1129baba110179cab7cb88d12fa154616c", "url": "https://github.com/apache/ozone/commit/c6ff9e1129baba110179cab7cb88d12fa154616c", "message": "Merge branch 'HDDS-3869-fix-block-keycodecs' into HDDS-3869\n\n* HDDS-3869-fix-block-keycodecs:\n  Fix checkstyle errors\n  Add comments and clearer class name for new codec\n  Add new codec for keys that passes unit tests\n  Add unit tests to expose issues with encoding/decoding block data keys", "committedDate": "2020-09-30T21:06:10Z", "type": "commit"}, {"oid": "f1646012420978445b519108df09e8419d904c31", "url": "https://github.com/apache/ozone/commit/f1646012420978445b519108df09e8419d904c31", "message": "Change incorrect string keys to longs in DB\n\nThe change was accidentally made in the target resource originally, not the source files.", "committedDate": "2020-10-01T13:17:30Z", "type": "commit"}, {"oid": "23283dfc3b1f0c8c15881249f1deda1c6224ba23", "url": "https://github.com/apache/ozone/commit/23283dfc3b1f0c8c15881249f1deda1c6224ba23", "message": "Fix codec bugs to pass acceptance tests", "committedDate": "2020-10-01T16:34:36Z", "type": "commit"}, {"oid": "183ffc86d83d678e39d0118abf0c8ff69839d23c", "url": "https://github.com/apache/ozone/commit/183ffc86d83d678e39d0118abf0c8ff69839d23c", "message": "Add test of block iterator", "committedDate": "2020-10-01T17:02:33Z", "type": "commit"}, {"oid": "82bcb8e452887216103db0b30631ba4912e0ca3e", "url": "https://github.com/apache/ozone/commit/82bcb8e452887216103db0b30631ba4912e0ca3e", "message": "Fix checkstyle violations", "committedDate": "2020-10-01T18:29:15Z", "type": "commit"}, {"oid": "fce5e9fdaf108c4c89024c34593a9f5db1d7088e", "url": "https://github.com/apache/ozone/commit/fce5e9fdaf108c4c89024c34593a9f5db1d7088e", "message": "Switch key type of block data table from long to string\n\nAlthough the block IDs are longs, the prefixes sometimes used requires the type\nto be a string.", "committedDate": "2020-07-21T13:37:18Z", "type": "commit"}, {"oid": "e1cf80afe17efd50826d5a61b837ab53ae1e08c1", "url": "https://github.com/apache/ozone/commit/e1cf80afe17efd50826d5a61b837ab53ae1e08c1", "message": "Switch calls to sequential and non sequential getRangeKV() calls to the new interface", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "af84fa54db12378b951d929de7654e2bd623b47b", "url": "https://github.com/apache/ozone/commit/af84fa54db12378b951d929de7654e2bd623b47b", "message": "Switch KeyValueBlockIterator to new iterator interface", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "bb87771e44f99c31a4e0ec2ad1a3b8f45f5b4e7a", "url": "https://github.com/apache/ozone/commit/bb87771e44f99c31a4e0ec2ad1a3b8f45f5b4e7a", "message": "Make ContainerCache.getDB() not require the schema version\n\nIf it pulls a cached container, we don't need to set it up.\nIf it needs to create a new container, we always use the two table version.", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "ecd876e2e32c5ce39396c746d7b5734266c9afe1", "url": "https://github.com/apache/ozone/commit/ecd876e2e32c5ce39396c746d7b5734266c9afe1", "message": "Remove unused imports", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "c81e2fc6de792be8ef65dc71de24b03cfa7a279c", "url": "https://github.com/apache/ozone/commit/c81e2fc6de792be8ef65dc71de24b03cfa7a279c", "message": "Add new constructor to DBStore builder that allows specifying the DBDefintiion", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "a5bd13cd2d8ddedd265956270d4b94d928535863", "url": "https://github.com/apache/ozone/commit/a5bd13cd2d8ddedd265956270d4b94d928535863", "message": "Add getDBLocation method for Datanode DBDefinition to use instead of location config key", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "c5974bd908b4fe87258305d623567a09c89c0d42", "url": "https://github.com/apache/ozone/commit/c5974bd908b4fe87258305d623567a09c89c0d42", "message": "Update container cache test to provide a schema version when using the cache", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "765d5aea2595fbc7e09ad068175976117ca760a9", "url": "https://github.com/apache/ozone/commit/765d5aea2595fbc7e09ad068175976117ca760a9", "message": "Update method header and fix string comparison for schema version", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "0d760943e7bf237851913b8314d1f36895a00e4c", "url": "https://github.com/apache/ozone/commit/0d760943e7bf237851913b8314d1f36895a00e4c", "message": "Remove extra import in BlockManagerImpl that was accidentally added", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "ca442c36655e64e13daacd12c6c2a96910b5abbb", "url": "https://github.com/apache/ozone/commit/ca442c36655e64e13daacd12c6c2a96910b5abbb", "message": "Re add schema version parameter to in BlockUtils after it was removed by the merge", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "94d373b45c401b65987d46c67a12f2d2879de280", "url": "https://github.com/apache/ozone/commit/94d373b45c401b65987d46c67a12f2d2879de280", "message": "Remove extra import that was already added", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "157b427d8b4d89ba950a7f06ec1be6450f8dab65", "url": "https://github.com/apache/ozone/commit/157b427d8b4d89ba950a7f06ec1be6450f8dab65", "message": "Fix bug where DB name was used instead of column family name", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "a58773fd9fd6f500be52db34baeace802306f344", "url": "https://github.com/apache/ozone/commit/a58773fd9fd6f500be52db34baeace802306f344", "message": "Fix bugs with missing column families and mismatched ContainerCache params", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "39389c9a58303f63dc62a3d447c80665c372c872", "url": "https://github.com/apache/ozone/commit/39389c9a58303f63dc62a3d447c80665c372c872", "message": "Fix bug to allow null key to be used as start key in key range query", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "242452a49747141453aef9b1853feee1aa67c6b1", "url": "https://github.com/apache/ozone/commit/242452a49747141453aef9b1853feee1aa67c6b1", "message": "Make DeleteBlocksCommandHandler.deleteKeyValueContainerBlocks() use block data table instead of metadata table", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "300f445e566288f96d9ab600d548681b4f4f649d", "url": "https://github.com/apache/ozone/commit/300f445e566288f96d9ab600d548681b4f4f649d", "message": "Fix null start key error that was missed in the original fix", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "8967675dee11a361126d7f01f573eb4596803376", "url": "https://github.com/apache/ozone/commit/8967675dee11a361126d7f01f573eb4596803376", "message": "Remove early batch operation commit in BlockDeletingService", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "0ae45e213d0712067f8732499b623575a57d7714", "url": "https://github.com/apache/ozone/commit/0ae45e213d0712067f8732499b623575a57d7714", "message": "Fix bug where .db file was created in wrong directory\n\nThe added method DBDefinition.getDBLocation() is now defined to return the parent directory of the .db file.", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "0941deec80a88cdc36790dcac6a5e39f468ea7b2", "url": "https://github.com/apache/ozone/commit/0941deec80a88cdc36790dcac6a5e39f468ea7b2", "message": "Fix bug where default column family was being registered twice", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "ab2fd6cf98bb52fb6dc32b74fcbda803d7b9e6cf", "url": "https://github.com/apache/ozone/commit/ab2fd6cf98bb52fb6dc32b74fcbda803d7b9e6cf", "message": "Make testContainerImportExport write to block and metadata tables where needed", "committedDate": "2020-07-21T13:37:22Z", "type": "commit"}, {"oid": "2e89fc0e08c010216df686d7083aa81f3a197e79", "url": "https://github.com/apache/ozone/commit/2e89fc0e08c010216df686d7083aa81f3a197e79", "message": "Minor readability change", "committedDate": "2020-07-21T13:37:23Z", "type": "commit"}, {"oid": "ccc58847c7f62afd939a6a2b283075067188823a", "url": "https://github.com/apache/ozone/commit/ccc58847c7f62afd939a6a2b283075067188823a", "message": "Remove missed git artifacts after conflict", "committedDate": "2020-07-21T13:39:00Z", "type": "commit"}, {"oid": "79b7fc4973e396b7c461ed3538120e2fad56ed8d", "url": "https://github.com/apache/ozone/commit/79b7fc4973e396b7c461ed3538120e2fad56ed8d", "message": "Switch merged in unit test to new interface", "committedDate": "2020-07-21T13:39:00Z", "type": "commit"}, {"oid": "6f53850bacb8a1aee64082605ac8576805ef4cfa", "url": "https://github.com/apache/ozone/commit/6f53850bacb8a1aee64082605ac8576805ef4cfa", "message": "Update merged unit test to use new interface", "committedDate": "2020-07-21T13:39:00Z", "type": "commit"}, {"oid": "a3bc1866528efb8c9ea48210b470f5bd5623e9e7", "url": "https://github.com/apache/ozone/commit/a3bc1866528efb8c9ea48210b470f5bd5623e9e7", "message": "Add schema version param to ContainerCache.getDB() after it was accidentally removed in merge", "committedDate": "2020-07-21T13:39:00Z", "type": "commit"}, {"oid": "8807d0c61350572be827e4229fd85d3531f2df4a", "url": "https://github.com/apache/ozone/commit/8807d0c61350572be827e4229fd85d3531f2df4a", "message": "Remove duplicate method call introduced accidentally when resolving merge conflict", "committedDate": "2020-07-21T13:39:01Z", "type": "commit"}, {"oid": "081e3e064133aa181a2a9e396948638d8b286e47", "url": "https://github.com/apache/ozone/commit/081e3e064133aa181a2a9e396948638d8b286e47", "message": "Move new key value block iterator implementation and tests to new interface", "committedDate": "2020-07-21T13:40:31Z", "type": "commit"}, {"oid": "5b59a73793f3a5414d748f54933b263f29ff2425", "url": "https://github.com/apache/ozone/commit/5b59a73793f3a5414d748f54933b263f29ff2425", "message": "Import schema version when importing container data from export", "committedDate": "2020-07-21T13:40:35Z", "type": "commit"}, {"oid": "98dc6b772bbdae17f50afaa2f332e1b5aaae755e", "url": "https://github.com/apache/ozone/commit/98dc6b772bbdae17f50afaa2f332e1b5aaae755e", "message": "Move block delete to correct table and remove debugging print statement", "committedDate": "2020-07-21T13:40:35Z", "type": "commit"}, {"oid": "c08d6b99e6021e3551df7aa3e03f221b3fdc1bf5", "url": "https://github.com/apache/ozone/commit/c08d6b99e6021e3551df7aa3e03f221b3fdc1bf5", "message": "Have block deleting service test look for #deleted# keys in metadata table", "committedDate": "2020-07-21T13:40:35Z", "type": "commit"}, {"oid": "690f8178dff8aea10ed7e89d2600253229fc427f", "url": "https://github.com/apache/ozone/commit/690f8178dff8aea10ed7e89d2600253229fc427f", "message": "HDDS-3987. Encrypted bucket creation failed with INVALID_REQUEST Encryption cannot be set for bucket links (#1221)", "committedDate": "2020-07-21T13:40:35Z", "type": "commit"}, {"oid": "da17469aff5ece05810568fceac4b0d650b14914", "url": "https://github.com/apache/ozone/commit/da17469aff5ece05810568fceac4b0d650b14914", "message": "HDDS-3982. Disable moveToTrash in o3fs and ofs temporarily (#1215)", "committedDate": "2020-07-21T13:40:35Z", "type": "commit"}, {"oid": "75d1273da6bb400aec0bc7611592cb8346756077", "url": "https://github.com/apache/ozone/commit/75d1273da6bb400aec0bc7611592cb8346756077", "message": "Update ratis to 1.0.0 (#1222)", "committedDate": "2020-07-21T13:40:35Z", "type": "commit"}, {"oid": "801d9aebd51eeb770b4675b7b76f3f1d59601500", "url": "https://github.com/apache/ozone/commit/801d9aebd51eeb770b4675b7b76f3f1d59601500", "message": "HDDS-3813. Upgrade Ratis third-party, too (#1229)", "committedDate": "2020-07-21T13:40:35Z", "type": "commit"}, {"oid": "5e7f2e4610dca4dd5de55cc39d9e5e700464cac3", "url": "https://github.com/apache/ozone/commit/5e7f2e4610dca4dd5de55cc39d9e5e700464cac3", "message": "Make block iterator tests use deleted blocks table, and remove the now unused #deleted#\n\nUnit tests for BlockDeletingService and KeyValueBlockIterator pass.", "committedDate": "2020-07-21T14:59:52Z", "type": "commit"}, {"oid": "2fa068b69faa2e9f2690094f1374745157777ca3", "url": "https://github.com/apache/ozone/commit/2fa068b69faa2e9f2690094f1374745157777ca3", "message": "Merge branch 'add-deleted-block-table' into HDDS-3869\n\n* add-deleted-block-table: (63 commits)\n  Make block iterator tests use deleted blocks table, and remove the now unused #deleted#\n  Replace uses of #deleted# key prefix with access to new deleted blocks table\n  Add deleted blocks table to base level DB wrappers\n  Have block deleting service test look for #deleted# keys in metadata table\n  Move block delete to correct table and remove debugging print statement\n  Import schema version when importing container data from export\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  Move new key value block iterator implementation and tests to new interface\n  Fix checkstyle violations\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)\n  Update comments\n  Add comments on added helper method\n  Remove seekToLast() from iterator interface, implementation, and tests\n  Add more robust unit test with alternating key matches\n  All unit tests pass after allowing keys with deleted and deleting prefixes to be made\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  ...", "committedDate": "2020-07-21T15:26:18Z", "type": "commit"}, {"oid": "5c911b0e6cb9004eb63438f30a68bdc2d3d13e3b", "url": "https://github.com/apache/ozone/commit/5c911b0e6cb9004eb63438f30a68bdc2d3d13e3b", "message": "Fix checkstyle and findbugs errors", "committedDate": "2020-07-21T20:51:44Z", "type": "commit"}, {"oid": "cf6fb9bd0358fb47f6af9cebc9ac6dd7fb52c93a", "url": "https://github.com/apache/ozone/commit/cf6fb9bd0358fb47f6af9cebc9ac6dd7fb52c93a", "message": "Remove now unused key prefix encodings from OzoneConsts", "committedDate": "2020-07-22T15:00:22Z", "type": "commit"}, {"oid": "24036f595871df92b9c06da5d9f43fc9aa905856", "url": "https://github.com/apache/ozone/commit/24036f595871df92b9c06da5d9f43fc9aa905856", "message": "Make new OzoneConst SCHEMA_LATEST variable to hold newest schema version", "committedDate": "2020-07-22T15:06:22Z", "type": "commit"}, {"oid": "d71a8e509a390615317f266e2c7d395b7e8d91a2", "url": "https://github.com/apache/ozone/commit/d71a8e509a390615317f266e2c7d395b7e8d91a2", "message": "Create placeholder and codec for empty byte array", "committedDate": "2020-07-22T15:25:35Z", "type": "commit"}, {"oid": "5d94ed15244b731f1c6702efb0ab61e7eebb9627", "url": "https://github.com/apache/ozone/commit/5d94ed15244b731f1c6702efb0ab61e7eebb9627", "message": "Update classes to use the new NoData value for deleted blocks table", "committedDate": "2020-07-22T15:34:33Z", "type": "commit"}, {"oid": "b5af1b8b4e1bf30840fba7bf9b51cf76e39d9f5e", "url": "https://github.com/apache/ozone/commit/b5af1b8b4e1bf30840fba7bf9b51cf76e39d9f5e", "message": "Add test for write and read of schema version to container data file", "committedDate": "2020-07-22T19:28:27Z", "type": "commit"}, {"oid": "a74145552ef7c04133a4833e4f2d6e3922749772", "url": "https://github.com/apache/ozone/commit/a74145552ef7c04133a4833e4f2d6e3922749772", "message": "Add test of schema version set and get for KeyValueContainerData", "committedDate": "2020-07-23T13:36:30Z", "type": "commit"}, {"oid": "5f03f9d37ef0f32adf3eff94bd842058418fcb08", "url": "https://github.com/apache/ozone/commit/5f03f9d37ef0f32adf3eff94bd842058418fcb08", "message": "Create initial outline of files for testing schema version backwards compatability", "committedDate": "2020-07-23T15:29:56Z", "type": "commit"}, {"oid": "83270063d1e8076cf7c48f700736f5ab4597ec96", "url": "https://github.com/apache/ozone/commit/83270063d1e8076cf7c48f700736f5ab4597ec96", "message": "Fix naming of db resources so they are picked up by unit tests", "committedDate": "2020-07-23T20:47:46Z", "type": "commit"}, {"oid": "1842c0f3b75ed6c4bb5a5cc7665c00d1b414945c", "url": "https://github.com/apache/ozone/commit/1842c0f3b75ed6c4bb5a5cc7665c00d1b414945c", "message": "Correct values in test resources and copy and r/w them from temp folder", "committedDate": "2020-07-24T14:55:00Z", "type": "commit"}, {"oid": "8669e6dc5b2c829a673f96ef69637af8b9c1cc12", "url": "https://github.com/apache/ozone/commit/8669e6dc5b2c829a673f96ef69637af8b9c1cc12", "message": "Add test for reading container when metadata is absent", "committedDate": "2020-07-24T16:31:24Z", "type": "commit"}, {"oid": "f31ae5e506efdacd5ef2401e5e5ccd13adc93cf6", "url": "https://github.com/apache/ozone/commit/f31ae5e506efdacd5ef2401e5e5ccd13adc93cf6", "message": "Make class to hold information about the database under test", "committedDate": "2020-07-24T18:56:27Z", "type": "commit"}, {"oid": "5fe2cff8bab045576eeeb507b636b9b57dfd4fe4", "url": "https://github.com/apache/ozone/commit/5fe2cff8bab045576eeeb507b636b9b57dfd4fe4", "message": "Add mostly complete deltion and iteration tests", "committedDate": "2020-07-24T20:34:47Z", "type": "commit"}, {"oid": "9cdc4b244bfa8387097c04ea3667e2f600192d16", "url": "https://github.com/apache/ozone/commit/9cdc4b244bfa8387097c04ea3667e2f600192d16", "message": "Rename classes to reflect which database schema version they correspond to", "committedDate": "2020-07-24T20:48:20Z", "type": "commit"}, {"oid": "da5c33c7a0e8c5983c55971c1a6cc722befaf2bc", "url": "https://github.com/apache/ozone/commit/da5c33c7a0e8c5983c55971c1a6cc722befaf2bc", "message": "Add statistics to new datanode store to match old implementation", "committedDate": "2020-07-24T21:00:32Z", "type": "commit"}, {"oid": "2e122108122ff59913aa564c7672d43f5721cf82", "url": "https://github.com/apache/ozone/commit/2e122108122ff59913aa564c7672d43f5721cf82", "message": "Start outline of prefix speific solution\n\nAdd table implemention (WIP) that warns when keys are added without prefix, or table is scanned without filtering for prefix.", "committedDate": "2020-07-28T00:18:40Z", "type": "commit"}, {"oid": "acd0ca45c5162442fd3b7fdde17719b0fe4387b3", "url": "https://github.com/apache/ozone/commit/acd0ca45c5162442fd3b7fdde17719b0fe4387b3", "message": "Switch key type of deleted blocks table to String", "committedDate": "2020-07-28T13:19:12Z", "type": "commit"}, {"oid": "c18658f604d17534835635903bdcc4cfb0580530", "url": "https://github.com/apache/ozone/commit/c18658f604d17534835635903bdcc4cfb0580530", "message": "Update block deleting code to use #deleted# prefix in the deleted blocks table\n\nPasses unit tests for TestblockDeletingService.", "committedDate": "2020-07-28T13:40:38Z", "type": "commit"}, {"oid": "464223c5d23d0c8697381bfb199e9e0ee0ca7082", "url": "https://github.com/apache/ozone/commit/464223c5d23d0c8697381bfb199e9e0ee0ca7082", "message": "Fix test container contents so it has 2 of each block type", "committedDate": "2020-07-28T14:39:19Z", "type": "commit"}, {"oid": "a62f4731ccc858210f8197466507a4edee565dcc", "url": "https://github.com/apache/ozone/commit/a62f4731ccc858210f8197466507a4edee565dcc", "message": "Iteration using key filters passes on schema version 1 format", "committedDate": "2020-07-28T14:39:49Z", "type": "commit"}, {"oid": "d2faaa6fe1676dcfc9170d24323c4cd12c4b1e82", "url": "https://github.com/apache/ozone/commit/d2faaa6fe1676dcfc9170d24323c4cd12c4b1e82", "message": "Merge branch 'caller-specifies-prefix-solution' into fix-prefix-bugs\n\n* caller-specifies-prefix-solution:\n  Iteration using key filters passes on schema version 1 format\n  Fix test container contents so it has 2 of each block type\n  Update block deleting code to use #deleted# prefix in the deleted blocks table\n  Switch key type of deleted blocks table to String\n  Start outline of prefix speific solution", "committedDate": "2020-07-28T15:19:49Z", "type": "commit"}, {"oid": "de8a6acf09d8c8b2779436b6a47a1234cea5e287", "url": "https://github.com/apache/ozone/commit/de8a6acf09d8c8b2779436b6a47a1234cea5e287", "message": "Merge branch 'fix-prefix-bugs' into HDDS-3869\n\n* fix-prefix-bugs:\n  Iteration using key filters passes on schema version 1 format\n  Fix test container contents so it has 2 of each block type\n  Update block deleting code to use #deleted# prefix in the deleted blocks table\n  Switch key type of deleted blocks table to String\n  Start outline of prefix speific solution", "committedDate": "2020-07-28T15:20:11Z", "type": "commit"}, {"oid": "97a64c728a73d1162deb7ff225916b974242da01", "url": "https://github.com/apache/ozone/commit/97a64c728a73d1162deb7ff225916b974242da01", "message": "Fix warnings and update documentation for unit tests", "committedDate": "2020-07-28T16:04:23Z", "type": "commit"}, {"oid": "6abdae88f8ab7b1ba79826b0a5c016d3c292efee", "url": "https://github.com/apache/ozone/commit/6abdae88f8ab7b1ba79826b0a5c016d3c292efee", "message": "Update name and documentation header of unit tests for schema version 1", "committedDate": "2020-07-28T16:13:45Z", "type": "commit"}, {"oid": "af24c6fd3b4b5486a0957d3dcd8f6271b2c41faf", "url": "https://github.com/apache/ozone/commit/af24c6fd3b4b5486a0957d3dcd8f6271b2c41faf", "message": "Fix improper usages of key prefix filters\n\nFix representation exposure in MetadataKeyFilter class that allowed callers to\nmodify private static instances of filters, affecting all others holding\nreferences to those values.\n\nRemove instances of manual prefix filter creation, and replace them with\nidentical calls to existing getters in MetadataKeyFilter.", "committedDate": "2020-07-28T16:38:41Z", "type": "commit"}, {"oid": "98ad1084a84167e80de282ed99a137abf88ea41a", "url": "https://github.com/apache/ozone/commit/98ad1084a84167e80de282ed99a137abf88ea41a", "message": "Initial attempt at block deleting service tests\n\nTests currently fail with a storage exception.", "committedDate": "2020-07-28T20:50:15Z", "type": "commit"}, {"oid": "53d42efaba93dbb69f839eb43b0dda2fcd67330a", "url": "https://github.com/apache/ozone/commit/53d42efaba93dbb69f839eb43b0dda2fcd67330a", "message": "Fix bug where codecs were not added for multiple DBDefinitions of default table", "committedDate": "2020-07-28T21:55:02Z", "type": "commit"}, {"oid": "e92af54c50d5ad5ee4e212063afaf6c4daf7c2d0", "url": "https://github.com/apache/ozone/commit/e92af54c50d5ad5ee4e212063afaf6c4daf7c2d0", "message": "Fix copy paste error in key filter getter", "committedDate": "2020-07-29T13:43:07Z", "type": "commit"}, {"oid": "2274dbee33bb4d55c5bd1af6b08bf96408dbcc0e", "url": "https://github.com/apache/ozone/commit/2274dbee33bb4d55c5bd1af6b08bf96408dbcc0e", "message": "Fix the bytes used value in the test database and update the tests accordingly", "committedDate": "2020-07-29T13:43:50Z", "type": "commit"}, {"oid": "b61138a5de7d2dd144e3332b4a102aa3646a550d", "url": "https://github.com/apache/ozone/commit/b61138a5de7d2dd144e3332b4a102aa3646a550d", "message": "Block deleting service runs and deletes blocks", "committedDate": "2020-07-29T17:51:57Z", "type": "commit"}, {"oid": "c39bd71277b989a185ff5433660bc7201c99a911", "url": "https://github.com/apache/ozone/commit/c39bd71277b989a185ff5433660bc7201c99a911", "message": "All tests pass except for key count after delete\n\nIssue is documented, and related to\nKeyValueContainerUtil.initializeUsedBytesAndBlockCount() not counting\ndeleting blocks as part of the key count.\nThis will be fixed once it is confirmed if the current implementation is\nincorrect.", "committedDate": "2020-07-30T14:10:15Z", "type": "commit"}, {"oid": "a2402b988d73763fe78513f58b189b05233d70a0", "url": "https://github.com/apache/ozone/commit/a2402b988d73763fe78513f58b189b05233d70a0", "message": "Remove unused imported from TestSchemaOneBackwardsCompatibility", "committedDate": "2020-07-30T16:11:21Z", "type": "commit"}, {"oid": "953f98afa09f3f3950e9d519f216c67133206a57", "url": "https://github.com/apache/ozone/commit/953f98afa09f3f3950e9d519f216c67133206a57", "message": "Create initial implementation of SchemaOneDeletedBlocksTable", "committedDate": "2020-07-30T16:12:04Z", "type": "commit"}, {"oid": "584e565958626cad0aee59f5beae90e8382147d6", "url": "https://github.com/apache/ozone/commit/584e565958626cad0aee59f5beae90e8382147d6", "message": "Remove explicit usages of #deleted# prefix\n\nThis prepares these classes to use the new table implementation that will not require this.", "committedDate": "2020-07-30T18:08:45Z", "type": "commit"}, {"oid": "77f67379b4e9888642599857f1ba4c7f43f20bb7", "url": "https://github.com/apache/ozone/commit/77f67379b4e9888642599857f1ba4c7f43f20bb7", "message": "Switch from inheritance to composition relationship to TypedTable", "committedDate": "2020-07-30T20:59:04Z", "type": "commit"}, {"oid": "457ba132f63f5e30a073f8ad49db29c19d37685a", "url": "https://github.com/apache/ozone/commit/457ba132f63f5e30a073f8ad49db29c19d37685a", "message": "Fix errors with new table implementation and usages", "committedDate": "2020-07-30T21:40:00Z", "type": "commit"}, {"oid": "674e89f8e9f6ffd4d2f9c68353366f0104d6e934", "url": "https://github.com/apache/ozone/commit/674e89f8e9f6ffd4d2f9c68353366f0104d6e934", "message": "Swap user passed prefixes with #deleted# prefix for key range queries\n\nAllowing user passed prefixes in addition to the #deleted# prefix could cause\nprefix collisions and return results that are not actually deleted blocks.", "committedDate": "2020-07-30T22:10:14Z", "type": "commit"}, {"oid": "2d3623c625aa0f818eee20787ad9adef04bf7f2f", "url": "https://github.com/apache/ozone/commit/2d3623c625aa0f818eee20787ad9adef04bf7f2f", "message": "Change MetadataKeyFilters#getNormalKeyFilter to MetadataKeyFilters#getUnprefixedKeyFilter\n\nThe new method ignores keys that begin with #. The previous implementation\nrequired hard coding all prefixes used. This is no\nlonger effective now that schema version 1 will use the #deleted# prefix\ninternally to make up for a deleted blocks table not exisitng. The old\nimplementation returned these keys, even though they were prefixed.", "committedDate": "2020-07-31T13:32:55Z", "type": "commit"}, {"oid": "e725a5b4f8abef6c3ebe773c3f160d8798e4464d", "url": "https://github.com/apache/ozone/commit/e725a5b4f8abef6c3ebe773c3f160d8798e4464d", "message": "Remove unused variable in DatanodeStoreSchemaOneImpl", "committedDate": "2020-07-31T15:54:43Z", "type": "commit"}, {"oid": "bd1d4b43fff2a51ba5b2c17cd6d6b0306d9a6996", "url": "https://github.com/apache/ozone/commit/bd1d4b43fff2a51ba5b2c17cd6d6b0306d9a6996", "message": "Make KeyValueContainerUtil include #deleting# blocks when setting key count\n\nUnit tests are updated to reflect this change, which is now consistent with how\nBlockDeletingService handles key count.", "committedDate": "2020-07-31T15:55:00Z", "type": "commit"}, {"oid": "a3c64636b55eb000b0664da6acfa6b584c1b1859", "url": "https://github.com/apache/ozone/commit/a3c64636b55eb000b0664da6acfa6b584c1b1859", "message": "Update documentation and fix warnings in newly created of modified classes.", "committedDate": "2020-07-31T18:06:37Z", "type": "commit"}, {"oid": "2f2ed9051192879ff15d65e55c507c88aaecdba8", "url": "https://github.com/apache/ozone/commit/2f2ed9051192879ff15d65e55c507c88aaecdba8", "message": "Merge branch 'master' into HDDS-3869\n\n* master: (55 commits)\n  HDDS-4052. Remove master/slave terminology from Ozone (#1281)\n  HDDS-4047. OzoneManager met NPE exception while getServiceList (#1277)\n  HDDS-3990. Test Kubernetes examples with acceptance tests (#1223)\n  HDDS-4045. Add more ignore rules to the RAT ignore list (#1273)\n  HDDS-3970. Enabling TestStorageContainerManager with all failures addressed (#1257)\n  HDDS-4033. Make the acceptance test reports hierarchical (#1263)\n  HDDS-3423. Enabling TestContainerReplicationEndToEnd and addressing failures (#1260)\n  HDDS-4027. Suppress ERROR message when SCM attempt to create additional pipelines. (#1265)\n  HDDS-4024. Avoid while loop too soon when exception happen (#1253)\n  HDDS-3809. Make number of open containers on a datanode a function of no of volumes reported by it. (#1081)\n  HDDS-4019. Show the storageDir while need init om or scm (#1248)\n  HDDS-3511. Fix javadoc comment in OmMetadataManager (#1247)\n  HDDS-4041. Ozone /conf endpoint triggers kerberos replay error when SPNEGO is enabled. (#1267)\n  HDDS-4031. Run shell tests in CI (#1261)\n  HDDS-4038. Eliminate GitHub check warnings (#1268)\n  HDDS-4011. Update S3 related documentation. (#1245)\n  HDDS-4030. Remember the selected columns and make the X-axis scrollable in recon datanodes UI (#1259)\n  HDDS-4032. Run author check without docker (#1262)\n  HDDS-4026. Dir rename failed when sets 'ozone.om.enable.filesystem.paths' to true (#1256)\n  HDDS-4017. Acceptance check may run against wrong commit (#1249)\n  ...", "committedDate": "2020-07-31T21:59:06Z", "type": "commit"}, {"oid": "21eaf712a7f03fd889453dbcacac299b8112111c", "url": "https://github.com/apache/ozone/commit/21eaf712a7f03fd889453dbcacac299b8112111c", "message": "Remove caching test for DBStoreBuilder\n\nThe new interface does not have this caching structure. The data needed from\nthe config file is always read from memory, not disk.", "committedDate": "2020-07-31T22:02:10Z", "type": "commit"}, {"oid": "862e374bab1e993063e4d35a7551370b442b41e1", "url": "https://github.com/apache/ozone/commit/862e374bab1e993063e4d35a7551370b442b41e1", "message": "Remove commented out code to fix the test DB isntance", "committedDate": "2020-07-31T22:14:16Z", "type": "commit"}, {"oid": "b6ad8161a80f6bcf8e3e63cfb6de70b236b60b20", "url": "https://github.com/apache/ozone/commit/b6ad8161a80f6bcf8e3e63cfb6de70b236b60b20", "message": "Add schema version specifier to code merged from master", "committedDate": "2020-08-03T15:39:52Z", "type": "commit"}, {"oid": "709ad0891478e6bfe21783aa533c3fe57c73ac6a", "url": "https://github.com/apache/ozone/commit/709ad0891478e6bfe21783aa533c3fe57c73ac6a", "message": "Fix generics issue that prevented code form building", "committedDate": "2020-08-03T15:40:27Z", "type": "commit"}, {"oid": "2948cd0e79ad3c96207c0573947817f2526cadd6", "url": "https://github.com/apache/ozone/commit/2948cd0e79ad3c96207c0573947817f2526cadd6", "message": "Create ChunkInfoCodec for storing chunk info in the deleted blocks table", "committedDate": "2020-08-03T15:49:02Z", "type": "commit"}, {"oid": "eb0f8550cb73fff09ba9eede1ffd35c45dbabd86", "url": "https://github.com/apache/ozone/commit/eb0f8550cb73fff09ba9eede1ffd35c45dbabd86", "message": "Make datanode store classes use ChunkInfo as value in deleted blocks table", "committedDate": "2020-08-03T16:03:08Z", "type": "commit"}, {"oid": "b2a8b5497e476ef6058e54ba869126cd5f7f380c", "url": "https://github.com/apache/ozone/commit/b2a8b5497e476ef6058e54ba869126cd5f7f380c", "message": "Update block deleting service to get chunk info list to store in table before doing deletes", "committedDate": "2020-08-03T17:36:18Z", "type": "commit"}, {"oid": "9c05d42574b073d568a7d99566f91a9e223812df", "url": "https://github.com/apache/ozone/commit/9c05d42574b073d568a7d99566f91a9e223812df", "message": "Move corrected tabase values out of target and in to test resources", "committedDate": "2020-08-03T19:42:07Z", "type": "commit"}, {"oid": "e7ee02485b65a044198565a074924679b04f5b68", "url": "https://github.com/apache/ozone/commit/e7ee02485b65a044198565a074924679b04f5b68", "message": "Restore KeyValueBlockIterator from master branch\n\nThe \"fix\" that was applied earlier was unnecessary.", "committedDate": "2020-08-03T21:06:58Z", "type": "commit"}, {"oid": "9832145964687efcc616d0921994746000ded7c0", "url": "https://github.com/apache/ozone/commit/9832145964687efcc616d0921994746000ded7c0", "message": "Add RocksDB database that is part of unit testing resources to rat exclude list", "committedDate": "2020-08-03T21:45:44Z", "type": "commit"}, {"oid": "41d596216d94b5bc1e3898179b5892d852fa8d1b", "url": "https://github.com/apache/ozone/commit/41d596216d94b5bc1e3898179b5892d852fa8d1b", "message": "Add container reader fix from HDDS-4061", "committedDate": "2020-08-04T15:47:16Z", "type": "commit"}, {"oid": "6ad6d07dd862857ed9bcd72db12140c4eb4e8aaf", "url": "https://github.com/apache/ozone/commit/6ad6d07dd862857ed9bcd72db12140c4eb4e8aaf", "message": "Create protobuf, codec, and helper class for serializing lists of ChunkInfo objects", "committedDate": "2020-08-04T19:11:48Z", "type": "commit"}, {"oid": "7cef40a86b63de5a365e77aed5999d43c3d9725a", "url": "https://github.com/apache/ozone/commit/7cef40a86b63de5a365e77aed5999d43c3d9725a", "message": "Switch all uses of deleted blocks table to use new ChunkInfoList object as value", "committedDate": "2020-08-04T19:26:23Z", "type": "commit"}, {"oid": "04ba6896120fddf50efb2bd867279372aac64555", "url": "https://github.com/apache/ozone/commit/04ba6896120fddf50efb2bd867279372aac64555", "message": "Add unit test to check that chunk information for blocks is preserved\n\nTestBlockDeletion unit tests are currently not starting for some reason.", "committedDate": "2020-08-04T20:16:59Z", "type": "commit"}, {"oid": "f796bf0a66fbd6ae99965d6d51e99c7a6a3533b5", "url": "https://github.com/apache/ozone/commit/f796bf0a66fbd6ae99965d6d51e99c7a6a3533b5", "message": "Add and verify unit test for preserving chunk info after block deletion", "committedDate": "2020-08-05T15:06:31Z", "type": "commit"}, {"oid": "aa9f1f6acd36685d346a665f354ff80b51c67f64", "url": "https://github.com/apache/ozone/commit/aa9f1f6acd36685d346a665f354ff80b51c67f64", "message": "Add test for reading the chunk info lists from deleted blocks before and after upgrade\n\nThe test fails (as expected) because there is currently no support for handling\ndeleted blocks written with the pre upgrade code, where the value is the block\nID, not the chunk info list.", "committedDate": "2020-08-05T16:07:46Z", "type": "commit"}, {"oid": "1e6bf3d663247a7e69b5e7d37e2756ff5d10b231", "url": "https://github.com/apache/ozone/commit/1e6bf3d663247a7e69b5e7d37e2756ff5d10b231", "message": "Add support for chunk information saved with deleted blocks in both pre and post upgrade versions of the code", "committedDate": "2020-08-05T16:42:20Z", "type": "commit"}, {"oid": "956b150f668a22d5dd7979cb14e2ba762dbfde82", "url": "https://github.com/apache/ozone/commit/956b150f668a22d5dd7979cb14e2ba762dbfde82", "message": "Fix checkstyle violations", "committedDate": "2020-08-05T17:25:37Z", "type": "commit"}, {"oid": "e442c6af9241b08aec75225ff084bf1c4fac99b7", "url": "https://github.com/apache/ozone/commit/e442c6af9241b08aec75225ff084bf1c4fac99b7", "message": "Fix bug in unit test where reading of post upgrade deletion blocks was skipped", "committedDate": "2020-08-05T17:42:43Z", "type": "commit"}, {"oid": "b73de108e1371937766eeddeb6e3685ac8abb434", "url": "https://github.com/apache/ozone/commit/b73de108e1371937766eeddeb6e3685ac8abb434", "message": "Merge branch 'test-chunkinfo-store' into store-chunkinfo-with-deleted-blocks\n\n* test-chunkinfo-store:\n  Fix bug in unit test where reading of post upgrade deletion blocks was skipped\n  Fix checkstyle violations\n  Add support for chunk information saved with deleted blocks in both pre and post upgrade versions of the code\n  Add test for reading the chunk info lists from deleted blocks before and after upgrade\n  Add and verify unit test for preserving chunk info after block deletion", "committedDate": "2020-08-05T17:51:50Z", "type": "commit"}, {"oid": "cf20035ab3a8f243d8ff5e5e42ee797eb658ed9e", "url": "https://github.com/apache/ozone/commit/cf20035ab3a8f243d8ff5e5e42ee797eb658ed9e", "message": "Merge branch 'store-chunkinfo-with-deleted-blocks' into HDDS-3869\n\n* store-chunkinfo-with-deleted-blocks:\n  Fix bug in unit test where reading of post upgrade deletion blocks was skipped\n  Fix checkstyle violations\n  Add support for chunk information saved with deleted blocks in both pre and post upgrade versions of the code\n  Add test for reading the chunk info lists from deleted blocks before and after upgrade\n  Add and verify unit test for preserving chunk info after block deletion\n  Add unit test to check that chunk information for blocks is preserved\n  Switch all uses of deleted blocks table to use new ChunkInfoList object as value\n  Create protobuf, codec, and helper class for serializing lists of ChunkInfo objects\n  Update block deleting service to get chunk info list to store in table before doing deletes\n  Make datanode store classes use ChunkInfo as value in deleted blocks table\n  Create ChunkInfoCodec for storing chunk info in the deleted blocks table", "committedDate": "2020-08-05T17:52:15Z", "type": "commit"}, {"oid": "11dbefc84f33f54d5ff64881f66050662332eda1", "url": "https://github.com/apache/ozone/commit/11dbefc84f33f54d5ff64881f66050662332eda1", "message": "Rename DatanodeStore implementations to indicate which table config they use", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "4c61bd4dac7541331e50637e5cdce632054a1f2c", "url": "https://github.com/apache/ozone/commit/4c61bd4dac7541331e50637e5cdce632054a1f2c", "message": "Delete levelDB classes\n\nWe will not be implementing levelDB support", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "f80d5434d2e08b66df22247d3fd332f605ed5baa", "url": "https://github.com/apache/ozone/commit/f80d5434d2e08b66df22247d3fd332f605ed5baa", "message": "Make ReferenceCountedDB use DatanodeStore\n\nMay change to a more generic interface if one is created.", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "ec0a0cdcb15a6681ae507f5cf1d7ffd4dfdaee67", "url": "https://github.com/apache/ozone/commit/ec0a0cdcb15a6681ae507f5cf1d7ffd4dfdaee67", "message": "Add schemaVersion field to yaml key value container data", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "bc0c2c05a755eb05ffa230f6c99cf4e82269dcce", "url": "https://github.com/apache/ozone/commit/bc0c2c05a755eb05ffa230f6c99cf4e82269dcce", "message": "Fix types and add comments to one and two table db definitions", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "8ff6dfd218ea2873b4098627fe52f2c2215f5c8c", "url": "https://github.com/apache/ozone/commit/8ff6dfd218ea2873b4098627fe52f2c2215f5c8c", "message": "Create abstract layer for shared implementations between one and two column family datanode DBs", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "3cbefd105000a36f5b644e6d673d6a6185e61571", "url": "https://github.com/apache/ozone/commit/3cbefd105000a36f5b644e6d673d6a6185e61571", "message": "Create abstract datanodestore layer to support one and two table implementations", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "3c78ab87a9bae1b67220a1e62e02a01c1fb37e07", "url": "https://github.com/apache/ozone/commit/3c78ab87a9bae1b67220a1e62e02a01c1fb37e07", "message": "Use generic ConfigurationSource instead of more specific OzoneConfiguration", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "b228f883f08e5980b9d64bdc84ba85b15bea9ad4", "url": "https://github.com/apache/ozone/commit/b228f883f08e5980b9d64bdc84ba85b15bea9ad4", "message": "Outline process for determining how to set schema version", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "7969bed1580492a8b3aecfb37904eaeb438f75ea", "url": "https://github.com/apache/ozone/commit/7969bed1580492a8b3aecfb37904eaeb438f75ea", "message": "Implement block data codec", "committedDate": "2020-07-16T13:09:00Z", "type": "commit"}, {"oid": "b028681dfbf7fc751346c1d0372104c68145b27e", "url": "https://github.com/apache/ozone/commit/b028681dfbf7fc751346c1d0372104c68145b27e", "message": "Add schemaVersion yaml field as string to .container options", "committedDate": "2020-07-16T13:12:39Z", "type": "commit"}, {"oid": "7c32b50b93783279a6f9eea316554f3f44c55413", "url": "https://github.com/apache/ozone/commit/7c32b50b93783279a6f9eea316554f3f44c55413", "message": "Make AbstractDatanodeStore DB created if missing\n\nThis behavior is identical to the code it is replacing.", "committedDate": "2020-07-16T13:13:08Z", "type": "commit"}, {"oid": "58d21abbf540f253fbcd46975637574d7f86f2d3", "url": "https://github.com/apache/ozone/commit/58d21abbf540f253fbcd46975637574d7f86f2d3", "message": "Add schemaVersion field to yaml key value container data", "committedDate": "2020-07-16T13:17:45Z", "type": "commit"}, {"oid": "5a5de40dfdc7ab9ee160cfc6cc194f5863243a63", "url": "https://github.com/apache/ozone/commit/5a5de40dfdc7ab9ee160cfc6cc194f5863243a63", "message": "Add schemaVersion yaml field as string to .container options", "committedDate": "2020-07-16T13:20:04Z", "type": "commit"}, {"oid": "d9082ee5da67859cdc8c76da661ed02d2bc2b952", "url": "https://github.com/apache/ozone/commit/d9082ee5da67859cdc8c76da661ed02d2bc2b952", "message": "Add flush and compact methods to datanodestore", "committedDate": "2020-07-16T13:20:07Z", "type": "commit"}, {"oid": "e0ad69b3c28965856e1dc8e239c658d25d62921c", "url": "https://github.com/apache/ozone/commit/e0ad69b3c28965856e1dc8e239c658d25d62921c", "message": "Replace old puts, gets, deletes and batch operations with the new versions", "committedDate": "2020-07-16T13:20:07Z", "type": "commit"}, {"oid": "3398689cc193542ceecbb96a322f0d5c4f42f87e", "url": "https://github.com/apache/ozone/commit/3398689cc193542ceecbb96a322f0d5c4f42f87e", "message": "Begin changing interface usage in nonKeyValue or Test classes\n\nSome changes are pending furhter investigation and are not yet completed.", "committedDate": "2020-07-16T13:20:07Z", "type": "commit"}, {"oid": "75cc4f3c87adac9837a49a3b05d63db70686c0cb", "url": "https://github.com/apache/ozone/commit/75cc4f3c87adac9837a49a3b05d63db70686c0cb", "message": "Switch key type of block data table from string to long", "committedDate": "2020-07-16T13:20:07Z", "type": "commit"}, {"oid": "78d82663f8d781ae6d2e80ed52f5f29f2debed7a", "url": "https://github.com/apache/ozone/commit/78d82663f8d781ae6d2e80ed52f5f29f2debed7a", "message": "Update gets, puts and batch operations in KeyValue classes to use new interface", "committedDate": "2020-07-16T13:22:33Z", "type": "commit"}, {"oid": "90d534e959549431d9a41ccce0f0aa4adf5bf5ab", "url": "https://github.com/apache/ozone/commit/90d534e959549431d9a41ccce0f0aa4adf5bf5ab", "message": "Update get, put, delete, and write batch calls to use new interface in unit tests", "committedDate": "2020-07-16T13:38:01Z", "type": "commit"}, {"oid": "b94c1e0093692face52082b875dced0617380381", "url": "https://github.com/apache/ozone/commit/b94c1e0093692face52082b875dced0617380381", "message": "Add getRangeKVs and getSequentialRangeKVs methods to Table interface\n\nAdd implementations based on the original implementations in MetadataStore,\nexcept that these new versions will only operate on one table.", "committedDate": "2020-07-16T13:38:04Z", "type": "commit"}, {"oid": "84e6c1aceca423ed7e4c2f86d685568228e38412", "url": "https://github.com/apache/ozone/commit/84e6c1aceca423ed7e4c2f86d685568228e38412", "message": "Fix typo in method call to new interface", "committedDate": "2020-07-16T13:38:04Z", "type": "commit"}, {"oid": "2221896290b01f9520726519f1d4f040caf22efe", "url": "https://github.com/apache/ozone/commit/2221896290b01f9520726519f1d4f040caf22efe", "message": "Add separate flushLog() and flushDB() methods to new interface to preserve old functionality", "committedDate": "2020-07-16T13:38:04Z", "type": "commit"}, {"oid": "add07ada86576b5f63d1d015165e51b221cc1392", "url": "https://github.com/apache/ozone/commit/add07ada86576b5f63d1d015165e51b221cc1392", "message": "Switch key type of block data table from long to string\n\nAlthough the block IDs are longs, the prefixes sometimes used requires the type\nto be a string.", "committedDate": "2020-07-16T13:38:04Z", "type": "commit"}, {"oid": "f3985dc5e472875cbe848c900223cebd7c42a2e2", "url": "https://github.com/apache/ozone/commit/f3985dc5e472875cbe848c900223cebd7c42a2e2", "message": "Switch calls to sequential and non sequential getRangeKV() calls to the new interface", "committedDate": "2020-07-16T13:38:59Z", "type": "commit"}, {"oid": "6dcfc786febe107a6d7b2a20ea6a7f35efec96be", "url": "https://github.com/apache/ozone/commit/6dcfc786febe107a6d7b2a20ea6a7f35efec96be", "message": "Switch KeyValueBlockIterator to new iterator interface", "committedDate": "2020-07-16T13:41:02Z", "type": "commit"}, {"oid": "7761b944bb192903dcf9cedb64548d7691520e8d", "url": "https://github.com/apache/ozone/commit/7761b944bb192903dcf9cedb64548d7691520e8d", "message": "Make ContainerCache.getDB() not require the schema version\n\nIf it pulls a cached container, we don't need to set it up.\nIf it needs to create a new container, we always use the two table version.", "committedDate": "2020-07-16T13:41:05Z", "type": "commit"}, {"oid": "f251c8fdd9b9121abe9c33927318de54039e7cd5", "url": "https://github.com/apache/ozone/commit/f251c8fdd9b9121abe9c33927318de54039e7cd5", "message": "Remove unused imports", "committedDate": "2020-07-16T13:41:05Z", "type": "commit"}, {"oid": "580c6f5d3f17e0eda12e0b7e04e3b8566ffb8ae4", "url": "https://github.com/apache/ozone/commit/580c6f5d3f17e0eda12e0b7e04e3b8566ffb8ae4", "message": "Add new constructor to DBStore builder that allows specifying the DBDefintiion", "committedDate": "2020-07-16T13:41:05Z", "type": "commit"}, {"oid": "fc4041006f99dc0206e021612327400d76467768", "url": "https://github.com/apache/ozone/commit/fc4041006f99dc0206e021612327400d76467768", "message": "Add getDBLocation method for Datanode DBDefinition to use instead of location config key", "committedDate": "2020-07-16T13:41:05Z", "type": "commit"}, {"oid": "9b64ab4bedc06870e3ee505ee4ed59feaf0ad44f", "url": "https://github.com/apache/ozone/commit/9b64ab4bedc06870e3ee505ee4ed59feaf0ad44f", "message": "Update container cache test to provide a schema version when using the cache", "committedDate": "2020-07-16T13:41:05Z", "type": "commit"}, {"oid": "7d35d43d1f3e0187876cc858af20cd4a9d865512", "url": "https://github.com/apache/ozone/commit/7d35d43d1f3e0187876cc858af20cd4a9d865512", "message": "Update method header and fix string comparison for schema version", "committedDate": "2020-07-16T13:47:00Z", "type": "commit"}, {"oid": "3a722ffafc1c786b46b9bf3bcf8eaedbe7155a28", "url": "https://github.com/apache/ozone/commit/3a722ffafc1c786b46b9bf3bcf8eaedbe7155a28", "message": "Remove extra import in BlockManagerImpl that was accidentally added", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "3496c11d7d08eb530ae53906d988a6295c209a0f", "url": "https://github.com/apache/ozone/commit/3496c11d7d08eb530ae53906d988a6295c209a0f", "message": "Re add schema version parameter to in BlockUtils after it was removed by the merge", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "584fb9a261f3fcc40d9f1e96b24f678e9b36127c", "url": "https://github.com/apache/ozone/commit/584fb9a261f3fcc40d9f1e96b24f678e9b36127c", "message": "Remove extra import that was already added", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "cf08fce3fee578eae7d08ebca53238ac562da370", "url": "https://github.com/apache/ozone/commit/cf08fce3fee578eae7d08ebca53238ac562da370", "message": "Fix bug where DB name was used instead of column family name", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "a0d6182e16c0d87c0f523f6c8ac19db32a20929d", "url": "https://github.com/apache/ozone/commit/a0d6182e16c0d87c0f523f6c8ac19db32a20929d", "message": "Fix bugs with missing column families and mismatched ContainerCache params", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "65213cbe29adcc11d9fd2214b59a55f4f8287152", "url": "https://github.com/apache/ozone/commit/65213cbe29adcc11d9fd2214b59a55f4f8287152", "message": "Fix bug to allow null key to be used as start key in key range query", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "091f013acd42d2893a1dbff48e92197456fa96fb", "url": "https://github.com/apache/ozone/commit/091f013acd42d2893a1dbff48e92197456fa96fb", "message": "Make DeleteBlocksCommandHandler.deleteKeyValueContainerBlocks() use block data table instead of metadata table", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "0df4c4f65ee8b396d115428c26ba3ea384881cd1", "url": "https://github.com/apache/ozone/commit/0df4c4f65ee8b396d115428c26ba3ea384881cd1", "message": "Fix null start key error that was missed in the original fix", "committedDate": "2020-07-16T13:47:02Z", "type": "commit"}, {"oid": "ba80ef481891423014252c49f43dccd5cf45699e", "url": "https://github.com/apache/ozone/commit/ba80ef481891423014252c49f43dccd5cf45699e", "message": "Remove early batch operation commit in BlockDeletingService", "committedDate": "2020-07-16T13:47:03Z", "type": "commit"}, {"oid": "8c4f37f4175982172ac5b2caba67250835cdfb3d", "url": "https://github.com/apache/ozone/commit/8c4f37f4175982172ac5b2caba67250835cdfb3d", "message": "Fix bug where .db file was created in wrong directory\n\nThe added method DBDefinition.getDBLocation() is now defined to return the parent directory of the .db file.", "committedDate": "2020-07-16T13:47:03Z", "type": "commit"}, {"oid": "3be476a01a4e7190eaae7c34555a4190c6049933", "url": "https://github.com/apache/ozone/commit/3be476a01a4e7190eaae7c34555a4190c6049933", "message": "Fix bug where default column family was being registered twice", "committedDate": "2020-07-16T13:47:03Z", "type": "commit"}, {"oid": "67ee0958b54c29c2a8b67bc1d530dea08516f1be", "url": "https://github.com/apache/ozone/commit/67ee0958b54c29c2a8b67bc1d530dea08516f1be", "message": "Make testContainerImportExport write to block and metadata tables where needed", "committedDate": "2020-07-16T13:47:03Z", "type": "commit"}, {"oid": "232347fb746b7ebe1db0b7964977f072cd2f77d4", "url": "https://github.com/apache/ozone/commit/232347fb746b7ebe1db0b7964977f072cd2f77d4", "message": "Minor readability change", "committedDate": "2020-07-16T13:47:03Z", "type": "commit"}, {"oid": "8feede3f78ddf6788d9a2b2c2b5847e6ff51f879", "url": "https://github.com/apache/ozone/commit/8feede3f78ddf6788d9a2b2c2b5847e6ff51f879", "message": "Switch KeyValueBlockIterator to new interface after patch using old interface was integrated", "committedDate": "2020-07-16T16:00:23Z", "type": "commit"}, {"oid": "9faca0270c92c2cdc7b7c530dcab609d99dbe36b", "url": "https://github.com/apache/ozone/commit/9faca0270c92c2cdc7b7c530dcab609d99dbe36b", "message": "Remove missed git artifacts after conflict", "committedDate": "2020-07-16T16:04:34Z", "type": "commit"}, {"oid": "da05a25a2eb753514d7258743847bc13f0d716ef", "url": "https://github.com/apache/ozone/commit/da05a25a2eb753514d7258743847bc13f0d716ef", "message": "Switch merged in unit test to new interface", "committedDate": "2020-07-16T16:05:37Z", "type": "commit"}, {"oid": "ed668971db69514a42ad4e38f3bfb3b1fb8f8ac9", "url": "https://github.com/apache/ozone/commit/ed668971db69514a42ad4e38f3bfb3b1fb8f8ac9", "message": "Update merged unit test to use new interface", "committedDate": "2020-07-16T16:12:01Z", "type": "commit"}, {"oid": "c1b6e056c0591ad269882e2806c46a7d8012000d", "url": "https://github.com/apache/ozone/commit/c1b6e056c0591ad269882e2806c46a7d8012000d", "message": "Add schema version param to ContainerCache.getDB() after it was accidentally removed in merge", "committedDate": "2020-07-16T16:22:15Z", "type": "commit"}, {"oid": "786b49c2bacaf243a3560ac6fdef0273fec016b0", "url": "https://github.com/apache/ozone/commit/786b49c2bacaf243a3560ac6fdef0273fec016b0", "message": "Remove duplicate method call introduced accidentally when resolving merge conflict", "committedDate": "2020-07-16T18:08:07Z", "type": "commit"}, {"oid": "90dd3f0137b15e11a3ddc1b02dfd45994f84b0db", "url": "https://github.com/apache/ozone/commit/90dd3f0137b15e11a3ddc1b02dfd45994f84b0db", "message": "New iterator implementation passes existing unit tests", "committedDate": "2020-07-16T20:25:42Z", "type": "commit"}, {"oid": "3b8cdbb69ffdc18c026bae2c2b117d1504030fa0", "url": "https://github.com/apache/ozone/commit/3b8cdbb69ffdc18c026bae2c2b117d1504030fa0", "message": "Merge branch 'HDDS-3869' of https://github.com/errose28/hadoop-ozone into HDDS-3869", "committedDate": "2020-07-17T13:40:36Z", "type": "commit"}, {"oid": "dc13c5a9b7c21810525505345d3cfdbb41f9ef7a", "url": "https://github.com/apache/ozone/commit/dc13c5a9b7c21810525505345d3cfdbb41f9ef7a", "message": "All unit tests pass after allowing keys with deleted and deleting prefixes to be made", "committedDate": "2020-07-17T14:10:52Z", "type": "commit"}, {"oid": "a89787835150b2eb39f95fee660245f86f357a39", "url": "https://github.com/apache/ozone/commit/a89787835150b2eb39f95fee660245f86f357a39", "message": "Add more robust unit test with alternating key matches", "committedDate": "2020-07-17T16:21:05Z", "type": "commit"}, {"oid": "b5ca1574ca1d04de43bf8963d6a2ce210fbc8210", "url": "https://github.com/apache/ozone/commit/b5ca1574ca1d04de43bf8963d6a2ce210fbc8210", "message": "Remove seekToLast() from iterator interface, implementation, and tests\n\nIt will require additional work to implement correctly and was only being called by tests anyways.", "committedDate": "2020-07-17T16:22:40Z", "type": "commit"}, {"oid": "731e96c1c67545d6a729b726215e3057d8c4837b", "url": "https://github.com/apache/ozone/commit/731e96c1c67545d6a729b726215e3057d8c4837b", "message": "Add comments on added helper method", "committedDate": "2020-07-17T16:30:23Z", "type": "commit"}, {"oid": "378f27ca3f82a0837abd3d8dba217874eeec7e60", "url": "https://github.com/apache/ozone/commit/378f27ca3f82a0837abd3d8dba217874eeec7e60", "message": "Update comments", "committedDate": "2020-07-17T16:31:51Z", "type": "commit"}, {"oid": "3ef5841be91f4d21bbb8d2700fef604a3ec4d186", "url": "https://github.com/apache/ozone/commit/3ef5841be91f4d21bbb8d2700fef604a3ec4d186", "message": "Merge branch 'master' into HDDS-3976\n\n* master:\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  HDDS-3612. Allow mounting bucket under other volume (#1104)\n  HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)\n  HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)", "committedDate": "2020-07-17T16:40:14Z", "type": "commit"}, {"oid": "28dbbfe47989cd519dc8dba04d9cc05c681c20be", "url": "https://github.com/apache/ozone/commit/28dbbfe47989cd519dc8dba04d9cc05c681c20be", "message": "Fix checkstyle violations", "committedDate": "2020-07-17T18:54:44Z", "type": "commit"}, {"oid": "548da86696d9e3fbdcb60f1e51f96b6e107c78e6", "url": "https://github.com/apache/ozone/commit/548da86696d9e3fbdcb60f1e51f96b6e107c78e6", "message": "Merge remote-tracking branch 'upstream/master' into switch-to-new-interface\n\n* upstream/master:\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  HDDS-3612. Allow mounting bucket under other volume (#1104)\n  HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)\n  HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)\n  HDDS-3966. Disable flaky TestOMRatisSnapshots", "committedDate": "2020-07-17T19:25:40Z", "type": "commit"}, {"oid": "1bb61649c8f4e40bb18e69f205f5d8f51fa9d835", "url": "https://github.com/apache/ozone/commit/1bb61649c8f4e40bb18e69f205f5d8f51fa9d835", "message": "Merge branch 'HDDS-3976' into switch-to-new-interface\n\n* HDDS-3976:\n  Fix checkstyle violations\n  Update comments\n  Add comments on added helper method\n  Remove seekToLast() from iterator interface, implementation, and tests\n  Add more robust unit test with alternating key matches\n  All unit tests pass after allowing keys with deleted and deleting prefixes to be made\n  New iterator implementation passes existing unit tests", "committedDate": "2020-07-17T19:28:23Z", "type": "commit"}, {"oid": "3d771f7aacbc14ca60ffa024f91faa6c74f94622", "url": "https://github.com/apache/ozone/commit/3d771f7aacbc14ca60ffa024f91faa6c74f94622", "message": "Move new key value block iterator implementation and tests to new interface", "committedDate": "2020-07-17T19:34:25Z", "type": "commit"}, {"oid": "af8b4d43d6f2d0e64599bf909b724e3052bb2a75", "url": "https://github.com/apache/ozone/commit/af8b4d43d6f2d0e64599bf909b724e3052bb2a75", "message": "Merge branch 'master' into HDDS-3869\n\n* master:\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  HDDS-3612. Allow mounting bucket under other volume (#1104)\n  HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)\n  HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)\n  HDDS-3966. Disable flaky TestOMRatisSnapshots", "committedDate": "2020-07-20T13:19:43Z", "type": "commit"}, {"oid": "cf6e0c472d932c3e39cf0082b7b6eb52b8ffc5b6", "url": "https://github.com/apache/ozone/commit/cf6e0c472d932c3e39cf0082b7b6eb52b8ffc5b6", "message": "Merge branch 'HDDS-3869' into switch-to-new-interface\n\n* HDDS-3869:\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)", "committedDate": "2020-07-20T13:19:58Z", "type": "commit"}, {"oid": "3d208ea220790e170a313e9f7cbeb5e8a8dd9f99", "url": "https://github.com/apache/ozone/commit/3d208ea220790e170a313e9f7cbeb5e8a8dd9f99", "message": "Import schema version when importing container data from export", "committedDate": "2020-07-20T15:43:59Z", "type": "commit"}, {"oid": "3b02529ed09fac25151d9adbe4b6ef79469a27c2", "url": "https://github.com/apache/ozone/commit/3b02529ed09fac25151d9adbe4b6ef79469a27c2", "message": "Move block delete to correct table and remove debugging print statement", "committedDate": "2020-07-20T18:39:22Z", "type": "commit"}, {"oid": "257ee9ee290c28fc6a9cf1fa915fcfa3630ef6c4", "url": "https://github.com/apache/ozone/commit/257ee9ee290c28fc6a9cf1fa915fcfa3630ef6c4", "message": "Have block deleting service test look for #deleted# keys in metadata table", "committedDate": "2020-07-20T19:22:51Z", "type": "commit"}, {"oid": "18f6a18d5ffd33fd296f028c4e701272cadf402b", "url": "https://github.com/apache/ozone/commit/18f6a18d5ffd33fd296f028c4e701272cadf402b", "message": "Merge branch 'master' into HDDS-3976\n\n* master:\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)", "committedDate": "2020-07-20T19:52:01Z", "type": "commit"}, {"oid": "19857237dafffa86af9fd6a601bf2993b1cf7b0a", "url": "https://github.com/apache/ozone/commit/19857237dafffa86af9fd6a601bf2993b1cf7b0a", "message": "Merge branch 'switch-to-new-interface' into HDDS-3869\n\n* switch-to-new-interface: (67 commits)\n  Have block deleting service test look for #deleted# keys in metadata table\n  Move block delete to correct table and remove debugging print statement\n  Import schema version when importing container data from export\n  Move new key value block iterator implementation and tests to new interface\n  Remove duplicate method call introduced accidentally when resolving merge conflict\n  Add schema version param to ContainerCache.getDB() after it was accidentally removed in merge\n  Update merged unit test to use new interface\n  Switch merged in unit test to new interface\n  Remove missed git artifacts after conflict\n  Switch KeyValueBlockIterator to new interface after patch using old interface was integrated\n  Minor readability change\n  Make testContainerImportExport write to block and metadata tables where needed\n  Fix bug where default column family was being registered twice\n  Fix bug where .db file was created in wrong directory\n  Remove early batch operation commit in BlockDeletingService\n  Fix null start key error that was missed in the original fix\n  Make DeleteBlocksCommandHandler.deleteKeyValueContainerBlocks() use block data table instead of metadata table\n  Fix bug to allow null key to be used as start key in key range query\n  Fix bugs with missing column families and mismatched ContainerCache params\n  Fix bug where DB name was used instead of column family name\n  ...", "committedDate": "2020-07-20T20:48:36Z", "type": "commit"}, {"oid": "24d4d788d725d7248c6659ca374dc8326223042e", "url": "https://github.com/apache/ozone/commit/24d4d788d725d7248c6659ca374dc8326223042e", "message": "Add deleted blocks table to base level DB wrappers", "committedDate": "2020-07-20T21:09:12Z", "type": "commit"}, {"oid": "444f75f9abe2e520610e9e626cf2d915db87ccf0", "url": "https://github.com/apache/ozone/commit/444f75f9abe2e520610e9e626cf2d915db87ccf0", "message": "Replace uses of #deleted# key prefix with access to new deleted blocks table\n\nStill need to do the more involved fix of carrying this out for\nTestKeyValueBlockIterator and MetadataKeyFilters.", "committedDate": "2020-07-21T13:32:12Z", "type": "commit"}, {"oid": "ef4272493c47271b4b28e2bf73aae9a62a1971ae", "url": "https://github.com/apache/ozone/commit/ef4272493c47271b4b28e2bf73aae9a62a1971ae", "message": "New iterator implementation passes existing unit tests", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "5e7e9df47d6def80e971b910b2bc676e17c62b09", "url": "https://github.com/apache/ozone/commit/5e7e9df47d6def80e971b910b2bc676e17c62b09", "message": "All unit tests pass after allowing keys with deleted and deleting prefixes to be made", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "54e83a6bb302a49821300bb6b1545521542b05a3", "url": "https://github.com/apache/ozone/commit/54e83a6bb302a49821300bb6b1545521542b05a3", "message": "Add more robust unit test with alternating key matches", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "f19e77f7e896a047c77cddc90c6437944fde8fd8", "url": "https://github.com/apache/ozone/commit/f19e77f7e896a047c77cddc90c6437944fde8fd8", "message": "Remove seekToLast() from iterator interface, implementation, and tests\n\nIt will require additional work to implement correctly and was only being called by tests anyways.", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "6916e4ad3bfef17a3c0a083f112d3723e0613c14", "url": "https://github.com/apache/ozone/commit/6916e4ad3bfef17a3c0a083f112d3723e0613c14", "message": "Add comments on added helper method", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "27ec1fbc53975e6dfc62a066cb0e1f282d374614", "url": "https://github.com/apache/ozone/commit/27ec1fbc53975e6dfc62a066cb0e1f282d374614", "message": "Update comments", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "7fe3dc4ab4a81381f3196ff32a6f5c12463ff4df", "url": "https://github.com/apache/ozone/commit/7fe3dc4ab4a81381f3196ff32a6f5c12463ff4df", "message": "HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "8a9e8331302c1811e8c1f06c1f22f23b4d8bc3fa", "url": "https://github.com/apache/ozone/commit/8a9e8331302c1811e8c1f06c1f22f23b4d8bc3fa", "message": "HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "68493482bc84890096a4dd1e2e23dff83d9ad99c", "url": "https://github.com/apache/ozone/commit/68493482bc84890096a4dd1e2e23dff83d9ad99c", "message": "HDDS-3612. Allow mounting bucket under other volume (#1104)", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "072da921ab23c5377fd4eb20b2f01de6da4a116b", "url": "https://github.com/apache/ozone/commit/072da921ab23c5377fd4eb20b2f01de6da4a116b", "message": "HDDS-3964. Ratis config key mismatch (#1204)", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "223d59c503ab9c326e43fdb909bbe4a3f6fa25a1", "url": "https://github.com/apache/ozone/commit/223d59c503ab9c326e43fdb909bbe4a3f6fa25a1", "message": "HDDS-3855. Add upgrade smoketest (#1142)", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "a231ff82c0be2d1ac6da3d1f1b01d5cdc314f453", "url": "https://github.com/apache/ozone/commit/a231ff82c0be2d1ac6da3d1f1b01d5cdc314f453", "message": "Fix checkstyle violations", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "8878e9da634551542588517b893d55b5ff3256ed", "url": "https://github.com/apache/ozone/commit/8878e9da634551542588517b893d55b5ff3256ed", "message": "HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "3f7b4eb51679a567c41dc7e69ac978de9d463cf3", "url": "https://github.com/apache/ozone/commit/3f7b4eb51679a567c41dc7e69ac978de9d463cf3", "message": "HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)", "committedDate": "2020-07-21T13:35:07Z", "type": "commit"}, {"oid": "7acfb125b63273776c056fac62362496216a9d44", "url": "https://github.com/apache/ozone/commit/7acfb125b63273776c056fac62362496216a9d44", "message": "HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "81cdb949fa89f17d4ea3e13bdbf1bd67b010b4a3", "url": "https://github.com/apache/ozone/commit/81cdb949fa89f17d4ea3e13bdbf1bd67b010b4a3", "message": "HDDS-3806. Support recognize aws v2 Authorization header. (#1098)", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "8517e380a1c759e7f0d941db8a0a1c1efa42fcdb", "url": "https://github.com/apache/ozone/commit/8517e380a1c759e7f0d941db8a0a1c1efa42fcdb", "message": "HDDS-3984. Support filter and search the columns in recon UI (#1218)", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "fc2b8774f070f824a4c88d366241c5ba55076393", "url": "https://github.com/apache/ozone/commit/fc2b8774f070f824a4c88d366241c5ba55076393", "message": "Add schemaVersion field to yaml key value container data", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "3f2dc9510fd01a3882ef581177ab364354293dd8", "url": "https://github.com/apache/ozone/commit/3f2dc9510fd01a3882ef581177ab364354293dd8", "message": "Add schemaVersion yaml field as string to .container options", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "85711f94cd4e3520ff0676958c0a7eb387e5b4d8", "url": "https://github.com/apache/ozone/commit/85711f94cd4e3520ff0676958c0a7eb387e5b4d8", "message": "Add flush and compact methods to datanodestore", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "43d315b84466d72dd961febc67f75d349308cc77", "url": "https://github.com/apache/ozone/commit/43d315b84466d72dd961febc67f75d349308cc77", "message": "Replace old puts, gets, deletes and batch operations with the new versions", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "bcfe27f3d929a785dbe63475489d2de171b67ab0", "url": "https://github.com/apache/ozone/commit/bcfe27f3d929a785dbe63475489d2de171b67ab0", "message": "Begin changing interface usage in nonKeyValue or Test classes\n\nSome changes are pending furhter investigation and are not yet completed.", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "6cdba76b69ad0026db640f23814e442f2286e961", "url": "https://github.com/apache/ozone/commit/6cdba76b69ad0026db640f23814e442f2286e961", "message": "Switch key type of block data table from string to long", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "8bfffc8cd972191b7c94ea444a983394b702a2ae", "url": "https://github.com/apache/ozone/commit/8bfffc8cd972191b7c94ea444a983394b702a2ae", "message": "Update gets, puts and batch operations in KeyValue classes to use new interface", "committedDate": "2020-07-21T13:35:08Z", "type": "commit"}, {"oid": "c5b3f8b0c1dea86de1eb819ff486dab052057a81", "url": "https://github.com/apache/ozone/commit/c5b3f8b0c1dea86de1eb819ff486dab052057a81", "message": "Update get, put, delete, and write batch calls to use new interface in unit tests", "committedDate": "2020-07-21T13:36:08Z", "type": "commit"}, {"oid": "8569557ec289bd908aad187c0ebe5e4ef86e2db7", "url": "https://github.com/apache/ozone/commit/8569557ec289bd908aad187c0ebe5e4ef86e2db7", "message": "Add getRangeKVs and getSequentialRangeKVs methods to Table interface\n\nAdd implementations based on the original implementations in MetadataStore,\nexcept that these new versions will only operate on one table.", "committedDate": "2020-07-21T13:36:12Z", "type": "commit"}, {"oid": "30b15e7a065b4599ad3c229b04e7427393bc9378", "url": "https://github.com/apache/ozone/commit/30b15e7a065b4599ad3c229b04e7427393bc9378", "message": "Fix typo in method call to new interface", "committedDate": "2020-07-21T13:36:12Z", "type": "commit"}, {"oid": "9175db605d188d78e0333fc21c3cbf3779607754", "url": "https://github.com/apache/ozone/commit/9175db605d188d78e0333fc21c3cbf3779607754", "message": "Add separate flushLog() and flushDB() methods to new interface to preserve old functionality", "committedDate": "2020-07-21T13:36:12Z", "type": "commit"}]}