{"pr_number": 4639, "pr_title": "[Timelock Partitioning] Part 53: Latest sequence cache concurrent", "pr_createdAt": "2020-03-05T21:15:16Z", "pr_url": "https://github.com/palantir/atlasdb/pull/4639", "timeline": [{"oid": "11d723bc1d91160066cbf651d958a7e57d5021ab", "url": "https://github.com/palantir/atlasdb/commit/11d723bc1d91160066cbf651d958a7e57d5021ab", "message": "Concurrent cache.", "committedDate": "2020-03-05T17:03:04Z", "type": "commit"}, {"oid": "c9c540e1173c2e30b0eb19bb25bf8a5cdc0e7d4b", "url": "https://github.com/palantir/atlasdb/commit/c9c540e1173c2e30b0eb19bb25bf8a5cdc0e7d4b", "message": "Fix up comment.", "committedDate": "2020-03-05T17:06:53Z", "type": "commit"}, {"oid": "d93303b9dca0055f18880e00250fd982403a10ec", "url": "https://github.com/palantir/atlasdb/commit/d93303b9dca0055f18880e00250fd982403a10ec", "message": "More clarifying details around the CAS.", "committedDate": "2020-03-05T17:09:57Z", "type": "commit"}, {"oid": "f1ad8bfc46dbac3ae2c8933d6020e71c73868edb", "url": "https://github.com/palantir/atlasdb/commit/f1ad8bfc46dbac3ae2c8933d6020e71c73868edb", "message": "Fix compile error.", "committedDate": "2020-03-05T17:43:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2OTk0MQ==", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r388569941", "bodyText": "should I have passed in the cache key that generated this request? I think it's probably fine, but might make sense", "author": "felixdesouza", "createdAt": "2020-03-05T21:16:03Z", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,159 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n+import org.immutables.value.Value;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<TimestampedAcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");\n     }\n \n-    private Map<Client, PaxosLong> unsafeGetLatest(Set<Client> clients) throws InvalidAcceptorCacheKeyException {\n-        if (cacheKey == null) {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clients));\n-            return getResponseMap(clients);\n-        }\n+    private Map<Client, PaxosLong> populateNewCache(Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        AcceptorCacheDigest digest = delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clientsSeenSoFar);\n+        ConcurrentMap<Client, PaxosLong> newEntriesToCache =\n+                cacheKeysToCaches.get(TimestampedAcceptorCacheKey.of(digest));\n+        processDigest(newEntriesToCache, digest);\n+        return getResponseMap(newEntriesToCache, requestedClients);\n+    }\n \n-        Set<Client> newClients = Sets.difference(clients, cachedEntries.keySet());\n+    private Map<Client, PaxosLong> populateExistingCache(\n+            TimestampedAcceptorCacheKey timestampedCacheKey,\n+            ConcurrentMap<Client, PaxosLong> currentCachedEntries,\n+            Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        Set<Client> newClients = ImmutableSet.copyOf(Sets.difference(requestedClients, currentCachedEntries.keySet()));\n         if (newClients.isEmpty()) {\n-            delegate.latestSequencesPreparedOrAcceptedCached(cacheKey).ifPresent(this::processDigest);\n-            return getResponseMap(clients);\n+            delegate.latestSequencesPreparedOrAcceptedCached(timestampedCacheKey.cacheKey())\n+                    .ifPresent(digest -> processDigest(currentCachedEntries, digest));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         } else {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.of(cacheKey), newClients));\n-            return getResponseMap(clients);\n+            processDigest(currentCachedEntries, delegate.latestSequencesPreparedOrAccepted(\n+                    Optional.of(timestampedCacheKey.cacheKey()),\n+                    newClients));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n+        }\n+    }\n+\n+    private void processDigest(ConcurrentMap<Client, PaxosLong> currentCachedEntries, AcceptorCacheDigest digest) {\n+        TimestampedAcceptorCacheKey newCacheKey = TimestampedAcceptorCacheKey.of(digest);\n+        // this shares the same map with \"previous\" cache keys, if it's too confusing we can always copy it potentially\n+        ConcurrentMap<Client, PaxosLong> newCachedEntries =\n+                cacheKeysToCaches.get(newCacheKey, $ -> currentCachedEntries);\n+        KeyedStream.stream(digest.updates())\n+                .map(PaxosLong::of)\n+                .forEach((client, paxosLong) ->\n+                        newCachedEntries.merge(client, paxosLong, BatchingPaxosLatestSequenceCache::max));\n+\n+        // for a *new* mapping, setting the cache key must happen *after* we've setup the mapping, so that concurrent\n+        // clients will not reference an in-progress populating map which can be empty.\n+        maybeSetNewCacheKey(newCacheKey);\n+    }\n+\n+    private void maybeSetNewCacheKey(TimestampedAcceptorCacheKey newCacheKey) {\n+        while (true) {\n+            TimestampedAcceptorCacheKey current = cacheKey.get();", "originalCommit": "f1ad8bfc46dbac3ae2c8933d6020e71c73868edb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA0NzE2MQ==", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r389047161", "bodyText": "I think this is fine, as written we have a standard CAS algorithm and it makes the most current role of cacheKey more obvious", "author": "jeremyk-91", "createdAt": "2020-03-06T17:41:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2OTk0MQ=="}], "type": "inlineReview"}, {"oid": "7daa4c3499227e18bc8d885fc1e210e5686156dc", "url": "https://github.com/palantir/atlasdb/commit/7daa4c3499227e18bc8d885fc1e210e5686156dc", "message": "Fix server side runtime errors and pass the timestamp around.", "committedDate": "2020-03-05T21:16:42Z", "type": "commit"}, {"oid": "ee3931053e3285b59cb5f735bff5bbc4c322bf08", "url": "https://github.com/palantir/atlasdb/commit/ee3931053e3285b59cb5f735bff5bbc4c322bf08", "message": "clarifying rename", "committedDate": "2020-03-06T14:21:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg4MTc5NA==", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r388881794", "bodyText": "The new and old cache key are never the same at this point", "author": "jeremyk-91", "createdAt": "2020-03-06T12:43:33Z", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey.cacheKey()),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");\n     }\n \n-    private Map<Client, PaxosLong> unsafeGetLatest(Set<Client> clients) throws InvalidAcceptorCacheKeyException {\n-        if (cacheKey == null) {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clients));\n-            return getResponseMap(clients);\n-        }\n+    private Map<Client, PaxosLong> populateNewCache(Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        AcceptorCacheDigest digest = delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clientsSeenSoFar);\n+        ConcurrentMap<Client, PaxosLong> newEntriesToCache =\n+                cacheKeysToCaches.get(digest.newCacheKey());\n+        processDigest(newEntriesToCache, digest);\n+        return getResponseMap(newEntriesToCache, requestedClients);\n+    }\n \n-        Set<Client> newClients = Sets.difference(clients, cachedEntries.keySet());\n+    private Map<Client, PaxosLong> populateExistingCache(\n+            TimestampedAcceptorCacheKey timestampedCacheKey,\n+            ConcurrentMap<Client, PaxosLong> currentCachedEntries,\n+            Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        Set<Client> newClients = ImmutableSet.copyOf(Sets.difference(requestedClients, currentCachedEntries.keySet()));\n         if (newClients.isEmpty()) {\n-            delegate.latestSequencesPreparedOrAcceptedCached(cacheKey).ifPresent(this::processDigest);\n-            return getResponseMap(clients);\n+            delegate.latestSequencesPreparedOrAcceptedCached(timestampedCacheKey.cacheKey())\n+                    .ifPresent(digest -> processDigest(currentCachedEntries, digest));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         } else {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.of(cacheKey), newClients));\n-            return getResponseMap(clients);\n+            processDigest(currentCachedEntries, delegate.latestSequencesPreparedOrAccepted(\n+                    Optional.of(timestampedCacheKey.cacheKey()),\n+                    newClients));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         }\n     }\n \n-    private Map<Client, PaxosLong> getResponseMap(Set<Client> clientsInRequest) {\n-        return Maps.toMap(clientsInRequest, client -> cachedEntries.getOrDefault(client, DEFAULT_VALUE));\n+    private void processDigest(ConcurrentMap<Client, PaxosLong> currentCachedEntries, AcceptorCacheDigest digest) {\n+        TimestampedAcceptorCacheKey newCacheKey = TimestampedAcceptorCacheKey.of(digest);\n+        // this shares the same map with \"previous\" cache keys, if it's too confusing we can always copy it potentially\n+        ConcurrentMap<Client, PaxosLong> newCachedEntries =\n+                cacheKeysToCaches.get(newCacheKey.cacheKey(), $ -> currentCachedEntries);", "originalCommit": "7daa4c3499227e18bc8d885fc1e210e5686156dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg4MzExNQ==", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r388883115", "bodyText": "so this map stores the largest known version of the updates from the reference point indicated by the cache key", "author": "jeremyk-91", "createdAt": "2020-03-06T12:46:33Z", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =", "originalCommit": "7daa4c3499227e18bc8d885fc1e210e5686156dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA0NDMyOA==", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r389044328", "bodyText": "nit: double request in message\nI think this is safe even with 3 attempts, that usually indicates leader churn or getting strangely far behind.", "author": "jeremyk-91", "createdAt": "2020-03-06T17:36:51Z", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey.cacheKey()),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");", "originalCommit": "7daa4c3499227e18bc8d885fc1e210e5686156dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA1ODM5Nw==", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r389058397", "bodyText": "nit: accumulateLatestCacheKey?", "author": "jeremyk-91", "createdAt": "2020-03-06T18:05:14Z", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey.cacheKey()),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");\n     }\n \n-    private Map<Client, PaxosLong> unsafeGetLatest(Set<Client> clients) throws InvalidAcceptorCacheKeyException {\n-        if (cacheKey == null) {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clients));\n-            return getResponseMap(clients);\n-        }\n+    private Map<Client, PaxosLong> populateNewCache(Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        AcceptorCacheDigest digest = delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clientsSeenSoFar);\n+        ConcurrentMap<Client, PaxosLong> newEntriesToCache =\n+                cacheKeysToCaches.get(digest.newCacheKey());\n+        processDigest(newEntriesToCache, digest);\n+        return getResponseMap(newEntriesToCache, requestedClients);\n+    }\n \n-        Set<Client> newClients = Sets.difference(clients, cachedEntries.keySet());\n+    private Map<Client, PaxosLong> populateExistingCache(\n+            TimestampedAcceptorCacheKey timestampedCacheKey,\n+            ConcurrentMap<Client, PaxosLong> currentCachedEntries,\n+            Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        Set<Client> newClients = ImmutableSet.copyOf(Sets.difference(requestedClients, currentCachedEntries.keySet()));\n         if (newClients.isEmpty()) {\n-            delegate.latestSequencesPreparedOrAcceptedCached(cacheKey).ifPresent(this::processDigest);\n-            return getResponseMap(clients);\n+            delegate.latestSequencesPreparedOrAcceptedCached(timestampedCacheKey.cacheKey())\n+                    .ifPresent(digest -> processDigest(currentCachedEntries, digest));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         } else {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.of(cacheKey), newClients));\n-            return getResponseMap(clients);\n+            processDigest(currentCachedEntries, delegate.latestSequencesPreparedOrAccepted(\n+                    Optional.of(timestampedCacheKey.cacheKey()),\n+                    newClients));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         }\n     }\n \n-    private Map<Client, PaxosLong> getResponseMap(Set<Client> clientsInRequest) {\n-        return Maps.toMap(clientsInRequest, client -> cachedEntries.getOrDefault(client, DEFAULT_VALUE));\n+    private void processDigest(ConcurrentMap<Client, PaxosLong> currentCachedEntries, AcceptorCacheDigest digest) {\n+        TimestampedAcceptorCacheKey newCacheKey = TimestampedAcceptorCacheKey.of(digest);\n+        // this shares the same map with \"previous\" cache keys, if it's too confusing we can always copy it potentially\n+        ConcurrentMap<Client, PaxosLong> newCachedEntries =\n+                cacheKeysToCaches.get(newCacheKey.cacheKey(), $ -> currentCachedEntries);\n+        KeyedStream.stream(digest.updates())\n+                .map(PaxosLong::of)\n+                .forEach((client, paxosLong) ->\n+                        newCachedEntries.merge(client, paxosLong, BatchingPaxosLatestSequenceCache::max));\n+\n+        // for a *new* mapping, setting the cache key must happen *after* we've setup the mapping, so that concurrent\n+        // clients will not reference an in-progress populating map which can be empty.\n+        maybeSetNewCacheKey(newCacheKey);", "originalCommit": "7daa4c3499227e18bc8d885fc1e210e5686156dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "110f30d2dd22a4e94db7da0414166bf8dcbed1f8", "url": "https://github.com/palantir/atlasdb/commit/110f30d2dd22a4e94db7da0414166bf8dcbed1f8", "message": "Add clarifying comments.", "committedDate": "2020-03-19T11:48:13Z", "type": "commit"}, {"oid": "4a61353d80ef451828d27fc5aa31b77b5624c528", "url": "https://github.com/palantir/atlasdb/commit/4a61353d80ef451828d27fc5aa31b77b5624c528", "message": "fix up comments again.", "committedDate": "2020-03-19T12:11:12Z", "type": "commit"}]}