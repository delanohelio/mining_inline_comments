{"pr_number": 757, "pr_title": "PARQUET-41: Add bloom filter ", "pr_createdAt": "2020-02-12T13:14:17Z", "pr_url": "https://github.com/apache/parquet-mr/pull/757", "timeline": [{"oid": "d473d170be64aec6042a58f3681796220d9bcece", "url": "https://github.com/apache/parquet-mr/commit/d473d170be64aec6042a58f3681796220d9bcece", "message": "PARQUET-1328: Add Bloom filter reader and writer (#587)", "committedDate": "2019-01-21T10:47:17Z", "type": "commit"}, {"oid": "96c2fef80c8d433cf2e247e28f3af07562a8065e", "url": "https://github.com/apache/parquet-mr/commit/96c2fef80c8d433cf2e247e28f3af07562a8065e", "message": "PARQUET-1516: Store Bloom filters near to footer (#608)\n\n* PARQUET-1516: Store Bloom filters near to footer", "committedDate": "2019-02-12T09:28:25Z", "type": "commit"}, {"oid": "dd7e655de908cb4b35e1ea94f5b54525865dce6d", "url": "https://github.com/apache/parquet-mr/commit/dd7e655de908cb4b35e1ea94f5b54525865dce6d", "message": "PARQUET-1391: Integrate Bloom filter logic (#619)", "committedDate": "2019-03-19T09:36:02Z", "type": "commit"}, {"oid": "21c45edef6be575df813a332adae4d011915ca36", "url": "https://github.com/apache/parquet-mr/commit/21c45edef6be575df813a332adae4d011915ca36", "message": "Merge branch 'master' into bloom-filter", "committedDate": "2019-05-30T08:40:19Z", "type": "commit"}, {"oid": "1fc273329cd6d54085de81ecccfe99be181f24be", "url": "https://github.com/apache/parquet-mr/commit/1fc273329cd6d54085de81ecccfe99be181f24be", "message": "Merge branch 'master' into bloom-filter", "committedDate": "2019-09-20T11:11:44Z", "type": "commit"}, {"oid": "ba28686ac84e22ef8694e4b9d25ec89295267b91", "url": "https://github.com/apache/parquet-mr/commit/ba28686ac84e22ef8694e4b9d25ec89295267b91", "message": "PARQUET-1660: align Bloom filter implementation with format (#686)", "committedDate": "2020-01-07T11:16:42Z", "type": "commit"}, {"oid": "1e44aa4cb1fc0d7a6c107fad23e92e1fbe36160e", "url": "https://github.com/apache/parquet-mr/commit/1e44aa4cb1fc0d7a6c107fad23e92e1fbe36160e", "message": "Merge remote-tracking branch 'apache/master' into bloom-filter\n\nConflicts:\n\tparquet-column/src/main/java/org/apache/parquet/column/ParquetProperties.java\n\tparquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java\n\tpom.xml", "committedDate": "2020-01-09T08:57:14Z", "type": "commit"}, {"oid": "e396d15350d2910b6a39c0fc1af4b16675c06104", "url": "https://github.com/apache/parquet-mr/commit/e396d15350d2910b6a39c0fc1af4b16675c06104", "message": "Merge branch 'master' into bloom-filter\n\nConflicts:\n\tparquet-column/src/main/java/org/apache/parquet/column/ParquetProperties.java\n\tparquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n\tparquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java\n\tparquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java", "committedDate": "2020-02-12T12:58:47Z", "type": "commit"}, {"oid": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "url": "https://github.com/apache/parquet-mr/commit/6260ca49b315e13e8732fb4e4633b2ed1258ac35", "message": "remove some useless changes", "committedDate": "2020-02-12T14:21:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc0MjU5OA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378742598", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                super (schema, pageWriteStore, bloomFilterWriteStore, props);\n          \n          \n            \n                super(schema, pageWriteStore, bloomFilterWriteStore, props);", "author": "gszadovszky", "createdAt": "2020-02-13T09:30:16Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriteStoreV1.java", "diffHunk": "@@ -36,8 +38,15 @@ public ColumnWriteStoreV1(final PageWriteStore pageWriteStore,\n     super(pageWriteStore, props);\n   }\n \n+  public ColumnWriteStoreV1(MessageType schema, PageWriteStore pageWriteStore,\n+                            BloomFilterWriteStore bloomFilterWriteStore,\n+                            ParquetProperties props) {\n+    super (schema, pageWriteStore, bloomFilterWriteStore, props);", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc1NzA0Nw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378757047", "bodyText": "Is it fine to have less bytes than the optimal if optimalNumOfBits % 8 != 0?", "author": "gszadovszky", "createdAt": "2020-02-13T09:56:59Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -66,6 +74,42 @@\n     this.dataColumn = props.newValuesWriter(path);\n   }\n \n+  ColumnWriterBase(\n+    ColumnDescriptor path,\n+    PageWriter pageWriter,\n+    BloomFilterWriter bloomFilterWriter,\n+    ParquetProperties props\n+  ) {\n+    this(path, pageWriter, props);\n+\n+    // Bloom filters don't support nested columns yet; see PARQUET-1453.\n+    if (path.getPath().length != 1 || bloomFilterWriter == null) {\n+      return;\n+    }\n+    String column = path.getPath()[0];\n+\n+    this.bloomFilterWriter = bloomFilterWriter;\n+    Set<String> bloomFilterColumns = props.getBloomFilterColumns();\n+    if (!bloomFilterColumns.contains(column)) {\n+      return;\n+    }\n+    int maxBloomFilterSize = props.getMaxBloomFilterBytes();\n+\n+    Map<String, Long> bloomFilterColumnExpectedNDVs = props.getBloomFilterColumnExpectedNDVs();\n+    if (bloomFilterColumnExpectedNDVs.size() > 0) {\n+      // If user specify the column NDV, we construct Bloom filter from it.\n+      if (bloomFilterColumnExpectedNDVs.keySet().contains(column)) {\n+        int optimalNumOfBits = BlockSplitBloomFilter.optimalNumOfBits(bloomFilterColumnExpectedNDVs.get(column).intValue(),\n+          BlockSplitBloomFilter.DEFAULT_FPP);\n+\n+        this.bloomFilter = new BlockSplitBloomFilter(optimalNumOfBits / 8, maxBloomFilterSize);", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk0NDgzMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378944831", "bodyText": "I think it is OK because it only allows having one byte less.", "author": "chenjunjiedada", "createdAt": "2020-02-13T15:46:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc1NzA0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI4OTcyMg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379289722", "bodyText": "I'm not an expert in bloom filters so if you say so, I'm fine with that. Is it possible that optionalNumOfBits < 8?", "author": "gszadovszky", "createdAt": "2020-02-14T07:50:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc1NzA0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDAwNjA4Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380006083", "bodyText": "The optimalNumOfBit should be within range[minimumBytes * 8, maximumBytes *8] as the computation as below:\n    // Handle overflow.\n    if (m > MAX || m < 0) {\n      numBits = (int)MAX;\n    }\n\n    // Round to BITS_PER_BLOCK\n    numBits = (numBits + BITS_PER_BLOCK -1) & ~BITS_PER_BLOCK;\n\n    if (numBits < (DEFAULT_MINIMUM_BYTES << 3)) {\n      numBits = DEFAULT_MINIMUM_BYTES << 3;\n    }", "author": "chenjunjiedada", "createdAt": "2020-02-17T06:41:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc1NzA0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2MDA2OQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378760069", "bodyText": "I am not sure if we want to count the bloom filter in. The statistics is not counted either.", "author": "gszadovszky", "createdAt": "2020-02-13T10:02:41Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -265,20 +348,24 @@ long getCurrentPageBufferedSize() {\n    * @return the number of bytes of memory used to buffer the current data and the previously written pages\n    */\n   long getTotalBufferedSize() {\n+    long bloomBufferSize = bloomFilter == null ? 0 : bloomFilter.getBitsetSize();\n     return repetitionLevelColumn.getBufferedSize()\n         + definitionLevelColumn.getBufferedSize()\n         + dataColumn.getBufferedSize()\n-        + pageWriter.getMemSize();\n+        + pageWriter.getMemSize()\n+        + bloomBufferSize;", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk0NzUyOA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378947528", "bodyText": "Statistics should be smaller than the bloom filter. but the bloom filter size is not that large according to computation. I 'm ok to remove this.", "author": "chenjunjiedada", "createdAt": "2020-02-13T15:50:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2MDA2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5MTMzOA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379291338", "bodyText": "As per the method comment it is \"Used to decide when to write a page or row group\" bloom filters shall not be counted.", "author": "gszadovszky", "createdAt": "2020-02-14T07:55:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2MDA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2MDE1Mg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378760152", "bodyText": "See above.", "author": "gszadovszky", "createdAt": "2020-02-13T10:02:51Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -265,20 +348,24 @@ long getCurrentPageBufferedSize() {\n    * @return the number of bytes of memory used to buffer the current data and the previously written pages\n    */\n   long getTotalBufferedSize() {\n+    long bloomBufferSize = bloomFilter == null ? 0 : bloomFilter.getBitsetSize();\n     return repetitionLevelColumn.getBufferedSize()\n         + definitionLevelColumn.getBufferedSize()\n         + dataColumn.getBufferedSize()\n-        + pageWriter.getMemSize();\n+        + pageWriter.getMemSize()\n+        + bloomBufferSize;\n   }\n \n   /**\n    * @return actual memory used\n    */\n   long allocatedSize() {\n+    long bloomAllocatedSize = bloomFilter == null ? 0 : bloomFilter.getBitsetSize();\n     return repetitionLevelColumn.getAllocatedSize()\n         + definitionLevelColumn.getAllocatedSize()\n         + dataColumn.getAllocatedSize()\n-        + pageWriter.allocatedSize();\n+        + pageWriter.allocatedSize()\n+        + bloomAllocatedSize;", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5MjU3NA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379292574", "bodyText": "Based on the invocations this method seems to represent a memory footprint related to the column chunk. So, bloom filters should not be counted here either.", "author": "gszadovszky", "createdAt": "2020-02-14T07:59:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2MDE1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2NzMxNg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378767316", "bodyText": "Based on the java naming conventions you shall not use camel case in package names.", "author": "gszadovszky", "createdAt": "2020-02-13T10:16:08Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/filter2/BloomFilterLevel/BloomFilterImpl.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.filter2.BloomFilterLevel;", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk0Nzc3MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378947770", "bodyText": "ok", "author": "chenjunjiedada", "createdAt": "2020-02-13T15:50:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc2NzMxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc3NDcyMg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378774722", "bodyText": "You have specific hash methods for every primitive type supported in Parquet. Do you really need this one?\n(The implementation of this method would have quite bad performance so, you shall never invoke for each values to be written.)", "author": "gszadovszky", "createdAt": "2020-02-13T10:30:20Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BloomFilter.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+\n+/**\n+ * A Bloom filter is a compact structure to indicate whether an item is not in a set or probably\n+ * in a set. The Bloom filter usually consists of a bit set that represents a elements set,\n+ * a hash strategy and a Bloom filter algorithm.\n+ */\n+public interface BloomFilter {\n+  /* Bloom filter Hash strategy.\n+   *\n+   * xxHash is an extremely fast hash algorithm, running at RAM speed limits. It successfully\n+   * completes the SMHasher test suite which evaluates collision, dispersion and randomness qualities\n+   * of hash functions. It shows good performance advantage from its benchmark result.\n+   * (see https://github.com/Cyan4973/xxHash).\n+   */\n+  enum HashStrategy {\n+    XXH64(0);\n+    HashStrategy(int value) {\n+      this.value = value;\n+    }\n+    int value;\n+  }\n+\n+  // Bloom filter algorithm.\n+  enum Algorithm {\n+    BLOCK(0);\n+    Algorithm(int value) {\n+      this.value = value;\n+    }\n+    int value;\n+  }\n+\n+  // Bloom filter compression.\n+  enum Compression {\n+    UNCOMPRESSED(0);\n+    Compression(int value) {\n+      this.value = value;\n+    }\n+    int value;\n+  }\n+\n+  /**\n+   * Write the Bloom filter to an output stream. It writes the Bloom filter header including the\n+   * bitset's length in bytes, the hash strategy, the algorithm, and the bitset.\n+   *\n+   * @param out the output stream to write\n+   */\n+  void writeTo(OutputStream out) throws IOException;\n+\n+  /**\n+   * Insert an element to the Bloom filter, the element content is represented by\n+   * the hash value of its plain encoding result.\n+   *\n+   * @param hash the hash result of element.\n+   */\n+  void insertHash(long hash);\n+\n+  /**\n+   * Determine whether an element is in set or not.\n+   *\n+   * @param hash the hash value of element plain encoding result.\n+   * @return false if element is must not in set, true if element probably in set.\n+   */\n+  boolean findHash(long hash);\n+\n+  /**\n+   * Get the number of bytes for bitset in this Bloom filter.\n+   *\n+   * @return The number of bytes for bitset in this Bloom filter.\n+   */\n+  long getBitsetSize();\n+\n+  /**\n+   * Compute hash for int value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(int value);\n+\n+  /**\n+   * Compute hash for long value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(long value) ;\n+\n+  /**\n+   * Compute hash for double value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(double value);\n+\n+  /**\n+   * Compute hash for float value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(float value);\n+\n+  /**\n+   * Compute hash for Binary value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(Binary value);\n+\n+  /**\n+   * Compute hash for Object value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(Object value);", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk0OTc4Ng==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378949786", "bodyText": "This is used in filtering logic, it is supposed to be called only when doing predicate.", "author": "chenjunjiedada", "createdAt": "2020-02-13T15:53:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc3NDcyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5MzQxMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379293411", "bodyText": "Thanks for the explanation.", "author": "gszadovszky", "createdAt": "2020-02-14T08:02:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc3NDcyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc3NTY2Ng==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378775666", "bodyText": "You don't have a hash(boolean) method. I think, it is fine to not support boolean values in bloom filters but it should be documented.\nAlso, this path shall be covered by the tests as well.", "author": "gszadovszky", "createdAt": "2020-02-13T10:31:59Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BloomFilter.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+\n+/**\n+ * A Bloom filter is a compact structure to indicate whether an item is not in a set or probably\n+ * in a set. The Bloom filter usually consists of a bit set that represents a elements set,\n+ * a hash strategy and a Bloom filter algorithm.\n+ */\n+public interface BloomFilter {\n+  /* Bloom filter Hash strategy.\n+   *\n+   * xxHash is an extremely fast hash algorithm, running at RAM speed limits. It successfully\n+   * completes the SMHasher test suite which evaluates collision, dispersion and randomness qualities\n+   * of hash functions. It shows good performance advantage from its benchmark result.\n+   * (see https://github.com/Cyan4973/xxHash).\n+   */\n+  enum HashStrategy {\n+    XXH64(0);\n+    HashStrategy(int value) {\n+      this.value = value;\n+    }\n+    int value;\n+  }\n+\n+  // Bloom filter algorithm.\n+  enum Algorithm {\n+    BLOCK(0);\n+    Algorithm(int value) {\n+      this.value = value;\n+    }\n+    int value;\n+  }\n+\n+  // Bloom filter compression.\n+  enum Compression {\n+    UNCOMPRESSED(0);\n+    Compression(int value) {\n+      this.value = value;\n+    }\n+    int value;\n+  }\n+\n+  /**\n+   * Write the Bloom filter to an output stream. It writes the Bloom filter header including the\n+   * bitset's length in bytes, the hash strategy, the algorithm, and the bitset.\n+   *\n+   * @param out the output stream to write\n+   */\n+  void writeTo(OutputStream out) throws IOException;\n+\n+  /**\n+   * Insert an element to the Bloom filter, the element content is represented by\n+   * the hash value of its plain encoding result.\n+   *\n+   * @param hash the hash result of element.\n+   */\n+  void insertHash(long hash);\n+\n+  /**\n+   * Determine whether an element is in set or not.\n+   *\n+   * @param hash the hash value of element plain encoding result.\n+   * @return false if element is must not in set, true if element probably in set.\n+   */\n+  boolean findHash(long hash);\n+\n+  /**\n+   * Get the number of bytes for bitset in this Bloom filter.\n+   *\n+   * @return The number of bytes for bitset in this Bloom filter.\n+   */\n+  long getBitsetSize();\n+\n+  /**\n+   * Compute hash for int value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(int value);\n+\n+  /**\n+   * Compute hash for long value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(long value) ;\n+\n+  /**\n+   * Compute hash for double value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(double value);\n+\n+  /**\n+   * Compute hash for float value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(float value);\n+\n+  /**\n+   * Compute hash for Binary value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(Binary value);\n+\n+  /**\n+   * Compute hash for Object value by using its plain encoding result.\n+   *\n+   * @param value the value to hash\n+   * @return hash result\n+   */\n+  long hash(Object value);\n+}", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk1MDEwOA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378950108", "bodyText": "Agreed. Will do.", "author": "chenjunjiedada", "createdAt": "2020-02-13T15:54:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc3NTY2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc3OTQ1MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378779450", "bodyText": "I think, this logging is not very useful. Other filters neither logs the dropped row groups. If you would like to have a more useful log, it might be a good idea to log the index of the row group as well and the filter type which triggered the drop.\nAlso, please use curly braces for any blocks and format accordingly.", "author": "gszadovszky", "createdAt": "2020-02-13T10:39:17Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/filter2/compat/RowGroupFilter.java", "diffHunk": "@@ -103,6 +108,11 @@ private RowGroupFilter(List<FilterLevel> levels, List<BlockMetaData> blocks, Par\n         drop = DictionaryFilter.canDrop(filterPredicate, block.getColumns(), reader.getDictionaryReader(block));\n       }\n \n+      if (!drop && levels.contains(FilterLevel.BLOOMFILTER)) {\n+        drop = BloomFilterImpl.canDrop(filterPredicate, block.getColumns(), reader.getBloomFilterDataReader(block));\n+        if (drop) logger.info(\"Block drop by Bloom filter\");", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk1MTEzNg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378951136", "bodyText": "Agreed to remove this.", "author": "chenjunjiedada", "createdAt": "2020-02-13T15:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc3OTQ1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc4MDA3Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378780073", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *", "author": "gszadovszky", "createdAt": "2020-02-13T10:40:26Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/BloomFilterReader.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.parquet.column.values.bloomfilter.BloomFilter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+\n+/**\n+ * Bloom filter reader that reads Bloom filter data from an open {@link ParquetFileReader}.\n+ *", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc4ODExMg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378788112", "bodyText": "An error during reading a bloom filter should not block the reading of the data. For example if the bloom filter is corrupt somehow the you throw a RuntimeException which would blocks the user to read the data. If it is a low level I/O problem then it will fail with an IOException at reading the data anyway.\nI would suggest logging the issue and return null instead of throwing an exception.", "author": "gszadovszky", "createdAt": "2020-02-13T10:54:51Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/BloomFilterReader.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.parquet.column.values.bloomfilter.BloomFilter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+\n+/**\n+ * Bloom filter reader that reads Bloom filter data from an open {@link ParquetFileReader}.\n+ *\n+ */\n+public class BloomFilterReader {\n+  private final ParquetFileReader reader;\n+  private final Map<ColumnPath, ColumnChunkMetaData> columns;\n+  private final Map<ColumnPath, BloomFilter> cache = new HashMap<>();\n+\n+  public BloomFilterReader(ParquetFileReader fileReader, BlockMetaData block) {\n+    this.reader = fileReader;\n+    this.columns = new HashMap<>();\n+    for (ColumnChunkMetaData column : block.getColumns()) {\n+      columns.put(column.getPath(), column);\n+    }\n+  }\n+\n+  public BloomFilter readBloomFilter(ColumnChunkMetaData meta) {\n+    if (cache.containsKey(meta.getPath())) {\n+      return cache.get(meta.getPath());\n+    }\n+    try {\n+      synchronized (cache) {\n+        if (!cache.containsKey(meta.getPath())) {\n+          BloomFilter bloomFilter = reader.readBloomFilter(meta);\n+          if (bloomFilter == null) return null;\n+          cache.put(meta.getPath(), bloomFilter);\n+        }\n+      }\n+      return cache.get(meta.getPath());\n+    } catch (IOException e) {\n+      throw new RuntimeException(", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk1NDA5Nw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378954097", "bodyText": "Make sense to return null.", "author": "chenjunjiedada", "createdAt": "2020-02-13T16:00:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc4ODExMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyMTA2OA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378821068", "bodyText": "Why do we need this synchronized? Parquet library is prepared for concurrent reading/writing anyway.", "author": "gszadovszky", "createdAt": "2020-02-13T12:07:42Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/BloomFilterReader.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.parquet.column.values.bloomfilter.BloomFilter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+\n+/**\n+ * Bloom filter reader that reads Bloom filter data from an open {@link ParquetFileReader}.\n+ *\n+ */\n+public class BloomFilterReader {\n+  private final ParquetFileReader reader;\n+  private final Map<ColumnPath, ColumnChunkMetaData> columns;\n+  private final Map<ColumnPath, BloomFilter> cache = new HashMap<>();\n+\n+  public BloomFilterReader(ParquetFileReader fileReader, BlockMetaData block) {\n+    this.reader = fileReader;\n+    this.columns = new HashMap<>();\n+    for (ColumnChunkMetaData column : block.getColumns()) {\n+      columns.put(column.getPath(), column);\n+    }\n+  }\n+\n+  public BloomFilter readBloomFilter(ColumnChunkMetaData meta) {\n+    if (cache.containsKey(meta.getPath())) {\n+      return cache.get(meta.getPath());\n+    }\n+    try {\n+      synchronized (cache) {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk2MDIyOQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378960229", "bodyText": "It will return if the cache has the bloom filter in memory, so it doesn't block concurrently reading. The synchronized  here ensures reading bloom filter just once when preforming concurrent predicating.", "author": "chenjunjiedada", "createdAt": "2020-02-13T16:09:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyMTA2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NjY2MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379296661", "bodyText": "parquet-mr is designed to be executed on one thread only when reading/filtering a file. When starting a filtering using a bloom filter a new BloomFilterReader object will be created. So, no synchronization is required.\nIf you think otherwise, please provide unit tests for concurrency.", "author": "gszadovszky", "createdAt": "2020-02-14T08:12:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyMTA2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4NTU1NA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380185554", "bodyText": "Hm, make sense. Thanks for the explanation.", "author": "chenjunjiedada", "createdAt": "2020-02-17T13:37:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyMTA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyNDQ4Nw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378824487", "bodyText": "Seems to be a regression in the merge: the checksum flag is missing.", "author": "gszadovszky", "createdAt": "2020-02-13T12:15:49Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordWriter.java", "diffHunk": "@@ -101,9 +103,12 @@ public ParquetMetadata getFooter() {\n   }\n \n   private void initStore() {\n-    pageStore = new ColumnChunkPageWriteStore(compressor, schema, props.getAllocator(),\n-        props.getColumnIndexTruncateLength(), props.getPageWriteChecksumEnabled());\n-    columnStore = props.newColumnWriteStore(schema, pageStore);\n+    ColumnChunkPageWriteStore columnChunkPageWriteStore = new ColumnChunkPageWriteStore(compressor,\n+        schema, props.getAllocator(), props.getColumnIndexTruncateLength());", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk2MDYxMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378960611", "bodyText": "Will check again. Thanks to point this out.", "author": "chenjunjiedada", "createdAt": "2020-02-13T16:09:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyNDQ4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyODIyMg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378828222", "bodyText": "This is not how we should do the parsing. We have the corresponding thrift structures generated (see org.apache.parquet.format.BloomFilterHeader). Let thrift do the parsing.\nSee the implementation of column index reading for details.", "author": "gszadovszky", "createdAt": "2020-02-13T12:24:36Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1050,6 +1054,51 @@ private DictionaryPage readCompressedDictionary(\n         converter.getEncoding(dictHeader.getEncoding()));\n   }\n \n+  public BloomFilterReader getBloomFilterDataReader(BlockMetaData block) {\n+    return new BloomFilterReader(this, block);\n+  }\n+\n+  /**\n+   * Reads Bloom filter data for the given column chunk.\n+   *\n+   * @param meta a column's ColumnChunkMetaData to read the dictionary from\n+   * @return an BloomFilter object.\n+   * @throws IOException if there is an error while reading the Bloom filter.\n+   */\n+  public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException {\n+    long bloomFilterOffset = meta.getBloomFilterOffset();\n+    f.seek(bloomFilterOffset);\n+\n+    // Read Bloom filter data header.", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzMDkyNQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378830925", "bodyText": "I think the name of this method is misleading. This class implements the file writing while this method does not actually write the bloom filter but adds to the list of bloom filters to be written when the file is being closed.\nAlso, why do you need this method? If it is for testing only, please document it here.", "author": "gszadovszky", "createdAt": "2020-02-13T12:30:50Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -574,6 +584,14 @@ private void innerWriteDataPage(\n     currentEncodings.add(valuesEncoding);\n   }\n \n+  /**\n+   * Write a Bloom filter\n+   * @param bloomFilter the bloom filter of column values\n+   */\n+  void writeBloomFilter(BloomFilter bloomFilter)  {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkwMjUyMw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379902523", "bodyText": "OK", "author": "chenjunjiedada", "createdAt": "2020-02-16T13:15:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzMDkyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzMjc1Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378832753", "bodyText": "Why else? I don't think bloom filter has any relation to the dictionary page.\nAlso, you are about adding a bloom filter only if it is not null. It would mix up the indexing so you won't write the correct bloom filter to the related column.", "author": "gszadovszky", "createdAt": "2020-02-13T12:35:05Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -674,6 +694,8 @@ void writeColumnChunk(ColumnDescriptor descriptor,\n     state = state.write();\n     if (dictionaryPage != null) {\n       writeDictionaryPage(dictionaryPage);\n+    }  else if (bloomFilter != null) {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkwMjQ5Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379902493", "bodyText": "My original thought is if the data is encoded in the dictionary, then there is no need to build the bloom filter because the dictionary filter can do the filtering job. But you are right that it will mix up the indexing.  Will use a separated sentence.", "author": "chenjunjiedada", "createdAt": "2020-02-16T13:15:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzMjc1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzNTkzNA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378835934", "bodyText": "We shall not write the header this way. That's why we have thrift. (See my other comment at bloom filter reading.)\nRead/write shall be done by converting our internal objects to the related thrift objects and make them serialized by thrift. This conversion is done at ParquetMetadataConverter which is in parquet-hadoop. parquet-hadoop depends on parquet-column and not the other way around. So, you are not really able to implement the serialization here anyway.", "author": "gszadovszky", "createdAt": "2020-02-13T12:41:55Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // Default minimum Bloom filter size, set to the size of a tiny Bloom filter block\n+  public static final int DEFAULT_MINIMUM_BYTES = 32;\n+\n+  // Default Maximum Bloom filter size, set to 1MB which should cover most cases.\n+  public static final int DEFAULT_MAXIMUM_BYTES = 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // Hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = DEFAULT_MAXIMUM_BYTES;\n+  private int minimumBytes = DEFAULT_MINIMUM_BYTES;\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, DEFAULT_MAXIMUM_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, DEFAULT_MAXIMUM_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (maximumBytes > DEFAULT_MINIMUM_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+    initBitset(numBytes);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkwMjU3Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379902573", "bodyText": "Make sense. Will change to read/write bloom filter header via thrift.", "author": "chenjunjiedada", "createdAt": "2020-02-16T13:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzNTkzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg0MDg2MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378840861", "bodyText": "Does it make sense without NDVs?", "author": "gszadovszky", "createdAt": "2020-02-13T12:52:39Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -543,6 +545,21 @@ public SELF withPageWriteChecksumEnabled(boolean enablePageWriteChecksum) {\n       return self();\n     }\n \n+    /**\n+     * Enables bloom filter column names for the constructed writer.\n+     *\n+     * @return this builder for method chaining.\n+     */\n+    public SELF withBloomFilterColumnNames(String... columnNames) {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk2NDkxNw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378964917", "bodyText": "If no NDVs is specified, it creates the bloom filter with specified maximum size(or default maximum size).", "author": "chenjunjiedada", "createdAt": "2020-02-13T16:16:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg0MDg2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5ODgyMg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379298822", "bodyText": "Thanks for the explanation.", "author": "gszadovszky", "createdAt": "2020-02-14T08:18:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg0MDg2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg0NDc0MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378844740", "bodyText": "Who adds FilterLevel.BLOOMFILTER to levels? It seems for me that in the current way bloom filters are never used at filtering.", "author": "gszadovszky", "createdAt": "2020-02-13T13:00:48Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/filter2/compat/RowGroupFilter.java", "diffHunk": "@@ -103,6 +108,11 @@ private RowGroupFilter(List<FilterLevel> levels, List<BlockMetaData> blocks, Par\n         drop = DictionaryFilter.canDrop(filterPredicate, block.getColumns(), reader.getDictionaryReader(block));\n       }\n \n+      if (!drop && levels.contains(FilterLevel.BLOOMFILTER)) {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk2NjQ3Ng==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378966476", "bodyText": "The upstream compute engine can add it through filterRowGroups.", "author": "chenjunjiedada", "createdAt": "2020-02-13T16:19:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg0NDc0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNjQyMw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379306423", "bodyText": "RowGroupFilter.filterRowGroups is invoked by ParquetFileReader.filterRowGroups. That's where filtering configuration runs down to the code via ParquetReadOptions. I would suggest following the path of the dictionary filtering from the config key parquet.filter.dictionary.enabled.", "author": "gszadovszky", "createdAt": "2020-02-14T08:39:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg0NDc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg1MzQ4NQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378853485", "bodyText": "I am not sure if anything stands in the way of supporting nested columns but this very statement. Filtering is supported for primitive types (leaf nodes in the schema tree) even if they are nested. From parquet filtering point of view the only difference is there are '.' characters in the column name.", "author": "gszadovszky", "createdAt": "2020-02-13T13:19:03Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -66,6 +74,42 @@\n     this.dataColumn = props.newValuesWriter(path);\n   }\n \n+  ColumnWriterBase(\n+    ColumnDescriptor path,\n+    PageWriter pageWriter,\n+    BloomFilterWriter bloomFilterWriter,\n+    ParquetProperties props\n+  ) {\n+    this(path, pageWriter, props);\n+\n+    // Bloom filters don't support nested columns yet; see PARQUET-1453.\n+    if (path.getPath().length != 1 || bloomFilterWriter == null) {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkwMjY0MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379902641", "bodyText": "Make sense, let me implement this and add some tests.", "author": "chenjunjiedada", "createdAt": "2020-02-16T13:17:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg1MzQ4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg2MjAyNQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r378862025", "bodyText": "Seems that it never happens because the related map is initialized.", "author": "gszadovszky", "createdAt": "2020-02-13T13:35:40Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriteStoreBase.java", "diffHunk": "@@ -105,7 +107,38 @@ public ColumnWriter getColumnWriter(ColumnDescriptor path) {\n     };\n   }\n \n-  abstract ColumnWriterBase createColumnWriter(ColumnDescriptor path, PageWriter pageWriter, ParquetProperties props);\n+  // The Bloom filter is written to a specified bitset instead of pages, so it needs a separate write store abstract.\n+  ColumnWriteStoreBase(\n+    MessageType schema,\n+    PageWriteStore pageWriteStore,\n+    BloomFilterWriteStore bloomFilterWriteStore,\n+    ParquetProperties props) {\n+    this.props = props;\n+    this.thresholdTolerance = (long) (props.getPageSizeThreshold() * THRESHOLD_TOLERANCE_RATIO);\n+    Map<ColumnDescriptor, ColumnWriterBase> mcolumns = new TreeMap<>();\n+    for (ColumnDescriptor path : schema.getColumns()) {\n+      PageWriter pageWriter = pageWriteStore.getPageWriter(path);\n+      if (props.getBloomFilterColumnExpectedNDVs() != null) {", "originalCommit": "6260ca49b315e13e8732fb4e4633b2ed1258ac35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkwMzIxMw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r379903213", "bodyText": "You are right, I will change this by checking specified column names instead since we may not specify NDVs but use the default maximum bloom filter size.", "author": "chenjunjiedada", "createdAt": "2020-02-16T13:26:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg2MjAyNQ=="}], "type": "inlineReview"}, {"oid": "a67480d37e19aa81539df5af9d3b98fc6f82fa9f", "url": "https://github.com/apache/parquet-mr/commit/a67480d37e19aa81539df5af9d3b98fc6f82fa9f", "message": "address comments first round", "committedDate": "2020-02-16T16:37:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDA2NzIyMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380067221", "bodyText": "Here, you are using DEFAULT_MAXIMUM_BYTES as an un-configurable hard limit (as well as DEFAULT_MINIMUM_BYTES). Meanwhile, you have the configurable fields minimumBytes and maximumBytes.\nSometimes you used the DEFAULT_* constants as defaults, sometimes they are hard limits even here or for configurating the *Bytes fields. It shall be more clear which value is a default, which is a hard limit and which is configurable.", "author": "gszadovszky", "createdAt": "2020-02-17T09:27:44Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // Default minimum Bloom filter size, set to the size of a tiny Bloom filter block\n+  public static final int DEFAULT_MINIMUM_BYTES = 32;\n+\n+  // Default Maximum Bloom filter size, set to 1MB which should cover most cases.\n+  public static final int DEFAULT_MAXIMUM_BYTES = 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // Hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = DEFAULT_MAXIMUM_BYTES;\n+  private int minimumBytes = DEFAULT_MINIMUM_BYTES;\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, DEFAULT_MAXIMUM_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, DEFAULT_MAXIMUM_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (maximumBytes > DEFAULT_MINIMUM_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+    initBitset(numBytes);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = key * SALT[i];\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = mask[i] >>> 27;\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = 0x1 << mask[i];\n+    }\n+\n+    return mask;\n+  }\n+\n+  @Override\n+  public void insertHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for bucket.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      int value = intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i);\n+      value |= mask[i];\n+      intBuffer.put(blockIndex * (BYTES_PER_BLOCK / 4) + i, value);\n+    }\n+  }\n+\n+  @Override\n+  public boolean findHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for the tiny Bloom filter.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      if (0 == (intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i) & mask[i])) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Calculate optimal size according to the number of distinct values and false positive probability.\n+   *\n+   * @param n: The number of distinct values.\n+   * @param p: The false positive probability.\n+   * @return optimal number of bits of given n and p.\n+   */\n+  public static int optimalNumOfBits(long n, double p) {\n+    Preconditions.checkArgument((p > 0.0 && p < 1.0),\n+      \"FPP should be less than 1.0 and great than 0.0\");\n+    final double m = -8 * n / Math.log(1 - Math.pow(p, 1.0 / 8));\n+    final double MAX = DEFAULT_MAXIMUM_BYTES << 3;", "originalCommit": "a67480d37e19aa81539df5af9d3b98fc6f82fa9f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDIwMTI2Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380201263", "bodyText": "Agreed, will clean this.", "author": "chenjunjiedada", "createdAt": "2020-02-17T14:07:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDA2NzIyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDEwMDYyMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380100621", "bodyText": "This method is used when we want to write a constructed bloom filter to the file. If anything is missing or invalid than it is a bug so, I would throw an exception here instead of simply returning null.", "author": "gszadovszky", "createdAt": "2020-02-17T10:29:33Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1613,4 +1620,30 @@ public static OffsetIndex toParquetOffsetIndex(org.apache.parquet.internal.colum\n     }\n     return builder.build();\n   }\n+\n+  public static BloomFilterHeader toBloomFilterHeader(\n+    org.apache.parquet.column.values.bloomfilter.BloomFilter bloomFilter) {\n+\n+    BloomFilterAlgorithm algorithm = null;\n+    BloomFilterHash hashStrategy = null;\n+    BloomFilterCompression compression = null;\n+\n+    if (bloomFilter.getAlgorithm() == BloomFilter.Algorithm.BLOCK) {\n+      algorithm = BloomFilterAlgorithm.BLOCK(new SplitBlockAlgorithm());\n+    }\n+\n+    if (bloomFilter.getHashStrategy() == BloomFilter.HashStrategy.XXH64) {\n+      hashStrategy = BloomFilterHash.XXHASH(new XxHash());\n+    }\n+\n+    if (bloomFilter.getCompression() == BloomFilter.Compression.UNCOMPRESSED) {\n+      compression = BloomFilterCompression.UNCOMPRESSED(new Uncompressed());\n+    }\n+\n+    if (algorithm != null && hashStrategy != null && compression != null) {\n+      return new BloomFilterHeader(bloomFilter.getBitsetSize(), algorithm, hashStrategy, compression);\n+    } else {\n+      return null;", "originalCommit": "a67480d37e19aa81539df5af9d3b98fc6f82fa9f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDIwMTQzMA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380201430", "bodyText": "Agreed.", "author": "chenjunjiedada", "createdAt": "2020-02-17T14:07:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDEwMDYyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDEwNjQ5NQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380106495", "bodyText": "It is good that you return null in case of the bloom filter is not compatible with the one supported by this implementation. This way this is forward compatible with format changes that may add other bloom filter related options.\nHowever, one would have no clue why a newer bloom filter is not used. I would suggest adding warn logs with the actual problem and the bloomFilterHeader itself. (Thrift generates proper toString implementation for the objects.)", "author": "gszadovszky", "createdAt": "2020-02-17T10:41:05Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1070,27 +1069,14 @@ public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException\n     f.seek(bloomFilterOffset);\n \n     // Read Bloom filter data header.\n-    byte[] bytes = new byte[BlockSplitBloomFilter.HEADER_SIZE];\n-    f.read(bytes);\n-    ByteBuffer bloomHeader = ByteBuffer.wrap(bytes);\n-    IntBuffer headerBuffer = bloomHeader.order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n-    int numBytes = headerBuffer.get();\n-    if (numBytes <= 0 || numBytes > BlockSplitBloomFilter.DEFAULT_MAXIMUM_BYTES) {\n+    BloomFilterHeader bloomFilterHeader = Util.readBloomFilterHeader(f);\n+    int numBytes = bloomFilterHeader.getNumBytes();\n+    if ( numBytes <= 0 || numBytes > BlockSplitBloomFilter.DEFAULT_MAXIMUM_BYTES) {", "originalCommit": "a67480d37e19aa81539df5af9d3b98fc6f82fa9f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDIwMTc2MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380201760", "bodyText": "Make sense, will do.", "author": "chenjunjiedada", "createdAt": "2020-02-17T14:08:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDEwNjQ5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE1OTg0MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r380159841", "bodyText": "It still mixes up the indexing. In serializeBloomFilters you are checking if the bloom filter for the column index is null but this way it will never be null.\nA comment for the previous discussion pointed here. (Somehow I am not able to add comments the previous discussions anymore.) It can happen that you have a dictionary page but not all the pages are dictionary encoded because the dictionary reaches the maximum limit. I this case adding a bloom filter makes sense. If you don't want to write bloom filters for column chunks where all the values are dictionary encoded you should check dataEncodings instead.", "author": "gszadovszky", "createdAt": "2020-02-17T12:42:06Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -675,6 +695,10 @@ void writeColumnChunk(ColumnDescriptor descriptor,\n     if (dictionaryPage != null) {\n       writeDictionaryPage(dictionaryPage);\n     }\n+\n+    if (bloomFilter != null) {", "originalCommit": "a67480d37e19aa81539df5af9d3b98fc6f82fa9f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "28216ce060103b6071406c73082578e14863d282", "url": "https://github.com/apache/parquet-mr/commit/28216ce060103b6071406c73082578e14863d282", "message": "address comments second round", "committedDate": "2020-02-17T13:51:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI1Mzk3Ng==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381253976", "bodyText": "Why don't we enable it by default? I think, if a bloom filter is written, we should use it. In addition, in HadoopReadOptions it is enabled by default.", "author": "gszadovszky", "createdAt": "2020-02-19T12:15:05Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/ParquetReadOptions.java", "diffHunk": "@@ -42,13 +42,15 @@\n   private static final boolean COLUMN_INDEX_FILTERING_ENABLED_DEFAULT = true;\n   private static final int ALLOCATION_SIZE_DEFAULT = 8388608; // 8MB\n   private static final boolean PAGE_VERIFY_CHECKSUM_ENABLED_DEFAULT = false;\n+  private static final boolean BLOOM_FILTER_ENABLED_DEFAULT = false;", "originalCommit": "28216ce060103b6071406c73082578e14863d282", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI3Nzg4OA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381277888", "bodyText": "Thanks for pointing this out, I will go through all these configurations and clean them up.", "author": "chenjunjiedada", "createdAt": "2020-02-19T13:07:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI1Mzk3Ng=="}], "type": "inlineReview"}, {"oid": "1519967d5d79c616fc70adcb1ad2465d8d313555", "url": "https://github.com/apache/parquet-mr/commit/1519967d5d79c616fc70adcb1ad2465d8d313555", "message": "address comments, add unit tests, shade jar", "committedDate": "2020-02-20T09:01:06Z", "type": "commit"}, {"oid": "1519967d5d79c616fc70adcb1ad2465d8d313555", "url": "https://github.com/apache/parquet-mr/commit/1519967d5d79c616fc70adcb1ad2465d8d313555", "message": "address comments, add unit tests, shade jar", "committedDate": "2020-02-20T09:01:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkwNTEyNg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381905126", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            import java.util.LinkedList;\n          \n      \n    \n    \n  \n\nDoes not seem to be used.", "author": "gszadovszky", "createdAt": "2020-02-20T10:16:24Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/ParquetProperties.java", "diffHunk": "@@ -18,8 +18,11 @@\n  */\n package org.apache.parquet.column;\n \n+import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkxMTQ0MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381911441", "bodyText": "Here you want to match the NDVs and the list of columns marked to write bloom filters for. But this is not that simple. I can see the following strategies to do that:\n\nUse the union of the NDV keys and the list of column names.\nMatch the NDV keys and the list of the column names and throw and exception if there is a mismatch.\nImplement the builder in a way that it ensures that no mismatch can happen. E.g. adding NDVs or the column names one by one and manage only the NDV map by adding the default NDV to the columns where only name was set.\n\n(If PARQUET-1784 gets in I would like to refactor the using facing configuration of bloom filters according to that if you agree.)", "author": "gszadovszky", "createdAt": "2020-02-20T10:27:42Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/ParquetProperties.java", "diffHunk": "@@ -261,7 +262,11 @@ public boolean getPageWriteChecksumEnabled() {\n   }\n \n   public Set<String> getBloomFilterColumns() {\n-    return bloomFilterColumns;\n+    if (bloomFilterColumns != null && bloomFilterColumns.size() > 0){\n+      return new HashSet<>(bloomFilterColumns);\n+    }\n+\n+    return bloomFilterExpectedDistinctNumbers.keySet();", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjA4MzIwNQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382083205", "bodyText": "There are two use cases I thought,\n\n\nuser doesn't know the NDV, he or she just want to enable bloom filtering for the column, so I assume only column names would be specified, maybe maximum byte as well.\n\n\nuser knows the NDV, he or she is able to specify both column names and NDVs, and maybe maximum bytes as well\n\n\nCurrently, I think the implementation could cover the correct usage, but it cannot handle some error usage even we get result map, for example, some may pass in the wrong ordering values. I think the best way is the solution from PARQUET-1784. So +1 to you.", "author": "chenjunjiedada", "createdAt": "2020-02-20T15:43:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkxMTQ0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjA4NTc4Mg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382085782", "bodyText": "Fine for me.", "author": "gszadovszky", "createdAt": "2020-02-20T15:47:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkxMTQ0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkxNjY2Ng==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381916666", "bodyText": "You are not checking whether minimumBytes < maximumBytes.", "author": "gszadovszky", "createdAt": "2020-02-20T10:37:30Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -111,22 +111,28 @@ public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n    * @param hashStrategy The hash strategy of Bloom filter.\n    */\n   private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n-    this(numBytes, DEFAULT_MAXIMUM_BYTES, hashStrategy);\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n   }\n \n   /**\n    * Constructor of block-based Bloom filter.\n    *\n    * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n-   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n    *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n    * @param maximumBytes The maximum bytes of the Bloom filter.\n    * @param hashStrategy The adopted hash strategy of the Bloom filter.\n    */\n-  public BlockSplitBloomFilter(int numBytes, int maximumBytes, HashStrategy hashStrategy) {\n-    if (maximumBytes > DEFAULT_MINIMUM_BYTES) {\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyMTY0MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381921641", "bodyText": "You have the absolute interval of [32B, 20MB] then, you choose the default interval to be the same as the absolute one. So, the user is not able to extend the default interval only to narrow. Is it intentional?\nAlso, you say that 20MB (...) should cover most real cases. What if the current case is a special one? Then, the user is not able to go above 20MB.\n32B seems to be a fair absolute and default minimum but for maximum I would suggest having a practical value which is suitable for the most cases as a default and an absolute maximum that we can handle.", "author": "gszadovszky", "createdAt": "2020-02-20T10:46:58Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -43,11 +42,12 @@\n   // Bits in a tiny Bloom filter block.\n   private static final int BITS_PER_BLOCK = 256;\n \n-  // Default minimum Bloom filter size, set to the size of a tiny Bloom filter block\n-  public static final int DEFAULT_MINIMUM_BYTES = 32;\n+  // the lower bound of Bloom filter size, set to the size of a tiny Bloom filter block", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjA5MzgzNg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382093836", "bodyText": "Yes, It is intentional. For the maximum value 20MB, it is the size for a row group of 128 MB with only one long type column. Since we have a default maximum value, so I agree to increase the upper bound to a larger value, how about 256MB?\nFor the minimum value, I think the bloom filter maybe use for other cases in the future other than just row group filter, so set it to a minimum value that could work. I 'm OK to set to a larger value, how about 1k?", "author": "chenjunjiedada", "createdAt": "2020-02-20T15:58:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyMTY0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjQ0NzM5OQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382447399", "bodyText": "I am not an expert on bloom filters and I think defining good defaults is not easy. However, this is about to define a range not an exact value so it does not seem to be that though.\nMy concern was only the fact that the hard limits are the same as the defaults which is strange. If it is intentional that we allow the user to only narrow the range then it is fine however, the reason should be documented. Otherwise, the hard limits should be because of technical limitations (e.g. memory/time limits, meaningless values like -1) and default limits should be practical values for the most cases.", "author": "gszadovszky", "createdAt": "2020-02-21T08:15:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyMTY0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjkyMzM0Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382923343", "bodyText": "The default maximum is 1MB,  and the upper bound is 20MB.  You are right about the upper bound, I think I should set upper bound to row group size since a row group bloom filter should not larger than the row group itself. Does that make sense to you?", "author": "chenjunjiedada", "createdAt": "2020-02-22T16:12:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyMTY0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk4MzU2NQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381983565", "bodyText": "It shall be the responsibility of the caller to prepare the buffer. A ByteBuffer can be used in several ways and it is the caller who shall know from which part it needs a hash from.", "author": "gszadovszky", "createdAt": "2020-02-20T13:01:37Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/XxHash.java", "diffHunk": "@@ -35,6 +35,7 @@ public long hashBytes(byte[] input) {\n \n   @Override\n   public long hashByteBuffer(ByteBuffer input) {\n+    input.flip();", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk4NTAwMA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r381985000", "bodyText": "I would call the related method instead. (E.g. call hash(int) for Integer).", "author": "gszadovszky", "createdAt": "2020-02-20T13:04:36Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,372 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // the lower bound of Bloom filter size, set to the size of a tiny Bloom filter block\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // the upper bound of Bloom filter size, it sets to 20MB which is enough for a row group of 128MB\n+  // with only one column of long type which should covers most real cases.\n+  public static final int UPPER_BOUND_BYTES = 1024 * 1024 * 20;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = key * SALT[i];\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = mask[i] >>> 27;\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = 0x1 << mask[i];\n+    }\n+\n+    return mask;\n+  }\n+\n+  @Override\n+  public void insertHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for bucket.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      int value = intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i);\n+      value |= mask[i];\n+      intBuffer.put(blockIndex * (BYTES_PER_BLOCK / 4) + i, value);\n+    }\n+  }\n+\n+  @Override\n+  public boolean findHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for the tiny Bloom filter.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      if (0 == (intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i) & mask[i])) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Calculate optimal size according to the number of distinct values and false positive probability.\n+   *\n+   * @param n: The number of distinct values.\n+   * @param p: The false positive probability.\n+   *\n+   * @return optimal number of bits of given n and p.\n+   */\n+  public static int optimalNumOfBits(long n, double p) {\n+    Preconditions.checkArgument((p > 0.0 && p < 1.0),\n+      \"FPP should be less than 1.0 and great than 0.0\");\n+    final double m = -8 * n / Math.log(1 - Math.pow(p, 1.0 / 8));\n+    final double MAX = UPPER_BOUND_BYTES << 3;\n+    int numBits = (int)m;\n+\n+    // Handle overflow.\n+    if (m > MAX || m < 0) {\n+      numBits = (int)MAX;\n+    }\n+\n+    // Round to BITS_PER_BLOCK\n+    numBits = (numBits + BITS_PER_BLOCK -1) & ~BITS_PER_BLOCK;\n+\n+    if (numBits < (LOWER_BOUND_BYTES << 3)) {\n+      numBits = LOWER_BOUND_BYTES << 3;\n+    }\n+\n+    return numBits;\n+  }\n+\n+  @Override\n+  public int getBitsetSize() {\n+    return this.bitset.length;\n+  }\n+\n+  @Override\n+  public long hash(Object value) {\n+    ByteBuffer plain;\n+\n+    if (value instanceof Binary) {\n+      return hashFunction.hashBytes(((Binary) value).getBytes());\n+    }\n+\n+    if (value instanceof Integer) {", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAwMDc2NA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382000764", "bodyText": "You may speed up the hashing by having a ByteBuffer cache created at class level. You may create a 64bit long buffer and set the ordering once. Then, you can clear the buffer before putting a value in it then flip before invoking hashByteBuffer.\nIn the other hand I can see the functions hashLong and hashInt in LongHashFunction. Maybe it worth to update the HashFunction interface so you can call these directly. I am not sure which options is the best or which one performs better.", "author": "gszadovszky", "createdAt": "2020-02-20T13:35:26Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,372 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // the lower bound of Bloom filter size, set to the size of a tiny Bloom filter block\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // the upper bound of Bloom filter size, it sets to 20MB which is enough for a row group of 128MB\n+  // with only one column of long type which should covers most real cases.\n+  public static final int UPPER_BOUND_BYTES = 1024 * 1024 * 20;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = key * SALT[i];\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = mask[i] >>> 27;\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = 0x1 << mask[i];\n+    }\n+\n+    return mask;\n+  }\n+\n+  @Override\n+  public void insertHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for bucket.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      int value = intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i);\n+      value |= mask[i];\n+      intBuffer.put(blockIndex * (BYTES_PER_BLOCK / 4) + i, value);\n+    }\n+  }\n+\n+  @Override\n+  public boolean findHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for the tiny Bloom filter.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      if (0 == (intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i) & mask[i])) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Calculate optimal size according to the number of distinct values and false positive probability.\n+   *\n+   * @param n: The number of distinct values.\n+   * @param p: The false positive probability.\n+   *\n+   * @return optimal number of bits of given n and p.\n+   */\n+  public static int optimalNumOfBits(long n, double p) {\n+    Preconditions.checkArgument((p > 0.0 && p < 1.0),\n+      \"FPP should be less than 1.0 and great than 0.0\");\n+    final double m = -8 * n / Math.log(1 - Math.pow(p, 1.0 / 8));\n+    final double MAX = UPPER_BOUND_BYTES << 3;\n+    int numBits = (int)m;\n+\n+    // Handle overflow.\n+    if (m > MAX || m < 0) {\n+      numBits = (int)MAX;\n+    }\n+\n+    // Round to BITS_PER_BLOCK\n+    numBits = (numBits + BITS_PER_BLOCK -1) & ~BITS_PER_BLOCK;\n+\n+    if (numBits < (LOWER_BOUND_BYTES << 3)) {\n+      numBits = LOWER_BOUND_BYTES << 3;\n+    }\n+\n+    return numBits;\n+  }\n+\n+  @Override\n+  public int getBitsetSize() {\n+    return this.bitset.length;\n+  }\n+\n+  @Override\n+  public long hash(Object value) {\n+    ByteBuffer plain;\n+\n+    if (value instanceof Binary) {\n+      return hashFunction.hashBytes(((Binary) value).getBytes());\n+    }\n+\n+    if (value instanceof Integer) {\n+      plain = ByteBuffer.allocate(Integer.SIZE/Byte.SIZE);\n+      plain.order(ByteOrder.LITTLE_ENDIAN).putInt(((Integer)value));\n+    } else if (value instanceof Long) {\n+      plain = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+      plain.order(ByteOrder.LITTLE_ENDIAN).putLong(((Long)value));\n+    } else if (value instanceof Float) {\n+      plain = ByteBuffer.allocate(Float.SIZE/Byte.SIZE);\n+      plain.order(ByteOrder.LITTLE_ENDIAN).putFloat(((Float)value));\n+    } else if (value instanceof Double) {\n+      plain = ByteBuffer.allocate(Double.SIZE/ Byte.SIZE);\n+      plain.order(ByteOrder.LITTLE_ENDIAN).putDouble(((Double)value));\n+    } else {\n+      throw new RuntimeException(\"Parquet Bloom filter: Not supported type\");\n+    }\n+\n+    return hashFunction.hashByteBuffer(plain);\n+  }\n+\n+  @Override\n+  public HashStrategy getHashStrategy() {\n+    return HashStrategy.XXH64;\n+  }\n+\n+  @Override\n+  public Algorithm getAlgorithm() {\n+    return Algorithm.BLOCK;\n+  }\n+\n+  @Override\n+  public Compression getCompression() {\n+    return Compression.UNCOMPRESSED;\n+  }\n+\n+  @Override\n+  public long hash(int value) {\n+    ByteBuffer plain = ByteBuffer.allocate(Integer.SIZE/Byte.SIZE);", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjExNzkyMw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382117923", "bodyText": "I agree to use a class level ByteBuffer to speed up.  The reason for using hashByteBuffer is to avoid endianness issue, though LongHashFunction seems to use little-endian by default. Let me check the xxhash implementation again. I will put comments in the code to address this.", "author": "chenjunjiedada", "createdAt": "2020-02-20T16:36:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAwMDc2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAwNzM0Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382007343", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                numBits = -8 * ndv / Math.log(1 - Math.pow(fpp, 1.0 / 8));\n          \n          \n            \n                numBits = 8 * ndv / Math.log(1 - Math.pow(fpp, 1.0 / 8));", "author": "gszadovszky", "createdAt": "2020-02-20T13:46:48Z", "path": "parquet-column/src/test/java/org/apache/parquet/column/values/bloomfilter/TestBlockSplitBloomFilter.java", "diffHunk": "@@ -54,73 +52,74 @@ public void testConstructor () {\n    * serializing and de-serializing.\n    */\n   @Test\n-  public void testBasic() throws IOException {\n+  public void testBloomFilterBasicReadWrite() throws IOException {\n     final String[] testStrings = {\"hello\", \"parquet\", \"bloom\", \"filter\"};\n     BloomFilter bloomFilter = new BlockSplitBloomFilter(1024);\n \n-    for(int i = 0; i < testStrings.length; i++) {\n-      bloomFilter.insertHash(bloomFilter.hash(Binary.fromString(testStrings[i])));\n+    for (String string : testStrings) {\n+      bloomFilter.insertHash(bloomFilter.hash(Binary.fromString(string)));\n     }\n \n     File testFile = temp.newFile();\n     FileOutputStream fileOutputStream = new FileOutputStream(testFile);\n-    fileOutputStream.write(BytesUtils.intToBytes(bloomFilter.getBitsetSize()));\n-    fileOutputStream.write(BytesUtils.intToBytes(bloomFilter.getAlgorithm().value));\n-    fileOutputStream.write(BytesUtils.intToBytes(bloomFilter.getHashStrategy().value));\n-    fileOutputStream.write(BytesUtils.intToBytes(bloomFilter.getCompression().value));\n     bloomFilter.writeTo(fileOutputStream);\n     fileOutputStream.close();\n     FileInputStream fileInputStream = new FileInputStream(testFile);\n \n-    byte[] value = new byte[4];\n-    fileInputStream.read(value);\n-    int length = ByteBuffer.wrap(value).order(ByteOrder.LITTLE_ENDIAN).getInt();\n-    assertEquals(length, 1024);\n-\n-    fileInputStream.read(value);\n-    int hash = ByteBuffer.wrap(value).order(ByteOrder.LITTLE_ENDIAN).getInt();\n-    assertEquals(hash, BloomFilter.HashStrategy.XXH64.ordinal());\n-\n-    fileInputStream.read(value);\n-    int algorithm = ByteBuffer.wrap(value).order(ByteOrder.LITTLE_ENDIAN).getInt();\n-    assertEquals(algorithm, BloomFilter.Algorithm.BLOCK.ordinal());\n-\n-    fileInputStream.read(value);\n-    int compression = ByteBuffer.wrap(value).order(ByteOrder.LITTLE_ENDIAN).getInt();\n-    assertEquals(compression, BloomFilter.Compression.UNCOMPRESSED.ordinal());\n-\n-    byte[] bitset = new byte[length];\n-    fileInputStream.read(bitset);\n+    byte[] bitset = new byte[1024];\n+    int bytes = fileInputStream.read(bitset);\n+    assertEquals(bytes, 1024);\n     bloomFilter = new BlockSplitBloomFilter(bitset);\n     for (String testString : testStrings) {\n       assertTrue(bloomFilter.findHash(bloomFilter.hash(Binary.fromString(testString))));\n     }\n   }\n \n   @Test\n-  public void testFPP() throws IOException {\n+  public void testBloomFilterFPPAccuracy() {\n     final int totalCount = 100000;\n     final double FPP = 0.01;\n-    final long SEED = 104729;\n \n-    BloomFilter bloomFilter = new BlockSplitBloomFilter(BlockSplitBloomFilter.optimalNumOfBits(totalCount, FPP));\n-    List<String> strings = new ArrayList<>();\n-    for(int i = 0; i < totalCount; i++) {\n-      String str = RandomStringUtils.randomAlphabetic(10);\n-      strings.add(str);\n-      bloomFilter.insertHash(bloomFilter.hash(Binary.fromString(str)));\n+    BloomFilter bloomFilter = new BlockSplitBloomFilter(BlockSplitBloomFilter.optimalNumOfBits(totalCount, FPP) / 8);\n+\n+    Set<String> distinctStrings = new HashSet<>();\n+    while (distinctStrings.size() < totalCount) {\n+      String str = RandomStringUtils.randomAlphabetic(12);\n+      if (distinctStrings.add(str)) {\n+        bloomFilter.insertHash(bloomFilter.hash(Binary.fromString(str)));\n+      }\n     }\n \n+    distinctStrings.clear();\n     // The exist counts the number of times FindHash returns true.\n     int exist = 0;\n-    for (int i = 0; i < totalCount; i++) {\n-      String str = RandomStringUtils.randomAlphabetic(8);\n-      if (bloomFilter.findHash(bloomFilter.hash(Binary.fromString(str)))) {\n+    while(distinctStrings.size() < totalCount) {\n+      String str = RandomStringUtils.randomAlphabetic(10);\n+      if (distinctStrings.add(str) && bloomFilter.findHash(bloomFilter.hash(Binary.fromString(str)))) {\n         exist ++;\n       }\n     }\n \n-    // The exist should be probably less than 1000 according FPP 0.01.\n-    assertTrue(exist < totalCount * FPP);\n+    // The exist should be probably less than 1000 according FPP 0.01. Add 10% here for error space.\n+    assertTrue(exist < totalCount * (FPP * 1.1));\n   }\n+\n+  @Test\n+  public void testBloomFilterNDVs(){\n+    // a row group of 128M with one column of long type.\n+    int ndv = 128 * 1024 * 1024 / 8;\n+    double fpp = 0.01;\n+\n+    // the optimal value formula\n+    double numBits = -8 * ndv / Math.log(1 - Math.pow(0.01, 1.0 / 8));\n+    int bytes = (int)numBits / 8;\n+    assertTrue(bytes < BlockSplitBloomFilter.UPPER_BOUND_BYTES);\n+\n+    // a row group of 128MB with one column of UUID type\n+    ndv = 128 * 1024 * 1024 / java.util.UUID.randomUUID().toString().length();\n+    numBits = -8 * ndv / Math.log(1 - Math.pow(fpp, 1.0 / 8));", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEwNzcxMw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382107713", "bodyText": "I think this is not a typo.", "author": "chenjunjiedada", "createdAt": "2020-02-20T16:21:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAwNzM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjQ1MzQzMg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382453432", "bodyText": "Yep, my fault.", "author": "gszadovszky", "createdAt": "2020-02-21T08:32:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAwNzM0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAwOTQzNw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382009437", "bodyText": "I would use an IllegalArgumentException and would also add bloomFilter to the message.", "author": "gszadovszky", "createdAt": "2020-02-20T13:50:32Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1643,7 +1643,7 @@ public static BloomFilterHeader toBloomFilterHeader(\n     if (algorithm != null && hashStrategy != null && compression != null) {\n       return new BloomFilterHeader(bloomFilter.getBitsetSize(), algorithm, hashStrategy, compression);\n     } else {\n-      return null;\n+      throw new RuntimeException(\"Failed to build thrift structure for BloomFilterHeader\");", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAyNDUzNA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382024534", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // Check with only column index based filtering\n          \n          \n            \n                // Check with only bloom filter based filtering", "author": "gszadovszky", "createdAt": "2020-02-20T14:15:19Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomFiltering.java", "diffHunk": "@@ -0,0 +1,257 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.filter2.predicate.FilterPredicate;\n+import org.apache.parquet.filter2.recordlevel.PhoneBookWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.io.api.Binary;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.parquet.filter2.predicate.FilterApi.*;\n+import static org.apache.parquet.hadoop.ParquetFileWriter.Mode.OVERWRITE;\n+import static org.junit.Assert.*;\n+\n+@RunWith(Parameterized.class)\n+public class TestBloomFiltering {\n+  private static final Path FILE_V1 = createTempFile();\n+  private static final Path FILE_V2 = createTempFile();\n+  private static final Logger LOGGER = LoggerFactory.getLogger(TestBloomFiltering.class);\n+  private static final Random RANDOM = new Random(42);\n+  private static final String[] PHONE_KINDS = { null, \"mobile\", \"home\", \"work\" };\n+  private static final List<PhoneBookWriter.User> DATA = Collections.unmodifiableList(generateData(10000));\n+\n+  private final Path file;\n+  public TestBloomFiltering(Path file) {\n+    this.file = file;\n+  }\n+\n+  private static Path createTempFile() {\n+    try {\n+      return new Path(Files.createTempFile(\"test-bloom-filter_\", \".parquet\").toAbsolutePath().toString());\n+    } catch (IOException e) {\n+      throw new AssertionError(\"Unable to create temporary file\", e);\n+    }\n+  }\n+\n+  @Parameterized.Parameters\n+  public static Collection<Object[]> params() {\n+    return Arrays.asList(new Object[] { FILE_V1 }, new Object[] { FILE_V2 });\n+  }\n+\n+  private static List<PhoneBookWriter.User> generateData(int rowCount) {\n+    List<PhoneBookWriter.User> users = new ArrayList<>();\n+    List<String> names = generateNames(rowCount);\n+    for (int i = 0; i < rowCount; ++i) {\n+      users.add(new PhoneBookWriter.User(i, names.get(i), generatePhoneNumbers(), generateLocation(i, rowCount)));\n+    }\n+    return users;\n+  }\n+\n+  private static List<String> generateNames(int rowCount) {\n+    List<String> list = new ArrayList<>();\n+\n+    // Adding fix values for filtering\n+    list.add(\"anderson\");\n+    list.add(\"anderson\");\n+    list.add(\"miller\");\n+    list.add(\"miller\");\n+    list.add(\"miller\");\n+    list.add(\"thomas\");\n+    list.add(\"thomas\");\n+    list.add(\"williams\");\n+\n+    int nullCount = rowCount / 100;\n+\n+    String alphabet = \"aabcdeefghiijklmnoopqrstuuvwxyz\";\n+    int maxLength = 8;\n+    for (int i = rowCount - list.size() - nullCount; i >= 0; --i) {\n+      int l = RANDOM.nextInt(maxLength);\n+      StringBuilder builder = new StringBuilder(l);\n+      for (int j = 0; j < l; ++j) {\n+        builder.append(alphabet.charAt(RANDOM.nextInt(alphabet.length())));\n+      }\n+      list.add(builder.toString());\n+    }\n+    list.sort((str1, str2) -> -str1.compareTo(str2));\n+\n+    // Adding nulls to random places\n+    for (int i = 0; i < nullCount; ++i) {\n+      list.add(RANDOM.nextInt(list.size()), null);\n+    }\n+\n+    return list;\n+  }\n+\n+  private static List<PhoneBookWriter.PhoneNumber> generatePhoneNumbers() {\n+    int length = RANDOM.nextInt(5) - 1;\n+    if (length < 0) {\n+      return null;\n+    }\n+    List<PhoneBookWriter.PhoneNumber> phoneNumbers = new ArrayList<>(length);\n+    for (int i = 0; i < length; ++i) {\n+      // 6 digits numbers\n+      long number = Math.abs(RANDOM.nextLong() % 900000) + 100000;\n+      phoneNumbers.add(new PhoneBookWriter.PhoneNumber(number, PHONE_KINDS[RANDOM.nextInt(PHONE_KINDS.length)]));\n+    }\n+    return phoneNumbers;\n+  }\n+\n+  private static PhoneBookWriter.Location generateLocation(int id, int rowCount) {\n+    if (RANDOM.nextDouble() < 0.01) {\n+      return null;\n+    }\n+\n+    if (RANDOM.nextDouble() < 0.001) {\n+      return new PhoneBookWriter.Location(99.9, 99.9);\n+    }\n+\n+    double lat = RANDOM.nextDouble() * 90.0 - (id < rowCount / 2 ? 90.0 : 0.0);\n+    double lon = RANDOM.nextDouble() * 90.0 - (id < rowCount / 4 || id >= 3 * rowCount / 4 ? 90.0 : 0.0);\n+\n+    return new PhoneBookWriter.Location(RANDOM.nextDouble() < 0.01 ? null : lat, RANDOM.nextDouble() < 0.01 ? null : lon);\n+  }\n+\n+  private List<PhoneBookWriter.User> readUsers(FilterPredicate filter, boolean useOtherFiltering,\n+                                               boolean useBloomFilter) throws IOException {\n+    return PhoneBookWriter.readUsers(ParquetReader.builder(new GroupReadSupport(), file)\n+      .withFilter(FilterCompat.get(filter))\n+      .useDictionaryFilter(useOtherFiltering)\n+      .useStatsFilter(useOtherFiltering)\n+      .useRecordFilter(useOtherFiltering)\n+      .useBloomFilter(useBloomFilter)\n+      .useColumnIndexFilter(useOtherFiltering));\n+  }\n+\n+  // Assumes that both lists are in the same order\n+  private static void assertContains(Stream<PhoneBookWriter.User> expected, List<PhoneBookWriter.User> actual) {\n+    Iterator<PhoneBookWriter.User> expIt = expected.iterator();\n+    if (!expIt.hasNext()) {\n+      return;\n+    }\n+    PhoneBookWriter.User exp = expIt.next();\n+    for (PhoneBookWriter.User act : actual) {\n+      if (act.equals(exp)) {\n+        if (!expIt.hasNext()) {\n+          break;\n+        }\n+        exp = expIt.next();\n+      }\n+    }\n+    assertFalse(\"Not all expected elements are in the actual list. E.g.: \" + exp, expIt.hasNext());\n+  }\n+\n+  private void assertCorrectFiltering(Predicate<PhoneBookWriter.User> expectedFilter, FilterPredicate actualFilter)\n+    throws IOException {\n+    // Check with only column index based filtering", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjAyNTQxOQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r382025419", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return location != null && location.getLat() != null &&location.getLat() == 99.9;\n          \n          \n            \n                    return location != null && location.getLat() != null && location.getLat() == 99.9;", "author": "gszadovszky", "createdAt": "2020-02-20T14:16:40Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomFiltering.java", "diffHunk": "@@ -0,0 +1,257 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.filter2.predicate.FilterPredicate;\n+import org.apache.parquet.filter2.recordlevel.PhoneBookWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.io.api.Binary;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.parquet.filter2.predicate.FilterApi.*;\n+import static org.apache.parquet.hadoop.ParquetFileWriter.Mode.OVERWRITE;\n+import static org.junit.Assert.*;\n+\n+@RunWith(Parameterized.class)\n+public class TestBloomFiltering {\n+  private static final Path FILE_V1 = createTempFile();\n+  private static final Path FILE_V2 = createTempFile();\n+  private static final Logger LOGGER = LoggerFactory.getLogger(TestBloomFiltering.class);\n+  private static final Random RANDOM = new Random(42);\n+  private static final String[] PHONE_KINDS = { null, \"mobile\", \"home\", \"work\" };\n+  private static final List<PhoneBookWriter.User> DATA = Collections.unmodifiableList(generateData(10000));\n+\n+  private final Path file;\n+  public TestBloomFiltering(Path file) {\n+    this.file = file;\n+  }\n+\n+  private static Path createTempFile() {\n+    try {\n+      return new Path(Files.createTempFile(\"test-bloom-filter_\", \".parquet\").toAbsolutePath().toString());\n+    } catch (IOException e) {\n+      throw new AssertionError(\"Unable to create temporary file\", e);\n+    }\n+  }\n+\n+  @Parameterized.Parameters\n+  public static Collection<Object[]> params() {\n+    return Arrays.asList(new Object[] { FILE_V1 }, new Object[] { FILE_V2 });\n+  }\n+\n+  private static List<PhoneBookWriter.User> generateData(int rowCount) {\n+    List<PhoneBookWriter.User> users = new ArrayList<>();\n+    List<String> names = generateNames(rowCount);\n+    for (int i = 0; i < rowCount; ++i) {\n+      users.add(new PhoneBookWriter.User(i, names.get(i), generatePhoneNumbers(), generateLocation(i, rowCount)));\n+    }\n+    return users;\n+  }\n+\n+  private static List<String> generateNames(int rowCount) {\n+    List<String> list = new ArrayList<>();\n+\n+    // Adding fix values for filtering\n+    list.add(\"anderson\");\n+    list.add(\"anderson\");\n+    list.add(\"miller\");\n+    list.add(\"miller\");\n+    list.add(\"miller\");\n+    list.add(\"thomas\");\n+    list.add(\"thomas\");\n+    list.add(\"williams\");\n+\n+    int nullCount = rowCount / 100;\n+\n+    String alphabet = \"aabcdeefghiijklmnoopqrstuuvwxyz\";\n+    int maxLength = 8;\n+    for (int i = rowCount - list.size() - nullCount; i >= 0; --i) {\n+      int l = RANDOM.nextInt(maxLength);\n+      StringBuilder builder = new StringBuilder(l);\n+      for (int j = 0; j < l; ++j) {\n+        builder.append(alphabet.charAt(RANDOM.nextInt(alphabet.length())));\n+      }\n+      list.add(builder.toString());\n+    }\n+    list.sort((str1, str2) -> -str1.compareTo(str2));\n+\n+    // Adding nulls to random places\n+    for (int i = 0; i < nullCount; ++i) {\n+      list.add(RANDOM.nextInt(list.size()), null);\n+    }\n+\n+    return list;\n+  }\n+\n+  private static List<PhoneBookWriter.PhoneNumber> generatePhoneNumbers() {\n+    int length = RANDOM.nextInt(5) - 1;\n+    if (length < 0) {\n+      return null;\n+    }\n+    List<PhoneBookWriter.PhoneNumber> phoneNumbers = new ArrayList<>(length);\n+    for (int i = 0; i < length; ++i) {\n+      // 6 digits numbers\n+      long number = Math.abs(RANDOM.nextLong() % 900000) + 100000;\n+      phoneNumbers.add(new PhoneBookWriter.PhoneNumber(number, PHONE_KINDS[RANDOM.nextInt(PHONE_KINDS.length)]));\n+    }\n+    return phoneNumbers;\n+  }\n+\n+  private static PhoneBookWriter.Location generateLocation(int id, int rowCount) {\n+    if (RANDOM.nextDouble() < 0.01) {\n+      return null;\n+    }\n+\n+    if (RANDOM.nextDouble() < 0.001) {\n+      return new PhoneBookWriter.Location(99.9, 99.9);\n+    }\n+\n+    double lat = RANDOM.nextDouble() * 90.0 - (id < rowCount / 2 ? 90.0 : 0.0);\n+    double lon = RANDOM.nextDouble() * 90.0 - (id < rowCount / 4 || id >= 3 * rowCount / 4 ? 90.0 : 0.0);\n+\n+    return new PhoneBookWriter.Location(RANDOM.nextDouble() < 0.01 ? null : lat, RANDOM.nextDouble() < 0.01 ? null : lon);\n+  }\n+\n+  private List<PhoneBookWriter.User> readUsers(FilterPredicate filter, boolean useOtherFiltering,\n+                                               boolean useBloomFilter) throws IOException {\n+    return PhoneBookWriter.readUsers(ParquetReader.builder(new GroupReadSupport(), file)\n+      .withFilter(FilterCompat.get(filter))\n+      .useDictionaryFilter(useOtherFiltering)\n+      .useStatsFilter(useOtherFiltering)\n+      .useRecordFilter(useOtherFiltering)\n+      .useBloomFilter(useBloomFilter)\n+      .useColumnIndexFilter(useOtherFiltering));\n+  }\n+\n+  // Assumes that both lists are in the same order\n+  private static void assertContains(Stream<PhoneBookWriter.User> expected, List<PhoneBookWriter.User> actual) {\n+    Iterator<PhoneBookWriter.User> expIt = expected.iterator();\n+    if (!expIt.hasNext()) {\n+      return;\n+    }\n+    PhoneBookWriter.User exp = expIt.next();\n+    for (PhoneBookWriter.User act : actual) {\n+      if (act.equals(exp)) {\n+        if (!expIt.hasNext()) {\n+          break;\n+        }\n+        exp = expIt.next();\n+      }\n+    }\n+    assertFalse(\"Not all expected elements are in the actual list. E.g.: \" + exp, expIt.hasNext());\n+  }\n+\n+  private void assertCorrectFiltering(Predicate<PhoneBookWriter.User> expectedFilter, FilterPredicate actualFilter)\n+    throws IOException {\n+    // Check with only column index based filtering\n+    List<PhoneBookWriter.User> result = readUsers(actualFilter, false, true);\n+\n+    assertTrue(\"Bloom filtering should drop some row groups\", result.size() < DATA.size());\n+    LOGGER.info(\"{}/{} records read; filtering ratio: {}%\", result.size(), DATA.size(),\n+      100 * result.size() / DATA.size());\n+    // Asserts that all the required records are in the result\n+    assertContains(DATA.stream().filter(expectedFilter), result);\n+    // Asserts that all the retrieved records are in the file (validating non-matching records)\n+    assertContains(result.stream(), DATA);\n+\n+    // Check with all the filtering filtering to ensure the result contains exactly the required values\n+    result = readUsers(actualFilter, true, false);\n+    assertEquals(DATA.stream().filter(expectedFilter).collect(Collectors.toList()), result);\n+  }\n+\n+\n+  @BeforeClass\n+  public static void createFile() throws IOException {\n+    int pageSize = DATA.size() / 100;     // Ensure that several pages will be created\n+    int rowGroupSize = pageSize * 4;    // Ensure that there are more row-groups created\n+    Map<String, Long> column2NDVMap = new HashMap<>();\n+    column2NDVMap.put(\"location.lat\", 10000L);\n+    column2NDVMap.put(\"name\", 10000L);\n+    column2NDVMap.put(\"id\", 10000L);\n+    PhoneBookWriter.write(ExampleParquetWriter.builder(FILE_V1)\n+        .withWriteMode(OVERWRITE)\n+        .withRowGroupSize(rowGroupSize)\n+        .withPageSize(pageSize)\n+        .withBloomFilterColumnToNDVMap(column2NDVMap)\n+        .withWriterVersion(ParquetProperties.WriterVersion.PARQUET_1_0),\n+      DATA);\n+    PhoneBookWriter.write(ExampleParquetWriter.builder(FILE_V2)\n+        .withWriteMode(OVERWRITE)\n+        .withRowGroupSize(rowGroupSize)\n+        .withPageSize(pageSize)\n+        .withBloomFilterColumnToNDVMap(column2NDVMap)\n+        .withWriterVersion(ParquetProperties.WriterVersion.PARQUET_2_0),\n+      DATA);\n+  }\n+\n+  @AfterClass\n+  public static void deleteFile() throws IOException {\n+    FILE_V1.getFileSystem(new Configuration()).delete(FILE_V1, false);\n+    FILE_V2.getFileSystem(new Configuration()).delete(FILE_V2, false);\n+  }\n+\n+\n+  @Test\n+  public void testSimpleFiltering() throws IOException {\n+    assertCorrectFiltering(\n+      record -> record.getId() == 1234L,\n+      eq(longColumn(\"id\"), 1234L));\n+\n+    assertCorrectFiltering(\n+      record -> \"miller\".equals(record.getName()),\n+      eq(binaryColumn(\"name\"), Binary.fromString(\"miller\")));\n+  }\n+\n+  @Test\n+  public void testNestedFiltering() throws IOException {\n+    assertCorrectFiltering(\n+      record -> {\n+        PhoneBookWriter.Location location = record.getLocation();\n+        return location != null && location.getLat() != null &&location.getLat() == 99.9;", "originalCommit": "1519967d5d79c616fc70adcb1ad2465d8d313555", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b9125870c422c576b061419f7626f2e039266390", "url": "https://github.com/apache/parquet-mr/commit/b9125870c422c576b061419f7626f2e039266390", "message": "Add xxhash correctness unit test, update shading, upper bound and some minir updates", "committedDate": "2020-02-22T16:28:32Z", "type": "forcePushed"}, {"oid": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "url": "https://github.com/apache/parquet-mr/commit/6a8e641573899e24a62e6b1db2f25d61faa65e6c", "message": "Add xxhash correctness unit test, update shading, upper bound and some minir updates", "committedDate": "2020-02-23T02:31:58Z", "type": "commit"}, {"oid": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "url": "https://github.com/apache/parquet-mr/commit/6a8e641573899e24a62e6b1db2f25d61faa65e6c", "message": "Add xxhash correctness unit test, update shading, upper bound and some minir updates", "committedDate": "2020-02-23T02:31:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE0NTk5Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383145993", "bodyText": "Having these variables final would help JIT to optimize out the bloomFilter != null parts.", "author": "gszadovszky", "createdAt": "2020-02-24T09:11:29Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -53,6 +58,9 @@\n   private long rowsWrittenSoFar = 0;\n   private int pageRowCount;\n \n+  private BloomFilterWriter bloomFilterWriter;\n+  private BloomFilter bloomFilter;", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1MTIyMA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383351220", "bodyText": "The final variable need to be intialized while we still have some constructor that not intialize these variables.", "author": "chenjunjiedada", "createdAt": "2020-02-24T15:56:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE0NTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzcyNTM1Ng==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383725356", "bodyText": "As far as I can see bloomFilter is either initialized in a constructor (line 100 and 104) or null. Similarly for bloomFilterWriter. So, you can declare them final only that you might need to initialize them null in the constructors and code paths where no non-null value is not assigned.", "author": "gszadovszky", "createdAt": "2020-02-25T08:34:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE0NTk5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzg1MTQ0Mw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383851443", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-02-25T12:35:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE0NTk5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE1MDM1MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383150350", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                else {\n          \n          \n            \n                } else {", "author": "gszadovszky", "createdAt": "2020-02-24T09:21:22Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -66,6 +74,37 @@\n     this.dataColumn = props.newValuesWriter(path);\n   }\n \n+  ColumnWriterBase(\n+    ColumnDescriptor path,\n+    PageWriter pageWriter,\n+    BloomFilterWriter bloomFilterWriter,\n+    ParquetProperties props\n+  ) {\n+    this(path, pageWriter, props);\n+\n+    this.bloomFilterWriter = bloomFilterWriter;\n+    Set<String> bloomFilterColumns = props.getBloomFilterColumns();\n+    String column = String.join(\".\", path.getPath());\n+    if (!bloomFilterColumns.contains(column)) {\n+      return;\n+    }\n+    int maxBloomFilterSize = props.getMaxBloomFilterBytes();\n+\n+    Map<String, Long> bloomFilterColumnExpectedNDVs = props.getBloomFilterColumnExpectedNDVs();\n+    if (bloomFilterColumnExpectedNDVs.size() > 0) {\n+      // If user specify the column NDV, we construct Bloom filter from it.\n+      if (bloomFilterColumnExpectedNDVs.containsKey(column)) {\n+        int optimalNumOfBits = BlockSplitBloomFilter.optimalNumOfBits(\n+          bloomFilterColumnExpectedNDVs.get(column).intValue(), BlockSplitBloomFilter.DEFAULT_FPP);\n+\n+        this.bloomFilter = new BlockSplitBloomFilter(optimalNumOfBits / 8, maxBloomFilterSize);\n+      }\n+    }\n+    else {", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE1MDQ3Mg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383150472", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                }", "author": "gszadovszky", "createdAt": "2020-02-24T09:21:36Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -66,6 +74,37 @@\n     this.dataColumn = props.newValuesWriter(path);\n   }\n \n+  ColumnWriterBase(\n+    ColumnDescriptor path,\n+    PageWriter pageWriter,\n+    BloomFilterWriter bloomFilterWriter,\n+    ParquetProperties props\n+  ) {\n+    this(path, pageWriter, props);\n+\n+    this.bloomFilterWriter = bloomFilterWriter;\n+    Set<String> bloomFilterColumns = props.getBloomFilterColumns();\n+    String column = String.join(\".\", path.getPath());\n+    if (!bloomFilterColumns.contains(column)) {\n+      return;\n+    }\n+    int maxBloomFilterSize = props.getMaxBloomFilterBytes();\n+\n+    Map<String, Long> bloomFilterColumnExpectedNDVs = props.getBloomFilterColumnExpectedNDVs();\n+    if (bloomFilterColumnExpectedNDVs.size() > 0) {\n+      // If user specify the column NDV, we construct Bloom filter from it.\n+      if (bloomFilterColumnExpectedNDVs.containsKey(column)) {\n+        int optimalNumOfBits = BlockSplitBloomFilter.optimalNumOfBits(\n+          bloomFilterColumnExpectedNDVs.get(column).intValue(), BlockSplitBloomFilter.DEFAULT_FPP);\n+\n+        this.bloomFilter = new BlockSplitBloomFilter(optimalNumOfBits / 8, maxBloomFilterSize);\n+      }\n+    }", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE1MzIyMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383153221", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n          \n          \n            \n                if (maximumBytes > LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {", "author": "gszadovszky", "createdAt": "2020-02-24T09:27:39Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > maximumBytes) {\n+      throw new IllegalArgumentException(\"the minimum bytes should be less or equal than maximum bytes\");\n+    }\n+\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE1NDU2OA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383154568", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n          \n          \n            \n              private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.BYTES);", "author": "gszadovszky", "createdAt": "2020-02-24T09:30:32Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE2NjE2OA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383166168", "bodyText": "From a naive point of view this three loops can be simplified into one by doing the calculation in one line. If it is done this way to optimize for vectorization it shall be commented so later it won't be simplified.", "author": "gszadovszky", "createdAt": "2020-02-24T09:53:05Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > maximumBytes) {\n+      throw new IllegalArgumentException(\"the minimum bytes should be less or equal than maximum bytes\");\n+    }\n+\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1MTU4NQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383351585", "bodyText": "Make sense, doc added.", "author": "chenjunjiedada", "createdAt": "2020-02-24T15:57:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE2NjE2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE3MTI2NA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383171264", "bodyText": "As the generated mask is read only and privately used it might be better to create once and reuse the same array.", "author": "gszadovszky", "createdAt": "2020-02-24T10:02:54Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > maximumBytes) {\n+      throw new IllegalArgumentException(\"the minimum bytes should be less or equal than maximum bytes\");\n+    }\n+\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1MTc2NA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383351764", "bodyText": "Make sense, updated.", "author": "chenjunjiedada", "createdAt": "2020-02-24T15:57:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE3MTI2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE3OTk4MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383179981", "bodyText": "You already have the casted int value. Why do you use m instead of numBits?", "author": "gszadovszky", "createdAt": "2020-02-24T10:19:45Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > maximumBytes) {\n+      throw new IllegalArgumentException(\"the minimum bytes should be less or equal than maximum bytes\");\n+    }\n+\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = key * SALT[i];\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = mask[i] >>> 27;\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = 0x1 << mask[i];\n+    }\n+\n+    return mask;\n+  }\n+\n+  @Override\n+  public void insertHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for bucket.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      int value = intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i);\n+      value |= mask[i];\n+      intBuffer.put(blockIndex * (BYTES_PER_BLOCK / 4) + i, value);\n+    }\n+  }\n+\n+  @Override\n+  public boolean findHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for the tiny Bloom filter.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      if (0 == (intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i) & mask[i])) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Calculate optimal size according to the number of distinct values and false positive probability.\n+   *\n+   * @param n: The number of distinct values.\n+   * @param p: The false positive probability.\n+   *\n+   * @return optimal number of bits of given n and p.\n+   */\n+  public static int optimalNumOfBits(long n, double p) {\n+    Preconditions.checkArgument((p > 0.0 && p < 1.0),\n+      \"FPP should be less than 1.0 and great than 0.0\");\n+    final double m = -8 * n / Math.log(1 - Math.pow(p, 1.0 / 8));\n+    int numBits = (int)m ;\n+\n+    // Handle overflow.\n+    if (m > UPPER_BOUND_BYTES << 3 || m < 0) {", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1NTIxMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383355211", "bodyText": "The casting drops decimal value. I just updated to compare upper bound with integer and keep to compare 0 with double.", "author": "chenjunjiedada", "createdAt": "2020-02-24T16:03:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE3OTk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE4MDM4NQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383180385", "bodyText": "Shouldn't it be UPPER_BOUND_BYTES << 3?", "author": "gszadovszky", "createdAt": "2020-02-24T10:20:32Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > maximumBytes) {\n+      throw new IllegalArgumentException(\"the minimum bytes should be less or equal than maximum bytes\");\n+    }\n+\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = key * SALT[i];\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = mask[i] >>> 27;\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = 0x1 << mask[i];\n+    }\n+\n+    return mask;\n+  }\n+\n+  @Override\n+  public void insertHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for bucket.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      int value = intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i);\n+      value |= mask[i];\n+      intBuffer.put(blockIndex * (BYTES_PER_BLOCK / 4) + i, value);\n+    }\n+  }\n+\n+  @Override\n+  public boolean findHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for the tiny Bloom filter.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      if (0 == (intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i) & mask[i])) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Calculate optimal size according to the number of distinct values and false positive probability.\n+   *\n+   * @param n: The number of distinct values.\n+   * @param p: The false positive probability.\n+   *\n+   * @return optimal number of bits of given n and p.\n+   */\n+  public static int optimalNumOfBits(long n, double p) {\n+    Preconditions.checkArgument((p > 0.0 && p < 1.0),\n+      \"FPP should be less than 1.0 and great than 0.0\");\n+    final double m = -8 * n / Math.log(1 - Math.pow(p, 1.0 / 8));\n+    int numBits = (int)m ;\n+\n+    // Handle overflow.\n+    if (m > UPPER_BOUND_BYTES << 3 || m < 0) {\n+      numBits = UPPER_BOUND_BYTES;", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1NTMzOA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383355338", "bodyText": "My bad. Fixed", "author": "chenjunjiedada", "createdAt": "2020-02-24T16:03:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE4MDM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE4MjQwOA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383182408", "bodyText": "clear is always invoked before writing to the buffer (line 312). I think, this one is unnecessary.", "author": "gszadovszky", "createdAt": "2020-02-24T10:24:18Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > maximumBytes) {\n+      throw new IllegalArgumentException(\"the minimum bytes should be less or equal than maximum bytes\");\n+    }\n+\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = key * SALT[i];\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = mask[i] >>> 27;\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = 0x1 << mask[i];\n+    }\n+\n+    return mask;\n+  }\n+\n+  @Override\n+  public void insertHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for bucket.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      int value = intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i);\n+      value |= mask[i];\n+      intBuffer.put(blockIndex * (BYTES_PER_BLOCK / 4) + i, value);\n+    }\n+  }\n+\n+  @Override\n+  public boolean findHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for the tiny Bloom filter.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      if (0 == (intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i) & mask[i])) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Calculate optimal size according to the number of distinct values and false positive probability.\n+   *\n+   * @param n: The number of distinct values.\n+   * @param p: The false positive probability.\n+   *\n+   * @return optimal number of bits of given n and p.\n+   */\n+  public static int optimalNumOfBits(long n, double p) {\n+    Preconditions.checkArgument((p > 0.0 && p < 1.0),\n+      \"FPP should be less than 1.0 and great than 0.0\");\n+    final double m = -8 * n / Math.log(1 - Math.pow(p, 1.0 / 8));\n+    int numBits = (int)m ;\n+\n+    // Handle overflow.\n+    if (m > UPPER_BOUND_BYTES << 3 || m < 0) {\n+      numBits = UPPER_BOUND_BYTES;\n+    }\n+\n+    // Round to BITS_PER_BLOCK\n+    numBits = (numBits + BITS_PER_BLOCK -1) & ~BITS_PER_BLOCK;\n+\n+    if (numBits < (LOWER_BOUND_BYTES << 3)) {\n+      numBits = LOWER_BOUND_BYTES << 3;\n+    }\n+\n+    return numBits;\n+  }\n+\n+  @Override\n+  public int getBitsetSize() {\n+    return this.bitset.length;\n+  }\n+\n+  @Override\n+  public long hash(Object value) {\n+    if (value instanceof Binary) {\n+      return hashFunction.hashBytes(((Binary) value).getBytes());\n+    }\n+\n+    cacheBuffer.clear();\n+    if (value instanceof Integer) {\n+      cacheBuffer.putInt((Integer)value);\n+    } else if (value instanceof Long) {\n+      cacheBuffer.putLong((Long)value);\n+    } else if (value instanceof Float) {\n+      cacheBuffer.putFloat((Float)value);\n+    } else if (value instanceof Double) {\n+      cacheBuffer.putDouble((Double) value);\n+    } else {\n+      throw new RuntimeException(\"Parquet Bloom filter: Not supported type\");\n+    }\n+\n+    return doHash();\n+  }\n+\n+  @Override\n+  public HashStrategy getHashStrategy() {\n+    return HashStrategy.XXH64;\n+  }\n+\n+  @Override\n+  public Algorithm getAlgorithm() {\n+    return Algorithm.BLOCK;\n+  }\n+\n+  @Override\n+  public Compression getCompression() {\n+    return Compression.UNCOMPRESSED;\n+  }\n+\n+  private long doHash() {\n+    cacheBuffer.flip();\n+    long hashResult = hashFunction.hashByteBuffer(cacheBuffer);\n+    cacheBuffer.clear();", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1Nzg2MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383357860", "bodyText": "The hashes for primitive values don't clear the buffer, so we need to clear them. The hash for object values is supposed to be used when perform filtering.", "author": "chenjunjiedada", "createdAt": "2020-02-24T16:07:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE4MjQwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzczMTIwNA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383731204", "bodyText": "You are right. Then, line 312 seems to be unnecessary. (I am trying to be critical for this code because we have to invoke the hashing for every values to be written.)", "author": "gszadovszky", "createdAt": "2020-02-25T08:46:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE4MjQwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE5MTgzNg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383191836", "bodyText": "I think, this one would be more accurate\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // Round to BITS_PER_BLOCK\n          \n          \n            \n                // Round numBits up to (k * BITS_PER_BLOCK)", "author": "gszadovszky", "createdAt": "2020-02-24T10:42:38Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.IntBuffer;\n+\n+/*\n+ * This Bloom filter is implemented using block-based Bloom filter algorithm from Putze et al.'s\n+ * \"Cache-, Hash- and Space-Efficient Bloom filters\". The basic idea is to hash the item to a tiny\n+ * Bloom filter which size fit a single cache line or smaller. This implementation sets 8 bits in\n+ * each tiny Bloom filter. Each tiny Bloom filter is 32 bytes to take advantage of 32-byte SIMD\n+ * instruction.\n+ */\n+public class BlockSplitBloomFilter implements BloomFilter {\n+  // Bytes in a tiny Bloom filter block.\n+  private static final int BYTES_PER_BLOCK = 32;\n+\n+  // Bits in a tiny Bloom filter block.\n+  private static final int BITS_PER_BLOCK = 256;\n+\n+  // The lower bound of bloom filter size, set to the size of a tiny Bloom filter block.\n+  public static final int LOWER_BOUND_BYTES = 32;\n+\n+  // The upper bound of bloom filter size, set to default row group size.\n+  public static final int UPPER_BOUND_BYTES = 128 * 1024 * 1024;\n+\n+  // The number of bits to set in a tiny Bloom filter\n+  private static final int BITS_SET_PER_BLOCK = 8;\n+\n+  // The metadata in the header of a serialized Bloom filter is four four-byte values: the number of bytes,\n+  // the filter algorithm, the hash algorithm, and the compression.\n+  public static final int HEADER_SIZE = 16;\n+\n+  // The default false positive probability value\n+  public static final double DEFAULT_FPP = 0.01;\n+\n+  // The hash strategy used in this Bloom filter.\n+  private final HashStrategy hashStrategy;\n+\n+  // The underlying byte array for Bloom filter bitset.\n+  private byte[] bitset;\n+\n+  // A integer array buffer of underlying bitset to help setting bits.\n+  private IntBuffer intBuffer;\n+\n+  // Hash function use to compute hash for column value.\n+  private HashFunction hashFunction;\n+\n+  private int maximumBytes = UPPER_BOUND_BYTES;\n+  private int minimumBytes = LOWER_BOUND_BYTES;\n+\n+  // A cache used for hashing\n+  private ByteBuffer cacheBuffer = ByteBuffer.allocate(Long.SIZE/Byte.SIZE);\n+\n+  // The block-based algorithm needs 8 odd SALT values to calculate eight indexes\n+  // of bits to set, one per 32-bit word.\n+  private static final int[] SALT = {0x47b6137b, 0x44974d91, 0x8824ad5b, 0xa2b7289d,\n+    0x705495c7, 0x2df1424b, 0x9efc4947, 0x5c6bfb31};\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, DEFAULT_MAXIMUM_BYTES], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   */\n+  public BlockSplitBloomFilter(int numBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [DEFAULT_MINIMUM_BYTES, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range. It will also be rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int maximumBytes) {\n+    this(numBytes, LOWER_BOUND_BYTES, maximumBytes, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset\n+   * @param hashStrategy The hash strategy of Bloom filter.\n+   */\n+  private BlockSplitBloomFilter(int numBytes, HashStrategy hashStrategy) {\n+    this(numBytes, LOWER_BOUND_BYTES, UPPER_BOUND_BYTES, hashStrategy);\n+  }\n+\n+  /**\n+   * Constructor of block-based Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down to lower/upper bound if\n+   *                 num_bytes is out of range. It will also be rounded up to a power of 2.\n+   * @param minimumBytes The minimum bytes of the Bloom filter.\n+   * @param maximumBytes The maximum bytes of the Bloom filter.\n+   * @param hashStrategy The adopted hash strategy of the Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(int numBytes, int minimumBytes, int maximumBytes, HashStrategy hashStrategy) {\n+    if (minimumBytes > maximumBytes) {\n+      throw new IllegalArgumentException(\"the minimum bytes should be less or equal than maximum bytes\");\n+    }\n+\n+    if (minimumBytes > LOWER_BOUND_BYTES && minimumBytes < UPPER_BOUND_BYTES) {\n+      this.minimumBytes = minimumBytes;\n+    }\n+\n+    if (maximumBytes> LOWER_BOUND_BYTES && maximumBytes < UPPER_BOUND_BYTES) {\n+      this.maximumBytes = maximumBytes;\n+    }\n+\n+    initBitset(numBytes);\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file. It use XXH64 as its default hash\n+   * function.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   */\n+  public BlockSplitBloomFilter(byte[] bitset) {\n+    this(bitset, HashStrategy.XXH64);\n+  }\n+\n+  /**\n+   * Construct the Bloom filter with given bitset, it is used when reconstructing\n+   * Bloom filter from parquet file.\n+   *\n+   * @param bitset The given bitset to construct Bloom filter.\n+   * @param hashStrategy The hash strategy Bloom filter apply.\n+   */\n+  private BlockSplitBloomFilter(byte[] bitset, HashStrategy hashStrategy) {\n+    if (bitset == null) {\n+      throw new RuntimeException(\"Given bitset is null\");\n+    }\n+\n+    cacheBuffer.order(ByteOrder.LITTLE_ENDIAN);\n+    this.bitset = bitset;\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+    switch (hashStrategy) {\n+      case XXH64:\n+        this.hashStrategy = hashStrategy;\n+        hashFunction = new XxHash();\n+        break;\n+      default:\n+        throw new RuntimeException(\"Unsupported hash strategy\");\n+    }\n+  }\n+\n+  /**\n+   * Create a new bitset for Bloom filter.\n+   *\n+   * @param numBytes The number of bytes for Bloom filter bitset. The range of num_bytes should be within\n+   *                 [minimumBytes, maximumBytes], it will be rounded up/down\n+   *                 to lower/upper bound if num_bytes is out of range and also will rounded up to a power\n+   *                 of 2. It uses XXH64 as its default hash function and block-based algorithm\n+   *                 as default algorithm.\n+   */\n+  private void initBitset(int numBytes) {\n+    if (numBytes < minimumBytes) {\n+      numBytes = minimumBytes;\n+    }\n+    // Get next power of 2 if it is not power of 2.\n+    if ((numBytes & (numBytes - 1)) != 0) {\n+      numBytes = Integer.highestOneBit(numBytes) << 1;\n+    }\n+    if (numBytes > maximumBytes || numBytes < 0) {\n+      numBytes = maximumBytes;\n+    }\n+    this.bitset = new byte[numBytes];\n+    this.intBuffer = ByteBuffer.wrap(bitset).order(ByteOrder.LITTLE_ENDIAN).asIntBuffer();\n+  }\n+\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(bitset);\n+  }\n+\n+  private int[] setMask(int key) {\n+    int[] mask = new int[BITS_SET_PER_BLOCK];\n+\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = key * SALT[i];\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = mask[i] >>> 27;\n+    }\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; ++i) {\n+      mask[i] = 0x1 << mask[i];\n+    }\n+\n+    return mask;\n+  }\n+\n+  @Override\n+  public void insertHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for bucket.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      int value = intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i);\n+      value |= mask[i];\n+      intBuffer.put(blockIndex * (BYTES_PER_BLOCK / 4) + i, value);\n+    }\n+  }\n+\n+  @Override\n+  public boolean findHash(long hash) {\n+    long numBlocks = bitset.length / BYTES_PER_BLOCK;\n+    long lowHash = hash >>> 32;\n+    int blockIndex = (int)((lowHash * numBlocks) >> 32);\n+    int key = (int)hash;\n+\n+    // Calculate mask for the tiny Bloom filter.\n+    int[] mask = setMask(key);\n+    for (int i = 0; i < BITS_SET_PER_BLOCK; i++) {\n+      if (0 == (intBuffer.get(blockIndex * (BYTES_PER_BLOCK / 4) + i) & mask[i])) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Calculate optimal size according to the number of distinct values and false positive probability.\n+   *\n+   * @param n: The number of distinct values.\n+   * @param p: The false positive probability.\n+   *\n+   * @return optimal number of bits of given n and p.\n+   */\n+  public static int optimalNumOfBits(long n, double p) {\n+    Preconditions.checkArgument((p > 0.0 && p < 1.0),\n+      \"FPP should be less than 1.0 and great than 0.0\");\n+    final double m = -8 * n / Math.log(1 - Math.pow(p, 1.0 / 8));\n+    int numBits = (int)m ;\n+\n+    // Handle overflow.\n+    if (m > UPPER_BOUND_BYTES << 3 || m < 0) {\n+      numBits = UPPER_BOUND_BYTES;\n+    }\n+\n+    // Round to BITS_PER_BLOCK", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1Nzk2NQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383357965", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-02-24T16:07:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE5MTgzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzIzNDQ3MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383234470", "bodyText": "Why do you need value for these enums?", "author": "gszadovszky", "createdAt": "2020-02-24T12:22:21Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BloomFilter.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.column.values.bloomfilter;\n+\n+import org.apache.parquet.io.api.Binary;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+\n+/**\n+ * A Bloom filter is a compact structure to indicate whether an item is not in a set or probably\n+ * in a set. The Bloom filter usually consists of a bit set that represents a elements set,\n+ * a hash strategy and a Bloom filter algorithm.\n+ */\n+public interface BloomFilter {\n+  /* Bloom filter Hash strategy.\n+   *\n+   * xxHash is an extremely fast hash algorithm, running at RAM speed limits. It successfully\n+   * completes the SMHasher test suite which evaluates collision, dispersion and randomness qualities\n+   * of hash functions. It shows good performance advantage from its benchmark result.\n+   * (see https://github.com/Cyan4973/xxHash).\n+   */\n+  enum HashStrategy {\n+    XXH64(0);\n+    HashStrategy(int value) {\n+      this.value = value;\n+    }\n+    int value;", "originalCommit": "6a8e641573899e24a62e6b1db2f25d61faa65e6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzM1ODcxMg==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383358712", "bodyText": "That is used for serialization before we using thrift. We don't need them now. Removed.", "author": "chenjunjiedada", "createdAt": "2020-02-24T16:08:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzIzNDQ3MA=="}], "type": "inlineReview"}, {"oid": "069cc22218cf06bfd3037744b62acf5ab810ef3a", "url": "https://github.com/apache/parquet-mr/commit/069cc22218cf06bfd3037744b62acf5ab810ef3a", "message": "add more unit tests, some minor fixes", "committedDate": "2020-02-24T15:19:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzcyNjk2Nw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383726967", "bodyText": "As the first loop assigns values for all entries without using the original value this line is not necessary.", "author": "gszadovszky", "createdAt": "2020-02-25T08:37:53Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/values/bloomfilter/BlockSplitBloomFilter.java", "diffHunk": "@@ -220,14 +223,18 @@ public void writeTo(OutputStream out) throws IOException {\n   }\n \n   private int[] setMask(int key) {\n-    int[] mask = new int[BITS_SET_PER_BLOCK];\n+    Arrays.fill(mask, 0);", "originalCommit": "069cc22218cf06bfd3037744b62acf5ab810ef3a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzczNjgzOA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383736838", "bodyText": "As you already invoke this 4 times what do you think about varying the size of the bloom filter as well? It shall not generate any false negatives even if the bloom filter size is not optimal.", "author": "gszadovszky", "createdAt": "2020-02-25T08:57:13Z", "path": "parquet-column/src/test/java/org/apache/parquet/column/values/bloomfilter/TestBlockSplitBloomFilter.java", "diffHunk": "@@ -54,29 +51,69 @@ public void testConstructor () {\n    * serializing and de-serializing.\n    */\n   @Test\n-  public void testBloomFilterBasicReadWrite() throws IOException {\n-    final String[] testStrings = {\"hello\", \"parquet\", \"bloom\", \"filter\"};\n-    BloomFilter bloomFilter = new BlockSplitBloomFilter(1024);\n-\n-    for (String string : testStrings) {\n-      bloomFilter.insertHash(bloomFilter.hash(Binary.fromString(string)));\n+  public void testBloomFilterForString() {\n+    final int numValues = 1024 * 1024;\n+    int numBytes = BlockSplitBloomFilter.optimalNumOfBits(numValues , 0.01) / 8;\n+    BloomFilter bloomFilter = new BlockSplitBloomFilter(numBytes);\n+\n+    Set<String> testStrings = new HashSet<>();\n+    for (int i = 0; i < numValues; i ++) {\n+      String str = RandomStringUtils.randomAlphabetic(1, 64);\n+      bloomFilter.insertHash(bloomFilter.hash(Binary.fromString(str)));\n+      testStrings.add(str);\n     }\n \n-    File testFile = temp.newFile();\n-    FileOutputStream fileOutputStream = new FileOutputStream(testFile);\n-    bloomFilter.writeTo(fileOutputStream);\n-    fileOutputStream.close();\n-    FileInputStream fileInputStream = new FileInputStream(testFile);\n-\n-    byte[] bitset = new byte[1024];\n-    int bytes = fileInputStream.read(bitset);\n-    assertEquals(bytes, 1024);\n-    bloomFilter = new BlockSplitBloomFilter(bitset);\n     for (String testString : testStrings) {\n       assertTrue(bloomFilter.findHash(bloomFilter.hash(Binary.fromString(testString))));\n     }\n   }\n \n+  @Test\n+  public void testBloomFilterForPrimitives() {\n+    for (int i = 0; i < 4; i++) {\n+      long seed = System.nanoTime();\n+      testBloomFilterForPrimitives(seed);\n+    }\n+  }\n+\n+  private void testBloomFilterForPrimitives(long seed) {\n+    final int numValues = 1024 * 1024;\n+    final int numBytes = BlockSplitBloomFilter.optimalNumOfBits(numValues , 0.01) / 8;", "originalCommit": "069cc22218cf06bfd3037744b62acf5ab810ef3a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzc0ODc0OQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383748749", "bodyText": "How about setting FPP randomly?", "author": "chenjunjiedada", "createdAt": "2020-02-25T09:18:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzczNjgzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzc2NjYzNQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383766635", "bodyText": "great idea", "author": "gszadovszky", "createdAt": "2020-02-25T09:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzczNjgzOA=="}], "type": "inlineReview"}, {"oid": "c16e8e8b8f133071cd98004c8bccc5bb92d7ca9e", "url": "https://github.com/apache/parquet-mr/commit/c16e8e8b8f133071cd98004c8bccc5bb92d7ca9e", "message": "minor updates", "committedDate": "2020-02-25T10:36:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzg0MTA4NQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383841085", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final int numBytes = BlockSplitBloomFilter.optimalNumOfBits(numValues , random.nextDouble()/10) / 8;\n          \n          \n            \n                final int numBytes = BlockSplitBloomFilter.optimalNumOfBits(numValues, random.nextDouble() / 10) / 8;", "author": "gszadovszky", "createdAt": "2020-02-25T12:12:12Z", "path": "parquet-column/src/test/java/org/apache/parquet/column/values/bloomfilter/TestBlockSplitBloomFilter.java", "diffHunk": "@@ -77,11 +77,11 @@ public void testBloomFilterForPrimitives() {\n   }\n \n   private void testBloomFilterForPrimitives(long seed) {\n+    Random random = new Random(seed);\n     final int numValues = 1024 * 1024;\n-    final int numBytes = BlockSplitBloomFilter.optimalNumOfBits(numValues , 0.01) / 8;\n-\n+    final int numBytes = BlockSplitBloomFilter.optimalNumOfBits(numValues , random.nextDouble()/10) / 8;", "originalCommit": "c16e8e8b8f133071cd98004c8bccc5bb92d7ca9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzg1MTUyMw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r383851523", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-02-25T12:35:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzg0MTA4NQ=="}], "type": "inlineReview"}, {"oid": "8f04bb8c8318202613da5e6ce6a399a729c23fe8", "url": "https://github.com/apache/parquet-mr/commit/8f04bb8c8318202613da5e6ce6a399a729c23fe8", "message": "minor updates", "committedDate": "2020-02-25T12:31:47Z", "type": "commit"}, {"oid": "8f04bb8c8318202613da5e6ce6a399a729c23fe8", "url": "https://github.com/apache/parquet-mr/commit/8f04bb8c8318202613da5e6ce6a399a729c23fe8", "message": "minor updates", "committedDate": "2020-02-25T12:31:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxNzU1MA==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r384017550", "bodyText": "Travis fails because bloomFilter and bloomFilterWriter gets value twice (once in the other constructor because of this call and once in this constructor) while they are final.\nI suggest moving all the code from the other constructor to this one so the initialization would be done only once. Then, you won't invoke the other constructor from this one but the other way around. The other constructor would only call this one (with bloomFilterWriter = null) and nothing else.", "author": "gszadovszky", "createdAt": "2020-02-25T17:24:54Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnWriterBase.java", "diffHunk": "@@ -64,6 +72,38 @@\n     this.repetitionLevelColumn = createRLWriter(props, path);\n     this.definitionLevelColumn = createDLWriter(props, path);\n     this.dataColumn = props.newValuesWriter(path);\n+    this.bloomFilter = null;\n+    this.bloomFilterWriter = null;\n+  }\n+\n+  ColumnWriterBase(\n+    ColumnDescriptor path,\n+    PageWriter pageWriter,\n+    BloomFilterWriter bloomFilterWriter,\n+    ParquetProperties props\n+  ) {\n+    this(path, pageWriter, props);", "originalCommit": "8f04bb8c8318202613da5e6ce6a399a729c23fe8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDIxMjg2Nw==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r384212867", "bodyText": "Fixed, thanks.", "author": "chenjunjiedada", "createdAt": "2020-02-26T00:37:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxNzU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDIxMDE0MQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r384210141", "bodyText": "@chenjunjiedada\nI'm curious about the maximum default value. Could you please explain why you choose 1 MB?", "author": "garawalid", "createdAt": "2020-02-26T00:28:19Z", "path": "parquet-column/src/main/java/org/apache/parquet/column/ParquetProperties.java", "diffHunk": "@@ -56,6 +60,7 @@\n   public static final int DEFAULT_COLUMN_INDEX_TRUNCATE_LENGTH = 64;\n   public static final int DEFAULT_STATISTICS_TRUNCATE_LENGTH = Integer.MAX_VALUE;\n   public static final int DEFAULT_PAGE_ROW_COUNT_LIMIT = 20_000;\n+  public static final int DEFAULT_MAX_BLOOM_FILTER_BYTES = 1024 * 1024;", "originalCommit": "8f04bb8c8318202613da5e6ce6a399a729c23fe8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDIxMzIxMQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r384213211", "bodyText": "Assume we have a row group with only one column of UUID (36 bytes), according to the formula and FPP = 0.01 we will need about 4MB. I expect that we will have more columns in the real scenario.", "author": "chenjunjiedada", "createdAt": "2020-02-26T00:38:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDIxMDE0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDYxMjQwNQ==", "url": "https://github.com/apache/parquet-mr/pull/757#discussion_r384612405", "bodyText": "@chenjunjiedada Thanks for the clarification!", "author": "garawalid", "createdAt": "2020-02-26T16:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDIxMDE0MQ=="}], "type": "inlineReview"}, {"oid": "5a98e780a5231decb72740da9347cd3856b1c07e", "url": "https://github.com/apache/parquet-mr/commit/5a98e780a5231decb72740da9347cd3856b1c07e", "message": "fix build", "committedDate": "2020-02-26T00:31:54Z", "type": "commit"}, {"oid": "373d811b46b38463407fa67e0383cf093144be3f", "url": "https://github.com/apache/parquet-mr/commit/373d811b46b38463407fa67e0383cf093144be3f", "message": "Merge branch 'master' into bloom-filter", "committedDate": "2020-02-26T11:49:33Z", "type": "commit"}]}