{"pr_number": 824, "pr_title": "PARQUET-1920: Fix Parquet writer's memory check interval calculation", "pr_createdAt": "2020-10-07T22:24:05Z", "pr_url": "https://github.com/apache/parquet-mr/pull/824", "timeline": [{"oid": "f7bcc4a64fbb4a23e6f6c5b97dd31f11f15b4c05", "url": "https://github.com/apache/parquet-mr/commit/f7bcc4a64fbb4a23e6f6c5b97dd31f11f15b4c05", "message": "Fix Parquet writer's memory check interval calculation, and throw helpful message while dealing with too large column chunks.", "committedDate": "2020-10-07T22:05:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc3MzkyMQ==", "url": "https://github.com/apache/parquet-mr/pull/824#discussion_r501773921", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  throw new IOException(\"Page size, \" + size + \", is larger than allowed \" + Integer.MAX_VALUE +\".\" +\n          \n          \n            \n                    \" Usually caused by a Parquet writer writing too big column chunks on encountering highly skewed dataset.\" +\n          \n          \n            \n                    \" Please set page.size.row.check.max to a lower value on the writer, default value is 10000.\" +\n          \n          \n            \n                    \" You can try setting it to \" + (10000 / (size/ Integer.MAX_VALUE)) + \" or lower.\");\n          \n          \n            \n                  throw new IOException(\"Page size, \" + size + \", is larger than allowed \" + Integer.MAX_VALUE + \".\" +\n          \n          \n            \n                    \" Usually caused by a Parquet writer writing too big column chunks on encountering highly skewed dataset.\" +\n          \n          \n            \n                    \" Please set page.size.row.check.max to a lower value on the writer, default value is 10000.\" +\n          \n          \n            \n                    \" You can try setting it to \" + (10000 / (size / Integer.MAX_VALUE)) + \" or lower.\");", "author": "gszadovszky", "createdAt": "2020-10-08T14:37:51Z", "path": "parquet-common/src/main/java/org/apache/parquet/bytes/BytesInput.java", "diffHunk": "@@ -215,6 +215,13 @@ public static BytesInput copy(BytesInput bytesInput) throws IOException {\n    * @throws IOException if there is an exception reading\n    */\n   public byte[] toByteArray() throws IOException {\n+    long size = size();\n+    if (size > Integer.MAX_VALUE) {\n+      throw new IOException(\"Page size, \" + size + \", is larger than allowed \" + Integer.MAX_VALUE +\".\" +\n+        \" Usually caused by a Parquet writer writing too big column chunks on encountering highly skewed dataset.\" +\n+        \" Please set page.size.row.check.max to a lower value on the writer, default value is 10000.\" +\n+        \" You can try setting it to \" + (10000 / (size/ Integer.MAX_VALUE)) + \" or lower.\");", "originalCommit": "f7bcc4a64fbb4a23e6f6c5b97dd31f11f15b4c05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4806f5df9f20f7b60c8ba6d25b15c38277bfe6db", "url": "https://github.com/apache/parquet-mr/commit/4806f5df9f20f7b60c8ba6d25b15c38277bfe6db", "message": "Update parquet-common/src/main/java/org/apache/parquet/bytes/BytesInput.java\n\nCo-authored-by: Gabor Szadovszky <gabor@apache.org>", "committedDate": "2020-10-08T14:56:04Z", "type": "commit"}]}