{"pr_number": 755, "pr_title": "PARQUET-1791: Add 'prune' command to parquet-tools", "pr_createdAt": "2020-02-09T14:51:24Z", "pr_url": "https://github.com/apache/parquet-mr/pull/755", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjkxNDM0Mw==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r376914343", "bodyText": "Please, remove this line.", "author": "gszadovszky", "createdAt": "2020-02-10T08:15:55Z", "path": "parquet-tools/src/main/java/org/apache/parquet/tools/command/PruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.tools.Main;\n+\n+import java.io.PrintWriter;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.HashSet;\n+\n+public class PruneColumnsCommand extends ArgsOnlyCommand {\n+\n+  public static final String[] USAGE = new String[] {\n+    \"<input> <output> [<column> ...]\",\n+\n+    \"where <input> is the source parquet file\",\n+    \"    <output> is the destination parquet file,\" +\n+    \"    [<column> ...] are the columns in the case senstive dot format\" +\n+      \" to be pruned, for example a.b.c\"\n+  };\n+\n+  /**\n+   * Biggest number of columns we can prune.\n+   */\n+  private static final int MAX_COL_NUM = 100;\n+\n+  private Configuration conf;\n+\n+  public PruneColumnsCommand() {\n+    super(3, MAX_COL_NUM + 1);\n+\n+    conf = new Configuration();\n+  }\n+\n+  @Override\n+  public String[] getUsageDescription() {\n+    return USAGE;\n+  }\n+\n+  @Override\n+  public String getCommandDescription() {\n+    return \"Prune column(s) in a Parquet file and save it to a new file. \" +\n+      \"The columns left are not changed.\";\n+  }\n+\n+  @Override\n+  public void execute(CommandLine options) throws Exception {\n+    List<String> args = options.getArgList();\n+    Path inputFile = new Path(args.get(0));\n+    Path outputFile = new Path(args.get(1));\n+    List<String> cols = args.subList(2, args.size());\n+\n+    Set<ColumnPath> prunePaths = convertToColumnPaths(cols);\n+\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, inputFile, ParquetMetadataConverter.NO_FILTER);\n+    FileMetaData metaData = pmd.getFileMetaData();\n+\n+    ParquetFileWriter writer = new ParquetFileWriter(conf,\n+      pruneColumnsInSchema(metaData.getSchema(), prunePaths), outputFile, ParquetFileWriter.Mode.CREATE);\n+\n+    writer.start();\n+    writer.appendFile(HadoopInputFile.fromPath(inputFile, conf));\n+    writer.end(metaData.getKeyValueMetaData());\n+  }\n+\n+  private MessageType pruneColumnsInSchema(MessageType schema, Set<ColumnPath> prunePaths) {\n+\n+    List<Type> fields = schema.getFields();\n+    List<String> currentPath = new ArrayList<>();\n+    List<Type> prunedFields = pruneColumnsInFields(fields, currentPath, prunePaths);\n+    MessageType newSchema = new MessageType(schema.getName(), prunedFields);\n+    return newSchema;\n+\n+    //return schema;", "originalCommit": "964f41d2c94554623a0927dd5bc5c459d4edffed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDAwNDA1MQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380004051", "bodyText": "sure", "author": "shangxinli", "createdAt": "2020-02-17T06:32:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjkxNDM0Mw=="}], "type": "inlineReview"}, {"oid": "6bef646243f867c1b208b92e2997917366716d69", "url": "https://github.com/apache/parquet-mr/commit/6bef646243f867c1b208b92e2997917366716d69", "message": "PARQUET-1791: Add 'prune' command to parquet-tools", "committedDate": "2020-02-17T06:24:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MTkzOA==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380181938", "bodyText": "Checking in binary parquet files is not really future proof. It is usually a better idea to create the input file in the test and remove it at the end. (There are many examples in the tests to do so.)", "author": "gszadovszky", "createdAt": "2020-02-17T13:30:05Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {", "originalCommit": "6bef646243f867c1b208b92e2997917366716d69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ0MzAyMw==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380443023", "bodyText": "Sounds good! I saw there are several places using the binary files, so I just followed. But you are right. We should create the files on the fly. I changed the tests to do that. Please have a look.", "author": "shangxinli", "createdAt": "2020-02-18T03:50:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MTkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MjU4MA==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380182580", "bodyText": "This path depends on the path where the maven command is executed from. Maybe, that's why it is failing on Travis.", "author": "gszadovszky", "createdAt": "2020-02-17T13:31:18Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private Configuration conf = new Configuration();\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // File test.parquet has 3 columns: \"name\", \"age\", \"gender\"\n+    String inputFile = \"src/test/java/org/apache/parquet/tools/data/test.parquet\";", "originalCommit": "6bef646243f867c1b208b92e2997917366716d69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ0MzE0Nw==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380443147", "bodyText": "Fixed.", "author": "shangxinli", "createdAt": "2020-02-18T03:50:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MjU4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MzQzOA==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380183438", "bodyText": "It would also be great to verify the actual data. Maybe, you may use the dump command?", "author": "gszadovszky", "createdAt": "2020-02-17T13:32:58Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private Configuration conf = new Configuration();\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // File test.parquet has 3 columns: \"name\", \"age\", \"gender\"\n+    String inputFile = \"src/test/java/org/apache/parquet/tools/data/test.parquet\";\n+    String outputFile =  rand.nextInt() + \".parquet\";\n+    String cargs[] = {inputFile, outputFile, \"gender\"};\n+    executeCommandLine(cargs);\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"name\");\n+    assertEquals(fields.get(1).getName(), \"age\");", "originalCommit": "6bef646243f867c1b208b92e2997917366716d69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ0MzEyNA==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380443124", "bodyText": "Yes. I just wrote a reader to verify.", "author": "shangxinli", "createdAt": "2020-02-18T03:50:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MzQzOA=="}], "type": "inlineReview"}, {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c", "url": "https://github.com/apache/parquet-mr/commit/e4f368c086ba739235a738c602586ac8826b6c4c", "message": "PARQUET-1791: Add 'prune' command to parquet-tools", "committedDate": "2020-02-18T03:48:08Z", "type": "commit"}, {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c", "url": "https://github.com/apache/parquet-mr/commit/e4f368c086ba739235a738c602586ac8826b6c4c", "message": "PARQUET-1791: Add 'prune' command to parquet-tools", "committedDate": "2020-02-18T03:48:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTE5NzE1Nw==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381197157", "bodyText": "I would prefer using creating the file in a temporary space instead. (See e.g. java.nio.file.Files.createTempFile)\nCreating a file int the hard-coded \"target/\" relative path is a bit error prone. What happens if the mvn command is executed from outside of the project structure?", "author": "gszadovszky", "createdAt": "2020-02-19T10:15:21Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, INT64, \"Backward\"),\n+        new PrimitiveType(REPEATED, INT64, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    Path file = new Path(root, + rand.nextInt(100000) + \"input.parquet\");", "originalCommit": "e4f368c086ba739235a738c602586ac8826b6c4c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM4OTk4OQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382389989", "bodyText": "Fixed", "author": "shangxinli", "createdAt": "2020-02-21T04:18:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTE5NzE1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTE5ODM4MA==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381198380", "bodyText": "nit: Using the builders in org.apache.parquet.schema.Types would result a nicer and more readable code.", "author": "gszadovszky", "createdAt": "2020-02-19T10:17:38Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",", "originalCommit": "e4f368c086ba739235a738c602586ac8826b6c4c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM5Mjk0OQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382392949", "bodyText": "I see this way is used in multiple places of the other tests. Since this is a pretty simple schema, it seems straightforward. But if you have a strong opinion, let me know and I will fix it.", "author": "shangxinli", "createdAt": "2020-02-21T04:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTE5ODM4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjQ1NDY2Ng==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382454666", "bodyText": "It was just nit picking, no strong opinion. (I guess the other places uses this way because they were developed before the schema builder design was created.)", "author": "gszadovszky", "createdAt": "2020-02-21T08:35:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTE5ODM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwMDI5OQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381200299", "bodyText": "nit: Using org.apache.parquet.hadoop.example.ExampleParquetWriter.builder makes creating the writer easier and more readable.", "author": "gszadovszky", "createdAt": "2020-02-19T10:20:59Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, INT64, \"Backward\"),\n+        new PrimitiveType(REPEATED, INT64, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    Path file = new Path(root, + rand.nextInt(100000) + \"input.parquet\");\n+\n+    ParquetWriter<Group> writer = new ParquetWriter<>(file, new GroupWriteSupport(), ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME,", "originalCommit": "e4f368c086ba739235a738c602586ac8826b6c4c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM5NDgxOQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382394819", "bodyText": "Great idea!", "author": "shangxinli", "createdAt": "2020-02-21T04:43:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwMDI5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwMDk1OQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381200959", "bodyText": "I would prefer using a try-with-resources construct instead.", "author": "gszadovszky", "createdAt": "2020-02-19T10:22:11Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, INT64, \"Backward\"),\n+        new PrimitiveType(REPEATED, INT64, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    Path file = new Path(root, + rand.nextInt(100000) + \"input.parquet\");\n+\n+    ParquetWriter<Group> writer = new ParquetWriter<>(file, new GroupWriteSupport(), ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME,\n+      ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.DEFAULT_PAGE_SIZE, 1024,\n+      ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED, ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,\n+      ParquetProperties.WriterVersion.PARQUET_1_0, conf);\n+\n+    for (int i = 0; i < numRecord; i++) {\n+      SimpleGroup g = new SimpleGroup(schema);\n+      g.add(\"DocId\", 1l);\n+      g.add(\"Name\", \"foo\");\n+      g.add(\"Gender\", \"male\");\n+      Group links = g.addGroup(\"Links\");\n+      links.add(0, 2l);\n+      links.add(1, 3l);\n+      writer.write(g);\n+    }\n+\n+    writer.close();", "originalCommit": "e4f368c086ba739235a738c602586ac8826b6c4c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM5NTQ3OA==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382395478", "bodyText": "sure", "author": "shangxinli", "createdAt": "2020-02-21T04:46:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwMDk1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwNTU3Ng==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381205576", "bodyText": "Let's check the following scenario.\nUser wants to prune the columns a, b and c from several parquet files. The schema of the files are not the same because they evolved (e.g. column c is added after some files were already created or column a is used for partitioning so many files do not contain it).\nIf you think the previous scenario is valid then the tool should not throw an exception in case a column to be pruned is not in the file. Meanwhile, the user might mistyped the column name and it may not notify. So, it is your choice how you should handle this case. :)", "author": "gszadovszky", "createdAt": "2020-02-19T10:30:36Z", "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {", "originalCommit": "e4f368c086ba739235a738c602586ac8826b6c4c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM5Njk1MQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382396951", "bodyText": "That is a really good point! I was trying to prevent 'silent failing' when user has a typo for the column names. Now I think we can just print a warning message.", "author": "shangxinli", "createdAt": "2020-02-21T04:53:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwNTU3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjQ1OTEwMg==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382459102", "bodyText": "You still expect the exception to be thrown in the two related tests. That's why Travis fails.", "author": "gszadovszky", "createdAt": "2020-02-21T08:46:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwNTU3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY2OTEyNQ==", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r382669125", "bodyText": "Thanks for pointing out! Just fixed both.", "author": "shangxinli", "createdAt": "2020-02-21T16:11:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwNTU3Ng=="}], "type": "inlineReview"}, {"oid": "fda592ed4ad0ca6c469472cb89c6966c59a2d7fa", "url": "https://github.com/apache/parquet-mr/commit/fda592ed4ad0ca6c469472cb89c6966c59a2d7fa", "message": "Address feedback", "committedDate": "2020-02-21T04:57:36Z", "type": "commit"}, {"oid": "75386f91dd6127447876ab9a80271edfadf5c739", "url": "https://github.com/apache/parquet-mr/commit/75386f91dd6127447876ab9a80271edfadf5c739", "message": "Remove unused imports", "committedDate": "2020-02-21T04:59:32Z", "type": "commit"}, {"oid": "c1a524198faadddd2854db9cfa1db8e532e38f5d", "url": "https://github.com/apache/parquet-mr/commit/c1a524198faadddd2854db9cfa1db8e532e38f5d", "message": "Fix failing test", "committedDate": "2020-02-21T16:21:20Z", "type": "commit"}]}