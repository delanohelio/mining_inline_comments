{"pr_number": 56860, "pr_title": "Add max_token_length setting to the CharGroupTokenizer", "pr_createdAt": "2020-05-16T17:24:44Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/56860", "timeline": [{"oid": "3ce8b1946e56fef39a87e19b197632fee29373c2", "url": "https://github.com/elastic/elasticsearch/commit/3ce8b1946e56fef39a87e19b197632fee29373c2", "message": "add failing test case", "committedDate": "2020-05-16T16:49:20Z", "type": "commit"}, {"oid": "9fa42dd312fa985e2418dc14301705a26d63d122", "url": "https://github.com/elastic/elasticsearch/commit/9fa42dd312fa985e2418dc14301705a26d63d122", "message": "fix code; test passes", "committedDate": "2020-05-16T16:55:25Z", "type": "commit"}, {"oid": "8cf5258fd99ecec4867dbfeb7e2090a1e905cb0b", "url": "https://github.com/elastic/elasticsearch/commit/8cf5258fd99ecec4867dbfeb7e2090a1e905cb0b", "message": "add more tests", "committedDate": "2020-05-16T17:12:43Z", "type": "commit"}, {"oid": "89cbcc7ab21918ca0e10461305b57e34f56aa554", "url": "https://github.com/elastic/elasticsearch/commit/89cbcc7ab21918ca0e10461305b57e34f56aa554", "message": "updated documentation", "committedDate": "2020-05-16T17:16:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0NjM2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56860#discussion_r427446363", "bodyText": "Super small nitpick: can you change to use the default from CharTokenizer.DEFAULT_MAX_WORD_LEN here? Its the same value (255) as StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH currently, just so we tie the default to the one thats used in the class we actually use.", "author": "cbuescher", "createdAt": "2020-05-19T16:43:19Z", "path": "modules/analysis-common/src/main/java/org/elasticsearch/analysis/common/CharGroupTokenizerFactory.java", "diffHunk": "@@ -41,6 +46,8 @@\n     public CharGroupTokenizerFactory(IndexSettings indexSettings, Environment environment, String name, Settings settings) {\n         super(indexSettings, settings, name);\n \n+        maxTokenLength = settings.getAsInt(MAX_TOKEN_LENGTH, StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH);", "originalCommit": "89cbcc7ab21918ca0e10461305b57e34f56aa554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ2MTE2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56860#discussion_r427461163", "bodyText": "nit: we have a 140 chars line length limit that gets checked via \"checkstyle\" in CI and it complains about this line being too long. Simply splitting this is fine.", "author": "cbuescher", "createdAt": "2020-05-19T17:06:23Z", "path": "modules/analysis-common/src/test/java/org/elasticsearch/analysis/common/CharGroupTokenizerFactoryTests.java", "diffHunk": "@@ -61,6 +64,52 @@ public void testParseTokenChars() {\n         }\n     }\n \n+    public void testMaxTokenLength() throws IOException {\n+        final Index index = new Index(\"test\", \"_na_\");\n+        final Settings indexSettings = newAnalysisSettingsBuilder().build();\n+        IndexSettings indexProperties = IndexSettingsModule.newIndexSettings(index, indexSettings);\n+        final String name = \"cg\";\n+\n+        String[] conf = new String[] {\"-\"};\n+\n+        final Settings defaultLengthSettings = newAnalysisSettingsBuilder()\n+            .putList(\"tokenize_on_chars\", conf)\n+            .build();\n+        CharTokenizer tokenizer = (CharTokenizer) new CharGroupTokenizerFactory(indexProperties, null, name, defaultLengthSettings).create();", "originalCommit": "89cbcc7ab21918ca0e10461305b57e34f56aa554", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3609c1dcd27678efb38e40c4cf22b78f5ef408f8", "url": "https://github.com/elastic/elasticsearch/commit/3609c1dcd27678efb38e40c4cf22b78f5ef408f8", "message": "small refactoring", "committedDate": "2020-05-19T20:25:42Z", "type": "commit"}, {"oid": "48366f3ff0df906aca2b2b208b71f6c2909219ed", "url": "https://github.com/elastic/elasticsearch/commit/48366f3ff0df906aca2b2b208b71f6c2909219ed", "message": "Removing unused import", "committedDate": "2020-05-20T08:04:59Z", "type": "commit"}, {"oid": "431fd1aa7c7a27beeea6dba3b5385939530220a5", "url": "https://github.com/elastic/elasticsearch/commit/431fd1aa7c7a27beeea6dba3b5385939530220a5", "message": "Merge branch 'master' into enhancement/max-char-limit-char-group-tokenizer", "committedDate": "2020-05-20T10:11:20Z", "type": "commit"}]}