{"pr_number": 53350, "pr_title": "Add logstash system index APIs", "pr_createdAt": "2020-03-10T17:22:16Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/53350", "timeline": [{"oid": "045aaaed2ac8f596371dafebfac98dbddea77b15", "url": "https://github.com/elastic/elasticsearch/commit/045aaaed2ac8f596371dafebfac98dbddea77b15", "message": "Add logstash module with wrapped APIs", "committedDate": "2020-03-04T17:55:10Z", "type": "commit"}, {"oid": "742c6216b4bf508b5752da70953bd1e2b401b35a", "url": "https://github.com/elastic/elasticsearch/commit/742c6216b4bf508b5752da70953bd1e2b401b35a", "message": "remove wrapped APIs and add dedicated logstash APIs", "committedDate": "2020-03-09T20:58:12Z", "type": "commit"}, {"oid": "5328277cbac4864d927745eaad0bbefa57687013", "url": "https://github.com/elastic/elasticsearch/commit/5328277cbac4864d927745eaad0bbefa57687013", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-03-10T14:49:16Z", "type": "commit"}, {"oid": "dd17265ff50a21fa0c1cb2dab80e64a8e3f68446", "url": "https://github.com/elastic/elasticsearch/commit/dd17265ff50a21fa0c1cb2dab80e64a8e3f68446", "message": "merge with x-pack plugin", "committedDate": "2020-03-10T18:55:50Z", "type": "commit"}, {"oid": "c94574fac1f45a5ee54005a98ac5ba03a02872b9", "url": "https://github.com/elastic/elasticsearch/commit/c94574fac1f45a5ee54005a98ac5ba03a02872b9", "message": "add system index access allowance", "committedDate": "2020-03-10T20:20:23Z", "type": "commit"}, {"oid": "d227480099926f5122329c60d250fd99cc09e9f1", "url": "https://github.com/elastic/elasticsearch/commit/d227480099926f5122329c60d250fd99cc09e9f1", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-03-11T14:40:35Z", "type": "commit"}, {"oid": "a63f268e34c7811a10c463ac053e9889d9ae3e07", "url": "https://github.com/elastic/elasticsearch/commit/a63f268e34c7811a10c463ac053e9889d9ae3e07", "message": "formatting", "committedDate": "2020-03-11T14:43:37Z", "type": "commit"}, {"oid": "922d3117dbd16e62267036789d6746f71de68f57", "url": "https://github.com/elastic/elasticsearch/commit/922d3117dbd16e62267036789d6746f71de68f57", "message": "serialization tests and fixes", "committedDate": "2020-03-11T17:10:47Z", "type": "commit"}, {"oid": "765963d13695e256144f2a2e9cb5484c713c087d", "url": "https://github.com/elastic/elasticsearch/commit/765963d13695e256144f2a2e9cb5484c713c087d", "message": "license", "committedDate": "2020-03-11T17:11:42Z", "type": "commit"}, {"oid": "b82253e1c0637fd7a05b7c329669ca488f5eb900", "url": "https://github.com/elastic/elasticsearch/commit/b82253e1c0637fd7a05b7c329669ca488f5eb900", "message": "format", "committedDate": "2020-03-11T17:25:12Z", "type": "commit"}, {"oid": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "url": "https://github.com/elastic/elasticsearch/commit/bcfa925ee3d699e23f4e5596876f8d1089e961b7", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-03-11T21:53:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM1NTk0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392355942", "bodyText": "Aesthetic nitpick: Could we move this to an import so this is just extends ActionType<DeletePipelineResponse>?", "author": "gwbrown", "createdAt": "2020-03-13T17:01:15Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/DeletePipelineAction.java", "diffHunk": "@@ -0,0 +1,19 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.elasticsearch.action.ActionType;\n+\n+public class DeletePipelineAction extends ActionType<org.elasticsearch.xpack.logstash.action.DeletePipelineResponse> {", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM1NjkxNA==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392356914", "bodyText": "Why not make this an AcknowledgedResponse?", "author": "gwbrown", "createdAt": "2020-03-13T17:03:04Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/DeletePipelineResponse.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+public class DeletePipelineResponse extends ActionResponse {", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyODY3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r459028672", "bodyText": "Is there a benefit? We're not acking anything here?", "author": "jaymode", "createdAt": "2020-07-22T19:23:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM1NjkxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM1OTM4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392359386", "bodyText": "Given that Logstash.LOGSTASH_CONCRETE_INDEX_NAME is now public, why not use it here?", "author": "gwbrown", "createdAt": "2020-03-13T17:07:45Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/TransportGetPipelineAction.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.get.MultiGetItemResponse;\n+import org.elasticsearch.action.search.ClearScrollRequest;\n+import org.elasticsearch.action.search.SearchResponse;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.search.SearchHit;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+public class TransportGetPipelineAction extends HandledTransportAction<GetPipelineRequest, GetPipelineResponse> {\n+\n+    private static final Logger logger = LogManager.getLogger(TransportGetPipelineAction.class);\n+    private final Client client;\n+\n+    @Inject\n+    public TransportGetPipelineAction(TransportService transportService, ActionFilters actionFilters, Client client) {\n+        super(GetPipelineAction.NAME, transportService, actionFilters, GetPipelineRequest::new);\n+        this.client = client;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {\n+        if (request.ids().isEmpty()) {\n+            client.prepareSearch(\".logstash\")", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM1OTY2OA==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392359668", "bodyText": "Same here about Logstash.LOGSTASH_CONCRETE_INDEX_NAME.", "author": "gwbrown", "createdAt": "2020-03-13T17:08:22Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/TransportGetPipelineAction.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.get.MultiGetItemResponse;\n+import org.elasticsearch.action.search.ClearScrollRequest;\n+import org.elasticsearch.action.search.SearchResponse;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.search.SearchHit;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+public class TransportGetPipelineAction extends HandledTransportAction<GetPipelineRequest, GetPipelineResponse> {\n+\n+    private static final Logger logger = LogManager.getLogger(TransportGetPipelineAction.class);\n+    private final Client client;\n+\n+    @Inject\n+    public TransportGetPipelineAction(TransportService transportService, ActionFilters actionFilters, Client client) {\n+        super(GetPipelineAction.NAME, transportService, actionFilters, GetPipelineRequest::new);\n+        this.client = client;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {\n+        if (request.ids().isEmpty()) {\n+            client.prepareSearch(\".logstash\")\n+                .setSource(\n+                    SearchSourceBuilder.searchSource()\n+                        .fetchSource(true)\n+                        .query(QueryBuilders.matchAllQuery())\n+                        .size(1000)\n+                        .trackTotalHits(true)\n+                )\n+                .setScroll(TimeValue.timeValueMinutes(1L))\n+                .execute(ActionListener.wrap(searchResponse -> {\n+                    final int numHits = Math.toIntExact(searchResponse.getHits().getTotalHits().value);\n+                    final Map<String, BytesReference> pipelineSources = new HashMap<>(numHits);\n+                    final Consumer<SearchResponse> clearScroll = (response) -> {\n+                        if (response != null && response.getScrollId() != null) {\n+                            ClearScrollRequest clearScrollRequest = new ClearScrollRequest();\n+                            clearScrollRequest.addScrollId(response.getScrollId());\n+                            client.clearScroll(\n+                                clearScrollRequest,\n+                                ActionListener.wrap(\n+                                    (r) -> {},\n+                                    e -> logger.warn(\n+                                        new ParameterizedMessage(\"clear scroll failed for scroll id [{}]\", response.getScrollId()),\n+                                        e\n+                                    )\n+                                )\n+                            );\n+                        }\n+                    };\n+                    handleSearchResponse(searchResponse, pipelineSources, clearScroll, listener);\n+                }, listener::onFailure));\n+        } else if (request.ids().size() == 1) {\n+            client.prepareGet(\".logstash\", request.ids().get(0)).setFetchSource(true).execute(ActionListener.wrap(response -> {", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM1OTg1MA==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392359850", "bodyText": "Same here about Logstash.LOGSTASH_CONCRETE_INDEX_NAME.", "author": "gwbrown", "createdAt": "2020-03-13T17:08:43Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/TransportGetPipelineAction.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.get.MultiGetItemResponse;\n+import org.elasticsearch.action.search.ClearScrollRequest;\n+import org.elasticsearch.action.search.SearchResponse;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.search.SearchHit;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+public class TransportGetPipelineAction extends HandledTransportAction<GetPipelineRequest, GetPipelineResponse> {\n+\n+    private static final Logger logger = LogManager.getLogger(TransportGetPipelineAction.class);\n+    private final Client client;\n+\n+    @Inject\n+    public TransportGetPipelineAction(TransportService transportService, ActionFilters actionFilters, Client client) {\n+        super(GetPipelineAction.NAME, transportService, actionFilters, GetPipelineRequest::new);\n+        this.client = client;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {\n+        if (request.ids().isEmpty()) {\n+            client.prepareSearch(\".logstash\")\n+                .setSource(\n+                    SearchSourceBuilder.searchSource()\n+                        .fetchSource(true)\n+                        .query(QueryBuilders.matchAllQuery())\n+                        .size(1000)\n+                        .trackTotalHits(true)\n+                )\n+                .setScroll(TimeValue.timeValueMinutes(1L))\n+                .execute(ActionListener.wrap(searchResponse -> {\n+                    final int numHits = Math.toIntExact(searchResponse.getHits().getTotalHits().value);\n+                    final Map<String, BytesReference> pipelineSources = new HashMap<>(numHits);\n+                    final Consumer<SearchResponse> clearScroll = (response) -> {\n+                        if (response != null && response.getScrollId() != null) {\n+                            ClearScrollRequest clearScrollRequest = new ClearScrollRequest();\n+                            clearScrollRequest.addScrollId(response.getScrollId());\n+                            client.clearScroll(\n+                                clearScrollRequest,\n+                                ActionListener.wrap(\n+                                    (r) -> {},\n+                                    e -> logger.warn(\n+                                        new ParameterizedMessage(\"clear scroll failed for scroll id [{}]\", response.getScrollId()),\n+                                        e\n+                                    )\n+                                )\n+                            );\n+                        }\n+                    };\n+                    handleSearchResponse(searchResponse, pipelineSources, clearScroll, listener);\n+                }, listener::onFailure));\n+        } else if (request.ids().size() == 1) {\n+            client.prepareGet(\".logstash\", request.ids().get(0)).setFetchSource(true).execute(ActionListener.wrap(response -> {\n+                if (response.isExists()) {\n+                    listener.onResponse(new GetPipelineResponse(Map.of(response.getId(), response.getSourceAsBytesRef())));\n+                } else {\n+                    listener.onResponse(new GetPipelineResponse(Map.of()));\n+                }\n+            }, listener::onFailure));\n+        } else {\n+            client.prepareMultiGet()\n+                .addIds(\".logstash\", request.ids())", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM2MDk3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392360972", "bodyText": "I'm not sure what to do instead, but if some of the GETs fail, do we really just want to silently swallow that? Should we at least log something here?", "author": "gwbrown", "createdAt": "2020-03-13T17:10:53Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/TransportGetPipelineAction.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.get.MultiGetItemResponse;\n+import org.elasticsearch.action.search.ClearScrollRequest;\n+import org.elasticsearch.action.search.SearchResponse;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.search.SearchHit;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+public class TransportGetPipelineAction extends HandledTransportAction<GetPipelineRequest, GetPipelineResponse> {\n+\n+    private static final Logger logger = LogManager.getLogger(TransportGetPipelineAction.class);\n+    private final Client client;\n+\n+    @Inject\n+    public TransportGetPipelineAction(TransportService transportService, ActionFilters actionFilters, Client client) {\n+        super(GetPipelineAction.NAME, transportService, actionFilters, GetPipelineRequest::new);\n+        this.client = client;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {\n+        if (request.ids().isEmpty()) {\n+            client.prepareSearch(\".logstash\")\n+                .setSource(\n+                    SearchSourceBuilder.searchSource()\n+                        .fetchSource(true)\n+                        .query(QueryBuilders.matchAllQuery())\n+                        .size(1000)\n+                        .trackTotalHits(true)\n+                )\n+                .setScroll(TimeValue.timeValueMinutes(1L))\n+                .execute(ActionListener.wrap(searchResponse -> {\n+                    final int numHits = Math.toIntExact(searchResponse.getHits().getTotalHits().value);\n+                    final Map<String, BytesReference> pipelineSources = new HashMap<>(numHits);\n+                    final Consumer<SearchResponse> clearScroll = (response) -> {\n+                        if (response != null && response.getScrollId() != null) {\n+                            ClearScrollRequest clearScrollRequest = new ClearScrollRequest();\n+                            clearScrollRequest.addScrollId(response.getScrollId());\n+                            client.clearScroll(\n+                                clearScrollRequest,\n+                                ActionListener.wrap(\n+                                    (r) -> {},\n+                                    e -> logger.warn(\n+                                        new ParameterizedMessage(\"clear scroll failed for scroll id [{}]\", response.getScrollId()),\n+                                        e\n+                                    )\n+                                )\n+                            );\n+                        }\n+                    };\n+                    handleSearchResponse(searchResponse, pipelineSources, clearScroll, listener);\n+                }, listener::onFailure));\n+        } else if (request.ids().size() == 1) {\n+            client.prepareGet(\".logstash\", request.ids().get(0)).setFetchSource(true).execute(ActionListener.wrap(response -> {\n+                if (response.isExists()) {\n+                    listener.onResponse(new GetPipelineResponse(Map.of(response.getId(), response.getSourceAsBytesRef())));\n+                } else {\n+                    listener.onResponse(new GetPipelineResponse(Map.of()));\n+                }\n+            }, listener::onFailure));\n+        } else {\n+            client.prepareMultiGet()\n+                .addIds(\".logstash\", request.ids())\n+                .execute(\n+                    ActionListener.wrap(\n+                        mGetResponse -> listener.onResponse(\n+                            new GetPipelineResponse(\n+                                Arrays.stream(mGetResponse.getResponses())\n+                                    .filter(itemResponse -> itemResponse.isFailed() == false)", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM2MTE5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392361191", "bodyText": "Same here about Logstash.LOGSTASH_CONCRETE_INDEX_NAME.", "author": "gwbrown", "createdAt": "2020-03-13T17:11:18Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/TransportPutPipelineAction.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+public class TransportPutPipelineAction extends HandledTransportAction<PutPipelineRequest, PutPipelineResponse> {\n+\n+    private final Client client;\n+\n+    @Inject\n+    public TransportPutPipelineAction(TransportService transportService, ActionFilters actionFilters, Client client) {\n+        super(PutPipelineAction.NAME, transportService, actionFilters, PutPipelineRequest::new);\n+        this.client = client;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, PutPipelineRequest request, ActionListener<PutPipelineResponse> listener) {\n+        client.prepareIndex(\".logstash\")", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM2NTE4Nw==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r392365187", "bodyText": "Weak recommendation: Break these out into AbstractWireSerializingTestCases and get equals/hashcode tests for free, plus make it a bit easier to find the tests for each class.", "author": "gwbrown", "createdAt": "2020-03-13T17:18:44Z", "path": "x-pack/plugin/logstash/src/test/java/org/elasticsearch/xpack/logstash/PipelineRequestResponseSerializationTests.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash;\n+\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.xcontent.XContentType;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.logstash.action.DeletePipelineRequest;\n+import org.elasticsearch.xpack.logstash.action.DeletePipelineResponse;\n+import org.elasticsearch.xpack.logstash.action.GetPipelineRequest;\n+import org.elasticsearch.xpack.logstash.action.GetPipelineResponse;\n+import org.elasticsearch.xpack.logstash.action.PutPipelineRequest;\n+import org.elasticsearch.xpack.logstash.action.PutPipelineResponse;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class PipelineRequestResponseSerializationTests extends ESTestCase {", "originalCommit": "bcfa925ee3d699e23f4e5596876f8d1089e961b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQzNDMxOA==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r465434318", "bodyText": "I've done this.", "author": "williamrandolph", "createdAt": "2020-08-05T02:25:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM2NTE4Nw=="}], "type": "inlineReview"}, {"oid": "59583e243e0589d18035be3973733fc44bd35d8a", "url": "https://github.com/elastic/elasticsearch/commit/59583e243e0589d18035be3973733fc44bd35d8a", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-07-22T17:25:53Z", "type": "commit"}, {"oid": "7fa4d07daadc6d0308d80d00668c79148ad46221", "url": "https://github.com/elastic/elasticsearch/commit/7fa4d07daadc6d0308d80d00668c79148ad46221", "message": "fixes", "committedDate": "2020-07-22T17:35:09Z", "type": "commit"}, {"oid": "c839bf7d8759790049d2f9762885259c735aaaa0", "url": "https://github.com/elastic/elasticsearch/commit/c839bf7d8759790049d2f9762885259c735aaaa0", "message": "aesthetics", "committedDate": "2020-07-22T17:35:42Z", "type": "commit"}, {"oid": "6a1be8cb80fadb6078dd0c8b913dec78ba0ed0c7", "url": "https://github.com/elastic/elasticsearch/commit/6a1be8cb80fadb6078dd0c8b913dec78ba0ed0c7", "message": "constants", "committedDate": "2020-07-22T17:38:48Z", "type": "commit"}, {"oid": "1cac8880bec12bbc0033c68389d2eb2be4cfc44b", "url": "https://github.com/elastic/elasticsearch/commit/1cac8880bec12bbc0033c68389d2eb2be4cfc44b", "message": "spotless", "committedDate": "2020-07-22T18:16:28Z", "type": "commit"}, {"oid": "45d0040ed051e77e77592b3b2236038204a0805f", "url": "https://github.com/elastic/elasticsearch/commit/45d0040ed051e77e77592b3b2236038204a0805f", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-07-31T19:44:12Z", "type": "commit"}, {"oid": "9f075cb26ffa616185cc6cbb275d17af081aadb8", "url": "https://github.com/elastic/elasticsearch/commit/9f075cb26ffa616185cc6cbb275d17af081aadb8", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-08-03T19:18:22Z", "type": "commit"}, {"oid": "2b3d16e7dd4420f40513297b33e880fae318f27e", "url": "https://github.com/elastic/elasticsearch/commit/2b3d16e7dd4420f40513297b33e880fae318f27e", "message": "Break out serialization tests into distinct classes", "committedDate": "2020-08-05T02:23:48Z", "type": "commit"}, {"oid": "0747a10804ba48feb3475f05a6637482ce0cbf70", "url": "https://github.com/elastic/elasticsearch/commit/0747a10804ba48feb3475f05a6637482ce0cbf70", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-08-05T02:24:32Z", "type": "commit"}, {"oid": "02d1cdadbe2f53ab33cf79fb275514945e3d349a", "url": "https://github.com/elastic/elasticsearch/commit/02d1cdadbe2f53ab33cf79fb275514945e3d349a", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-08-05T11:53:46Z", "type": "commit"}, {"oid": "de4060325b994f9f513e114c3f1f64214729dc1a", "url": "https://github.com/elastic/elasticsearch/commit/de4060325b994f9f513e114c3f1f64214729dc1a", "message": "Remove test of obsolete setting", "committedDate": "2020-08-05T20:29:07Z", "type": "commit"}, {"oid": "0d11fb353290544ab5ffd5dbb4d9b8aeb72d25f0", "url": "https://github.com/elastic/elasticsearch/commit/0d11fb353290544ab5ffd5dbb4d9b8aeb72d25f0", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-08-12T18:06:06Z", "type": "commit"}, {"oid": "453b2b1b97696d7248ebe2fa609a924037817466", "url": "https://github.com/elastic/elasticsearch/commit/453b2b1b97696d7248ebe2fa609a924037817466", "message": "Add test for pipeline multiget logic", "committedDate": "2020-08-13T00:51:52Z", "type": "commit"}, {"oid": "bc529ac7bdfe83948c8b9c68029902db337278e0", "url": "https://github.com/elastic/elasticsearch/commit/bc529ac7bdfe83948c8b9c68029902db337278e0", "message": "Log failures for partial multiget failure", "committedDate": "2020-08-13T20:59:28Z", "type": "commit"}, {"oid": "435df7b5afd4f9be33f2a5eecd325d83794c40b7", "url": "https://github.com/elastic/elasticsearch/commit/435df7b5afd4f9be33f2a5eecd325d83794c40b7", "message": "Clean up and add comments", "committedDate": "2020-08-14T19:22:26Z", "type": "commit"}, {"oid": "fa51c6270d6ca57129cf0464b25504c06ff26ed9", "url": "https://github.com/elastic/elasticsearch/commit/fa51c6270d6ca57129cf0464b25504c06ff26ed9", "message": "Fix Javadoc", "committedDate": "2020-08-18T16:23:51Z", "type": "commit"}, {"oid": "0e998033e4118013a48fcc3b5a14ef0e1c385355", "url": "https://github.com/elastic/elasticsearch/commit/0e998033e4118013a48fcc3b5a14ef0e1c385355", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-08-18T17:39:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODczMTM4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r478731389", "bodyText": "I don't think we need to change it for this API given it'll only be used by Logstash, but generally we're trying to move away from having dynamic keys like this because it makes API specs difficult to write.\n(no action item here, just to spread information)", "author": "gwbrown", "createdAt": "2020-08-27T22:31:23Z", "path": "x-pack/plugin/logstash/src/main/java/org/elasticsearch/xpack/logstash/action/GetPipelineResponse.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Objects;\n+\n+public class GetPipelineResponse extends ActionResponse implements ToXContentObject {\n+\n+    private final Map<String, BytesReference> pipelines;\n+\n+    public GetPipelineResponse(Map<String, BytesReference> pipelines) {\n+        this.pipelines = pipelines;\n+    }\n+\n+    public GetPipelineResponse(StreamInput in) throws IOException {\n+        super(in);\n+        this.pipelines = in.readMap(StreamInput::readString, StreamInput::readBytesReference);\n+    }\n+\n+    public Map<String, BytesReference> pipelines() {\n+        return pipelines;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeMap(pipelines, StreamOutput::writeString, StreamOutput::writeBytesReference);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        for (Entry<String, BytesReference> entry : pipelines.entrySet()) {\n+            builder.rawField(entry.getKey(), entry.getValue().streamInput());", "originalCommit": "0e998033e4118013a48fcc3b5a14ef0e1c385355", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODczNzU5MA==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r478737590", "bodyText": "You can remove this comment, the refresh API will allow system index access by default.", "author": "gwbrown", "createdAt": "2020-08-27T22:49:59Z", "path": "x-pack/plugin/src/test/java/org/elasticsearch/xpack/test/rest/LogstashSystemIndexIT.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.test.rest;\n+\n+import org.apache.http.util.EntityUtils;\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.ResponseException;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentHelper;\n+import org.elasticsearch.common.xcontent.XContentType;\n+import org.elasticsearch.common.xcontent.json.JsonXContent;\n+import org.elasticsearch.test.rest.ESRestTestCase;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.elasticsearch.xpack.test.rest.XPackRestIT.BASIC_AUTH_VALUE;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.is;\n+\n+public class LogstashSystemIndexIT extends ESRestTestCase {\n+\n+    @Override\n+    protected Settings restClientSettings() {\n+        return Settings.builder()\n+            .put(ThreadContext.PREFIX + \".Authorization\", BASIC_AUTH_VALUE)\n+            .build();\n+    }\n+\n+    public void testTemplateIsPut() throws Exception {\n+        assertBusy(\n+            () -> assertThat(\n+                client().performRequest(new Request(\"HEAD\", \"/_template/.logstash-management\")).getStatusLine().getStatusCode(),\n+                is(200)\n+            )\n+        );\n+    }\n+\n+    public void testPipelineCRUD() throws Exception {\n+        // put pipeline\n+        final String pipelineJson = getPipelineJson();\n+        createPipeline(\"test_pipeline\", pipelineJson);\n+\n+        // get pipeline\n+        Request getRequest = new Request(\"GET\", \"/_logstash/pipeline/test_pipeline\");\n+        Response getResponse = client().performRequest(getRequest);\n+        assertThat(getResponse.getStatusLine().getStatusCode(), is(200));\n+        assertThat(EntityUtils.toString(getResponse.getEntity()), containsString(pipelineJson));\n+\n+        // update\n+        final String updatedJson = getPipelineJson(\"2020-03-09T15:42:35.229Z\");\n+        Request putRequest = new Request(\"PUT\", \"/_logstash/pipeline/test_pipeline\");\n+        putRequest.setJsonEntity(updatedJson);\n+        Response putResponse = client().performRequest(putRequest);\n+        assertThat(putResponse.getStatusLine().getStatusCode(), is(200));\n+\n+        getRequest = new Request(\"GET\", \"/_logstash/pipeline/test_pipeline\");\n+        getResponse = client().performRequest(getRequest);\n+        assertThat(getResponse.getStatusLine().getStatusCode(), is(200));\n+        assertThat(EntityUtils.toString(getResponse.getEntity()), containsString(updatedJson));\n+\n+        // delete\n+        Request deleteRequest = new Request(\"DELETE\", \"/_logstash/pipeline/test_pipeline\");\n+        Response deleteResponse = client().performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), is(200));\n+\n+        // list is now empty\n+        Request listAll = new Request(\"GET\", \"/_logstash/pipeline\");\n+        Response listAllResponse = client().performRequest(listAll);\n+        assertThat(listAllResponse.getStatusLine().getStatusCode(), is(200));\n+        assertThat(EntityUtils.toString(listAllResponse.getEntity()), is(\"{}\"));\n+    }\n+\n+    public void testGetNonExistingPipeline() {\n+        Request getRequest = new Request(\"GET\", \"/_logstash/pipeline/test_pipeline\");\n+        ResponseException re = expectThrows(ResponseException.class, () -> client().performRequest(getRequest));\n+        Response getResponse = re.getResponse();\n+        assertThat(getResponse.getStatusLine().getStatusCode(), is(404));\n+    }\n+\n+    public void testDeleteNonExistingPipeline() {\n+        Request deleteRequest = new Request(\"DELETE\", \"/_logstash/pipeline/test_pipeline\");\n+        ResponseException re = expectThrows(ResponseException.class, () -> client().performRequest(deleteRequest));\n+        Response getResponse = re.getResponse();\n+        assertThat(getResponse.getStatusLine().getStatusCode(), is(404));\n+    }\n+\n+    public void testMultiplePipelines() throws IOException {\n+        final int numPipelines = scaledRandomIntBetween(2, 2000);\n+        final List<String> ids = new ArrayList<>(numPipelines);\n+        final String pipelineJson = getPipelineJson();\n+        for (int i = 0; i < numPipelines; i++) {\n+            final String id = \"id\" + i;\n+            ids.add(id);\n+            createPipeline(id, pipelineJson);\n+        }\n+\n+        // test mget-like\n+        final int numToGet = scaledRandomIntBetween(2, Math.min(100, numPipelines)); // limit number to avoid HTTP line length issues\n+        final List<String> mgetIds = randomSubsetOf(numToGet, ids);\n+        final String path = \"/_logstash/pipeline/\" + Strings.collectionToCommaDelimitedString(mgetIds);\n+        Request getRequest = new Request(\"GET\", path);\n+        Response getResponse = client().performRequest(getRequest);\n+        assertThat(getResponse.getStatusLine().getStatusCode(), is(200));\n+        Map<String, Object> responseMap = XContentHelper.convertToMap(\n+            XContentType.JSON.xContent(),\n+            EntityUtils.toString(getResponse.getEntity()),\n+            false\n+        );\n+\n+        for (String id : mgetIds) {\n+            assertTrue(responseMap.containsKey(id));\n+        }\n+\n+        // TODO need to update this after system indices are truly system indices to enable refresh", "originalCommit": "0e998033e4118013a48fcc3b5a14ef0e1c385355", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc0MzgxOA==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r478743818", "bodyText": "Can you make these request/response tests implement mutateInstance as well? That's required to automatically check .equals() for the case of an object of the same type that's not equal.", "author": "gwbrown", "createdAt": "2020-08-27T23:08:29Z", "path": "x-pack/plugin/logstash/src/test/java/org/elasticsearch/xpack/logstash/action/DeletePipelineRequestTests.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.logstash.action;\n+\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.test.AbstractWireSerializingTestCase;\n+\n+public class DeletePipelineRequestTests extends AbstractWireSerializingTestCase<DeletePipelineRequest> {", "originalCommit": "0e998033e4118013a48fcc3b5a14ef0e1c385355", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY0NjIxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53350#discussion_r486646219", "bodyText": "Done!", "author": "williamrandolph", "createdAt": "2020-09-10T21:33:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc0MzgxOA=="}], "type": "inlineReview"}, {"oid": "505104be5986f2e7890e0ef3265825d6109d05fe", "url": "https://github.com/elastic/elasticsearch/commit/505104be5986f2e7890e0ef3265825d6109d05fe", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-09-09T18:25:51Z", "type": "commit"}, {"oid": "b734883c7cd6e91fab4d18f8e6e68186f67658e7", "url": "https://github.com/elastic/elasticsearch/commit/b734883c7cd6e91fab4d18f8e6e68186f67658e7", "message": "Move LogstashSystemIndexIT to javaRestTest task", "committedDate": "2020-09-10T17:20:07Z", "type": "commit"}, {"oid": "551a789e20a0a111f9697e7bad7fe3c0683ed8fb", "url": "https://github.com/elastic/elasticsearch/commit/551a789e20a0a111f9697e7bad7fe3c0683ed8fb", "message": "Respond to PR feedback", "committedDate": "2020-09-10T21:17:58Z", "type": "commit"}, {"oid": "5f8a4c3aa409d5dd4165bcbccfd0e5733bb799fe", "url": "https://github.com/elastic/elasticsearch/commit/5f8a4c3aa409d5dd4165bcbccfd0e5733bb799fe", "message": "Remove unused imports, apply spotless", "committedDate": "2020-09-10T21:37:10Z", "type": "commit"}, {"oid": "11c6afb44003237739469316d6a915fa2599170d", "url": "https://github.com/elastic/elasticsearch/commit/11c6afb44003237739469316d6a915fa2599170d", "message": "Merge branch 'master' into logstash_system_index_plugin", "committedDate": "2020-09-14T20:24:33Z", "type": "commit"}]}