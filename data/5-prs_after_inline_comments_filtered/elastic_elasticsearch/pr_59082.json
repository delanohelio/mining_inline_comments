{"pr_number": 59082, "pr_title": "Share IT Infrastructure between Core Snapshot and SLM ITs", "pr_createdAt": "2020-07-06T15:09:58Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/59082", "timeline": [{"oid": "0c17c0f5e4f76c09289d461f9c4ed80cc4d2daec", "url": "https://github.com/elastic/elasticsearch/commit/0c17c0f5e4f76c09289d461f9c4ed80cc4d2daec", "message": "Share IT Infrastructure between Core Snapshot and SLM ITs\n\nFor #58994 it would be useful to be able to share test infrastructure.\nThis PR shares `AbstractSnapshotIntegTestCase` for that purpose, dries up SLM tests\naccordingly and adds a shared and efficient (compared to the previous implementations)\nway of waiting for no running snapshot operations to the test infrastructure to dry things up further.\n\nNote: the shared way of waiting for no more running operations was extracted from #56911", "committedDate": "2020-07-06T15:06:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NDY4MA==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450294680", "bodyText": "I like this a lot better than:\ndataNodeClient().admin().cluster().prepareState().get().getState();\n+ busy assert.\nOften times the tests use this kind of waiting when the busy assert will fail for a bit and then waste a second or two until the next run because of exponential back-off (over the large number of tests that do this kind of waiting for a certain CS it's well worth taking this approach IMO, especially with the concurrent snapshot ITs adding a large number of new tests that need this thing).\nAlso, the client() approach can (in disruption ITs) randomly go for the client of an isolated node and then waste effort and time for retrying in the transport master node action.", "author": "original-brownbear", "createdAt": "2020-07-06T15:17:47Z", "path": "test/framework/src/main/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java", "diffHunk": "@@ -366,4 +374,36 @@ protected long getCountForIndex(String indexName) {\n     protected void assertDocCount(String index, long count) {\n         assertEquals(getCountForIndex(index), count);\n     }\n+\n+    protected void awaitNoMoreRunningOperations(String viaNode) throws Exception {\n+        logger.info(\"--> verify no more operations in the cluster state\");\n+        awaitClusterState(viaNode, state -> state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries().isEmpty() &&\n+                state.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY).hasDeletionsInProgress() == false);\n+    }\n+\n+    private void awaitClusterState(String viaNode, Predicate<ClusterState> statePredicate) throws Exception {\n+        final ClusterService clusterService = internalCluster().getInstance(ClusterService.class, viaNode);\n+        final ThreadPool threadPool = internalCluster().getInstance(ThreadPool.class, viaNode);\n+        final ClusterStateObserver observer = new ClusterStateObserver(clusterService, logger, threadPool.getThreadContext());\n+        if (statePredicate.test(observer.setAndGetObservedState()) == false) {", "originalCommit": "0c17c0f5e4f76c09289d461f9c4ed80cc4d2daec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY3NDk4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450674986", "bodyText": "I understand the motivation; I'm wondering if it should always wait for next change?", "author": "tlrx", "createdAt": "2020-07-07T07:51:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NDY4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY4NzAzMA==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450687030", "bodyText": "I think waiting for the next change would make it very hard to avoid races. We often have this pattern:\n\ndo operation\nwait for no more operations running\n\nIf the first step completes before we start waiting we dead-lock. And this is just one example, the concurrent snapshotting tests make use of this logic in other spots where similar races could occur", "author": "original-brownbear", "createdAt": "2020-07-07T08:14:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NDY4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY5OTMzNA==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450699334", "bodyText": "Ok. I was wondering if something similar could happen: 1. do operation and 2. check cluster state on a data node that has not yet processed the updated cluster state", "author": "tlrx", "createdAt": "2020-07-07T08:35:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NDY4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDcwMTYzMA==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450701630", "bodyText": "\ud83d\udc4d yea that was an issue in the concurrency tests, in the end it just requires ensuring that stuff actually started by some other means before waiting for it to go away :)", "author": "original-brownbear", "createdAt": "2020-07-07T08:38:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NDY4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDcwMzY1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450703653", "bodyText": "Good! Thanks", "author": "tlrx", "createdAt": "2020-07-07T08:42:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NDY4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NTU3MA==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450295570", "bodyText": "All of these methods were just copies from AbstractSnapshotIntegTestCase that's why no other changes to the code were needed here.", "author": "original-brownbear", "createdAt": "2020-07-06T15:19:07Z", "path": "x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/slm/SLMSnapshotBlockingIntegTests.java", "diffHunk": "@@ -522,49 +487,4 @@ private String executePolicy(String policyId) {\n             return \"bad\";\n         }\n     }\n-\n-    public static String blockMasterFromFinalizingSnapshotOnIndexFile(final String repositoryName) {\n-        final String masterName = internalCluster().getMasterName();\n-        ((MockRepository)internalCluster().getInstance(RepositoriesService.class, masterName)\n-            .repository(repositoryName)).setBlockOnWriteIndexFile(true);\n-        return masterName;\n-    }\n-\n-    public static String unblockRepo(final String repositoryName) {\n-        final String masterName = internalCluster().getMasterName();\n-        ((MockRepository)internalCluster().getInstance(RepositoriesService.class, masterName)\n-            .repository(repositoryName)).unblock();\n-        return masterName;\n-    }\n-\n-    public static void blockAllDataNodes(String repository) {\n-        for(RepositoriesService repositoriesService : internalCluster().getDataNodeInstances(RepositoriesService.class)) {\n-            ((MockRepository)repositoriesService.repository(repository)).blockOnDataFiles(true);\n-        }\n-    }\n-\n-    public static void unblockAllDataNodes(String repository) {", "originalCommit": "0c17c0f5e4f76c09289d461f9c4ed80cc4d2daec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI5NjUxNw==", "url": "https://github.com/elastic/elasticsearch/pull/59082#discussion_r450296517", "bodyText": "Needed to add a new wait here because the AbstractSnapshotIntegTestCase does some repo consistency checks in after the tests (well worth it to have these here anyway) which will break if there's still work done in the cluster (which may be the case in this test).", "author": "original-brownbear", "createdAt": "2020-07-06T15:20:29Z", "path": "x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/slm/SLMSnapshotBlockingIntegTests.java", "diffHunk": "@@ -381,18 +368,14 @@ private void testUnsuccessfulSnapshotRetention(boolean partialSuccess) throws Ex\n                 assertEquals(SnapshotState.SUCCESS, snapshotInfo.state());\n             });\n         }\n+        awaitNoMoreRunningOperations(internalCluster().getMasterName());", "originalCommit": "0c17c0f5e4f76c09289d461f9c4ed80cc4d2daec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}