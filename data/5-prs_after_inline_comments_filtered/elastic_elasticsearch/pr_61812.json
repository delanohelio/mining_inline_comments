{"pr_number": 61812, "pr_title": "Faster Azure Blob InputStream", "pr_createdAt": "2020-09-01T18:15:13Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/61812", "timeline": [{"oid": "7e20aece4ac2ee8a513d1e941d9149fae9a99baf", "url": "https://github.com/elastic/elasticsearch/commit/7e20aece4ac2ee8a513d1e941d9149fae9a99baf", "message": "Faster Azure Blob InputStream\n\nBuilding our own that should perform better than the one in the SDK.", "committedDate": "2020-09-02T08:01:14Z", "type": "commit"}, {"oid": "552374293d8ab8bfa7d5ad8aca9ce7732aff3a53", "url": "https://github.com/elastic/elasticsearch/commit/552374293d8ab8bfa7d5ad8aca9ce7732aff3a53", "message": "little smarter", "committedDate": "2020-09-02T08:01:14Z", "type": "commit"}, {"oid": "38f516b6c3968eb1510a9835efdf03d44a5dd49d", "url": "https://github.com/elastic/elasticsearch/commit/38f516b6c3968eb1510a9835efdf03d44a5dd49d", "message": "16M", "committedDate": "2020-09-02T08:01:14Z", "type": "commit"}, {"oid": "9c297734ad7b669ce72006f23f82ad699511bb21", "url": "https://github.com/elastic/elasticsearch/commit/9c297734ad7b669ce72006f23f82ad699511bb21", "message": "way faster", "committedDate": "2020-09-02T08:01:14Z", "type": "commit"}, {"oid": "cddef3dae9e54d445cd49bc0b710ef3282ab2da7", "url": "https://github.com/elastic/elasticsearch/commit/cddef3dae9e54d445cd49bc0b710ef3282ab2da7", "message": "fixed", "committedDate": "2020-09-02T08:01:14Z", "type": "commit"}, {"oid": "072a1cc42de97d850979d6550af09609271ee302", "url": "https://github.com/elastic/elasticsearch/commit/072a1cc42de97d850979d6550af09609271ee302", "message": "comment", "committedDate": "2020-09-02T08:25:19Z", "type": "commit"}, {"oid": "072a1cc42de97d850979d6550af09609271ee302", "url": "https://github.com/elastic/elasticsearch/commit/072a1cc42de97d850979d6550af09609271ee302", "message": "comment", "committedDate": "2020-09-02T08:25:19Z", "type": "forcePushed"}, {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "url": "https://github.com/elastic/elasticsearch/commit/2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "message": "bck", "committedDate": "2020-09-02T08:51:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwNjU0NA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481906544", "bodyText": "The SDK wraps buffers in https://github.com/Azure/azure-storage-java/blob/legacy-master/microsoft-azure-storage/src/com/microsoft/azure/storage/core/WrappedByteArrayOutputStream.java#L24 which will do singly byte copies for every byte! Me and @hub-cap found this to take macroscopic time (~10% of the runtime when reading from the stream) so this hack is well worth it IMO.", "author": "original-brownbear", "createdAt": "2020-09-02T08:54:03Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {\n+\n+        /**\n+         * Maximum number of bytes to fetch per read request and thus to buffer on heap at a time.\n+         * Set to 4M because that's what {@link com.microsoft.azure.storage.blob.BlobInputStream} uses.\n+         */\n+        private static final int MAX_READ_CHUNK_SIZE = ByteSizeUnit.MB.toIntBytes(4);\n+\n+        /**\n+         * Using a {@link ByteArrayOutputStream} as a buffer instead of a byte array since the byte array APIs on the SDK are less\n+         * efficient.\n+         */\n+        private final ByteArrayOutputStream buffer;", "originalCommit": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwNzI2OA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481907268", "bodyText": "Still horrific to allocate 4M on heap but unlike the SDKs stream we at least reuse these bytes instead of allocating them over and over for larger blobs.", "author": "original-brownbear", "createdAt": "2020-09-02T08:54:50Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {\n+\n+        /**\n+         * Maximum number of bytes to fetch per read request and thus to buffer on heap at a time.\n+         * Set to 4M because that's what {@link com.microsoft.azure.storage.blob.BlobInputStream} uses.\n+         */\n+        private static final int MAX_READ_CHUNK_SIZE = ByteSizeUnit.MB.toIntBytes(4);\n+\n+        /**\n+         * Using a {@link ByteArrayOutputStream} as a buffer instead of a byte array since the byte array APIs on the SDK are less\n+         * efficient.\n+         */\n+        private final ByteArrayOutputStream buffer;\n+\n+        private final long limit;\n+\n+        private final CloudBlockBlob blockBlobReference;\n+\n+        private final long start;\n+\n+        private final OperationContext context;\n+\n+        // current read position on the byte array backing #buffer\n+        private int pos;\n+\n+        // current position up to which the contents of the blob where buffered\n+        private long offset;\n+\n+        BlobInputStream(long limit, CloudBlockBlob blockBlobReference, long start, OperationContext context) {\n+            this.limit = limit;\n+            this.blockBlobReference = blockBlobReference;\n+            this.start = start;\n+            this.context = context;\n+            buffer = new ByteArrayOutputStream(Math.min(MAX_READ_CHUNK_SIZE, Math.toIntExact(Math.min(limit, Integer.MAX_VALUE)))) {", "originalCommit": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk3MzkwOA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481973908", "bodyText": "Maybe we can write our own ByteArrayOutputStream version as all methods are synchronized and buffer.toByteArray() returns a copy that we copy again?.", "author": "fcofdez", "createdAt": "2020-09-02T10:43:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwNzI2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk3NjU5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481976596", "bodyText": "I do override toByteArray below to prevent that? :)\nAs far as the synchronized goes, I'm not sure it's worth the extra code. I would expect JIT to take care of that since we're never concurrently accessing the BAOS. Probably not worth the extra code? (also when I profiled this with @hub-cap the copying into buffer took trivial time only so not much to gain anyway)", "author": "original-brownbear", "createdAt": "2020-09-02T10:47:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwNzI2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk3NzkwMA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481977900", "bodyText": "Right, I missed that \ud83e\udd26. Thanks for clarifying :)", "author": "fcofdez", "createdAt": "2020-09-02T10:50:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwNzI2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwODE4MA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481908180", "bodyText": "Duplicating the exception handling here because we want the original exception unwrapped in the initial fill() call for ranged reads so that we correctly bubble up 404s.", "author": "original-brownbear", "createdAt": "2020-09-02T08:55:53Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {\n+\n+        /**\n+         * Maximum number of bytes to fetch per read request and thus to buffer on heap at a time.\n+         * Set to 4M because that's what {@link com.microsoft.azure.storage.blob.BlobInputStream} uses.\n+         */\n+        private static final int MAX_READ_CHUNK_SIZE = ByteSizeUnit.MB.toIntBytes(4);\n+\n+        /**\n+         * Using a {@link ByteArrayOutputStream} as a buffer instead of a byte array since the byte array APIs on the SDK are less\n+         * efficient.\n+         */\n+        private final ByteArrayOutputStream buffer;\n+\n+        private final long limit;\n+\n+        private final CloudBlockBlob blockBlobReference;\n+\n+        private final long start;\n+\n+        private final OperationContext context;\n+\n+        // current read position on the byte array backing #buffer\n+        private int pos;\n+\n+        // current position up to which the contents of the blob where buffered\n+        private long offset;\n+\n+        BlobInputStream(long limit, CloudBlockBlob blockBlobReference, long start, OperationContext context) {\n+            this.limit = limit;\n+            this.blockBlobReference = blockBlobReference;\n+            this.start = start;\n+            this.context = context;\n+            buffer = new ByteArrayOutputStream(Math.min(MAX_READ_CHUNK_SIZE, Math.toIntExact(Math.min(limit, Integer.MAX_VALUE)))) {\n+                @Override\n+                public byte[] toByteArray() {\n+                    return buf;\n+                }\n+            };\n+            pos = 0;\n+            offset = 0;\n+        }\n+\n+        @Override\n+        public int read() throws IOException {\n+            try {\n+                fill();\n+            } catch (StorageException | URISyntaxException ex) {", "originalCommit": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwODYwMA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481908600", "bodyText": "This should be a neat win for searchable snapshots IMO.", "author": "original-brownbear", "createdAt": "2020-09-02T08:56:12Z", "path": "plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobContainerRetriesTests.java", "diffHunk": "@@ -217,20 +217,13 @@ public void testReadBlobWithRetries() throws Exception {\n \n     public void testReadRangeBlobWithRetries() throws Exception {\n         final int maxRetries = randomIntBetween(1, 5);\n-        final CountDown countDownHead = new CountDown(maxRetries);\n         final CountDown countDownGet = new CountDown(maxRetries);\n         final byte[] bytes = randomBlobContent();\n         httpServer.createContext(\"/container/read_range_blob_max_retries\", exchange -> {\n             try {\n                 Streams.readFully(exchange.getRequestBody());\n                 if (\"HEAD\".equals(exchange.getRequestMethod())) {\n-                    if (countDownHead.countDown()) {\n-                        exchange.getResponseHeaders().add(\"Content-Type\", \"application/octet-stream\");\n-                        exchange.getResponseHeaders().add(\"x-ms-blob-content-length\", String.valueOf(bytes.length));\n-                        exchange.getResponseHeaders().add(\"x-ms-blob-type\", \"blockblob\");\n-                        exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1);\n-                        return;\n-                    }\n+                    throw new AssertionError(\"Should not HEAD blob for ranged reads\");", "originalCommit": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkxOTkwMA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481919900", "bodyText": "Yeah, I can't understand why the SDK is executing HEAD before every GET :/", "author": "tlrx", "createdAt": "2020-09-02T09:08:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwODYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkyMzgwOA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481923808", "bodyText": "Technically speaking, we could avoid this whole dance and make the Azure download as fast as those in GCS and S3 by offering a different blob container API that consumes an OutputStream, that would do away with the chunked reading and make things fast (far as I can tell, haven't tried it yet). An annoying hack for sure, but probably less effort+risk than upgrading to SDK v12. For now this is the best I could come up with from the profiling @hub-cap did.", "author": "original-brownbear", "createdAt": "2020-09-02T09:14:29Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {", "originalCommit": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk3NTc4MA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481975780", "bodyText": "I did some work to upgrade to SDK v12, but since it wasn't a priority I've been working on other things. I did some small benchmarks and the difference was notable, we can revisit the priority for future releases as all the past blockers are solved with the latest SDK versions.", "author": "fcofdez", "createdAt": "2020-09-02T10:46:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkyMzgwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk3NzMwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481977301", "bodyText": "Yea this certainly isn't meant as a long term solution. Just a stop-gap to :)", "author": "original-brownbear", "createdAt": "2020-09-02T10:49:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkyMzgwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYyNjQzMw==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r483626433", "bodyText": "space missing", "author": "ywelsch", "createdAt": "2020-09-04T13:47:38Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -245,9 +246,22 @@ public InputStream getInputStream(String blob, long position, @Nullable Long len\n         final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n         final CloudBlockBlob blockBlobReference = client.v1().getContainerReference(container).getBlockBlobReference(blob);\n         logger.trace(() -> new ParameterizedMessage(\"reading container [{}], blob [{}]\", container, blob));\n-        final BlobInputStream is = SocketAccess.doPrivilegedException(() ->\n-            blockBlobReference.openInputStream(position, length, null, null, context));\n-        return giveSocketPermissionsToStream(is);\n+        final long limit;\n+        if (length == null){", "originalCommit": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYzMDE3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r483630177", "bodyText": "This still executes a HEAD request?\nHow is length used afterwards?\nCould we always specify Long.MAX_VALUE here to avoid this call?", "author": "ywelsch", "createdAt": "2020-09-04T13:53:57Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -245,9 +246,22 @@ public InputStream getInputStream(String blob, long position, @Nullable Long len\n         final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n         final CloudBlockBlob blockBlobReference = client.v1().getContainerReference(container).getBlockBlobReference(blob);\n         logger.trace(() -> new ParameterizedMessage(\"reading container [{}], blob [{}]\", container, blob));\n-        final BlobInputStream is = SocketAccess.doPrivilegedException(() ->\n-            blockBlobReference.openInputStream(position, length, null, null, context));\n-        return giveSocketPermissionsToStream(is);\n+        final long limit;\n+        if (length == null){\n+            // Loading the blob attributes so we can get its length\n+            SocketAccess.doPrivilegedVoidException(() -> blockBlobReference.downloadAttributes(null, null, context));", "originalCommit": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzY3NTMzMA==", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r483675330", "bodyText": "We need it to correctly size the chunk's length to download. If we don't know the length and do an open ended request we could fill up the BAOS with the whole blob.", "author": "original-brownbear", "createdAt": "2020-09-04T15:02:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYzMDE3Nw=="}], "type": "inlineReview"}, {"oid": "0a20c0b67261538be59b367f36ccc5647d711bd7", "url": "https://github.com/elastic/elasticsearch/commit/0a20c0b67261538be59b367f36ccc5647d711bd7", "message": "Merge remote-tracking branch 'elastic/master' into faster-azure-reads-maybe", "committedDate": "2020-09-04T14:58:16Z", "type": "commit"}, {"oid": "876d2de332fc6b215fb3eb64667d5d4674bf32da", "url": "https://github.com/elastic/elasticsearch/commit/876d2de332fc6b215fb3eb64667d5d4674bf32da", "message": "CR: whitespace", "committedDate": "2020-09-04T14:58:40Z", "type": "commit"}, {"oid": "0c91d87097e6b25804233c7cdc21f8d3f9f3874c", "url": "https://github.com/elastic/elasticsearch/commit/0c91d87097e6b25804233c7cdc21f8d3f9f3874c", "message": "Merge remote-tracking branch 'elastic/master' into faster-azure-reads-maybe", "committedDate": "2020-09-05T07:26:58Z", "type": "commit"}]}