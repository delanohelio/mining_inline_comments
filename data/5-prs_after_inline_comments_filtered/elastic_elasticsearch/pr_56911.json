{"pr_number": 56911, "pr_title": "Enable Fully Concurrent Snapshot Operations", "pr_createdAt": "2020-05-18T13:43:58Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/56911", "timeline": [{"oid": "7627a3076d2867514fbb22974ae1ea4db6ea9a42", "url": "https://github.com/elastic/elasticsearch/commit/7627a3076d2867514fbb22974ae1ea4db6ea9a42", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-24T16:38:08Z", "type": "commit"}, {"oid": "c951e3bc35effe08f4ee374289853e98d28c3b2d", "url": "https://github.com/elastic/elasticsearch/commit/c951e3bc35effe08f4ee374289853e98d28c3b2d", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-25T18:57:11Z", "type": "commit"}, {"oid": "cedcd8838e4a8f6940a0168f444929d0faebb412", "url": "https://github.com/elastic/elasticsearch/commit/cedcd8838e4a8f6940a0168f444929d0faebb412", "message": "resolve some conflicts", "committedDate": "2020-06-25T20:43:03Z", "type": "commit"}, {"oid": "aa02752aef9e51f22d58c947777139febfdf939b", "url": "https://github.com/elastic/elasticsearch/commit/aa02752aef9e51f22d58c947777139febfdf939b", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-26T10:35:59Z", "type": "commit"}, {"oid": "5bf38b7f754703014ff257015c3835e8d2d1a0e3", "url": "https://github.com/elastic/elasticsearch/commit/5bf38b7f754703014ff257015c3835e8d2d1a0e3", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-26T12:59:32Z", "type": "commit"}, {"oid": "3eef5af7b5e02fff40b78faab050a21b7cd57236", "url": "https://github.com/elastic/elasticsearch/commit/3eef5af7b5e02fff40b78faab050a21b7cd57236", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-26T13:31:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMDIxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446210219", "bodyText": "can we assert that this is always greater than the current gen? Maybe that it's +1?", "author": "ywelsch", "createdAt": "2020-06-26T14:14:03Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -175,6 +175,16 @@ public Entry(Entry entry, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards)\n             this(entry, entry.state, shards, entry.failure);\n         }\n \n+        public Entry withRepoGen(long newRepoGen) {\n+            return new Entry(snapshot, includeGlobalState, partial, state, indices, dataStreams, startTime, newRepoGen, shards, failure,", "originalCommit": "cedcd8838e4a8f6940a0168f444929d0faebb412", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjgzNDc5OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446834798", "bodyText": "Sure can, but it's not always +1 (on master fail-over during finalization or delete we will skip a value of N here to avoid overwriting index-N)", "author": "original-brownbear", "createdAt": "2020-06-29T07:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMDIxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyNTQ5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446225496", "bodyText": "should we make this method static?", "author": "ywelsch", "createdAt": "2020-06-26T14:40:12Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1614,6 +1613,45 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerationsIfNecessary(ClusterState state, long oldGen, long newGen) {", "originalCommit": "cedcd8838e4a8f6940a0168f444929d0faebb412", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NjAwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446776009", "bodyText": "We're using this.metadata, found it nicer to use the field here than to pass in the name?", "author": "original-brownbear", "createdAt": "2020-06-29T05:13:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyNTQ5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI1NjEwMg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446256102", "bodyText": "Should we also test for the situation where a repository is mounted multiple times under different name, but same path, and check that the repo does not become corrupted when concurrently accessed?", "author": "ywelsch", "createdAt": "2020-06-26T15:32:37Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,971 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateObserver;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {", "originalCommit": "cedcd8838e4a8f6940a0168f444929d0faebb412", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjgzMzk4OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446833988", "bodyText": "We could but the result won't hold up for S3 anyway in edge cases (and we have checks for concurrent repo modification already e.g. here). I wonder if this is a good time to forbid mounting multiple writable repositories to the same path instead? (it's not too hard to do this on a best-effort basis I think and should prevent corruption issues from this kind of scenario in pretty much all realistic scenarios)", "author": "original-brownbear", "createdAt": "2020-06-29T07:47:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI1NjEwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3Nzk5NA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449477994", "bodyText": "I wonder if instead of forbidding, different repo definitions pointing to the same path should be treated the same way as if they were the same repo (i.e. concurrency limitations). Perhaps a first overapproximation would be to treat every repo as if it was the same (i.e. remove the \"isSameRepo\" checks in this PR).\nMost users are making use of one repository, not many. They get concurrency benefits with this PR even if it treats every repo as being the same. We can open the system to more concurrency possibly later then once we have a stronger notion of \"same repo\"?\nWDYT?", "author": "ywelsch", "createdAt": "2020-07-03T09:21:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI1NjEwMg=="}], "type": "inlineReview"}, {"oid": "cc88a1c6b50b83be2277385ce618473005378c9e", "url": "https://github.com/elastic/elasticsearch/commit/cc88a1c6b50b83be2277385ce618473005378c9e", "message": "changes", "committedDate": "2020-06-26T17:57:30Z", "type": "commit"}, {"oid": "72c4019fb07e939b32561583e4ea7a45b354c063", "url": "https://github.com/elastic/elasticsearch/commit/72c4019fb07e939b32561583e4ea7a45b354c063", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-26T18:04:28Z", "type": "commit"}, {"oid": "095023c23a65ff4583bd7fa031937b088c4a7ebc", "url": "https://github.com/elastic/elasticsearch/commit/095023c23a65ff4583bd7fa031937b088c4a7ebc", "message": "fix", "committedDate": "2020-06-26T18:07:59Z", "type": "commit"}, {"oid": "10092979aacea40b6d173705d575ffeb62df0f2e", "url": "https://github.com/elastic/elasticsearch/commit/10092979aacea40b6d173705d575ffeb62df0f2e", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-27T08:37:04Z", "type": "commit"}, {"oid": "291e5f55c4e3af55136e3af82363e81be1cef447", "url": "https://github.com/elastic/elasticsearch/commit/291e5f55c4e3af55136e3af82363e81be1cef447", "message": "fix corner case", "committedDate": "2020-06-27T15:10:02Z", "type": "commit"}, {"oid": "737cd485253a7fc19bae20200a0c303ae6670d43", "url": "https://github.com/elastic/elasticsearch/commit/737cd485253a7fc19bae20200a0c303ae6670d43", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-29T05:11:50Z", "type": "commit"}, {"oid": "86d00719e3fcf6fbde9039c00d5e0a86de994b0f", "url": "https://github.com/elastic/elasticsearch/commit/86d00719e3fcf6fbde9039c00d5e0a86de994b0f", "message": "rename", "committedDate": "2020-06-29T05:17:09Z", "type": "commit"}, {"oid": "d09f68e7fae97ce830b6033d3afbe24b576e102a", "url": "https://github.com/elastic/elasticsearch/commit/d09f68e7fae97ce830b6033d3afbe24b576e102a", "message": "add assertion", "committedDate": "2020-06-29T07:39:24Z", "type": "commit"}, {"oid": "3e7662f15abf9cac9b123f5bfd82d1bc56f2ef85", "url": "https://github.com/elastic/elasticsearch/commit/3e7662f15abf9cac9b123f5bfd82d1bc56f2ef85", "message": "add assertion", "committedDate": "2020-06-29T07:57:55Z", "type": "commit"}, {"oid": "974bd62ec687e5cde0b08adc6b847762440c7107", "url": "https://github.com/elastic/elasticsearch/commit/974bd62ec687e5cde0b08adc6b847762440c7107", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-29T09:02:32Z", "type": "commit"}, {"oid": "e388a6e1bbd26487e3384c68e2f360de7649ad7c", "url": "https://github.com/elastic/elasticsearch/commit/e388a6e1bbd26487e3384c68e2f360de7649ad7c", "message": "add queued state for shard snapshots", "committedDate": "2020-06-29T10:56:28Z", "type": "commit"}, {"oid": "647e2ed525bb8ebfdc652523b3a3ad5aa879b6c8", "url": "https://github.com/elastic/elasticsearch/commit/647e2ed525bb8ebfdc652523b3a3ad5aa879b6c8", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-29T11:36:48Z", "type": "commit"}, {"oid": "4b63b2f7fef1586c538a26576a497539f05f29fa", "url": "https://github.com/elastic/elasticsearch/commit/4b63b2f7fef1586c538a26576a497539f05f29fa", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-29T12:41:51Z", "type": "commit"}, {"oid": "f84c9b717cbcc386223ec75e25546014dab26ece", "url": "https://github.com/elastic/elasticsearch/commit/f84c9b717cbcc386223ec75e25546014dab26ece", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-29T15:34:29Z", "type": "commit"}, {"oid": "fba7536e206203f4988c014ee09d9836737fcba7", "url": "https://github.com/elastic/elasticsearch/commit/fba7536e206203f4988c014ee09d9836737fcba7", "message": "50% fix", "committedDate": "2020-06-29T20:43:34Z", "type": "commit"}, {"oid": "878fa07d6a108562e68dcc1e2eee48464fb336ac", "url": "https://github.com/elastic/elasticsearch/commit/878fa07d6a108562e68dcc1e2eee48464fb336ac", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-30T08:08:31Z", "type": "commit"}, {"oid": "5b4041628d1c4debb5bee92ce9e2314999f33c97", "url": "https://github.com/elastic/elasticsearch/commit/5b4041628d1c4debb5bee92ce9e2314999f33c97", "message": "fixes", "committedDate": "2020-06-30T08:37:29Z", "type": "commit"}, {"oid": "cefd32328b71daae7bd313f64e188d3f82200d3e", "url": "https://github.com/elastic/elasticsearch/commit/cefd32328b71daae7bd313f64e188d3f82200d3e", "message": "fix", "committedDate": "2020-06-30T09:54:31Z", "type": "commit"}, {"oid": "94d4608ed49554c337b60df40c1831cef8b8a0b4", "url": "https://github.com/elastic/elasticsearch/commit/94d4608ed49554c337b60df40c1831cef8b8a0b4", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-30T10:15:47Z", "type": "commit"}, {"oid": "d7674a69154e34c4992b51e9566d860c333e5514", "url": "https://github.com/elastic/elasticsearch/commit/d7674a69154e34c4992b51e9566d860c333e5514", "message": "moar coverage", "committedDate": "2020-06-30T10:47:03Z", "type": "commit"}, {"oid": "dcfc8fd85354fb37409d1331381df651bfa962ab", "url": "https://github.com/elastic/elasticsearch/commit/dcfc8fd85354fb37409d1331381df651bfa962ab", "message": "moar coverage", "committedDate": "2020-06-30T11:22:58Z", "type": "commit"}, {"oid": "d8a6b25c11edabc47171f1ffd8bcbb39815b57e3", "url": "https://github.com/elastic/elasticsearch/commit/d8a6b25c11edabc47171f1ffd8bcbb39815b57e3", "message": "docs", "committedDate": "2020-06-30T11:24:39Z", "type": "commit"}, {"oid": "6d97b421de62a1147144dcc24468442dd475c842", "url": "https://github.com/elastic/elasticsearch/commit/6d97b421de62a1147144dcc24468442dd475c842", "message": "even moar coverage", "committedDate": "2020-06-30T12:38:24Z", "type": "commit"}, {"oid": "f74e4d1af9f9e476e37f30b76cc93f6b55acbb3d", "url": "https://github.com/elastic/elasticsearch/commit/f74e4d1af9f9e476e37f30b76cc93f6b55acbb3d", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-30T12:38:36Z", "type": "commit"}, {"oid": "217137d88deb906dd740ca1250d87de48b27533e", "url": "https://github.com/elastic/elasticsearch/commit/217137d88deb906dd740ca1250d87de48b27533e", "message": "fix", "committedDate": "2020-06-30T12:47:57Z", "type": "commit"}, {"oid": "68aaa8ecea064a715fe0965def2260cd3e40371b", "url": "https://github.com/elastic/elasticsearch/commit/68aaa8ecea064a715fe0965def2260cd3e40371b", "message": "cover another tricky corner case", "committedDate": "2020-06-30T13:26:15Z", "type": "commit"}, {"oid": "cc36ffde9cd83d9ffc45431ea3942bafe825cfd4", "url": "https://github.com/elastic/elasticsearch/commit/cc36ffde9cd83d9ffc45431ea3942bafe825cfd4", "message": "and another corner case", "committedDate": "2020-06-30T13:37:31Z", "type": "commit"}, {"oid": "e2bfadd6b7ba493850ea0159876ccedc9cecde10", "url": "https://github.com/elastic/elasticsearch/commit/e2bfadd6b7ba493850ea0159876ccedc9cecde10", "message": "almost full coverage", "committedDate": "2020-06-30T14:03:25Z", "type": "commit"}, {"oid": "0afdbb52ca19ffba71de23b7a6eceb7cc87fc687", "url": "https://github.com/elastic/elasticsearch/commit/0afdbb52ca19ffba71de23b7a6eceb7cc87fc687", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-30T14:03:45Z", "type": "commit"}, {"oid": "b8c06c9db8c8b41d723c8a7fb1287a9abed6636c", "url": "https://github.com/elastic/elasticsearch/commit/b8c06c9db8c8b41d723c8a7fb1287a9abed6636c", "message": "another failure handler tested", "committedDate": "2020-06-30T14:28:33Z", "type": "commit"}, {"oid": "2f44a8f7f35ef0693521b75e0cd637dd2455dcd8", "url": "https://github.com/elastic/elasticsearch/commit/2f44a8f7f35ef0693521b75e0cd637dd2455dcd8", "message": "fix bugs", "committedDate": "2020-06-30T15:29:09Z", "type": "commit"}, {"oid": "b6cdba2e96e73b422214c384ffc5387ae3ce47de", "url": "https://github.com/elastic/elasticsearch/commit/b6cdba2e96e73b422214c384ffc5387ae3ce47de", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-01T00:24:47Z", "type": "commit"}, {"oid": "9c65059140535141282c739293088c4eda30fbae", "url": "https://github.com/elastic/elasticsearch/commit/9c65059140535141282c739293088c4eda30fbae", "message": "docs and simpler", "committedDate": "2020-07-01T01:36:58Z", "type": "commit"}, {"oid": "355432b063ee01f78113c23bfca565a4d490d9d5", "url": "https://github.com/elastic/elasticsearch/commit/355432b063ee01f78113c23bfca565a4d490d9d5", "message": "cover and fix very edgy edge case", "committedDate": "2020-07-01T02:57:13Z", "type": "commit"}, {"oid": "8cf751acbe431dcba5d31a978a577a2c6f1a1447", "url": "https://github.com/elastic/elasticsearch/commit/8cf751acbe431dcba5d31a978a577a2c6f1a1447", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-02T12:51:18Z", "type": "commit"}, {"oid": "773f90529ad292aa08a12a1312cf9af09eb0b631", "url": "https://github.com/elastic/elasticsearch/commit/773f90529ad292aa08a12a1312cf9af09eb0b631", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-02T13:00:32Z", "type": "commit"}, {"oid": "de169f0c3f2735235a2ee7684602d00c356a63c9", "url": "https://github.com/elastic/elasticsearch/commit/de169f0c3f2735235a2ee7684602d00c356a63c9", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-02T13:57:07Z", "type": "commit"}, {"oid": "945ef0cbf491f1fc3bd5870bfd61d70b421cc407", "url": "https://github.com/elastic/elasticsearch/commit/945ef0cbf491f1fc3bd5870bfd61d70b421cc407", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-02T14:05:23Z", "type": "commit"}, {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4", "url": "https://github.com/elastic/elasticsearch/commit/81fdf77e28c32ce4748cc315758ca517995329e4", "message": "fix conflict", "committedDate": "2020-07-02T14:14:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwODAwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449408009", "bodyText": "s/then/than", "author": "ywelsch", "createdAt": "2020-07-03T06:54:05Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -151,6 +152,18 @@ public Entry(Entry entry, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards)\n             this(entry, entry.state, shards, entry.failure);\n         }\n \n+        public Entry withRepoGen(long newRepoGen) {\n+            assert newRepoGen > repositoryStateId : \"Updated repository generation [\" + newRepoGen\n+                    + \"] must be higher then current generation [\" + repositoryStateId + \"]\";", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxNzgwMg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449417802", "bodyText": "Should we check that at the end of integration tests this is always properly cleaned up (same for the other state in this class)?\nContext for my question is that I'm wondering if all of this is always properly cleaned up on master failover", "author": "ywelsch", "createdAt": "2020-07-03T07:18:31Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -134,6 +141,14 @@\n     private final Map<Snapshot, List<ActionListener<Tuple<RepositoryData, SnapshotInfo>>>> snapshotCompletionListeners =\n         new ConcurrentHashMap<>();\n \n+    /**\n+     * Listeners for snapshot deletion keyed by delete uuid as returned from {@link SnapshotDeletionsInProgress.Entry#uuid()}\n+     */\n+    private final Map<String, List<ActionListener<Void>>> snapshotDeletionListeners = new HashMap<>();\n+\n+    //Set of repositories currently running either a snapshot finalization or a snapshot delete.\n+    private final Set<String> currentlyFinalizing = Collections.synchronizedSet(new HashSet<>());", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUzOTc4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449539782", "bodyText": "We do check that by running the assertion org.elasticsearch.snapshots.SnapshotsService#assertAllListenersResolved after each IT or are you looking for a different kind of check?", "author": "original-brownbear", "createdAt": "2020-07-03T11:43:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxNzgwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU0MTc4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449541782", "bodyText": "yeah, that only covers the listeners. I would like to cover also the  remaining  state here (i.e. snapshotDeletionListeners is empty, currentlyFinalizing is empty, ...)", "author": "ywelsch", "createdAt": "2020-07-03T11:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxNzgwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU0MzI0MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449543240", "bodyText": "We do check all of those in that method now https://github.com/elastic/elasticsearch/pull/56911/files#diff-a0853be4492c052f24917b5c1464003dR2012 ?", "author": "original-brownbear", "createdAt": "2020-07-03T11:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxNzgwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU0NTg5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449545893", "bodyText": "ok, missed  that. All good.", "author": "ywelsch", "createdAt": "2020-07-03T11:58:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxNzgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyMDEyMA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449420120", "bodyText": "Let's also add a node/cluster setting that limits the number of concurrent operations, just so that we have a safeguard (and something to set to 1 in case there's a concurrency bug).", "author": "ywelsch", "createdAt": "2020-07-03T07:23:53Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -204,24 +221,34 @@ public ClusterState execute(ClusterState currentState) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n+                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                if (runningSnapshots.stream().anyMatch(s -> {\n+                    final Snapshot running = s.snapshot();\n+                    return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+                })) {\n+                    throw new InvalidSnapshotNameException(\n+                            repository.getMetadata().name(), snapshotName, \"snapshot with the same name is already in-progress\");\n+                }\n                 validate(repositoryName, snapshotName, currentState);\n-                final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n+                SnapshotDeletionsInProgress deletionsInProgress =\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.hasDeletionsInProgress() && minNodeVersion.before(FULL_CONCURRENCY_VERSION)) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress =\n-                    currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+                        currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n                 if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n-                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n                 // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a\n                 // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the\n                 // cluster state anyway in #applyClusterState.\n-                if (snapshots.entries().stream().anyMatch(entry -> entry.state() != State.INIT)) {\n+                if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)\n+                        && runningSnapshots.stream().anyMatch(entry -> entry.state() != State.INIT)) {", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYxNjk2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449616961", "bodyText": "Done in 4c0fcaf", "author": "original-brownbear", "createdAt": "2020-07-03T14:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyMDEyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyNjQ3MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449426470", "bodyText": "I'm confused why we recheck this condition here and don't put it under the same if block as above", "author": "ywelsch", "createdAt": "2020-07-03T07:38:38Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1047,45 +1291,101 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n             };\n         }\n         return new ClusterStateUpdateTask(priority) {\n+\n+            private SnapshotDeletionsInProgress.Entry newDelete;\n+\n+            private boolean reusedExistingDelete = false;\n+\n+            private final Collection<SnapshotsInProgress.Entry> completedSnapshots = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n-                final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n-                    throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n-                        \"cannot delete - another snapshot is currently being deleted in [\" + deletionsInProgress + \"]\");\n+                SnapshotDeletionsInProgress deletionsInProgress =\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n+                if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)) {\n+                    if (deletionsInProgress.hasDeletionsInProgress()) {\n+                        throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n+                                \"cannot delete - another snapshot is currently being deleted in [\" + deletionsInProgress + \"]\");\n+                    }\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress =\n-                    currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+                        currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n                 if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n-                        \"cannot delete snapshots while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n+                            \"cannot delete snapshots while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n                 final RestoreInProgress restoreInProgress = currentState.custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY);\n                 // don't allow snapshot deletions while a restore is taking place,\n                 // otherwise we could end up deleting a snapshot that is being restored\n                 // and the files the restore depends on would all be gone\n+\n                 for (RestoreInProgress.Entry entry : restoreInProgress) {\n                     if (repoName.equals(entry.snapshot().getRepository()) && snapshotIds.contains(entry.snapshot().getSnapshotId())) {\n                         throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n-                            \"cannot delete snapshot during a restore in progress in [\" + restoreInProgress + \"]\");\n+                                \"cannot delete snapshot during a restore in progress in [\" + restoreInProgress + \"]\");\n                     }\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n-                if (snapshots.entries().isEmpty() == false) {\n-                    // However other snapshots are running - cannot continue\n-                    throw new ConcurrentSnapshotExecutionException(\n-                            repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final SnapshotsInProgress updatedSnapshots;\n+                if (minNodeVersion.onOrAfter(FULL_CONCURRENCY_VERSION)) {\n+                    updatedSnapshots = SnapshotsInProgress.of(snapshots.entries().stream()\n+                            .map(existing -> {\n+                                // snapshot is started - mark every non completed shard as aborted\n+                                if (existing.state() == State.STARTED && snapshotIds.contains(existing.snapshot().getSnapshotId())) {\n+                                    final ImmutableOpenMap<ShardId, ShardSnapshotStatus> abortedShards = abortEntry(existing);\n+                                    final boolean isCompleted = completed(abortedShards.values());\n+                                    final SnapshotsInProgress.Entry abortedEntry = new SnapshotsInProgress.Entry(\n+                                            existing, isCompleted ? State.SUCCESS : State.ABORTED, abortedShards,\n+                                            \"Snapshot was aborted by deletion\");\n+                                    if (isCompleted) {\n+                                        completedSnapshots.add(abortedEntry);\n+                                    }\n+                                    return abortedEntry;\n+                                }\n+                                return existing;\n+                            }).collect(Collectors.toUnmodifiableList()));\n+                } else {\n+                    if (snapshots.entries().isEmpty() == false) {\n+                        // However other snapshots are running - cannot continue\n+                        throw new ConcurrentSnapshotExecutionException(\n+                                repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                    }\n+                    updatedSnapshots = snapshots;\n                 }\n                 // add the snapshot deletion to the cluster state\n-                SnapshotDeletionsInProgress.Entry entry = new SnapshotDeletionsInProgress.Entry(\n+                SnapshotDeletionsInProgress.Entry replacedEntry = deletionsInProgress.getEntries().stream().filter(entry ->\n+                        entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.WAITING)\n+                        .findFirst().orElse(null);\n+                if (replacedEntry == null) {\n+                    final Optional<SnapshotDeletionsInProgress.Entry> foundDuplicate =\n+                            deletionsInProgress.getEntries().stream().filter(entry ->\n+                                    entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.STARTED\n+                                            && entry.getSnapshots().containsAll(snapshotIds)).findFirst();\n+                    if (foundDuplicate.isPresent()) {\n+                        newDelete = foundDuplicate.get();\n+                        reusedExistingDelete = true;\n+                        return currentState;\n+                    }\n+                }\n+                if (replacedEntry == null) {", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU1NTM5Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449555392", "bodyText": "Sorry this was a left-over from some previous iteration, obviously redundant condition -> removing.", "author": "original-brownbear", "createdAt": "2020-07-03T12:22:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyNjQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyODI5NA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449428294", "bodyText": "debug level? Are we also logging the actual deletion at info level?", "author": "ywelsch", "createdAt": "2020-07-03T07:42:39Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1095,11 +1395,64 @@ public void onFailure(String source, Exception e) {\n \n             @Override\n             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n-                deleteSnapshotsFromRepository(repoName, snapshotIds, listener, repositoryStateId, newState.nodes().getMinNodeVersion());\n+                addDeleteListener(newDelete.uuid(), listener);\n+                if (reusedExistingDelete) {\n+                    return;\n+                }\n+                if (newDelete.state() == SnapshotDeletionsInProgress.State.STARTED) {\n+                    if (tryEnterRepoLoop(repoName)) {\n+                        deleteSnapshotsFromRepository(newDelete, repositoryData, newState.nodes().getMinNodeVersion());\n+                    } else {\n+                        logger.trace(\"Delete [{}] could not execute directly and was queued\", newDelete);", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU0MTI1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449541255", "bodyText": "Idk, this is super trivial and I mainly just added it for my debugging of tests. It's just that this is logged in the  corner case of starting a delete in the short window of time between the cluster state update that removed the latest snapshot for a given repo and the listener for that snapshot getting resolved. We can't do synchronisation there because we delete outdated index-N between the two events so this is how I solved it for now, but it's extremely unlikely to get here and completely irrelevant outside of tricky debugging.", "author": "original-brownbear", "createdAt": "2020-07-03T11:46:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyODI5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQzMTI0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449431249", "bodyText": "let's add debug logging here", "author": "ywelsch", "createdAt": "2020-07-03T07:48:38Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1152,82 +1505,359 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n-     * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n+     * @param deleteEntry       delete entry in cluster state\n+     * @param minNodeVersion    minimum node version in the cluster\n+     */\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry, Version minNodeVersion) {\n+        final long expectedRepoGen = deleteEntry.repositoryStateId();\n+        repositoriesService.getRepositoryData(deleteEntry.repository(), new ActionListener<>() {\n+            @Override\n+            public void onResponse(RepositoryData repositoryData) {\n+                assert repositoryData.getGenId() == expectedRepoGen :\n+                        \"Repository generation should not change as long as a ready delete is found in the cluster state but found [\"\n+                                + expectedRepoGen + \"] in cluster state and [\" + repositoryData.getGenId() + \"] in the repository\";\n+                deleteSnapshotsFromRepository(deleteEntry, repositoryData, minNodeVersion);\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                clusterService.submitStateUpdateTask(\"fail repo tasks for [\" + deleteEntry.repository() + \"]\",\n+                        new FailPendingRepoTasksTask(deleteEntry.repository(), e));\n+            }\n+        });\n+    }\n+\n+    /** Deletes snapshot from repository\n+     *\n+     * @param deleteEntry       delete entry in cluster state\n+     * @param repositoryData    the {@link RepositoryData} of the repository to delete from\n      * @param minNodeVersion    minimum node version in the cluster\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n-                                               long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                               RepositoryData repositoryData, Version minNodeVersion) {\n+        if (repositoryOperations.startDeletion(deleteEntry.uuid())) {\n+            assert currentlyFinalizing.contains(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.STARTED :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repositoriesService.repository(deleteEntry.repository()).deleteSnapshots(\n                 snapshotIds,\n-                repositoryStateId,\n+                repositoryData.getGenId(),\n                 minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+                ActionListener.wrap(updatedRepoData -> {\n+                        logger.info(\"snapshots {} deleted\", snapshotIds);\n+                        removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                    }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                ));\n+        }\n     }\n \n     /**\n-     * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n+     * Removes a {@link SnapshotDeletionsInProgress.Entry} from {@link SnapshotDeletionsInProgress} in the cluster state after it executed\n+     * on the repository.\n+     *\n+     * @param deleteEntry delete entry to remove from the cluster state\n+     * @param failure     failure encountered while executing the delete on the repository or {@code null} if the delete executed\n+     *                    successfully\n+     * @param repositoryData current {@link RepositoryData} for the repository we just ran the delete on.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n-        clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n-            @Override\n-            public ClusterState execute(ClusterState currentState) {\n-                final SnapshotDeletionsInProgress deletions =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletions.hasDeletionsInProgress()) {\n-                    assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                    SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                    return ClusterState.builder(currentState).putCustom(\n-                        SnapshotDeletionsInProgress.TYPE, deletions.withRemovedEntry(entry)).build();\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, final RepositoryData repositoryData) {\n+        final ClusterStateUpdateTask clusterStateUpdateTask;\n+        if (failure == null) {\n+            // If we didn't have a failure during the snapshot delete we will remove all snapshot ids that the delete successfully removed\n+            // from the repository from enqueued snapshot delete entries during the cluster state update. After the cluster state update we\n+            // resolve the delete listeners with the latest repository data from after the delete.\n+            clusterStateUpdateTask = new RemoveSnapshotDeletionAndContinueTask(deleteEntry, repositoryData) {\n+                @Override\n+                protected SnapshotDeletionsInProgress filterDeletions(SnapshotDeletionsInProgress deletions) {\n+                    boolean changed = false;\n+                    List<SnapshotDeletionsInProgress.Entry> updatedEntries = new ArrayList<>(deletions.getEntries().size());\n+                    for (SnapshotDeletionsInProgress.Entry entry : deletions.getEntries()) {\n+                        if (entry.repository().equals(deleteEntry.repository())) {\n+                            final List<SnapshotId> updatedSnapshotIds = new ArrayList<>(entry.getSnapshots());\n+                            if (updatedSnapshotIds.removeAll(deleteEntry.getSnapshots())) {\n+                                changed = true;\n+                                updatedEntries.add(entry.withSnapshots(updatedSnapshotIds));\n+                            } else {\n+                                updatedEntries.add(entry);\n+                            }\n+                        } else {\n+                            updatedEntries.add(entry);\n+                        }\n+                    }\n+                    return changed ? SnapshotDeletionsInProgress.of(updatedEntries) : deletions;\n+                }\n+\n+                @Override\n+                protected void handleListeners(List<ActionListener<Void>> deleteListeners) {\n+                    assert repositoryData.getSnapshotIds().stream().noneMatch(deleteEntry.getSnapshots()::contains)\n+                            : \"Repository data contained snapshot ids \" + repositoryData.getSnapshotIds()\n+                            + \" that should should been deleted by [\" + deleteEntry + \"]\";\n+                    completeListenersIgnoringException(deleteListeners, null);\n+                }\n+            };\n+        } else {\n+            // The delete failed to execute on the repository. We remove it from the cluster state and then fail all listeners associated\n+            // with it.\n+            clusterStateUpdateTask = new RemoveSnapshotDeletionAndContinueTask(deleteEntry, repositoryData) {\n+                @Override\n+                protected void handleListeners(List<ActionListener<Void>> deleteListeners) {\n+                    failListenersIgnoringException(deleteListeners, failure);\n+                }\n+            };\n+        }\n+        clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", clusterStateUpdateTask);\n+    }\n+\n+    /**\n+     * Handle snapshot or delete failure due to not being master any more so we don't try to do run additional cluster state updates.\n+     * The next master will try handling the missing operations. All we can do is fail all the listeners on this master node so that\n+     * transport requests return and we don't leak listeners.\n+     *\n+     * @param e exception that caused us to realize we are not master any longer\n+     */\n+    private void failAllListenersOnMasterFailOver(Exception e) {\n+        synchronized (currentlyFinalizing) {", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYxNzE1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449617157", "bodyText": "Done", "author": "original-brownbear", "createdAt": "2020-07-03T14:44:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQzMTI0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3MzY4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449473682", "bodyText": "should we log this at info level? This is an important event?", "author": "ywelsch", "createdAt": "2020-07-03T09:13:06Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1463,4 +2157,171 @@ protected ClusterBlockException checkBlock(UpdateIndexShardSnapshotStatusRequest\n             return null;\n         }\n     }\n+\n+    /**\n+     * Cluster state update task that removes all {@link SnapshotsInProgress.Entry} and {@link SnapshotDeletionsInProgress.Entry} for a\n+     * given repository from the cluster state and afterwards fails all relevant listeners in {@link #snapshotCompletionListeners} and\n+     * {@link #snapshotDeletionListeners}.\n+     */\n+    private final class FailPendingRepoTasksTask extends ClusterStateUpdateTask {\n+\n+        // Snapshots to fail after the state update\n+        private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+        // Delete uuids to fail because after the state update\n+        private final List<String> deletionsToFail = new ArrayList<>();\n+\n+        // Failure that caused the decision to fail all snapshots and deletes for a repo\n+        private final Exception failure;\n+\n+        private final String repository;\n+\n+        FailPendingRepoTasksTask(String repository, Exception failure) {\n+            this.repository = repository;\n+            this.failure = failure;\n+        }\n+\n+        @Override\n+        public ClusterState execute(ClusterState currentState) {\n+            final SnapshotDeletionsInProgress deletionsInProgress =\n+                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+            boolean changed = false;\n+            final List<SnapshotDeletionsInProgress.Entry> remainingEntries = deletionsInProgress.getEntries();\n+            List<SnapshotDeletionsInProgress.Entry> updatedEntries = new ArrayList<>(remainingEntries.size());\n+            for (SnapshotDeletionsInProgress.Entry entry : remainingEntries) {\n+                if (entry.repository().equals(repository)) {\n+                    changed = true;\n+                    deletionsToFail.add(entry.uuid());\n+                } else {\n+                    updatedEntries.add(entry);\n+                }\n+            }\n+            final SnapshotDeletionsInProgress updatedDeletions = changed ? SnapshotDeletionsInProgress.of(updatedEntries) : null;\n+            final SnapshotsInProgress snapshotsInProgress =\n+                currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+            boolean changedSnapshots = false;\n+            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                if (entry.repository().equals(repository)) {\n+                    // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                    // retry these kinds of issues so we fail all the pending snapshots\n+                    snapshotsToFail.add(entry.snapshot());\n+                    changedSnapshots = true;\n+                } else {\n+                    // Entry is for another repository we just keep it as is\n+                    snapshotEntries.add(entry);\n+                }\n+            }\n+            final SnapshotsInProgress updatedSnapshotsInProgress = changedSnapshots ? SnapshotsInProgress.of(snapshotEntries) : null;\n+            return updateWithSnapshots(currentState, updatedSnapshotsInProgress, updatedDeletions);\n+        }\n+\n+        @Override\n+        public void onFailure(String source, Exception e) {\n+            logger.debug(\n+                    () -> new ParameterizedMessage(\"Failed to remove all snapshot tasks for repo [{}] from cluster state\", repository), e);\n+            failAllListenersOnMasterFailOver(e);\n+        }\n+\n+        @Override\n+        public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n+            logger.trace(\"Removed all snapshot tasks for repository [{}] from cluster state, now failing listeners\", repository);", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU0Mzk0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449543943", "bodyText": "Yea let's even do WARN here. The only way to run into this is when failing to load RepositoryData. This should be almost impossible with retries and caching of RepositoryData and if it happens at any sort of frequency it should be very visible.", "author": "original-brownbear", "createdAt": "2020-07-03T11:53:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3MzY4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3Mzg4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449473882", "bodyText": "should we log at info level? Important event, no?", "author": "ywelsch", "createdAt": "2020-07-03T09:13:33Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1463,4 +2157,171 @@ protected ClusterBlockException checkBlock(UpdateIndexShardSnapshotStatusRequest\n             return null;\n         }\n     }\n+\n+    /**\n+     * Cluster state update task that removes all {@link SnapshotsInProgress.Entry} and {@link SnapshotDeletionsInProgress.Entry} for a\n+     * given repository from the cluster state and afterwards fails all relevant listeners in {@link #snapshotCompletionListeners} and\n+     * {@link #snapshotDeletionListeners}.\n+     */\n+    private final class FailPendingRepoTasksTask extends ClusterStateUpdateTask {\n+\n+        // Snapshots to fail after the state update\n+        private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+        // Delete uuids to fail because after the state update\n+        private final List<String> deletionsToFail = new ArrayList<>();\n+\n+        // Failure that caused the decision to fail all snapshots and deletes for a repo\n+        private final Exception failure;\n+\n+        private final String repository;\n+\n+        FailPendingRepoTasksTask(String repository, Exception failure) {\n+            this.repository = repository;\n+            this.failure = failure;\n+        }\n+\n+        @Override\n+        public ClusterState execute(ClusterState currentState) {\n+            final SnapshotDeletionsInProgress deletionsInProgress =\n+                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+            boolean changed = false;\n+            final List<SnapshotDeletionsInProgress.Entry> remainingEntries = deletionsInProgress.getEntries();\n+            List<SnapshotDeletionsInProgress.Entry> updatedEntries = new ArrayList<>(remainingEntries.size());\n+            for (SnapshotDeletionsInProgress.Entry entry : remainingEntries) {\n+                if (entry.repository().equals(repository)) {\n+                    changed = true;\n+                    deletionsToFail.add(entry.uuid());\n+                } else {\n+                    updatedEntries.add(entry);\n+                }\n+            }\n+            final SnapshotDeletionsInProgress updatedDeletions = changed ? SnapshotDeletionsInProgress.of(updatedEntries) : null;\n+            final SnapshotsInProgress snapshotsInProgress =\n+                currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+            boolean changedSnapshots = false;\n+            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                if (entry.repository().equals(repository)) {\n+                    // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                    // retry these kinds of issues so we fail all the pending snapshots\n+                    snapshotsToFail.add(entry.snapshot());\n+                    changedSnapshots = true;\n+                } else {\n+                    // Entry is for another repository we just keep it as is\n+                    snapshotEntries.add(entry);\n+                }\n+            }\n+            final SnapshotsInProgress updatedSnapshotsInProgress = changedSnapshots ? SnapshotsInProgress.of(snapshotEntries) : null;\n+            return updateWithSnapshots(currentState, updatedSnapshotsInProgress, updatedDeletions);\n+        }\n+\n+        @Override\n+        public void onFailure(String source, Exception e) {\n+            logger.debug(", "originalCommit": "81fdf77e28c32ce4748cc315758ca517995329e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU0NTY0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449545645", "bodyText": "Yea we could log this more visibly. It's just something that will happen on master-failover and I didn't want to addd to the general noise of that but that's not an actual argument => info it is :)", "author": "original-brownbear", "createdAt": "2020-07-03T11:57:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3Mzg4Mg=="}], "type": "inlineReview"}, {"oid": "95a1dc53812d7254acf479ba805155b922d82a6e", "url": "https://github.com/elastic/elasticsearch/commit/95a1dc53812d7254acf479ba805155b922d82a6e", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-03T11:38:39Z", "type": "commit"}, {"oid": "5e79b5cfbbd7edc8b5c0b8456c4742258858ee1b", "url": "https://github.com/elastic/elasticsearch/commit/5e79b5cfbbd7edc8b5c0b8456c4742258858ee1b", "message": "CR: small fixes", "committedDate": "2020-07-03T12:34:10Z", "type": "commit"}, {"oid": "d2e97de304df5aca337205d572ca95bf10a96d07", "url": "https://github.com/elastic/elasticsearch/commit/d2e97de304df5aca337205d572ca95bf10a96d07", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-03T12:36:45Z", "type": "commit"}, {"oid": "4c0fcafb38491de387ab2db7720dc56f28426cf6", "url": "https://github.com/elastic/elasticsearch/commit/4c0fcafb38491de387ab2db7720dc56f28426cf6", "message": "CR: add setting to limit concurrency", "committedDate": "2020-07-03T14:42:49Z", "type": "commit"}, {"oid": "1f75a52688a63d47319987f65c2b87cdc3abe6e5", "url": "https://github.com/elastic/elasticsearch/commit/1f75a52688a63d47319987f65c2b87cdc3abe6e5", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-06T12:22:01Z", "type": "commit"}, {"oid": "bfff04ede64a14237e955b8ee74b934b1362a875", "url": "https://github.com/elastic/elasticsearch/commit/bfff04ede64a14237e955b8ee74b934b1362a875", "message": "merge in master", "committedDate": "2020-07-06T12:43:17Z", "type": "commit"}, {"oid": "528cc81f5feb672f854810f8d91aecd8cdadea47", "url": "https://github.com/elastic/elasticsearch/commit/528cc81f5feb672f854810f8d91aecd8cdadea47", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-07T09:00:49Z", "type": "commit"}, {"oid": "08177e1a9c97ea1562a52e1852ee9dc42b8800db", "url": "https://github.com/elastic/elasticsearch/commit/08177e1a9c97ea1562a52e1852ee9dc42b8800db", "message": "merge in master", "committedDate": "2020-07-07T09:09:25Z", "type": "commit"}, {"oid": "bb9cd4343e0efdf6330dc169d4de40adcb3363f1", "url": "https://github.com/elastic/elasticsearch/commit/bb9cd4343e0efdf6330dc169d4de40adcb3363f1", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-07T13:15:08Z", "type": "commit"}, {"oid": "883d2203f3000eb9d2784deaeed02468b789db95", "url": "https://github.com/elastic/elasticsearch/commit/883d2203f3000eb9d2784deaeed02468b789db95", "message": "Merge branch 'master' of github.com:elastic/elasticsearch into allow-multiple-snapshots", "committedDate": "2020-07-07T13:16:06Z", "type": "commit"}, {"oid": "906f51b294e6c6ab6020bd5804361ebd101a1be0", "url": "https://github.com/elastic/elasticsearch/commit/906f51b294e6c6ab6020bd5804361ebd101a1be0", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-07T14:36:11Z", "type": "commit"}, {"oid": "a7bb2d25eda5b3b101fb04ed1b80c887883eb6be", "url": "https://github.com/elastic/elasticsearch/commit/a7bb2d25eda5b3b101fb04ed1b80c887883eb6be", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-08T11:45:07Z", "type": "commit"}, {"oid": "cb9af07eca04727e3a9f6df148d880ceaa8ab021", "url": "https://github.com/elastic/elasticsearch/commit/cb9af07eca04727e3a9f6df148d880ceaa8ab021", "message": "add test for concurrent ops and parallel ops", "committedDate": "2020-07-08T12:25:44Z", "type": "commit"}, {"oid": "ca50bf4760a231b5a03cbbcad290651bdd6749dc", "url": "https://github.com/elastic/elasticsearch/commit/ca50bf4760a231b5a03cbbcad290651bdd6749dc", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-09T04:38:59Z", "type": "commit"}, {"oid": "f5a44d4bfa3d8bd5940cc09f694ec3bc99b15b1d", "url": "https://github.com/elastic/elasticsearch/commit/f5a44d4bfa3d8bd5940cc09f694ec3bc99b15b1d", "message": "be smarter with shard generations to always allow for concurrent stuff", "committedDate": "2020-07-09T05:58:50Z", "type": "commit"}, {"oid": "e9b03116ca4716488201b0775a87ed3ea222791e", "url": "https://github.com/elastic/elasticsearch/commit/e9b03116ca4716488201b0775a87ed3ea222791e", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-09T12:29:55Z", "type": "commit"}, {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "url": "https://github.com/elastic/elasticsearch/commit/897e6d1160b52aa70db2365907f0e8d9c19347f7", "message": "CR: limit 1k", "committedDate": "2020-07-09T12:36:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY2Njg0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452666841", "bodyText": "nit: add extra line", "author": "tlrx", "createdAt": "2020-07-10T07:19:01Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,1313 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return Arrays.asList(MockTransportService.TestPlugin.class, MockRepository.Plugin.class);\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))\n+                .put(AbstractDisruptionTestCase.DEFAULT_SETTINGS)\n+                .build();\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"slow-snapshot\", repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String indexFast = \"index-fast\";\n+        createIndexWithContent(indexFast, dataNode2, dataNode);\n+\n+        assertSuccessful(client().admin().cluster().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testDeletesAreBatched() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        createIndex(\"foo\");\n+        ensureGreen();\n+\n+        final int numSnapshots = randomIntBetween(1, 4);\n+        final PlainActionFuture<Collection<CreateSnapshotResponse>> allSnapshotsDone = PlainActionFuture.newFuture();\n+        final ActionListener<CreateSnapshotResponse> snapshotsListener = new GroupedActionListener<>(allSnapshotsDone, numSnapshots);\n+        final Collection<String> snapshotNames = new HashSet<>();\n+        for (int i = 0; i < numSnapshots; i++) {\n+            final String snapshot = \"snap-\" + i;\n+            snapshotNames.add(snapshot);\n+            client().admin().cluster().prepareCreateSnapshot(repoName, snapshot).setWaitForCompletion(true)\n+                    .execute(snapshotsListener);\n+        }\n+        final Collection<CreateSnapshotResponse> snapshotResponses = allSnapshotsDone.get();\n+        for (CreateSnapshotResponse snapshotResponse : snapshotResponses) {\n+            assertThat(snapshotResponse.getSnapshotInfo().state(), is(SnapshotState.SUCCESS));\n+        }\n+\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", repoName, dataNode);\n+\n+        final Collection<StepListener<AcknowledgedResponse>> deleteFutures = new ArrayList<>();\n+        while (snapshotNames.isEmpty() == false) {\n+            final Collection<String> toDelete = randomSubsetOf(snapshotNames);\n+            if (toDelete.isEmpty()) {\n+                continue;\n+            }\n+            snapshotNames.removeAll(toDelete);\n+            final StepListener<AcknowledgedResponse> future = new StepListener<>();\n+            client().admin().cluster().prepareDeleteSnapshot(repoName, toDelete.toArray(Strings.EMPTY_ARRAY)).execute(future);\n+            deleteFutures.add(future);\n+        }\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+\n+        final long repoGenAfterInitialSnapshots = getRepositoryData(repoName).getGenId();\n+        assertThat(repoGenAfterInitialSnapshots, is(numSnapshots - 1L));\n+        unblockNode(repoName, dataNode);\n+\n+        final SnapshotInfo slowSnapshotInfo = assertSuccessful(createSlowFuture);\n+\n+        logger.info(\"--> waiting for batched deletes to finish\");\n+        final PlainActionFuture<Collection<AcknowledgedResponse>> allDeletesDone = new PlainActionFuture<>();\n+        final ActionListener<AcknowledgedResponse> deletesListener = new GroupedActionListener<>(allDeletesDone, deleteFutures.size());\n+        for (StepListener<AcknowledgedResponse> deleteFuture : deleteFutures) {\n+            deleteFuture.whenComplete(deletesListener::onResponse, deletesListener::onFailure);\n+        }\n+        allDeletesDone.get();\n+\n+        logger.info(\"--> verifying repository state\");\n+        final RepositoryData repositoryDataAfterDeletes = getRepositoryData(repoName);\n+        // One increment for snapshot, one for all the deletes\n+        assertThat(repositoryDataAfterDeletes.getGenId(), is(repoGenAfterInitialSnapshots + 2));\n+        assertThat(repositoryDataAfterDeletes.getSnapshotIds(), contains(slowSnapshotInfo.snapshotId()));\n+    }\n+\n+    public void testBlockedRepoDoesNotBlockOtherRepos() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndex(\"foo\");\n+        ensureGreen();\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startAndBlockFailingFullSnapshot(blockedRepoName, \"blocked-snapshot\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(otherRepoName, \"snapshot\")\n+                .setIndices(\"does-not-exist-*\")\n+                .setWaitForCompletion(false).get();\n+\n+        unblockNode(blockedRepoName, internalCluster().getMasterName());\n+        expectThrows(SnapshotException.class, createSlowFuture::actionGet);\n+\n+        assertBusy(() -> assertThat(currentSnapshots(otherRepoName), empty()), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testMultipleReposAreIndependent() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second concurrent snapshot.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent2() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second repository's concurrent operations.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent3() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        createFullSnapshot( blockedRepoName, \"blocked-snapshot\");\n+        blockNodeOnAnyFiles(blockedRepoName, masterNode);\n+        final ActionFuture<AcknowledgedResponse> slowDeleteFuture = startDelete(blockedRepoName, \"*\");\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, masterNode);\n+        assertAcked(slowDeleteFuture.actionGet());\n+    }\n+\n+    private static String startDataNodeWithLargeSnapshotPool() {\n+        return internalCluster().startDataOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+    }\n+    public void testSnapshotRunsAfterInProgressDelete() throws Exception {", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY2OTg3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452669879", "bodyText": "Can we check that the 1st snapshot failed because it was aborted?", "author": "tlrx", "createdAt": "2020-07-10T07:25:37Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,1313 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return Arrays.asList(MockTransportService.TestPlugin.class, MockRepository.Plugin.class);\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))\n+                .put(AbstractDisruptionTestCase.DEFAULT_SETTINGS)\n+                .build();\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"slow-snapshot\", repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String indexFast = \"index-fast\";\n+        createIndexWithContent(indexFast, dataNode2, dataNode);\n+\n+        assertSuccessful(client().admin().cluster().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testDeletesAreBatched() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        createIndex(\"foo\");\n+        ensureGreen();\n+\n+        final int numSnapshots = randomIntBetween(1, 4);\n+        final PlainActionFuture<Collection<CreateSnapshotResponse>> allSnapshotsDone = PlainActionFuture.newFuture();\n+        final ActionListener<CreateSnapshotResponse> snapshotsListener = new GroupedActionListener<>(allSnapshotsDone, numSnapshots);\n+        final Collection<String> snapshotNames = new HashSet<>();\n+        for (int i = 0; i < numSnapshots; i++) {\n+            final String snapshot = \"snap-\" + i;\n+            snapshotNames.add(snapshot);\n+            client().admin().cluster().prepareCreateSnapshot(repoName, snapshot).setWaitForCompletion(true)\n+                    .execute(snapshotsListener);\n+        }\n+        final Collection<CreateSnapshotResponse> snapshotResponses = allSnapshotsDone.get();\n+        for (CreateSnapshotResponse snapshotResponse : snapshotResponses) {\n+            assertThat(snapshotResponse.getSnapshotInfo().state(), is(SnapshotState.SUCCESS));\n+        }\n+\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", repoName, dataNode);\n+\n+        final Collection<StepListener<AcknowledgedResponse>> deleteFutures = new ArrayList<>();\n+        while (snapshotNames.isEmpty() == false) {\n+            final Collection<String> toDelete = randomSubsetOf(snapshotNames);\n+            if (toDelete.isEmpty()) {\n+                continue;\n+            }\n+            snapshotNames.removeAll(toDelete);\n+            final StepListener<AcknowledgedResponse> future = new StepListener<>();\n+            client().admin().cluster().prepareDeleteSnapshot(repoName, toDelete.toArray(Strings.EMPTY_ARRAY)).execute(future);\n+            deleteFutures.add(future);\n+        }\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+\n+        final long repoGenAfterInitialSnapshots = getRepositoryData(repoName).getGenId();\n+        assertThat(repoGenAfterInitialSnapshots, is(numSnapshots - 1L));\n+        unblockNode(repoName, dataNode);\n+\n+        final SnapshotInfo slowSnapshotInfo = assertSuccessful(createSlowFuture);\n+\n+        logger.info(\"--> waiting for batched deletes to finish\");\n+        final PlainActionFuture<Collection<AcknowledgedResponse>> allDeletesDone = new PlainActionFuture<>();\n+        final ActionListener<AcknowledgedResponse> deletesListener = new GroupedActionListener<>(allDeletesDone, deleteFutures.size());\n+        for (StepListener<AcknowledgedResponse> deleteFuture : deleteFutures) {\n+            deleteFuture.whenComplete(deletesListener::onResponse, deletesListener::onFailure);\n+        }\n+        allDeletesDone.get();\n+\n+        logger.info(\"--> verifying repository state\");\n+        final RepositoryData repositoryDataAfterDeletes = getRepositoryData(repoName);\n+        // One increment for snapshot, one for all the deletes\n+        assertThat(repositoryDataAfterDeletes.getGenId(), is(repoGenAfterInitialSnapshots + 2));\n+        assertThat(repositoryDataAfterDeletes.getSnapshotIds(), contains(slowSnapshotInfo.snapshotId()));\n+    }\n+\n+    public void testBlockedRepoDoesNotBlockOtherRepos() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndex(\"foo\");\n+        ensureGreen();\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startAndBlockFailingFullSnapshot(blockedRepoName, \"blocked-snapshot\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(otherRepoName, \"snapshot\")\n+                .setIndices(\"does-not-exist-*\")\n+                .setWaitForCompletion(false).get();\n+\n+        unblockNode(blockedRepoName, internalCluster().getMasterName());\n+        expectThrows(SnapshotException.class, createSlowFuture::actionGet);\n+\n+        assertBusy(() -> assertThat(currentSnapshots(otherRepoName), empty()), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testMultipleReposAreIndependent() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second concurrent snapshot.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent2() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second repository's concurrent operations.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent3() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        createFullSnapshot( blockedRepoName, \"blocked-snapshot\");\n+        blockNodeOnAnyFiles(blockedRepoName, masterNode);\n+        final ActionFuture<AcknowledgedResponse> slowDeleteFuture = startDelete(blockedRepoName, \"*\");\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, masterNode);\n+        assertAcked(slowDeleteFuture.actionGet());\n+    }\n+\n+    private static String startDataNodeWithLargeSnapshotPool() {\n+        return internalCluster().startDataOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+    }\n+    public void testSnapshotRunsAfterInProgressDelete() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        ensureGreen();\n+        createIndexWithContent(\"index-test\");\n+\n+        final String firstSnapshot = \"first-snapshot\";\n+        createFullSnapshot(repoName, firstSnapshot);\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDelete(repoName, firstSnapshot);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"second-snapshot\");\n+\n+        unblockNode(repoName, masterNode);\n+        final UncategorizedExecutionException ex = expectThrows(UncategorizedExecutionException.class, deleteFuture::actionGet);\n+        assertThat(ex.getRootCause(), instanceOf(IOException.class));\n+\n+        assertSuccessful(snapshotFuture);\n+    }\n+\n+    public void testAbortOneOfMultipleSnapshots() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String firstIndex = \"index-one\";\n+        createIndexWithContent(firstIndex);\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse =\n+                startFullSnapshotBlockedOnDataNode(firstSnapshot, repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String secondIndex = \"index-two\";\n+        createIndexWithContent(secondIndex, dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> deleteSnapshotsResponse = startDelete(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        logger.info(\"--> start third snapshot\");\n+        final ActionFuture<CreateSnapshotResponse> thirdSnapshotResponse = client().admin().cluster()\n+                .prepareCreateSnapshot(repoName, \"snapshot-three\").setIndices(secondIndex).setWaitForCompletion(true).execute();\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        unblockNode(repoName, dataNode);\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY3NzIxMA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452677210", "bodyText": "Is there a meaningful error message we could check here?", "author": "tlrx", "createdAt": "2020-07-10T07:41:41Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,1313 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return Arrays.asList(MockTransportService.TestPlugin.class, MockRepository.Plugin.class);\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))\n+                .put(AbstractDisruptionTestCase.DEFAULT_SETTINGS)\n+                .build();\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"slow-snapshot\", repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String indexFast = \"index-fast\";\n+        createIndexWithContent(indexFast, dataNode2, dataNode);\n+\n+        assertSuccessful(client().admin().cluster().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testDeletesAreBatched() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        createIndex(\"foo\");\n+        ensureGreen();\n+\n+        final int numSnapshots = randomIntBetween(1, 4);\n+        final PlainActionFuture<Collection<CreateSnapshotResponse>> allSnapshotsDone = PlainActionFuture.newFuture();\n+        final ActionListener<CreateSnapshotResponse> snapshotsListener = new GroupedActionListener<>(allSnapshotsDone, numSnapshots);\n+        final Collection<String> snapshotNames = new HashSet<>();\n+        for (int i = 0; i < numSnapshots; i++) {\n+            final String snapshot = \"snap-\" + i;\n+            snapshotNames.add(snapshot);\n+            client().admin().cluster().prepareCreateSnapshot(repoName, snapshot).setWaitForCompletion(true)\n+                    .execute(snapshotsListener);\n+        }\n+        final Collection<CreateSnapshotResponse> snapshotResponses = allSnapshotsDone.get();\n+        for (CreateSnapshotResponse snapshotResponse : snapshotResponses) {\n+            assertThat(snapshotResponse.getSnapshotInfo().state(), is(SnapshotState.SUCCESS));\n+        }\n+\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", repoName, dataNode);\n+\n+        final Collection<StepListener<AcknowledgedResponse>> deleteFutures = new ArrayList<>();\n+        while (snapshotNames.isEmpty() == false) {\n+            final Collection<String> toDelete = randomSubsetOf(snapshotNames);\n+            if (toDelete.isEmpty()) {\n+                continue;\n+            }\n+            snapshotNames.removeAll(toDelete);\n+            final StepListener<AcknowledgedResponse> future = new StepListener<>();\n+            client().admin().cluster().prepareDeleteSnapshot(repoName, toDelete.toArray(Strings.EMPTY_ARRAY)).execute(future);\n+            deleteFutures.add(future);\n+        }\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+\n+        final long repoGenAfterInitialSnapshots = getRepositoryData(repoName).getGenId();\n+        assertThat(repoGenAfterInitialSnapshots, is(numSnapshots - 1L));\n+        unblockNode(repoName, dataNode);\n+\n+        final SnapshotInfo slowSnapshotInfo = assertSuccessful(createSlowFuture);\n+\n+        logger.info(\"--> waiting for batched deletes to finish\");\n+        final PlainActionFuture<Collection<AcknowledgedResponse>> allDeletesDone = new PlainActionFuture<>();\n+        final ActionListener<AcknowledgedResponse> deletesListener = new GroupedActionListener<>(allDeletesDone, deleteFutures.size());\n+        for (StepListener<AcknowledgedResponse> deleteFuture : deleteFutures) {\n+            deleteFuture.whenComplete(deletesListener::onResponse, deletesListener::onFailure);\n+        }\n+        allDeletesDone.get();\n+\n+        logger.info(\"--> verifying repository state\");\n+        final RepositoryData repositoryDataAfterDeletes = getRepositoryData(repoName);\n+        // One increment for snapshot, one for all the deletes\n+        assertThat(repositoryDataAfterDeletes.getGenId(), is(repoGenAfterInitialSnapshots + 2));\n+        assertThat(repositoryDataAfterDeletes.getSnapshotIds(), contains(slowSnapshotInfo.snapshotId()));\n+    }\n+\n+    public void testBlockedRepoDoesNotBlockOtherRepos() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndex(\"foo\");\n+        ensureGreen();\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startAndBlockFailingFullSnapshot(blockedRepoName, \"blocked-snapshot\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(otherRepoName, \"snapshot\")\n+                .setIndices(\"does-not-exist-*\")\n+                .setWaitForCompletion(false).get();\n+\n+        unblockNode(blockedRepoName, internalCluster().getMasterName());\n+        expectThrows(SnapshotException.class, createSlowFuture::actionGet);\n+\n+        assertBusy(() -> assertThat(currentSnapshots(otherRepoName), empty()), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testMultipleReposAreIndependent() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second concurrent snapshot.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent2() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second repository's concurrent operations.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent3() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        createFullSnapshot( blockedRepoName, \"blocked-snapshot\");\n+        blockNodeOnAnyFiles(blockedRepoName, masterNode);\n+        final ActionFuture<AcknowledgedResponse> slowDeleteFuture = startDelete(blockedRepoName, \"*\");\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, masterNode);\n+        assertAcked(slowDeleteFuture.actionGet());\n+    }\n+\n+    private static String startDataNodeWithLargeSnapshotPool() {\n+        return internalCluster().startDataOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+    }\n+    public void testSnapshotRunsAfterInProgressDelete() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        ensureGreen();\n+        createIndexWithContent(\"index-test\");\n+\n+        final String firstSnapshot = \"first-snapshot\";\n+        createFullSnapshot(repoName, firstSnapshot);\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDelete(repoName, firstSnapshot);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"second-snapshot\");\n+\n+        unblockNode(repoName, masterNode);\n+        final UncategorizedExecutionException ex = expectThrows(UncategorizedExecutionException.class, deleteFuture::actionGet);\n+        assertThat(ex.getRootCause(), instanceOf(IOException.class));\n+\n+        assertSuccessful(snapshotFuture);\n+    }\n+\n+    public void testAbortOneOfMultipleSnapshots() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String firstIndex = \"index-one\";\n+        createIndexWithContent(firstIndex);\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse =\n+                startFullSnapshotBlockedOnDataNode(firstSnapshot, repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String secondIndex = \"index-two\";\n+        createIndexWithContent(secondIndex, dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> deleteSnapshotsResponse = startDelete(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        logger.info(\"--> start third snapshot\");\n+        final ActionFuture<CreateSnapshotResponse> thirdSnapshotResponse = client().admin().cluster()\n+                .prepareCreateSnapshot(repoName, \"snapshot-three\").setIndices(secondIndex).setWaitForCompletion(true).execute();\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        unblockNode(repoName, dataNode);\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        final SnapshotInfo secondSnapshotInfo = assertSuccessful(secondSnapshotResponse);\n+        final SnapshotInfo thirdSnapshotInfo = assertSuccessful(thirdSnapshotResponse);\n+\n+        assertThat(deleteSnapshotsResponse.get().isAcknowledged(), is(true));\n+\n+        logger.info(\"--> verify that the first snapshot is gone\");\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName),\n+                containsInAnyOrder(secondSnapshotInfo, thirdSnapshotInfo));\n+    }\n+\n+    public void testCascadedAborts() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse =\n+                startFullSnapshotBlockedOnDataNode(firstSnapshot, repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        createIndexWithContent(\"index-two\", dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> deleteSnapshotsResponse = startDelete(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        final ActionFuture<CreateSnapshotResponse> thirdSnapshotResponse = startFullSnapshot(repoName, \"snapshot-three\");\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        logger.info(\"--> waiting for all three snapshots to show up as in-progress\");\n+        assertBusy(() -> assertThat(currentSnapshots(repoName), hasSize(3)), 30L, TimeUnit.SECONDS);\n+\n+        final ActionFuture<AcknowledgedResponse> allDeletedResponse = startDelete(repoName, \"*\");\n+\n+        logger.info(\"--> waiting for second and third snapshot to finish\");\n+        assertBusy(() -> {\n+            assertThat(currentSnapshots(repoName), hasSize(1));\n+            final SnapshotsInProgress snapshotsInProgress = clusterService().state().custom(SnapshotsInProgress.TYPE);\n+            assertThat(snapshotsInProgress.entries().get(0).state(), is(SnapshotsInProgress.State.ABORTED));\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        unblockNode(repoName, dataNode);\n+\n+        logger.info(\"--> verify all snapshots were aborted\");\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+        assertThat(secondSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+        assertThat(thirdSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        logger.info(\"--> verify both deletes have completed\");\n+        assertAcked(deleteSnapshotsResponse.get());\n+        assertAcked(allDeletedResponse.get());\n+\n+        logger.info(\"--> verify that all snapshots are gone\");\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+    }\n+\n+    public void testMasterFailOverWithQueuedDeletes() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String firstIndex = \"index-one\";\n+        createIndexWithContent(firstIndex);\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        blockDataNode(repoName, dataNode);\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse = startFullSnapshotFromNonMasterClient(repoName, firstSnapshot);\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(5);\n+        final String secondIndex = \"index-two\";\n+        createIndexWithContent(secondIndex, dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> firstDeleteFuture = startDeleteFromNonMasterClient(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        blockDataNode(repoName, dataNode2);\n+        final ActionFuture<CreateSnapshotResponse> snapshotThreeFuture = startFullSnapshotFromNonMasterClient(repoName, \"snapshot-three\");\n+        waitForBlock(dataNode2, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        logger.info(\"--> waiting for all three snapshots to show up as in-progress\");\n+        assertBusy(() -> assertThat(currentSnapshots(repoName), hasSize(3)), 30L, TimeUnit.SECONDS);\n+\n+        final ActionFuture<AcknowledgedResponse> deleteAllSnapshots = startDeleteFromNonMasterClient(repoName, \"*\");\n+        logger.info(\"--> wait for delete to be enqueued in cluster state\");\n+        awaitClusterState(state -> {\n+            final SnapshotDeletionsInProgress deletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE);\n+            return deletionsInProgress.getEntries().size() == 1 && deletionsInProgress.getEntries().get(0).getSnapshots().size() == 3;\n+        });\n+\n+        logger.info(\"--> waiting for second snapshot to finish and the other two snapshots to become aborted\");\n+        assertBusy(() -> {\n+            assertThat(currentSnapshots(repoName), hasSize(2));\n+            for (SnapshotsInProgress.Entry entry\n+                    : clusterService().state().custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n+                assertThat(entry.state(), is(SnapshotsInProgress.State.ABORTED));\n+                assertThat(entry.snapshot().getSnapshotId().getName(), not(secondSnapshot));\n+            }\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        logger.info(\"--> stopping current master node\");\n+        internalCluster().stopCurrentMasterNode();\n+\n+        unblockNode(repoName, dataNode);\n+        unblockNode(repoName, dataNode2);\n+\n+        assertAcked(firstDeleteFuture.get());\n+        assertAcked(deleteAllSnapshots.get());\n+        expectThrows(SnapshotException.class, snapshotThreeFuture::actionGet);\n+\n+        logger.info(\"--> verify that all snapshots are gone and no more work is left in the cluster state\");\n+        assertBusy(() -> {\n+            assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+            final ClusterState state = clusterService().state();\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE);\n+            assertThat(snapshotsInProgress.entries(), empty());\n+            final SnapshotDeletionsInProgress snapshotDeletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE);\n+            assertThat(snapshotDeletionsInProgress.getEntries(), empty());\n+        }, 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testAssertMultipleSnapshotsAndPrimaryFailOver() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String testIndex = \"index-one\";\n+        createIndex(testIndex, Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 1).put(SETTING_NUMBER_OF_REPLICAS, 1).build());\n+        ensureYellow(testIndex);\n+        indexDoc(testIndex, \"some_id\", \"foo\", \"bar\");\n+\n+        blockDataNode(repoName, dataNode);\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse = startFullSnapshotFromMasterClient(repoName, \"snapshot-one\");\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        ensureGreen(testIndex);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshotFromMasterClient(repoName, secondSnapshot);\n+\n+        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n+\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.PARTIAL));\n+        assertThat(secondSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.PARTIAL));\n+    }\n+\n+    public void testQueuedDeletesWithFailures() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> firstDeleteFuture = startDelete(repoName, \"*\");\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"snapshot-queued\");\n+        awaitNSnapshotsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> secondDeleteFuture = startDelete(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        unblockNode(repoName, masterNode);\n+        expectThrows(UncategorizedExecutionException.class, firstDeleteFuture::actionGet);\n+\n+        // Second delete works out cleanly since the repo is unblocked now\n+        assertThat(secondDeleteFuture.get().isAcknowledged(), is(true));\n+        // Snapshot should have been aborted\n+        assertThat(snapshotFuture.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+    }\n+\n+    public void testQueuedDeletesWithOverlap() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final ActionFuture<AcknowledgedResponse> firstDeleteFuture = startAndBlockOnDeleteSnapshot(repoName, \"*\");\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"snapshot-queued\");\n+        awaitNSnapshotsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> secondDeleteFuture = startDelete(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        unblockNode(repoName, masterNode);\n+        assertThat(firstDeleteFuture.get().isAcknowledged(), is(true));\n+\n+        // Second delete works out cleanly since the repo is unblocked now\n+        assertThat(secondDeleteFuture.get().isAcknowledged(), is(true));\n+        // Snapshot should have been aborted\n+        assertThat(snapshotFuture.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+    }\n+\n+    public void testQueuedOperationsOnMasterRestart() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        startAndBlockOnDeleteSnapshot(repoName, \"*\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(repoName, \"snapshot-three\").setWaitForCompletion(false).get();\n+\n+        startDelete(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        internalCluster().stopCurrentMasterNode();\n+        ensureStableCluster(3);\n+\n+        awaitNoMoreRunningOperations();\n+    }\n+\n+    public void testQueuedOperationsOnMasterDisconnect() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final String masterNode = internalCluster().getMasterName();\n+        final NetworkDisruption networkDisruption = isolateMasterDisruption(NetworkDisruption.DISCONNECT);\n+        internalCluster().setDisruptionScheme(networkDisruption);\n+\n+        blockNodeOnAnyFiles(repoName, masterNode);\n+        ActionFuture<AcknowledgedResponse> firstDeleteFuture = client(masterNode).admin().cluster()\n+                .prepareDeleteSnapshot(repoName, \"*\").execute();\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> createThirdSnapshot = client(masterNode).admin().cluster()\n+                .prepareCreateSnapshot(repoName, \"snapshot-three\").setWaitForCompletion(true).execute();\n+        awaitNSnapshotsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> secondDeleteFuture =\n+                client(masterNode).admin().cluster().prepareDeleteSnapshot(repoName, \"*\").execute();\n+        awaitNDeletionsInProgress(2);\n+\n+        networkDisruption.startDisrupting();\n+        ensureStableCluster(3, dataNode);\n+        unblockNode(repoName, masterNode);\n+        networkDisruption.stopDisrupting();\n+\n+        logger.info(\"--> make sure all failing requests get a response\");\n+        expectThrows(RepositoryException.class, firstDeleteFuture::actionGet);\n+        expectThrows(RepositoryException.class, secondDeleteFuture::actionGet);\n+        expectThrows(SnapshotException.class, createThirdSnapshot::actionGet);\n+\n+        awaitNoMoreRunningOperations();\n+    }\n+\n+    public void testQueuedOperationsOnMasterDisconnectAndRepoFailure() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final String masterNode = internalCluster().getMasterName();\n+        final NetworkDisruption networkDisruption = isolateMasterDisruption(NetworkDisruption.DISCONNECT);\n+        internalCluster().setDisruptionScheme(networkDisruption);\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<CreateSnapshotResponse> firstFailedSnapshotFuture =\n+                startFullSnapshotFromMasterClient(repoName, \"failing-snapshot-1\");\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        final ActionFuture<CreateSnapshotResponse> secondFailedSnapshotFuture =\n+                startFullSnapshotFromMasterClient(repoName, \"failing-snapshot-2\");\n+        awaitNSnapshotsInProgress(2);\n+\n+        final ActionFuture<AcknowledgedResponse> failedDeleteFuture =\n+                client(masterNode).admin().cluster().prepareDeleteSnapshot(repoName, \"*\").execute();\n+        awaitNDeletionsInProgress(1);\n+\n+        networkDisruption.startDisrupting();\n+        ensureStableCluster(3, dataNode);\n+        unblockNode(repoName, masterNode);\n+        networkDisruption.stopDisrupting();\n+\n+        logger.info(\"--> make sure all failing requests get a response\");\n+        expectThrows(SnapshotException.class, firstFailedSnapshotFuture::actionGet);\n+        expectThrows(SnapshotException.class, secondFailedSnapshotFuture::actionGet);\n+        expectThrows(RepositoryException.class, failedDeleteFuture::actionGet);\n+\n+        awaitNoMoreRunningOperations();\n+    }\n+\n+    public void testQueuedOperationsAndBrokenRepoOnMasterFailOver() throws Exception {\n+        disableRepoConsistencyCheck(\"This test corrupts the repository on purpose\");\n+\n+        internalCluster().startMasterOnlyNodes(3);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        final Path repoPath = randomRepoPath();\n+        createRepository(repoName, \"mock\", repoPath);\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final long generation = getRepositoryData(repoName).getGenId();\n+\n+        startAndBlockOnDeleteSnapshot(repoName, \"*\");\n+\n+        corruptIndexN(repoPath, generation);\n+\n+        client().admin().cluster().prepareCreateSnapshot(repoName, \"snapshot-three\").setWaitForCompletion(false).get();\n+\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDeleteFromNonMasterClient(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        internalCluster().stopCurrentMasterNode();\n+        ensureStableCluster(3);\n+\n+        awaitNoMoreRunningOperations();\n+        expectThrows(RepositoryException.class, deleteFuture::actionGet);", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjc5ODEyMw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452798123", "bodyText": "Not really, it's just \"failed to update repository\". It's all in the cause here, but that's also just a JSON parse failure.", "author": "original-brownbear", "createdAt": "2020-07-10T11:57:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY3NzIxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY4MjI2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452682269", "bodyText": "nit: assertConsistency -> assertNoConcurrentDeletionsForSameRepository() ?", "author": "tlrx", "createdAt": "2020-07-10T07:52:35Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -61,7 +67,18 @@ public static SnapshotDeletionsInProgress of(List<SnapshotDeletionsInProgress.En\n     }\n \n     public SnapshotDeletionsInProgress(StreamInput in) throws IOException {\n-        this.entries = Collections.unmodifiableList(in.readList(Entry::new));\n+        this(in.readList(Entry::new));\n+    }\n+\n+    private static boolean assertConsistency(List<Entry> entries) {", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY4NjQ1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452686456", "bodyText": "I'm not sure to understand why we catch and rethrow here", "author": "tlrx", "createdAt": "2020-07-10T08:00:31Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -438,16 +477,34 @@ public static State fromValue(byte value) {\n \n     private final List<Entry> entries;\n \n+    private static boolean assertConsistentEntries(List<Entry> entries) {\n+        final Map<String, Set<ShardId>> assignedShardsByRepo = new HashMap<>();\n+        for (Entry entry : entries) {\n+            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shard : entry.shards()) {\n+                if (shard.value.isActive()) {\n+                    assert assignedShardsByRepo.computeIfAbsent(entry.repository(), k -> new HashSet<>()).add(shard.key) :\n+                            \"Found duplicate shard assignments in \" + entries;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n     public static SnapshotsInProgress of(List<Entry> entries) {\n         if (entries.isEmpty()) {\n             return EMPTY;\n         }\n         return new SnapshotsInProgress(Collections.unmodifiableList(entries));\n     }\n \n-    private SnapshotsInProgress(List<Entry> entries) {\n-        this.entries = entries;\n-    }\n+    private SnapshotsInProgress(List <Entry> entries) {\n+            this.entries = entries;\n+            try {\n+                assert assertConsistentEntries(entries);\n+            } catch (AssertionError e) {\n+                throw e;", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjc5ODcxOA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452798718", "bodyText": "So I could put a debug breakpoint there :D  Thanks for spotting!", "author": "original-brownbear", "createdAt": "2020-07-10T11:58:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY4NjQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5MTIwNA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452691204", "bodyText": "allow -> allowed", "author": "tlrx", "createdAt": "2020-07-10T08:10:18Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -142,6 +158,18 @@\n \n     private final TransportService transportService;\n \n+    private final OngoingRepositoryOperations repositoryOperations = new OngoingRepositoryOperations();\n+\n+    /**\n+     * Setting that specifies the maximum number of allow concurrent snapshot create and delete operations in the", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5MjYzMg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452692632", "bodyText": "When backporting, we could maybe indicate in the error message that concurrent snapshot/deletions are possible in version 7.9?", "author": "tlrx", "createdAt": "2020-07-10T08:13:17Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -204,26 +235,36 @@ public ClusterState execute(ClusterState currentState) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                if (runningSnapshots.stream().anyMatch(s -> {\n+                    final Snapshot running = s.snapshot();\n+                    return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+                })) {\n+                    throw new InvalidSnapshotNameException(\n+                            repository.getMetadata().name(), snapshotName, \"snapshot with the same name is already in-progress\");\n+                }\n                 validate(repositoryName, snapshotName, currentState);\n+                final boolean concurrentOperationsAllowed = currentState.nodes().getMinNodeVersion().onOrAfter(FULL_CONCURRENCY_VERSION);\n                 final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.hasDeletionsInProgress() && concurrentOperationsAllowed == false) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjc5OTY0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452799641", "bodyText": "\ud83d\udc4d Right, I put down a note for that when doing the back-port work.", "author": "original-brownbear", "createdAt": "2020-07-10T12:00:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5MjYzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5NTA5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452695097", "bodyText": "What happen if multiple snapshot operations are started but the maxConcurrentOperations settings is updated to a value lower than the current number of concurrent ops? Would it still be possible to enque more ops?", "author": "tlrx", "createdAt": "2020-07-10T08:18:16Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -204,26 +235,36 @@ public ClusterState execute(ClusterState currentState) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                if (runningSnapshots.stream().anyMatch(s -> {\n+                    final Snapshot running = s.snapshot();\n+                    return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+                })) {\n+                    throw new InvalidSnapshotNameException(\n+                            repository.getMetadata().name(), snapshotName, \"snapshot with the same name is already in-progress\");\n+                }\n                 validate(repositoryName, snapshotName, currentState);\n+                final boolean concurrentOperationsAllowed = currentState.nodes().getMinNodeVersion().onOrAfter(FULL_CONCURRENCY_VERSION);\n                 final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.hasDeletionsInProgress() && concurrentOperationsAllowed == false) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress =\n-                    currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+                        currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n                 if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n-                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n                 // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a\n                 // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the\n                 // cluster state anyway in #applyClusterState.\n-                if (snapshots.entries().stream().anyMatch(entry -> entry.state() != State.INIT)) {\n+                if (concurrentOperationsAllowed == false && runningSnapshots.stream().anyMatch(entry -> entry.state() != State.INIT)) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \" a snapshot is already running\");\n                 }\n+                ensureBelowConcurrencyLimit(repositoryName, snapshotName, snapshots, deletionsInProgress);", "originalCommit": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgwMDIzNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452800237", "bodyText": "What happen if multiple snapshot operations are started but the maxConcurrentOperations settings is updated to a value lower than the current number of concurrent ops?\n\nExisting ops won't be affected but you can't start new ones.\n\nWould it still be possible to enque more ops?\nNo then the ops have to come down to below the new limit first before we can enqueue more.", "author": "original-brownbear", "createdAt": "2020-07-10T12:02:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5NTA5Nw=="}], "type": "inlineReview"}, {"oid": "382962b4026bf4c373e9f0e557c8588922fa0b87", "url": "https://github.com/elastic/elasticsearch/commit/382962b4026bf4c373e9f0e557c8588922fa0b87", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-07-10T11:53:28Z", "type": "commit"}, {"oid": "9e1a0db6e10e5882ddbc0b6ca80560e2bff38df5", "url": "https://github.com/elastic/elasticsearch/commit/9e1a0db6e10e5882ddbc0b6ca80560e2bff38df5", "message": "review comments", "committedDate": "2020-07-10T12:07:39Z", "type": "commit"}, {"oid": "346f306de9f8644f0a0b995377a6364c78f8abf5", "url": "https://github.com/elastic/elasticsearch/commit/346f306de9f8644f0a0b995377a6364c78f8abf5", "message": "Fully Concurrent Snapshots\n\nImplements fully concurrent snapshot operations. See documentation changes\nto snapshot package level JavaDoc for details.", "committedDate": "2020-06-15T09:10:38Z", "type": "commit"}, {"oid": "346f306de9f8644f0a0b995377a6364c78f8abf5", "url": "https://github.com/elastic/elasticsearch/commit/346f306de9f8644f0a0b995377a6364c78f8abf5", "message": "Fully Concurrent Snapshots\n\nImplements fully concurrent snapshot operations. See documentation changes\nto snapshot package level JavaDoc for details.", "committedDate": "2020-06-15T09:10:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA0NjYxNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440046617", "bodyText": "All scenarios covered by this test become obsolete. The actual premise of this test (checking that we don't dead-lock from blocked threads) is covered by the fact that SnapshotResiliencyTests work for the most part anyway.", "author": "original-brownbear", "createdAt": "2020-06-15T09:28:01Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/MinThreadsSnapshotRestoreIT.java", "diffHunk": "@@ -1,155 +0,0 @@\n-/*\n- * Licensed to Elasticsearch under one or more contributor\n- * license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright\n- * ownership. Elasticsearch licenses this file to you under\n- * the Apache License, Version 2.0 (the \"License\"); you may\n- * not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.snapshots;\n-\n-import org.elasticsearch.action.ActionFuture;\n-import org.elasticsearch.action.support.master.AcknowledgedResponse;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.common.unit.TimeValue;\n-import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.repositories.RepositoriesService;\n-import org.elasticsearch.snapshots.mockstore.MockRepository;\n-\n-import java.util.Collection;\n-import java.util.Collections;\n-\n-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n-import static org.hamcrest.Matchers.containsString;\n-\n-/**\n- * Tests for snapshot/restore that require at least 2 threads available\n- * in the thread pool (for example, tests that use the mock repository that\n- * block on master).\n- */\n-public class MinThreadsSnapshotRestoreIT extends AbstractSnapshotIntegTestCase {", "originalCommit": "346f306de9f8644f0a0b995377a6364c78f8abf5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0fffc109c03ae853f5a6f870b9a49d5f0b92fa1f", "url": "https://github.com/elastic/elasticsearch/commit/0fffc109c03ae853f5a6f870b9a49d5f0b92fa1f", "message": "cleanup todo", "committedDate": "2020-06-15T09:37:46Z", "type": "commit"}, {"oid": "26e4bced273146494dbb9cf5720944d26ddf5265", "url": "https://github.com/elastic/elasticsearch/commit/26e4bced273146494dbb9cf5720944d26ddf5265", "message": "docs", "committedDate": "2020-06-15T09:42:56Z", "type": "commit"}, {"oid": "7076a0f88008de717ff836d0ee00aa6d68806ba5", "url": "https://github.com/elastic/elasticsearch/commit/7076a0f88008de717ff836d0ee00aa6d68806ba5", "message": "cleanup", "committedDate": "2020-06-15T09:45:27Z", "type": "commit"}, {"oid": "7cd1c4e174648f4f00997d4d13fd84f650af89b8", "url": "https://github.com/elastic/elasticsearch/commit/7cd1c4e174648f4f00997d4d13fd84f650af89b8", "message": "doc", "committedDate": "2020-06-15T09:48:18Z", "type": "commit"}, {"oid": "a9b894c8be2321730e3e2ee77cd2528133a13da2", "url": "https://github.com/elastic/elasticsearch/commit/a9b894c8be2321730e3e2ee77cd2528133a13da2", "message": "reduce noise", "committedDate": "2020-06-15T09:51:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA2Mjk1MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440062950", "bodyText": "This fixes a tricky bug that might have been trouble for aborts with pre-7.6 repository metadata on S3 and generally wastes a lot of shard status updates. I'll fix this in a separate PR right away, just adding a node here to not forget to handle that first.", "author": "original-brownbear", "createdAt": "2020-06-15T09:56:06Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "diffHunk": "@@ -232,7 +232,7 @@ private void startNewSnapshots(SnapshotsInProgress snapshotsInProgress) {\n                     if (snapshotStatus == null) {\n                         // due to CS batching we might have missed the INIT state and straight went into ABORTED\n                         // notify master that abort has completed by moving to FAILED\n-                        if (shard.value.state() == ShardState.ABORTED) {\n+                        if (shard.value.state() == ShardState.ABORTED && localNodeId.equals(shard.value.nodeId())) {", "originalCommit": "a9b894c8be2321730e3e2ee77cd2528133a13da2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3MzE2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441273163", "bodyText": "#58214", "author": "original-brownbear", "createdAt": "2020-06-17T04:27:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA2Mjk1MA=="}], "type": "inlineReview"}, {"oid": "98516c6a2ba50a6237d94ab95c01f767c21d41cb", "url": "https://github.com/elastic/elasticsearch/commit/98516c6a2ba50a6237d94ab95c01f767c21d41cb", "message": "less noise", "committedDate": "2020-06-15T09:59:13Z", "type": "commit"}, {"oid": "37d439f3254ad08e6515f4e36c97507dff715499", "url": "https://github.com/elastic/elasticsearch/commit/37d439f3254ad08e6515f4e36c97507dff715499", "message": "fix test", "committedDate": "2020-06-15T10:50:34Z", "type": "commit"}, {"oid": "8d10c4c505ee209f514b29c7a3a7e412883be077", "url": "https://github.com/elastic/elasticsearch/commit/8d10c4c505ee209f514b29c7a3a7e412883be077", "message": "cover primary fail-over", "committedDate": "2020-06-15T11:04:20Z", "type": "commit"}, {"oid": "ca110eceade017a0bd3e530b2604d101f3a8df19", "url": "https://github.com/elastic/elasticsearch/commit/ca110eceade017a0bd3e530b2604d101f3a8df19", "message": "drier tests", "committedDate": "2020-06-15T11:38:18Z", "type": "commit"}, {"oid": "75a4aaddf1350e075fd1bfd60e2e34006fd02a8c", "url": "https://github.com/elastic/elasticsearch/commit/75a4aaddf1350e075fd1bfd60e2e34006fd02a8c", "message": "fix test", "committedDate": "2020-06-15T11:47:15Z", "type": "commit"}, {"oid": "37e799430b19d86e96d582749249438d19dbbfcc", "url": "https://github.com/elastic/elasticsearch/commit/37e799430b19d86e96d582749249438d19dbbfcc", "message": "update comment", "committedDate": "2020-06-15T11:54:19Z", "type": "commit"}, {"oid": "8a5eed5af63247266515abc432bd7b8e35f6f11d", "url": "https://github.com/elastic/elasticsearch/commit/8a5eed5af63247266515abc432bd7b8e35f6f11d", "message": "drier", "committedDate": "2020-06-15T12:04:58Z", "type": "commit"}, {"oid": "457c4438783b3ac466874c169805d71c99fc783b", "url": "https://github.com/elastic/elasticsearch/commit/457c4438783b3ac466874c169805d71c99fc783b", "message": "make things nicer looking", "committedDate": "2020-06-15T12:25:28Z", "type": "commit"}, {"oid": "66c642857c5ea4bdf69e61d775503d79c91539fe", "url": "https://github.com/elastic/elasticsearch/commit/66c642857c5ea4bdf69e61d775503d79c91539fe", "message": "drop pointless short-circuit", "committedDate": "2020-06-15T12:35:46Z", "type": "commit"}, {"oid": "72dbbcfd8f6c350d30374903ac9b7e1256d62993", "url": "https://github.com/elastic/elasticsearch/commit/72dbbcfd8f6c350d30374903ac9b7e1256d62993", "message": "doc", "committedDate": "2020-06-15T12:40:19Z", "type": "commit"}, {"oid": "fc267dcd23b13958eeb01308d6219d683ba16e8d", "url": "https://github.com/elastic/elasticsearch/commit/fc267dcd23b13958eeb01308d6219d683ba16e8d", "message": "moar test", "committedDate": "2020-06-15T12:59:51Z", "type": "commit"}, {"oid": "fa5176f102fe07b5e3488f32baca30ada7cb80fb", "url": "https://github.com/elastic/elasticsearch/commit/fa5176f102fe07b5e3488f32baca30ada7cb80fb", "message": "another corner case fixed", "committedDate": "2020-06-15T16:24:55Z", "type": "commit"}, {"oid": "c976fce0eafb1eda86ba79fd25bf6911f466af38", "url": "https://github.com/elastic/elasticsearch/commit/c976fce0eafb1eda86ba79fd25bf6911f466af38", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-15T16:25:01Z", "type": "commit"}, {"oid": "6cb598911b2d6e116d3b57f28bdbf74e59d0c1a6", "url": "https://github.com/elastic/elasticsearch/commit/6cb598911b2d6e116d3b57f28bdbf74e59d0c1a6", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-15T20:42:11Z", "type": "commit"}, {"oid": "075a0b74a2ed638574e88d998680bfa652734f17", "url": "https://github.com/elastic/elasticsearch/commit/075a0b74a2ed638574e88d998680bfa652734f17", "message": "more docs", "committedDate": "2020-06-15T21:12:27Z", "type": "commit"}, {"oid": "e27ec41367b339ce7c155233090b7b16247d35ae", "url": "https://github.com/elastic/elasticsearch/commit/e27ec41367b339ce7c155233090b7b16247d35ae", "message": "bck", "committedDate": "2020-06-15T21:13:36Z", "type": "commit"}, {"oid": "711770757a75a1822f1a4e59f71875c50c9f95f9", "url": "https://github.com/elastic/elasticsearch/commit/711770757a75a1822f1a4e59f71875c50c9f95f9", "message": "save some more repo data loading", "committedDate": "2020-06-16T07:48:29Z", "type": "commit"}, {"oid": "cd142a9602e21fc608975f0bb028a5bad8a1631c", "url": "https://github.com/elastic/elasticsearch/commit/cd142a9602e21fc608975f0bb028a5bad8a1631c", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-16T07:52:31Z", "type": "commit"}, {"oid": "34f934551f1ec665538cebc342c0204cfcb05732", "url": "https://github.com/elastic/elasticsearch/commit/34f934551f1ec665538cebc342c0204cfcb05732", "message": "bck", "committedDate": "2020-06-16T10:07:08Z", "type": "commit"}, {"oid": "1cdab7555676144c07d3ba03c4ee6dc26eaf3f84", "url": "https://github.com/elastic/elasticsearch/commit/1cdab7555676144c07d3ba03c4ee6dc26eaf3f84", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-16T10:10:37Z", "type": "commit"}, {"oid": "de9a3f77dcd0009a8ebdc90fb25036ac956c0dea", "url": "https://github.com/elastic/elasticsearch/commit/de9a3f77dcd0009a8ebdc90fb25036ac956c0dea", "message": "moar corner cases", "committedDate": "2020-06-16T12:30:19Z", "type": "commit"}, {"oid": "166e639b9013eaaa30eb17918cb4edf3649dd279", "url": "https://github.com/elastic/elasticsearch/commit/166e639b9013eaaa30eb17918cb4edf3649dd279", "message": "cleanup tests more", "committedDate": "2020-06-16T13:32:10Z", "type": "commit"}, {"oid": "cadad70d5bf03d0d88ecace425225ce506646b11", "url": "https://github.com/elastic/elasticsearch/commit/cadad70d5bf03d0d88ecace425225ce506646b11", "message": "cleanups", "committedDate": "2020-06-16T14:51:30Z", "type": "commit"}, {"oid": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24", "url": "https://github.com/elastic/elasticsearch/commit/0e726a009a0b6a4fa50871f346a7c3b912fd6e24", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-16T14:51:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzMjYwNg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440932606", "bodyText": "This is a scenario that was complicated before but became very complicated now, both fail-over on delete and create. We could put more effort into this by doing things like #54350 but I think it's not a priority relative to getting concurrent operations work functionally so I just went with the brute-force fail everything on master fail-over approach here as that's the current behavior for single snapshots.", "author": "original-brownbear", "createdAt": "2020-06-16T15:17:14Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1441,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            synchronized (currentlyFinalizing) {\n+                boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+                assert added;\n+                Repository repository = repositoriesService.repository(deleteEntry.repository());\n+                final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+                assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                        \"incorrect state for entry [\" + deleteEntry + \"]\";\n+                repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                        snapshotIds,\n+                        repositoryStateId,\n+                        minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                        ActionListener.wrap(updatedRepoData -> {\n+                                    logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                    removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                                }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                        )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+            }\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);\n+                        }\n+                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions)\n+                                .putCustom(SnapshotsInProgress.TYPE, updatedSnapshotsInProgress).build();\n                     }\n                 }\n                 return currentState;\n             }\n \n             @Override\n             public void onFailure(String source, Exception e) {\n-                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", snapshotIds), e);\n-                if (listener != null) {\n-                    listener.onFailure(e);\n+                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", deleteEntry), e);", "originalCommit": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzNzQ0MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440937440", "bodyText": "Here it was easy to expand coverage but unfortunately SnapshotResiliencyTests don't really cover repository exception handling so that's where ConcurrentSnapshotITs come from.", "author": "original-brownbear", "createdAt": "2020-06-16T15:21:53Z", "path": "server/src/test/java/org/elasticsearch/snapshots/SnapshotResiliencyTests.java", "diffHunk": "@@ -594,11 +582,16 @@ public void testBulkSnapshotDeleteWithAbort() {\n                 createIndexResponse -> client().admin().cluster().prepareCreateSnapshot(repoName, snapshotName)\n                         .setWaitForCompletion(true).execute(createSnapshotResponseStepListener));\n \n-        final StepListener<CreateSnapshotResponse> createOtherSnapshotResponseStepListener = new StepListener<>();\n+        final int inProgressSnapshots = randomIntBetween(1, 5);", "originalCommit": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzOTU4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441339582", "bodyText": "What do you mean by \"cover repository exception handling\"?", "author": "ywelsch", "createdAt": "2020-06-17T07:32:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzNzQ0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMjEwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444302109", "bodyText": "IOExceptions thrown by the repository. We don't have any simulation of that in SnapshotResiliencyTests", "author": "original-brownbear", "createdAt": "2020-06-23T15:13:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzNzQ0MA=="}], "type": "inlineReview"}, {"oid": "05d87183ba412dea5ddd36511ed9efeb5ff55cfc", "url": "https://github.com/elastic/elasticsearch/commit/05d87183ba412dea5ddd36511ed9efeb5ff55cfc", "message": "be smarter about failover handling", "committedDate": "2020-06-16T16:44:03Z", "type": "commit"}, {"oid": "cf39da642d4a2f14a85dc8fb25b8e14e81cdb181", "url": "https://github.com/elastic/elasticsearch/commit/cf39da642d4a2f14a85dc8fb25b8e14e81cdb181", "message": "fewer CS updates", "committedDate": "2020-06-16T17:19:10Z", "type": "commit"}, {"oid": "7762343df83fe8f630d7a593823dbc12ccf2c72b", "url": "https://github.com/elastic/elasticsearch/commit/7762343df83fe8f630d7a593823dbc12ccf2c72b", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-16T18:38:59Z", "type": "commit"}, {"oid": "e90de9dba7c9ae2dc51e164fc18be2b9e61f44fb", "url": "https://github.com/elastic/elasticsearch/commit/e90de9dba7c9ae2dc51e164fc18be2b9e61f44fb", "message": "nicer", "committedDate": "2020-06-16T19:54:05Z", "type": "commit"}, {"oid": "e69d0b72ce99a5f45c25cade22d442a6a8cdc454", "url": "https://github.com/elastic/elasticsearch/commit/e69d0b72ce99a5f45c25cade22d442a6a8cdc454", "message": "bck", "committedDate": "2020-06-16T20:15:14Z", "type": "commit"}, {"oid": "2abeda287ac168b509633c4763d1c6e9d78c52e4", "url": "https://github.com/elastic/elasticsearch/commit/2abeda287ac168b509633c4763d1c6e9d78c52e4", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-16T20:34:59Z", "type": "commit"}, {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "url": "https://github.com/elastic/elasticsearch/commit/c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-17T03:26:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3NTQ0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441275445", "bodyText": "I know this class is massive, but I did my best to get full coverage of all possible permutations of master fail-over and repository IO issues here (we definitely had some gaps in our testing without it even for single snapshots as this one caught #58214).", "author": "original-brownbear", "createdAt": "2020-06-17T04:37:20Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,889 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateObserver;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5NjE1MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446196150", "bodyText": "Given the complexity of the SnapshotsService class and friends, I wonder if we should try to get branch coverage on that class, to see if our tests (when run repeatedly) at least somehow cover all implementation aspects. Play a bit with a coverage tool (I used jacoco with Intellij for this when testing Zen2)", "author": "ywelsch", "createdAt": "2020-06-26T13:50:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3NTQ0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3NTg0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441275842", "bodyText": "Needed the new id here to get some fixed coordinate/key for a delete operation to connect completion listeners to when batching deletes up.", "author": "original-brownbear", "createdAt": "2020-06-17T04:39:09Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -176,17 +231,23 @@ public String toString() {\n     public static final class Entry implements Writeable, RepositoryOperation {\n         private final List<SnapshotId> snapshots;\n         private final String repoName;\n+        private final State state;\n         private final long startTime;\n         private final long repositoryStateId;\n+        private final String uuid;", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODAwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441278001", "bodyText": "Needed to know whether or not a delete can be executed yet or is still open to have snapshot ids added to it. Somewhat analogous to snapshot in progress state.\nTechnically, a boolean would be enough here. But I'd like to keep that state enum here to maybe further enhance the state machine with a cleanup step. As an improvement for concurrent operations, we could already move on to the next snapshot operation after master is done writing the new metadata during a delete but before it deleted all the unreferrenced blobs. It would be nice for the cleanup to still be recorded in the cluster state though as otherwise back-to-back deletes might list out and delete the same blobs concurrently+redundantly. It would just be easier to create that follow-up if we didn't go with a flag here now.", "author": "original-brownbear", "createdAt": "2020-06-17T04:47:35Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -176,17 +231,23 @@ public String toString() {\n     public static final class Entry implements Writeable, RepositoryOperation {\n         private final List<SnapshotId> snapshots;\n         private final String repoName;\n+        private final State state;", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODY0NA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441278644", "bodyText": "If two or more deletes share overlapping snapshots but couldn't be batched into one we need to \"clean up\" deleted snapshots from existing deletes so that the remaining snapshots don't fail trying to delete already deleted snapshots.", "author": "original-brownbear", "createdAt": "2020-06-17T04:50:12Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -76,13 +92,52 @@ public SnapshotDeletionsInProgress withAddedEntry(Entry entry) {\n     }\n \n     /**\n-     * Returns a new instance of {@link SnapshotDeletionsInProgress} which removes\n-     * the given entry from the invoking instance.\n+     * Returns a new instance of {@link SnapshotDeletionsInProgress} that has the entry with the given {@code deleteUUID} removed from its\n+     * entries.\n      */\n-    public SnapshotDeletionsInProgress withRemovedEntry(Entry entry) {\n-        List<Entry> entries = new ArrayList<>(getEntries());\n-        entries.remove(entry);\n-        return new SnapshotDeletionsInProgress(entries);\n+    public SnapshotDeletionsInProgress withRemovedEntry(String deleteUUID) {\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size() - 1);\n+        boolean removed = false;\n+        for (Entry entry : entries) {\n+            if (entry.uuid().equals(deleteUUID)) {\n+                removed = true;\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return removed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedSnapshotIds(String repository, Collection<SnapshotId> snapshotIds) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODkxMw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441278913", "bodyText": "If we fail loading repository data for a repository but have queued up deletes we just drop all the deletes for the repo on failure.", "author": "original-brownbear", "createdAt": "2020-06-17T04:51:08Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -76,13 +92,52 @@ public SnapshotDeletionsInProgress withAddedEntry(Entry entry) {\n     }\n \n     /**\n-     * Returns a new instance of {@link SnapshotDeletionsInProgress} which removes\n-     * the given entry from the invoking instance.\n+     * Returns a new instance of {@link SnapshotDeletionsInProgress} that has the entry with the given {@code deleteUUID} removed from its\n+     * entries.\n      */\n-    public SnapshotDeletionsInProgress withRemovedEntry(Entry entry) {\n-        List<Entry> entries = new ArrayList<>(getEntries());\n-        entries.remove(entry);\n-        return new SnapshotDeletionsInProgress(entries);\n+    public SnapshotDeletionsInProgress withRemovedEntry(String deleteUUID) {\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size() - 1);\n+        boolean removed = false;\n+        for (Entry entry : entries) {\n+            if (entry.uuid().equals(deleteUUID)) {\n+                removed = true;\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return removed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedSnapshotIds(String repository, Collection<SnapshotId> snapshotIds) {\n+        boolean changed = false;\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size());\n+        for (Entry entry : entries) {\n+            if (entry.repository().equals(repository)) {\n+                final List<SnapshotId> updatedSnapshotIds = new ArrayList<>(entry.getSnapshots());\n+                if (updatedSnapshotIds.removeAll(snapshotIds)) {\n+                    changed = true;\n+                    updatedEntries.add(entry.withSnapshots(updatedSnapshotIds));\n+                } else {\n+                    updatedEntries.add(entry);\n+                }\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return changed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedRepository(String repository) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQwMjE5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441402191", "bodyText": "Do we just silently drop the deletes? How are listeners of those notified?", "author": "ywelsch", "createdAt": "2020-06-17T09:14:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYwNjAzNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441606037", "bodyText": "No if we do this in a CS update we collect the relevant entries in the CS update and fail all the listeners for which we removed the job from the state.\nSee here https://github.com/elastic/elasticsearch/pull/56911/files#diff-a0853be4492c052f24917b5c1464003dR1512", "author": "original-brownbear", "createdAt": "2020-06-17T14:50:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODkxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MDE3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441280179", "bodyText": "It's somewhat questionable to even continue tracking the generation of an operation at this point since it's always equal to the safe repository generation (except for the corner case of having just mounted the repository maybe ...). I added the generation updating here for now to keep the change-set small but I'd look into removing the generation from deletes and snapshots in a follow-up (technically easy ... but BwC of that change is tricky).", "author": "original-brownbear", "createdAt": "2020-06-17T04:56:24Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1616,6 +1615,50 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerations(ClusterState state, long oldGen, long newGen) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MTIxNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441281217", "bodyText": "This is somewhat stupid, but I made this work the same way shard state updates are processed right now (setting success if all the shards completed). A follow-up to simplify and speed things up would be to simply remove snapshots that haven't done any work yet here right away instead of going through the redundant cycle of finalizing them and then deleting them again right away.", "author": "original-brownbear", "createdAt": "2020-06-17T05:00:38Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1046,25 +1247,76 @@ public ClusterState execute(ClusterState currentState) {\n                         }\n                     }\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n-                if (snapshots != null && snapshots.entries().isEmpty() == false) {\n-                    // However other snapshots are running - cannot continue\n-                    throw new ConcurrentSnapshotExecutionException(\n-                            repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final SnapshotsInProgress updatedSnapshots;\n+\n+                if (snapshots == null) {\n+                    updatedSnapshots = new SnapshotsInProgress();\n+                } else if (minNodeVersion.onOrAfter(FULL_CONCURRENCY_VERSION)) {\n+                    updatedSnapshots = new SnapshotsInProgress(snapshots.entries().stream()\n+                            .map(existing -> {\n+                                // snapshot is started - mark every non completed shard as aborted\n+                                if (existing.state() == State.STARTED && snapshotIds.contains(existing.snapshot().getSnapshotId())) {\n+                                    final ImmutableOpenMap<ShardId, ShardSnapshotStatus> abortedShards = abortEntry(existing);\n+                                    final boolean isCompleted = completed(abortedShards.values());\n+                                    final SnapshotsInProgress.Entry abortedEntry = new SnapshotsInProgress.Entry(\n+                                            existing, isCompleted ? State.SUCCESS : State.ABORTED, abortedShards,", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MjM4OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441282388", "bodyText": "This is fairly complex code now unfortunately since we have to deal with assigning new work where possible across queued up snapshots, truncate queued up deletes and handle the complex failure situation of having failed to read repository data before executing the delete.\nI did my best to comment reasonably and tried to reuse the shard assignment logic used during snapshot create to keep it simple instead of optimal (see added TODO about optimizing this logic).", "author": "original-brownbear", "createdAt": "2020-06-17T05:05:06Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MzIyMg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441283222", "bodyText": "Same as with snapshot finalizations, much easier to keep things consistent (and faster) by passing the repository data through the listeners instead of having to reload it over and over.", "author": "original-brownbear", "createdAt": "2020-06-17T05:08:37Z", "path": "server/src/main/java/org/elasticsearch/repositories/Repository.java", "diffHunk": "@@ -152,7 +152,7 @@ void finalizeSnapshot(SnapshotId snapshotId, ShardGenerations shardGenerations,\n      * @param listener              completion listener\n      */\n     void deleteSnapshots(Collection<SnapshotId> snapshotIds, long repositoryStateId, Version repositoryMetaVersion,\n-                         ActionListener<Void> listener);\n+                         ActionListener<RepositoryData> listener);", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5cc68d4e6717c57eb4f29a284f74e9d0411b845d", "url": "https://github.com/elastic/elasticsearch/commit/5cc68d4e6717c57eb4f29a284f74e9d0411b845d", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-17T09:55:12Z", "type": "commit"}, {"oid": "72d12f7a3bffabd455fa85b40dc55b16e44ab09e", "url": "https://github.com/elastic/elasticsearch/commit/72d12f7a3bffabd455fa85b40dc55b16e44ab09e", "message": "reproducer", "committedDate": "2020-06-17T10:37:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyNTczNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441325737", "bodyText": "for for", "author": "ywelsch", "createdAt": "2020-06-17T07:06:21Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -51,10 +54,23 @@\n \n     public SnapshotDeletionsInProgress(List<Entry> entries) {\n         this.entries = Collections.unmodifiableList(entries);\n+        assert entries.size() == entries.stream().map(Entry::uuid).distinct().count() : \"Found duplicate UUIDs in entries \" + entries;\n+        assert assertConsistency(entries);\n     }\n \n     public SnapshotDeletionsInProgress(StreamInput in) throws IOException {\n-        this.entries = Collections.unmodifiableList(in.readList(Entry::new));\n+        this(Collections.unmodifiableList(in.readList(Entry::new)));\n+    }\n+\n+    private static boolean assertConsistency(List<Entry> entries) {\n+        final Set<String> activeRepositories = new HashSet<>();\n+        for (Entry entry : entries) {\n+            if (entry.state() == State.META_DATA) {\n+                final boolean added = activeRepositories.add(entry.repository());\n+                assert added : \"Found multiple running deletes for for a single repository in \" + entries;", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyOTgxNg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441329816", "bodyText": "can you add some docs here as to what these States mean?", "author": "ywelsch", "createdAt": "2020-06-17T07:14:36Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -257,4 +361,26 @@ public long repositoryStateId() {\n             return repositoryStateId;\n         }\n     }\n+\n+    public enum State {\n+        WAITING((byte) 0),\n+        META_DATA((byte) 1);", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNzU4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441537585", "bodyText": "done :)", "author": "original-brownbear", "createdAt": "2020-06-17T13:18:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyOTgxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk0NjI3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444946271", "bodyText": "nit: what about renaming META_DATA to RUNNING ?", "author": "tlrx", "createdAt": "2020-06-24T14:42:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyOTgxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIwNzc3OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446207778", "bodyText": "or maybe STARTED. If you want to refine the states further here, you can still rename it from STARTED to META_DATA or whatever afterwards.", "author": "ywelsch", "createdAt": "2020-06-26T14:09:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyOTgxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzMjMyMw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441332323", "bodyText": "Instead of doing the lookup twice, perhaps just get the value and check if null", "author": "ywelsch", "createdAt": "2020-06-17T07:19:29Z", "path": "server/src/main/java/org/elasticsearch/repositories/RepositoryData.java", "diffHunk": "@@ -410,15 +410,22 @@ public IndexId resolveIndexId(final String indexName) {\n     /**\n      * Resolve the given index names to index ids, creating new index ids for\n      * new indices in the repository.\n+     *\n+     * @param indicesToResolve names of indices to resolve\n+     * @param inFlightIds      name to index mapping for currently in-flight snapshots not yet in the repository data to fall back to\n      */\n-    public List<IndexId> resolveNewIndices(final List<String> indicesToResolve) {\n+    public List<IndexId> resolveNewIndices(List<String> indicesToResolve, Map<String, IndexId> inFlightIds) {\n         List<IndexId> snapshotIndices = new ArrayList<>();\n         for (String index : indicesToResolve) {\n             final IndexId indexId;\n             if (indices.containsKey(index)) {\n                 indexId = indices.get(index);\n             } else {\n-                indexId = new IndexId(index, UUIDs.randomBase64UUID());\n+                if (inFlightIds.containsKey(index)) {\n+                    indexId = inFlightIds.get(index);", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyNTI4OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444325288", "bodyText": "Sure done", "author": "original-brownbear", "createdAt": "2020-06-23T15:46:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzMjMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzMzE0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441333145", "bodyText": "how do we expect the repositoryStateId to compare across the operations? Will it always be the same?", "author": "ywelsch", "createdAt": "2020-06-17T07:21:04Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -425,7 +425,6 @@ public void updateState(ClusterState state) {\n \n     private long bestGeneration(Collection<? extends RepositoryOperation> operations) {\n         final String repoName = metadata.name();\n-        assert operations.size() <= 1 : \"Assumed one or no operations but received \" + operations;", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU3MTQyMw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441571423", "bodyText": "removed because it was wrong", "author": "original-brownbear", "createdAt": "2020-06-17T14:04:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzMzE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxNDYxMA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444314610", "bodyText": "It's tricky. Technically, we could have different values here for a just mounted S3-like/eventually consistent repository and for back to back deletes into a freshly mounted repository.\nI looked into the details of them and all those weird corner cases we could run into here will either work out fine or fail in a not-repo-corrupting manner.\nIt's kind of hard to make these corner cases not happen without more massive refactorings so I figured this approach was the easiest to go with for now and it provides for the best safety as well I think. Happy to spell out all the failures modes I can think of in detail if you want/it helps :)", "author": "original-brownbear", "createdAt": "2020-06-23T15:30:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzMzE0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM0NzQ0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441347442", "bodyText": "should we disallow concurrent operations in this case where there are still entries with UNKNOWN_REPO_GEN? Just eliminates one more odd configuration to handle?", "author": "ywelsch", "createdAt": "2020-06-17T07:45:46Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -666,8 +753,36 @@ private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata) {\n             return;\n         }\n         final Snapshot snapshot = entry.snapshot();\n+        if (entry.repositoryStateId() == RepositoryData.UNKNOWN_REPO_GEN) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUwMzI1NA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441503254", "bodyText": "This is actually dead code (became obsolete after improving the master fail-over handling in a recent PR).  SnapshotsService#processExternalChanges will remove any such snapshot entry without taking any additional action these days (since 8.x never creates such a snapshot, it can't have listeners for it so quietly dropping it from the cluster state is just fine).", "author": "original-brownbear", "createdAt": "2020-06-17T12:22:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM0NzQ0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1MTUxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441351519", "bodyText": "This pattern is repeated quite often. I wonder if we should provide an overloaded method ClusterState.custom(String name, Supplier<Custom> defaultSupplier)", "author": "ywelsch", "createdAt": "2020-06-17T07:52:50Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1046,25 +1247,76 @@ public ClusterState execute(ClusterState currentState) {\n                         }\n                     }\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n-                if (snapshots != null && snapshots.entries().isEmpty() == false) {\n-                    // However other snapshots are running - cannot continue\n-                    throw new ConcurrentSnapshotExecutionException(\n-                            repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final SnapshotsInProgress updatedSnapshots;\n+\n+                if (snapshots == null) {\n+                    updatedSnapshots = new SnapshotsInProgress();", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg4NTc4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r442885782", "bodyText": "Gave this a go in #58382", "author": "original-brownbear", "createdAt": "2020-06-19T14:52:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1MTUxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NDI1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441354253", "bodyText": "can we assert something after doing all these transformations?", "author": "ywelsch", "createdAt": "2020-06-17T07:57:25Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU3Nzg2Nw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441577867", "bodyText": "We are asserting correctness of the state on the CS applier thread in SnapshotsService anyway, that's why I generally stayed away from adding individual assertions on all CS updates but I can move the assertions into the tasks as well for easier debugging if you want?", "author": "original-brownbear", "createdAt": "2020-06-17T14:13:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NDI1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NjU0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441356547", "bodyText": "These are two big objects to hold onto :(\nThere's a risk here for a node to go OOM when holding on to CS for too long. Is there a way to avoid this? We would have to write the metadata prior to enqueuing here, right?", "author": "ywelsch", "createdAt": "2020-06-17T08:01:11Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1433,4 +1943,16 @@ protected ClusterBlockException checkBlock(UpdateIndexShardSnapshotStatusRequest\n             return null;\n         }\n     }\n+\n+    private static final class SnapshotFinalization {\n+\n+        private final SnapshotsInProgress.Entry entry;\n+\n+        private final Metadata metadata;", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU5NDc0OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441594748", "bodyText": "We would have to write the metadata prior to enqueuing here, right?\n\nYea that would be one option. I like it in theory, but in practice I'd rather not do that as it makes the whole logic much more complex.\nFirst off, I wonder if this all matters much in practice:\nIf the snapshot finalizations come in, in a spike then the metadata should be almost the same across all snapshots and with the way we deserialize should be mostly the same objects shouldn't it? I'm having a hard time imagining a situation where finalizations would massively queue up and have different metadata. You could have a situation where a snapshot abort or a data node dropping out causes a bunch of concurrent snapshots to fail and get queued up for finalization, but in that case they all come from the same CS update and have the same metadata object. Outside of that, a finalization should generally be as fast or faster than a shard snapshot (I guess there's some strange corner cases with mixed data and master nodes where running shard snapshots make finalization slow for example but even in that case I wouldn't expect those metadata objects to take up a lot of dedicated heap?).\nSecond of all, I mainly added the metadata to the queue to make things \"subjectively more consistent\" by reducing the skew between data and metadata snapshot. Technically we don't give any guarantees as to the time of when we take the metadata snapshot (only that it's after all shards have finished).  Maybe it would be enough to simply hold on to the latest metadata seen when movign a snapshot to SUCESS state and use that for all the snapshots we finalize. No strong need to too keep a copy per snapshot?", "author": "original-brownbear", "createdAt": "2020-06-17T14:36:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NjU0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE4NDIyNA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446184224", "bodyText": "Maybe it would be enough to simply hold on to the latest metadata seen when movign a snapshot to SUCESS state and use that for all the snapshots we finalize. No strong need to too keep a copy per snapshot?\n\nWouldn't that also be the behavior when we  have a master failover (where the new master now uses a new cluster state as finalization point?", "author": "ywelsch", "createdAt": "2020-06-26T13:29:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NjU0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE4NTE4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446185183", "bodyText": "Yea exactly ... good point. I'll get rid of this. This \"felt\" smarter than it is ...", "author": "original-brownbear", "createdAt": "2020-06-26T13:30:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NjU0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM5OTExNg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441399116", "bodyText": "this is wrapping the list twice using unmodifiableList", "author": "ywelsch", "createdAt": "2020-06-17T09:09:58Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -51,10 +54,23 @@\n \n     public SnapshotDeletionsInProgress(List<Entry> entries) {\n         this.entries = Collections.unmodifiableList(entries);\n+        assert entries.size() == entries.stream().map(Entry::uuid).distinct().count() : \"Found duplicate UUIDs in entries \" + entries;\n+        assert assertConsistency(entries);\n     }\n \n     public SnapshotDeletionsInProgress(StreamInput in) throws IOException {\n-        this.entries = Collections.unmodifiableList(in.readList(Entry::new));\n+        this(Collections.unmodifiableList(in.readList(Entry::new)));", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQwNTY2Nw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441405667", "bodyText": "I would prefer a static readFrom method and implements Writeable on this class, so that we have the serialization covered here directly", "author": "ywelsch", "createdAt": "2020-06-17T09:20:36Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -257,4 +361,26 @@ public long repositoryStateId() {\n             return repositoryStateId;\n         }\n     }\n+\n+    public enum State {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyMTY5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444321693", "bodyText": "Sure done :)", "author": "original-brownbear", "createdAt": "2020-06-23T15:40:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQwNTY2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxMDI1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441410253", "bodyText": "This got me confused a bit, because we do not have a STARTED state on ShardState and because this talks about startedShardsByRepo.\nI wonder if we should add a new method isPossiblyActivelyRunning to ShardState, which only returns true for INIT and ABORTED, and use that here (with similar naming for the hashmap)", "author": "ywelsch", "createdAt": "2020-06-17T09:27:40Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -466,10 +487,25 @@ public static State fromValue(byte value) {\n \n     public SnapshotsInProgress(List<Entry> entries) {\n         this.entries = entries;\n+        assert assertConsistentEntries(entries);\n     }\n \n     public SnapshotsInProgress(Entry... entries) {\n-        this.entries = Arrays.asList(entries);\n+        this(Arrays.asList(entries));\n+    }\n+\n+    private static boolean assertConsistentEntries(List<Entry> entries) {\n+        final Map<String, Set<ShardId>> startedShardsByRepo = new HashMap<>();\n+        for (Entry entry : entries) {\n+            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shard : entry.shards()) {\n+                final ShardState shardState = shard.value.state();\n+                if (shardState == ShardState.INIT || shardState == ShardState.ABORTED) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYwMDE3MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441600170", "bodyText": "I added org.elasticsearch.cluster.SnapshotsInProgress.ShardSnapshotStatus#isAssigned now as a method to also cover the waiting-for-shard-relocation case here that we also need to track here. I'll update the name of the map accordingly", "author": "original-brownbear", "createdAt": "2020-06-17T14:43:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxMDI1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxODU4OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441418588", "bodyText": "Do we have a test somewhere that ensures that we're not resolving to new indices for concurrent snaps? (which would always work, but give us less incrementality)", "author": "ywelsch", "createdAt": "2020-06-17T09:40:59Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -202,37 +228,54 @@ public void createSnapshot(final CreateSnapshotRequest request, final ActionList\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 // check if the snapshot name already exists in the repository\n-                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) {\n+                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots == null ? List.of() : snapshots.entries();\n+                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName)) ||\n+                        runningSnapshots.stream().anyMatch(s -> {\n+                            final Snapshot running = s.snapshot();\n+                            return running.getRepository().equals(repositoryName)\n+                                    && running.getSnapshotId().getName().equals(snapshotName);\n+                        })) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n                 validate(repositoryName, snapshotName, currentState);\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n                 SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n+                boolean readyToExecute = true;\n                 if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) {\n-                    throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n-                        \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n+                    if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)) {\n+                        throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n+                                \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n+                    } else {\n+                        readyToExecute = deletionsInProgress.getEntries().stream().noneMatch(entry ->\n+                                entry.repository().equals(repositoryName) && entry.state() == SnapshotDeletionsInProgress.State.META_DATA);\n+                    }\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress = currentState.custom(RepositoryCleanupInProgress.TYPE);\n                 if (repositoryCleanupInProgress != null && repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n                 // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a\n                 // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the\n                 // cluster state anyway in #applyClusterState.\n-                if (snapshots != null && snapshots.entries().stream().anyMatch(entry -> entry.state() != State.INIT)) {\n+                if (minNodeVersion.before(FULL_CONCURRENCY_VERSION) && snapshots != null\n+                        && runningSnapshots.stream().anyMatch(entry -> entry.state() != State.INIT)) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \" a snapshot is already running\");\n                 }\n                 // Store newSnapshot here to be processed in clusterStateProcessed\n                 List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState,\n                     request.indicesOptions(), request.indices()));\n                 logger.trace(\"[{}][{}] creating snapshot for indices [{}]\", repositoryName, snapshotName, indices);\n \n-                final List<IndexId> indexIds = repositoryData.resolveNewIndices(indices);\n+                final List<IndexId> indexIds = repositoryData.resolveNewIndices(\n+                        indices, runningSnapshots.stream().filter(entry -> entry.repository().equals(repositoryName))\n+                                .flatMap(entry -> entry.indices().stream()).distinct()\n+                                .collect(Collectors.toMap(IndexId::getName, Function.identity())));", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYxMjI4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441612281", "bodyText": "Do we have a test somewhere that ensures that we're not resolving to new indices for concurrent snaps? (which would always work, but give us less incrementality)\n\nI don't think this would work. Remember, we're looking up index uuids (the repo uuids) by index name. If we were to cause collisions there with multiple IndexId for a single name, then we would a. trip assertions in RepositoryData when adding a snapshot to it (updated shard generations would conflict with existing RepositoryData) and b. fail repository consistency checks. We do have a bunch of tests that run two or more concurrent snapshots on an empty repo that cover this properly I think (it was one of the first things that blew up when I enabled concurrent snapshots :)).", "author": "original-brownbear", "createdAt": "2020-06-17T14:58:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxODU4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMDUyNA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441430524", "bodyText": "Should this be called \"updateRepositoryGenerationsIfNecessary\"?", "author": "ywelsch", "createdAt": "2020-06-17T10:00:38Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1616,6 +1615,50 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerations(ClusterState state, long oldGen, long newGen) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM0NDExNQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444344115", "bodyText": "Sure probably better :)", "author": "original-brownbear", "createdAt": "2020-06-23T16:13:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMDUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMDkyMw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441430923", "bodyText": "what are the situations where we expect entry.repositoryStateId() != oldGen? Is there something we can assert here?", "author": "ywelsch", "createdAt": "2020-06-17T10:01:17Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1616,6 +1615,50 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerations(ClusterState state, long oldGen, long newGen) {\n+        final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE);\n+        final String repoName = metadata.name();\n+        final List<SnapshotsInProgress.Entry> snapshotEntries;\n+        if (snapshotsInProgress == null) {\n+            snapshotEntries = List.of();\n+        } else {\n+            snapshotEntries = new ArrayList<>();\n+            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                if (entry.repository().equals(repoName) && entry.repositoryStateId() == oldGen) {\n+                    snapshotEntries.add(entry.withRepoGen(newGen));\n+                } else {\n+                    snapshotEntries.add(entry);\n+                }\n+            }\n+        }\n+        final SnapshotDeletionsInProgress snapshotDeletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE);\n+        final List<SnapshotDeletionsInProgress.Entry> deletionEntries;\n+        if (snapshotDeletionsInProgress == null) {\n+            deletionEntries = List.of();\n+        } else {\n+            deletionEntries = new ArrayList<>();\n+            for (SnapshotDeletionsInProgress.Entry entry : snapshotDeletionsInProgress.getEntries()) {\n+                if (entry.repositoryStateId() == oldGen) {\n+                    deletionEntries.add(entry.withRepoGen(newGen));\n+                } else {\n+                    deletionEntries.add(entry);", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzODU2NA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444338564", "bodyText": "Currently, this would all be situations where we concurrently run stuff before the repo has written anything since it was mounted (so some ops pick up a broken generation, others pick up a good one). All of these situations are somewhat safe from corruption thanks to the fact that the finalization step of writing a new index-N will break for them (at least as safe as the current single operation situation as far as I can tell). We can't really assert any hard truths here I'm afraid (S3 could technically always find some index-N with a weird N at any point ... theoretically).\nI mean technically we could also just not allow concurrent operations until after the first write to the repository to be in a spot that is easier to reason about. I rejected that idea to some degree, because:\n\nThe fact that concurrent operations would actually do extra listings for the last index-N in the unsafe startup scenario makes it more likely that we catch a broken operation.\nIt requires more logic that mixes repo and snapshot service concerns in a confusing way.\n\nHope that helps, same as with the other comment on this. I'm happy to spell out all the scenarios I can think of for this here if desired (it's quite a few) :)", "author": "original-brownbear", "createdAt": "2020-06-23T16:05:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMDkyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ1NTMxOA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441455318", "bodyText": "does this need adjustment? Not backported yet?", "author": "ywelsch", "createdAt": "2020-06-17T10:46:43Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -113,6 +117,8 @@\n  */\n public class SnapshotsService extends AbstractLifecycleComponent implements ClusterStateApplier {\n \n+    public static final Version FULL_CONCURRENCY_VERSION = Version.V_8_0_0;\n+\n     public static final Version SHARD_GEN_IN_REPO_DATA_VERSION = Version.V_7_6_0;\n \n     public static final Version INDEX_GEN_IN_REPO_DATA_VERSION = Version.V_8_0_0;", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyNjM0NA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444326344", "bodyText": "Not yet backported sorry, incoming though (we discussed this last week, very tricky backport ...).", "author": "original-brownbear", "createdAt": "2020-06-23T15:47:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ1NTMxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ1ODExNQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441458115", "bodyText": "I think I would prefer a different error message here for running snapshots, making it clear that this is conflicting with an already running snapshot.", "author": "ywelsch", "createdAt": "2020-06-17T10:52:17Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -202,37 +228,54 @@ public void createSnapshot(final CreateSnapshotRequest request, final ActionList\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 // check if the snapshot name already exists in the repository\n-                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) {\n+                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots == null ? List.of() : snapshots.entries();\n+                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName)) ||\n+                        runningSnapshots.stream().anyMatch(s -> {\n+                            final Snapshot running = s.snapshot();\n+                            return running.getRepository().equals(repositoryName)", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM0MjUxNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444342517", "bodyText": "Sure done", "author": "original-brownbear", "createdAt": "2020-06-23T16:10:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ1ODExNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MjkzNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441472937", "bodyText": "add javadocs here to explain what this method does?", "author": "ywelsch", "createdAt": "2020-06-17T11:23:23Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -725,12 +836,107 @@ private void handleFinalizationFailure(Exception e, SnapshotsInProgress.Entry en\n                 new SnapshotException(snapshot, \"Failed to update cluster state during snapshot finalization\", e));\n         } else {\n             logger.warn(() -> new ParameterizedMessage(\"[{}] failed to finalize snapshot\", snapshot), e);\n-            removeSnapshotFromClusterState(snapshot, e);\n+            removeSnapshotFromClusterState(snapshot, e,\n+                    ActionListener.wrap(() -> runNextQueuedOperation(entry.repositoryStateId(), entry.repository())));\n+        }\n+    }\n+\n+    /**\n+     * Run the next queued up repository operation for the given repository name.\n+     *\n+     * @param newGeneration current repository generation for given repository\n+     * @param repository    repository name\n+     */\n+    private void runNextQueuedOperation(long newGeneration, String repository) {\n+        synchronized (currentlyFinalizing) {\n+            assert currentlyFinalizing.contains(repository);\n+            final Deque<SnapshotFinalization> outstandingForRepo = snapshotsToFinalize.get(repository);\n+            final SnapshotFinalization nextFinalization;\n+            if (outstandingForRepo == null) {\n+                nextFinalization = null;\n+            } else {\n+                nextFinalization = outstandingForRepo.pollFirst();\n+                if (outstandingForRepo.isEmpty()) {\n+                    snapshotsToFinalize.remove(repository);\n+                }\n+            }\n+            if (nextFinalization == null) {\n+                final boolean removed = currentlyFinalizing.remove(repository);\n+                assert removed;\n+                runReadyDeletions();\n+            } else {\n+                logger.trace(\"Moving on to finalizing next snapshot [{}]\", nextFinalization);\n+                finalizeSnapshotEntry(nextFinalization.entry, nextFinalization.metadata, newGeneration);\n+            }\n         }\n     }\n \n+    /**\n+     * Runs a cluster state update that checks whether we have outstanding snapshot deletions that can be executed and executes them.\n+     *\n+     * TODO: optimize this to execute in a single CS update together with finalizing the latest snapshot\n+     */\n+    private void runReadyDeletions() {\n+        clusterService.submitStateUpdateTask(\"Run ready deletions\", new ClusterStateUpdateTask() {\n+\n+            private List<SnapshotDeletionsInProgress.Entry> deletionsToRun;\n+\n+            @Override\n+            public ClusterState execute(ClusterState currentState) {\n+                final Tuple<ClusterState, List<SnapshotDeletionsInProgress.Entry>> res = readyDeletions(currentState);\n+                assert res.v1() == currentState : \"Deletes should have been set to ready by finished snapshot deletes and finalizations\";\n+                deletionsToRun = res.v2();\n+                return res.v1();\n+            }\n+\n+            @Override\n+            public void onFailure(String source, Exception e) {\n+                logger.warn(\"Failed to run ready delete operations\", e);\n+            }\n+\n+            @Override\n+            public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n+                for (SnapshotDeletionsInProgress.Entry entry : deletionsToRun) {\n+                    deleteSnapshotsFromRepository(entry, entry.repositoryStateId(), newState.nodes().getMinNodeVersion());\n+                }\n+            }\n+        });\n+    }\n+\n+    private static Tuple<ClusterState, List<SnapshotDeletionsInProgress.Entry>> readyDeletions(ClusterState currentState) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM0MTE0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444341142", "bodyText": "Done :)", "author": "original-brownbear", "createdAt": "2020-06-23T16:08:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MjkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3NTYzMg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441475632", "bodyText": "why is there only at most one?", "author": "ywelsch", "createdAt": "2020-06-17T11:29:03Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -821,22 +1023,23 @@ public void deleteSnapshots(final DeleteSnapshotRequest request, final ActionLis\n \n             @Override\n             public ClusterState execute(ClusterState currentState) throws Exception {\n-                if (snapshotNames.length > 1 && currentState.nodes().getMinNodeVersion().before(MULTI_DELETE_VERSION)) {\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n+                if (snapshotNames.length > 1 && minNodeVersion.before(MULTI_DELETE_VERSION)) {\n                     throw new IllegalArgumentException(\"Deleting multiple snapshots in a single request is only supported in version [ \"\n-                            + MULTI_DELETE_VERSION + \"] but cluster contained node of version [\" + currentState.nodes().getMinNodeVersion()\n-                            + \"]\");\n+                            + MULTI_DELETE_VERSION + \"] but cluster contained node of version [\" + minNodeVersion + \"]\");\n                 }\n                 final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n-                final SnapshotsInProgress.Entry snapshotEntry = findInProgressSnapshot(snapshots, snapshotNames, repositoryName);\n+                final List<SnapshotsInProgress.Entry> snapshotEntries = findInProgressSnapshots(snapshots, snapshotNames, repositoryName);\n                 final List<SnapshotId> snapshotIds = matchingSnapshotIds(\n-                        snapshotEntry == null ? null : snapshotEntry.snapshot().getSnapshotId(),\n-                        repositoryData, snapshotNames, repositoryName);\n-                if (snapshotEntry == null) {\n+                        snapshotEntries.stream().map(e -> e.snapshot().getSnapshotId()).collect(Collectors.toList()), repositoryData,\n+                        snapshotNames, repositoryName);\n+                if (snapshotEntries.isEmpty() || minNodeVersion.onOrAfter(SnapshotsService.FULL_CONCURRENCY_VERSION)) {\n                     deleteFromRepoTask =\n                             createDeleteStateUpdate(snapshotIds, repositoryName, repositoryData.getGenId(), Priority.NORMAL, listener);\n                     return deleteFromRepoTask.execute(currentState);\n                 }\n-\n+                assert snapshotEntries.size() == 1 : \"Expected just a single running snapshot but saw \" + snapshotEntries;", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM0MjA3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444342072", "bodyText": "Because we break out above to the new logic if we have a cluster on or after FULL_CONCURRENCY_VERSION and hence never get here unless there  is precisely one running snapshot.", "author": "original-brownbear", "createdAt": "2020-06-23T16:10:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3NTYzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MDU1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441480553", "bodyText": "do we need to set changed to true here?", "author": "ywelsch", "createdAt": "2020-06-17T11:39:06Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MTE5OA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441481198", "bodyText": "this should only be a problem if the repository data isn't cached, right?", "author": "ywelsch", "createdAt": "2020-06-17T11:40:20Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY0MDk4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441640981", "bodyText": "Yea it's a highly unlikely scenario, that's why I figured brute force failing (just removing from CS) all things queued up for a given snapshot should be ok here.", "author": "original-brownbear", "createdAt": "2020-06-17T15:37:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MTE5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjE1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441482155", "bodyText": "Should there be a return after this line?\nI see usage later on of repositoryData where it's possibly null", "author": "ywelsch", "createdAt": "2020-06-17T11:42:15Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjI1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441482256", "bodyText": "Also sounds like we have a testing gap", "author": "ywelsch", "createdAt": "2020-06-17T11:42:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjE1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY0MjczNw==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441642737", "bodyText": "I think we're safe here because we don't do any actual shard assignments that would use RepositoryData if it's null, we just break out here: https://github.com/elastic/elasticsearch/pull/56911/files#diff-a0853be4492c052f24917b5c1464003dR1529 and fail all the queued up snapshots and deletes. This is actually covered by e.g. test org.elasticsearch.snapshots.ConcurrentSnapshotsIT#testQueuedDeletesWithFailures that has both a queued up snapshot and delete fail as a result of not being able to read RepositoryData", "author": "original-brownbear", "createdAt": "2020-06-17T15:39:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MzQ4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441483489", "bodyText": "As mentioned in Slack, this could possibly be result in shards to be done out-of-snapshot-order in case where there is a relocating primary.", "author": "ywelsch", "createdAt": "2020-06-17T11:45:01Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUwOTk3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441509971", "bodyText": "Fixed this case now in ee88275 and added test+assertion coverage for it.\n\ud83d\udcaf thanks for spotting this!", "author": "original-brownbear", "createdAt": "2020-06-17T12:34:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MzQ4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NDc2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441484769", "bodyText": "should we still call the listeners here?", "author": "ywelsch", "createdAt": "2020-06-17T11:47:36Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);\n+                        }\n+                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions)\n+                                .putCustom(SnapshotsInProgress.TYPE, updatedSnapshotsInProgress).build();\n                     }\n                 }\n                 return currentState;\n             }\n \n             @Override\n             public void onFailure(String source, Exception e) {\n-                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", snapshotIds), e);\n-                if (listener != null) {\n-                    listener.onFailure(e);\n+                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", deleteEntry), e);\n+                runningDeletions.remove(deleteEntry.uuid());\n+                final String repoName = deleteEntry.repository();\n+                synchronized (currentlyFinalizing) {\n+                    if (ExceptionsHelper.unwrap(e, NotMasterException.class, FailedToCommitClusterStateException.class) != null) {\n+                        // Failure due to not being master any more so we don't try to do run more cluster state updates. The next master\n+                        // will try handling the missing operations. All we can do is fail all the listeners on this master node so that\n+                        // transport requests return and we don't leak listeners\n+                        final Exception wrapped =\n+                                new RepositoryException(repoName, \"Failed to update cluster state during snapshot delete\", e);\n+                        final Deque<SnapshotFinalization> outstandingSnapshotsForRepo = snapshotsToFinalize.remove(repoName);\n+                        if (outstandingSnapshotsForRepo != null) {\n+                            SnapshotFinalization finalization;\n+                            while ((finalization = outstandingSnapshotsForRepo.poll()) != null) {\n+                                failSnapshotCompletionListeners(finalization.entry.snapshot(), wrapped);\n+                            }\n+                        }\n+                        for (Iterator<List<ActionListener<RepositoryData>>> iterator = snapshotDeletionListeners.values().iterator();\n+                             iterator.hasNext(); ) {\n+                            List<ActionListener<RepositoryData>> listeners = iterator.next();\n+                            iterator.remove();\n+                            failListenersIgnoringException(listeners, wrapped);\n+                        }\n+                        assert snapshotDeletionListeners.isEmpty() :\n+                                \"No new listeners should have been added but saw \" + snapshotDeletionListeners;\n+                    } else {\n+                        assert false : \"Removing snapshot entry should only ever fail because we failed to publish new state\";", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjcwMzY0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r442703641", "bodyText": "I don't know. It's a broken situation either way if we get here and come to think of it we have the same issue in the snapshot path. In the end, only a master fail-over can fix things here and it should be impossible to arrive here unless there is a bug.\nIf we call the listeners we'll return from the request but the cluster state is still stuck. If we don't call the listeners we won't return from the request so at least the issue is a little more visible? Seems to me it makes no difference what we do here practically.", "author": "original-brownbear", "createdAt": "2020-06-19T08:21:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NDc2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NTM2MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441485360", "bodyText": "how is this tested?", "author": "ywelsch", "createdAt": "2020-06-17T11:48:47Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);\n+                        }\n+                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions)\n+                                .putCustom(SnapshotsInProgress.TYPE, updatedSnapshotsInProgress).build();\n                     }\n                 }\n                 return currentState;\n             }\n \n             @Override\n             public void onFailure(String source, Exception e) {\n-                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", snapshotIds), e);\n-                if (listener != null) {\n-                    listener.onFailure(e);\n+                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", deleteEntry), e);\n+                runningDeletions.remove(deleteEntry.uuid());\n+                final String repoName = deleteEntry.repository();\n+                synchronized (currentlyFinalizing) {\n+                    if (ExceptionsHelper.unwrap(e, NotMasterException.class, FailedToCommitClusterStateException.class) != null) {\n+                        // Failure due to not being master any more so we don't try to do run more cluster state updates. The next master\n+                        // will try handling the missing operations. All we can do is fail all the listeners on this master node so that\n+                        // transport requests return and we don't leak listeners\n+                        final Exception wrapped =\n+                                new RepositoryException(repoName, \"Failed to update cluster state during snapshot delete\", e);\n+                        final Deque<SnapshotFinalization> outstandingSnapshotsForRepo = snapshotsToFinalize.remove(repoName);\n+                        if (outstandingSnapshotsForRepo != null) {\n+                            SnapshotFinalization finalization;\n+                            while ((finalization = outstandingSnapshotsForRepo.poll()) != null) {\n+                                failSnapshotCompletionListeners(finalization.entry.snapshot(), wrapped);\n+                            }\n+                        }\n+                        for (Iterator<List<ActionListener<RepositoryData>>> iterator = snapshotDeletionListeners.values().iterator();\n+                             iterator.hasNext(); ) {\n+                            List<ActionListener<RepositoryData>> listeners = iterator.next();\n+                            iterator.remove();\n+                            failListenersIgnoringException(listeners, wrapped);\n+                        }\n+                        assert snapshotDeletionListeners.isEmpty() :\n+                                \"No new listeners should have been added but saw \" + snapshotDeletionListeners;\n+                    } else {\n+                        assert false : \"Removing snapshot entry should only ever fail because we failed to publish new state\";\n+                    }\n+                    currentlyFinalizing.remove(repoName);\n                 }\n             }\n \n             @Override\n             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n-                if (listener != null) {\n-                    if (failure != null) {\n-                        listener.onFailure(failure);\n-                    } else {\n-                        logger.info(\"Successfully deleted snapshots {}\", snapshotIds);\n-                        listener.onResponse(null);\n+                final List<ActionListener<RepositoryData>> deleteListeners = snapshotDeletionListeners.remove(deleteEntry.uuid());\n+                if (failure == null) {\n+                    completeListenersIgnoringException(deleteListeners, repositoryData);\n+                } else {\n+                    failListenersIgnoringException(deleteListeners, failure);\n+                }\n+                runningDeletions.remove(deleteEntry.uuid());\n+                if (repositoryData == null) {", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUwOTA0Ng==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441509046", "bodyText": "org.elasticsearch.snapshots.ConcurrentSnapshotsIT#testQueuedOperationsAndBrokenRepoOnMasterFailOver reproduces this scenario by corrupting the index-N blob before master fail-over.", "author": "original-brownbear", "createdAt": "2020-06-17T12:33:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NTM2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NzQ1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441487455", "bodyText": "Does this logic still make sure that a snapshot that's taken contains a set of docs that existed sometime between start and end of snapshot command? Or will this batch up snapshots that had been running prior to the command?", "author": "ywelsch", "createdAt": "2020-06-17T11:52:45Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1322,20 +1811,41 @@ protected void doClose() {\n             if (snapshots != null) {\n                 int changedCount = 0;\n                 final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();\n+                final Map<String, Set<ShardId>> reusedShardIdsByRepo = new HashMap<>();\n                 for (SnapshotsInProgress.Entry entry : snapshots.entries()) {\n                     ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();\n                     boolean updated = false;\n \n                     for (UpdateIndexShardSnapshotStatusRequest updateSnapshotState : tasks) {\n+                        final ShardId finishedShardId = updateSnapshotState.shardId();\n                         if (entry.snapshot().equals(updateSnapshotState.snapshot())) {\n                             logger.trace(\"[{}] Updating shard [{}] with status [{}]\", updateSnapshotState.snapshot(),\n-                                updateSnapshotState.shardId(), updateSnapshotState.status().state());\n+                                    finishedShardId, updateSnapshotState.status().state());\n                             if (updated == false) {\n                                 shards.putAll(entry.shards());\n                                 updated = true;\n                             }\n-                            shards.put(updateSnapshotState.shardId(), updateSnapshotState.status());\n+                            shards.put(finishedShardId, updateSnapshotState.status());\n                             changedCount++;\n+                        } else {\n+                            final Set<ShardId> reusedShardIds =\n+                                    reusedShardIdsByRepo.computeIfAbsent(entry.repository(), k -> new HashSet<>());\n+                            if (entry.state().completed() == false && reusedShardIds.contains(finishedShardId) == false", "originalCommit": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUxMTg1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441511856", "bodyText": "Does this logic still make sure that a snapshot that's taken contains a set of docs that existed sometime between start and end of snapshot command?\n\nYes. There is no smart logic here. We simply reassign the finished shards as (new INIT state shard snapshots) if there's other outstanding snapshots for them and put them into reusedShardIds to ensure we only reassign once per shard+repo => a new snapshot will be taken from a fresh index commit so the guarantees about the timing are unchanged relative to what we have right now.", "author": "original-brownbear", "createdAt": "2020-06-17T12:38:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NzQ1NQ=="}], "type": "inlineReview"}, {"oid": "ee8827588c76bba0ab2181691a99027aa6df90be", "url": "https://github.com/elastic/elasticsearch/commit/ee8827588c76bba0ab2181691a99027aa6df90be", "message": "fix waiting+relocation failure propagation", "committedDate": "2020-06-17T12:03:11Z", "type": "commit"}, {"oid": "62559a560df20fdc5fd5043aa90c9b9340be12b4", "url": "https://github.com/elastic/elasticsearch/commit/62559a560df20fdc5fd5043aa90c9b9340be12b4", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-17T12:03:24Z", "type": "commit"}, {"oid": "318822558eb08fc729083d1737c829c568095ada", "url": "https://github.com/elastic/elasticsearch/commit/318822558eb08fc729083d1737c829c568095ada", "message": "docs fixes", "committedDate": "2020-06-17T12:12:45Z", "type": "commit"}, {"oid": "d28bddb1db6d3f5fddf2fdbedac6935db7aea52c", "url": "https://github.com/elastic/elasticsearch/commit/d28bddb1db6d3f5fddf2fdbedac6935db7aea52c", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-17T12:13:12Z", "type": "commit"}, {"oid": "7f961f57db67b9ee3ca96327a5f3ecdae2bfac7b", "url": "https://github.com/elastic/elasticsearch/commit/7f961f57db67b9ee3ca96327a5f3ecdae2bfac7b", "message": "drop dead code", "committedDate": "2020-06-17T12:23:03Z", "type": "commit"}, {"oid": "a7aa0bf7a628788daea0bc672f1311d6971c08d1", "url": "https://github.com/elastic/elasticsearch/commit/a7aa0bf7a628788daea0bc672f1311d6971c08d1", "message": "stop double wrapping list", "committedDate": "2020-06-17T14:38:39Z", "type": "commit"}, {"oid": "4ba1f6154ebc3b00ddcb6c0e9326ec49debbef67", "url": "https://github.com/elastic/elasticsearch/commit/4ba1f6154ebc3b00ddcb6c0e9326ec49debbef67", "message": "rename", "committedDate": "2020-06-17T14:43:37Z", "type": "commit"}, {"oid": "c0ff7ddca17f9208ce16f0dad45b41244a9ffd4f", "url": "https://github.com/elastic/elasticsearch/commit/c0ff7ddca17f9208ce16f0dad45b41244a9ffd4f", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-17T18:15:12Z", "type": "commit"}, {"oid": "a9d7a20468025f6b9a37538e308e711970288f01", "url": "https://github.com/elastic/elasticsearch/commit/a9d7a20468025f6b9a37538e308e711970288f01", "message": "further cleanups", "committedDate": "2020-06-18T05:58:14Z", "type": "commit"}, {"oid": "2f98b636e0d3d8cbd49f1cf1a65be855ef4b9492", "url": "https://github.com/elastic/elasticsearch/commit/2f98b636e0d3d8cbd49f1cf1a65be855ef4b9492", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-18T18:54:02Z", "type": "commit"}, {"oid": "38090b7d2f0ebec110700826049b5a65972a1bf0", "url": "https://github.com/elastic/elasticsearch/commit/38090b7d2f0ebec110700826049b5a65972a1bf0", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-19T06:29:09Z", "type": "commit"}, {"oid": "e766028e39e1d42dbd20751373d2306978b9e623", "url": "https://github.com/elastic/elasticsearch/commit/e766028e39e1d42dbd20751373d2306978b9e623", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-19T08:17:08Z", "type": "commit"}, {"oid": "962a76151cd4d692f9b07859c7f11f1357e5132e", "url": "https://github.com/elastic/elasticsearch/commit/962a76151cd4d692f9b07859c7f11f1357e5132e", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-19T10:40:05Z", "type": "commit"}, {"oid": "8200eba197ae4137821c7199a3282696e8b22391", "url": "https://github.com/elastic/elasticsearch/commit/8200eba197ae4137821c7199a3282696e8b22391", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-19T17:28:45Z", "type": "commit"}, {"oid": "a3c5196e7bcc04a469eece7da0dee7324031ea0f", "url": "https://github.com/elastic/elasticsearch/commit/a3c5196e7bcc04a469eece7da0dee7324031ea0f", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-20T17:15:02Z", "type": "commit"}, {"oid": "10b504b331ecef41fd0f854555b85b19c961ecd4", "url": "https://github.com/elastic/elasticsearch/commit/10b504b331ecef41fd0f854555b85b19c961ecd4", "message": "much cuter", "committedDate": "2020-06-20T22:33:10Z", "type": "commit"}, {"oid": "ea4f9eda94c82aeff1a695d0f0fa419c5d324a50", "url": "https://github.com/elastic/elasticsearch/commit/ea4f9eda94c82aeff1a695d0f0fa419c5d324a50", "message": "fix", "committedDate": "2020-06-21T07:22:17Z", "type": "commit"}, {"oid": "6ab07d9681d1d32dca6e88cbcc327db8d695d646", "url": "https://github.com/elastic/elasticsearch/commit/6ab07d9681d1d32dca6e88cbcc327db8d695d646", "message": "moar docs, cleaner logic", "committedDate": "2020-06-21T11:20:46Z", "type": "commit"}, {"oid": "01a3de8a72951e59b02c7d34cc066d16abcc16b6", "url": "https://github.com/elastic/elasticsearch/commit/01a3de8a72951e59b02c7d34cc066d16abcc16b6", "message": "tweaks", "committedDate": "2020-06-21T20:54:13Z", "type": "commit"}, {"oid": "685d5453ab0799b67c5c4a41e114f526898cac24", "url": "https://github.com/elastic/elasticsearch/commit/685d5453ab0799b67c5c4a41e114f526898cac24", "message": "fix", "committedDate": "2020-06-21T21:42:20Z", "type": "commit"}, {"oid": "5a72c285117410d0b61a6e1506954380eb07f5ad", "url": "https://github.com/elastic/elasticsearch/commit/5a72c285117410d0b61a6e1506954380eb07f5ad", "message": "test cleanups", "committedDate": "2020-06-22T07:42:44Z", "type": "commit"}, {"oid": "337643a89a5467c413d956677241f5c919482b0d", "url": "https://github.com/elastic/elasticsearch/commit/337643a89a5467c413d956677241f5c919482b0d", "message": "shorter", "committedDate": "2020-06-22T07:52:32Z", "type": "commit"}, {"oid": "40abfdd12e02973b76172fc3276f6823c501f22d", "url": "https://github.com/elastic/elasticsearch/commit/40abfdd12e02973b76172fc3276f6823c501f22d", "message": "much more readable tests", "committedDate": "2020-06-22T09:45:16Z", "type": "commit"}, {"oid": "ccfa36e687e6a436a8bd8feb6cb8efa54cc640c9", "url": "https://github.com/elastic/elasticsearch/commit/ccfa36e687e6a436a8bd8feb6cb8efa54cc640c9", "message": "much much stricter tests and fixes", "committedDate": "2020-06-22T19:29:37Z", "type": "commit"}, {"oid": "dc9efc20135d9cb70ea07c76ad33457649963380", "url": "https://github.com/elastic/elasticsearch/commit/dc9efc20135d9cb70ea07c76ad33457649963380", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-23T06:28:32Z", "type": "commit"}, {"oid": "8bf6f04d769f686945189da70046671a62c3a8c5", "url": "https://github.com/elastic/elasticsearch/commit/8bf6f04d769f686945189da70046671a62c3a8c5", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-23T07:25:49Z", "type": "commit"}, {"oid": "b566d482fa067101b14ed3e1460523ef339b92f8", "url": "https://github.com/elastic/elasticsearch/commit/b566d482fa067101b14ed3e1460523ef339b92f8", "message": "cleaner", "committedDate": "2020-06-23T08:05:18Z", "type": "commit"}, {"oid": "58981c208cc342877d06b0c54faea3139891dcc1", "url": "https://github.com/elastic/elasticsearch/commit/58981c208cc342877d06b0c54faea3139891dcc1", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-23T08:21:21Z", "type": "commit"}, {"oid": "6d31169421e33148a6e66b6314af97ed4267ed50", "url": "https://github.com/elastic/elasticsearch/commit/6d31169421e33148a6e66b6314af97ed4267ed50", "message": "even moar docs", "committedDate": "2020-06-23T09:23:20Z", "type": "commit"}, {"oid": "f207d1549592c0ee94fcc385ec10de94b2286dbb", "url": "https://github.com/elastic/elasticsearch/commit/f207d1549592c0ee94fcc385ec10de94b2286dbb", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-23T09:40:49Z", "type": "commit"}, {"oid": "cd2199423091914e1f662faccad490eaf7e93d55", "url": "https://github.com/elastic/elasticsearch/commit/cd2199423091914e1f662faccad490eaf7e93d55", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-23T10:39:58Z", "type": "commit"}, {"oid": "e3acf79e00f2808a57360f0df4c26f5f54c013ea", "url": "https://github.com/elastic/elasticsearch/commit/e3acf79e00f2808a57360f0df4c26f5f54c013ea", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-23T11:25:06Z", "type": "commit"}, {"oid": "1581aa8f0c8288c9fbcb3c1031ff68ce2333b0d0", "url": "https://github.com/elastic/elasticsearch/commit/1581aa8f0c8288c9fbcb3c1031ff68ce2333b0d0", "message": "test for and fix more listener leaks", "committedDate": "2020-06-23T13:56:50Z", "type": "commit"}, {"oid": "808d8279993665172ec381fb5544a4b889a5a71b", "url": "https://github.com/elastic/elasticsearch/commit/808d8279993665172ec381fb5544a4b889a5a71b", "message": "better doc + naming", "committedDate": "2020-06-23T14:07:12Z", "type": "commit"}, {"oid": "e19d0c6c5f1765837d2e003d2c0d965711d5f5c6", "url": "https://github.com/elastic/elasticsearch/commit/e19d0c6c5f1765837d2e003d2c0d965711d5f5c6", "message": "optimize", "committedDate": "2020-06-23T14:28:43Z", "type": "commit"}, {"oid": "fa822de031d4b0713871c29d8b2ee2445a55189f", "url": "https://github.com/elastic/elasticsearch/commit/fa822de031d4b0713871c29d8b2ee2445a55189f", "message": "writable", "committedDate": "2020-06-23T15:40:38Z", "type": "commit"}, {"oid": "fa1006bf5d5d7e416b30754d2ef738b3a5043557", "url": "https://github.com/elastic/elasticsearch/commit/fa1006bf5d5d7e416b30754d2ef738b3a5043557", "message": "faster", "committedDate": "2020-06-23T15:45:56Z", "type": "commit"}, {"oid": "a118af0319eb9acd01207e21d8f54af122405a9c", "url": "https://github.com/elastic/elasticsearch/commit/a118af0319eb9acd01207e21d8f54af122405a9c", "message": "different exceptions", "committedDate": "2020-06-23T16:10:28Z", "type": "commit"}, {"oid": "6d221d19835a10f872c3575acf26b773d2598344", "url": "https://github.com/elastic/elasticsearch/commit/6d221d19835a10f872c3575acf26b773d2598344", "message": "renaming", "committedDate": "2020-06-23T16:14:11Z", "type": "commit"}, {"oid": "13b3d4d313461790d684eefe389b1c9db29d1eef", "url": "https://github.com/elastic/elasticsearch/commit/13b3d4d313461790d684eefe389b1c9db29d1eef", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-23T16:53:13Z", "type": "commit"}, {"oid": "fc11ce13fab85ee07d00250b45a53ec3a22cdaec", "url": "https://github.com/elastic/elasticsearch/commit/fc11ce13fab85ee07d00250b45a53ec3a22cdaec", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-24T03:40:52Z", "type": "commit"}, {"oid": "5d3d447246ffa5c86522994a87db91385a878a81", "url": "https://github.com/elastic/elasticsearch/commit/5d3d447246ffa5c86522994a87db91385a878a81", "message": "simpler listener", "committedDate": "2020-06-24T03:58:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYzMjYzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444632635", "bodyText": "This synchronization on currentlyFinalizing in a bunch of spots is not great admittedly but for step 1 I traded efficiency for safety in many spots like this one.\nI have some planned simplifications that will move all the logic for triggering deletes and finalizations exclusively to the master update thread (most of them already are) which will remove the need for most synchronization on the fields in SnapshotsService.", "author": "original-brownbear", "createdAt": "2020-06-24T04:06:03Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -677,19 +795,44 @@ private static boolean removedNodesCleanupNeeded(SnapshotsInProgress snapshotsIn\n     }\n \n     /**\n-     * Finalizes the shard in repository and then removes it from cluster state\n-     * <p>\n-     * This is non-blocking method that runs on a thread from SNAPSHOT thread pool\n+     * Finalizes the snapshot in the repository.\n      *\n      * @param entry snapshot\n      */\n-    private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata) {\n-        if (endingSnapshots.add(entry.snapshot()) == false) {\n-            return;\n+    private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata, @Nullable RepositoryData repositoryData) {\n+        final boolean newFinalization = endingSnapshots.add(entry.snapshot());\n+        final String repoName = entry.repository();\n+        synchronized (currentlyFinalizing) {", "originalCommit": "5d3d447246ffa5c86522994a87db91385a878a81", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkyNzQ3MA==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444927470", "bodyText": "nit: isExecuting() sounds good to me", "author": "tlrx", "createdAt": "2020-06-24T14:17:46Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -404,6 +425,16 @@ public String reason() {\n             return reason;\n         }\n \n+        /**\n+         * Checks if this shard snapshot is actively executing.\n+         * A shard is defined as actively executing if it either is in a state that may write to the repository\n+         * ({@link ShardState#INIT} or {@link ShardState#ABORTED}) or is in state {@link ShardState#WAITING} with a concrete non-null\n+         * node id assignment (i.e. waiting for a shard relocation/initialization to finish).\n+         */\n+        public boolean isAssigned() {", "originalCommit": "5d3d447246ffa5c86522994a87db91385a878a81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMzQ4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446213489", "bodyText": "Can we assert that INIT state has always a non-null nodeId?\nIsn't it sufficient to look at shard states here that have an non-null node id?", "author": "ywelsch", "createdAt": "2020-06-26T14:19:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkyNzQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjgzNzE1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446837155", "bodyText": "Can we assert that INIT state has always a non-null nodeId?\n\nSure will do\n\nIsn't it sufficient to look at shard states here that have an non-null node id?\n\nAs a result of me (re-)using the WAITING state with node id null and with node id != null we can't do this. Might be worth while to go for a new kind of ShardState here after all. I'll do that before re-requesting review. On it :)", "author": "original-brownbear", "createdAt": "2020-06-29T07:52:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkyNzQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxMzg1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449413855", "bodyText": "maybe call this isActive now", "author": "ywelsch", "createdAt": "2020-07-03T07:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkyNzQ3MA=="}], "type": "inlineReview"}, {"oid": "d89ead6e6e5ab12b3da463a50f25132538c7226b", "url": "https://github.com/elastic/elasticsearch/commit/d89ead6e6e5ab12b3da463a50f25132538c7226b", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots", "committedDate": "2020-06-24T16:26:44Z", "type": "commit"}, {"oid": "a6d47f233a51efe69346adc730c1748ac98c89a4", "url": "https://github.com/elastic/elasticsearch/commit/a6d47f233a51efe69346adc730c1748ac98c89a4", "message": "shorter", "committedDate": "2020-06-24T16:37:55Z", "type": "commit"}]}