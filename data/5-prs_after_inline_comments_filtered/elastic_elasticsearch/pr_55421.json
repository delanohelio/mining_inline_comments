{"pr_number": 55421, "pr_title": "Fix security manager bug writing large blobs to GCS", "pr_createdAt": "2020-04-17T21:18:54Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/55421", "timeline": [{"oid": "7df9d78937d2b6928c42f590fab70d661daf8892", "url": "https://github.com/elastic/elasticsearch/commit/7df9d78937d2b6928c42f590fab70d661daf8892", "message": "Fix security manager bug writing large blobs to GCS\n\nThis commit addresses a security manager permissions issue writing large\nblobs (on the resumable upload path) to GCS. The underlying issue here\nis that we need to wrap the close and write calls on the channel. It is\nnot enough to do this:\n\nSocketAccess.doPrivilegedVoidIOException(\n  () -> Streams.copy(\n    inputStream,\n    Channels.newOutputStream(client().writer(blobInfo, writeOptions))));\n\nThis reason that this is not enough is because Streams#copy will be in\nthe stacktrace and it is not granted the security manager permissions\nneeded to close or write this channel. We only grant those permissions\nto classes loaded in the plugin classloader, and Streams#copy is from\nthe parent classloader. This is why we must wrap the close and write\ncalls as privileged, to truncate the Streams#copy call out of the\nstacktrace.\n\nThe reason that this issue is not caught in testing is because the size\nof data that we use in testing is too small to trigger the large blob\nresumable upload path. Therefore, we address this by adding a system\nproperty to control the threshold, which we can then set in tests to\nexercise this code path. Prior to rewriting the writeBlobResumable\nmethod to wrap the close and write calls as privileged, with this\nadditional test, we are able to reproduce the security manager\npermissions issue. After adding the wrapping, this test now passes.", "committedDate": "2020-04-17T21:17:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3NjUyMw==", "url": "https://github.com/elastic/elasticsearch/pull/55421#discussion_r410476523", "bodyText": "leftover debugging?", "author": "rjernst", "createdAt": "2020-04-17T21:22:49Z", "path": "plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageBlobStore.java", "diffHunk": "@@ -68,7 +73,29 @@\n     // request. Larger files should be uploaded over multiple requests (this is\n     // called \"resumable upload\")\n     // https://cloud.google.com/storage/docs/json_api/v1/how-tos/resumable-upload\n-    public static final int LARGE_BLOB_THRESHOLD_BYTE_SIZE = 5 * 1024 * 1024;\n+    public static final int LARGE_BLOB_THRESHOLD_BYTE_SIZE;\n+\n+    static {\n+        final String key = \"es.repository_gcs.large_blob_threshold_byte_size\";\n+        final String largeBlobThresholdByteSizeProperty = System.getProperty(key);\n+        if (largeBlobThresholdByteSizeProperty == null) {\n+            LARGE_BLOB_THRESHOLD_BYTE_SIZE = Math.toIntExact(new ByteSizeValue(5, ByteSizeUnit.MB).getBytes());\n+        } else {\n+            if (true) {", "originalCommit": "7df9d78937d2b6928c42f590fab70d661daf8892", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3OTE3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55421#discussion_r410479175", "bodyText": "Yes, lol, I was double checking that the property was getting set on the cluster in the largeBlobIntegTest test cluster. I pushed cc3d9a3.", "author": "jasontedor", "createdAt": "2020-04-17T21:29:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3NjUyMw=="}], "type": "inlineReview"}, {"oid": "8ad5470a7d4b051cf2ddb71e276555fd4efc759f", "url": "https://github.com/elastic/elasticsearch/commit/8ad5470a7d4b051cf2ddb71e276555fd4efc759f", "message": "Fix forbidden APIs issue", "committedDate": "2020-04-17T21:26:13Z", "type": "commit"}, {"oid": "cc3d9a315d7ed540f12404a75088e06c3d4d7ef5", "url": "https://github.com/elastic/elasticsearch/commit/cc3d9a315d7ed540f12404a75088e06c3d4d7ef5", "message": "Remove leftover debugging", "committedDate": "2020-04-17T21:28:50Z", "type": "commit"}]}