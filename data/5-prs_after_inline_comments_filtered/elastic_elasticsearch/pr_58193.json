{"pr_number": 58193, "pr_title": "Pipeline Inference Aggregation", "pr_createdAt": "2020-06-16T20:00:15Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/58193", "timeline": [{"oid": "aaedaed32d258431609359baf340cad96c31da73", "url": "https://github.com/elastic/elasticsearch/commit/aaedaed32d258431609359baf340cad96c31da73", "message": "Fix hashcode and tidy up", "committedDate": "2020-06-16T20:39:27Z", "type": "forcePushed"}, {"oid": "e5c2cbc07e9bff54eaa8065f995339e654b60226", "url": "https://github.com/elastic/elasticsearch/commit/e5c2cbc07e9bff54eaa8065f995339e654b60226", "message": "black list rest tests from security tests", "committedDate": "2020-06-17T09:55:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4OTY2MA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r441489660", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return (o == null || getClass() != o.getClass()) == false;\n          \n          \n            \n                    return o != null && getClass() == o.getClass();", "author": "benwtrent", "createdAt": "2020-06-17T11:57:08Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/trainedmodel/EmptyConfigUpdate.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.ml.inference.trainedmodel;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A config update that applies no changes.\n+ * Supports any type of {@link InferenceConfig}\n+ */\n+public class EmptyConfigUpdate implements InferenceConfigUpdate {\n+\n+    public static final ParseField NAME = new ParseField(\"empty\");\n+\n+    private static final ObjectParser<EmptyConfigUpdate, Void> PARSER =\n+        new ObjectParser<>(NAME.getPreferredName(), EmptyConfigUpdate::new);\n+\n+    public static EmptyConfigUpdate fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    public EmptyConfigUpdate() {\n+    }\n+\n+    public EmptyConfigUpdate(StreamInput in) {\n+    }\n+\n+    @Override\n+    public InferenceConfig apply(InferenceConfig originalConfig) {\n+        return originalConfig;\n+    }\n+\n+    @Override\n+    public InferenceConfig toConfig() {\n+        return RegressionConfig.EMPTY_PARAMS;\n+    }\n+\n+    @Override\n+    public boolean isSupported(InferenceConfig config) {\n+        return true;\n+    }\n+\n+    @Override\n+    public String getWriteableName() {\n+        return NAME.getPreferredName();\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return NAME.getPreferredName();\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.endObject();\n+        return builder;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        // Equal if o is not null and the same class\n+        return (o == null || getClass() != o.getClass()) == false;", "originalCommit": "e5c2cbc07e9bff54eaa8065f995339e654b60226", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5NzI2NA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r441497264", "bodyText": "Loading the model could take seconds to finish. Depending on the threadpool/context this is bad.\nThis needs to be addressed before merge.\nA potential solution to this would have pipeline aggs satisfy some sort of \"rewrite\" phase. This way the context has time to make an asynchronous call before executing.", "author": "benwtrent", "createdAt": "2020-06-17T12:11:56Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.EmptyConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.LenientlyParsedInferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(LenientlyParsedInferenceConfig.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfig inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, bucketsPath.values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfig.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfig inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }", "originalCommit": "e5c2cbc07e9bff54eaa8065f995339e654b60226", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5ODgzNw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r441498837", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public InferencePipelineAggregator(String name, Map<String,\n          \n          \n            \n                                                   String> bucketPathMap, Map<String, Object> metaData,\n          \n          \n            \n                public InferencePipelineAggregator(String name, \n          \n          \n            \n                                                   Map<String, String> bucketPathMap, \n          \n          \n            \n                                                   Map<String, Object> metaData,", "author": "benwtrent", "createdAt": "2020-06-17T12:14:58Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap, Map<String, Object> metaData,", "originalCommit": "e5c2cbc07e9bff54eaa8065f995339e654b60226", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwODgwMg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r442108802", "bodyText": "I agree with what Ben said about the need to remove the latch await, and if that is done there will hopefully be no need to catch InterruptedException here.  However, if we do still have to catch it, the handler should interrupt the current thread before throwing a different type of exception, because the class that owns the thread (e.g. a thread pool) may be relying on the interrupted status to decide what to do with the thread.", "author": "droberts195", "createdAt": "2020-06-18T09:54:07Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.EmptyConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.LenientlyParsedInferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(LenientlyParsedInferenceConfig.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfig inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, bucketsPath.values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfig.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfig inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);", "originalCommit": "c47d42d5c91ef55490abff9683f7a7c0b1bdde0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjExMDIyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r442110225", "bodyText": "Most exceptions used in Elasticsearch are already RuntimeExceptions.  It would be better if these weren't wrapped in a generic runtime exception, as that will look less friendly in error messages and especially if it ends up getting transported to a different node.  So it would be better to only wrap the exception if it's not already instanceof RuntimeException.", "author": "droberts195", "createdAt": "2020-06-18T09:56:29Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.EmptyConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.LenientlyParsedInferenceConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.RegressionConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(LenientlyParsedInferenceConfig.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfig inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, bucketsPath.values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfig.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfig inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }\n+\n+        if (error.get() != null) {\n+            throw new RuntimeException(error.get());", "originalCommit": "c47d42d5c91ef55490abff9683f7a7c0b1bdde0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5b9ecf60f23823e2c702b1bcbd7b554ef7c0c201", "url": "https://github.com/elastic/elasticsearch/commit/5b9ecf60f23823e2c702b1bcbd7b554ef7c0c201", "message": "Parsed Agg", "committedDate": "2020-06-24T11:55:33Z", "type": "forcePushed"}, {"oid": "0022c649b1f54dce121593c8ce26395112993d13", "url": "https://github.com/elastic/elasticsearch/commit/0022c649b1f54dce121593c8ce26395112993d13", "message": "Parsed Agg", "committedDate": "2020-06-24T12:33:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzMzg2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445633862", "bodyText": "It might be good to add a predictedValue() method to the InferenceResults interface. That way you can access it without these castings, especially since all inference results will either have a predicted value or null.", "author": "benwtrent", "createdAt": "2020-06-25T15:12:09Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -48,24 +51,44 @@ public InternalAggregation reduce(List<InternalAggregation> aggregations, Reduce\n     }\n \n     @Override\n-    @SuppressWarnings(\"unchecked\")\n     public Object getProperty(List<String> path) {\n-        Map<String, Object> resultMap = this.inferenceResult.writeResultToMap();\n-\n-        for (int i=0; i<path.size() -1; i++) {\n-            Object value = resultMap.get(path.get(i));\n-            if (value == null) {\n-                throw new InvalidAggregationPathException(\"Cannot find an key [\" + path.get(i) + \"] in \" + path);\n-            }\n-\n-            if (value instanceof Map<?, ?>) {\n-                resultMap = (Map<String, Object>)value;\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {\n+            String field = path.get(0);\n+            if (CommonFields.VALUE.getPreferredName().equals(field)) {\n+                if (inferenceResult instanceof ClassificationInferenceResults) {\n+                    return ((ClassificationInferenceResults)inferenceResult).transformedPredictedValue();\n+                } else if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    return ((SingleValueInferenceResults)inferenceResult).value();\n+                } else {\n+                    return null;\n+                }", "originalCommit": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY0MjI4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445642281", "bodyText": "Yes the problem is value returns a Double and transformedPredictedValue returns an Object that may be a Double, String or Boolean.\nI'll take another look though as it would be nice to simplify this", "author": "davidkyle", "createdAt": "2020-06-25T15:23:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzMzg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY0NTA5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445645093", "bodyText": "Object getPredictedValue() or something on the Interface could help. Fewer branches would be good :D", "author": "benwtrent", "createdAt": "2020-06-25T15:27:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzMzg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MDg3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446280872", "bodyText": "I went with Object getPredictedValue()", "author": "davidkyle", "createdAt": "2020-06-26T16:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzMzg2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzNDY5NA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445634694", "bodyText": "awe snap! Since we are handling the paths...I wonder if we could handle paths like feature_importance.0.importance and feature_importance.0.feature_name", "author": "benwtrent", "createdAt": "2020-06-25T15:13:18Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -48,24 +51,44 @@ public InternalAggregation reduce(List<InternalAggregation> aggregations, Reduce\n     }\n \n     @Override\n-    @SuppressWarnings(\"unchecked\")\n     public Object getProperty(List<String> path) {\n-        Map<String, Object> resultMap = this.inferenceResult.writeResultToMap();\n-\n-        for (int i=0; i<path.size() -1; i++) {\n-            Object value = resultMap.get(path.get(i));\n-            if (value == null) {\n-                throw new InvalidAggregationPathException(\"Cannot find an key [\" + path.get(i) + \"] in \" + path);\n-            }\n-\n-            if (value instanceof Map<?, ?>) {\n-                resultMap = (Map<String, Object>)value;\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {\n+            String field = path.get(0);\n+            if (CommonFields.VALUE.getPreferredName().equals(field)) {\n+                if (inferenceResult instanceof ClassificationInferenceResults) {\n+                    return ((ClassificationInferenceResults)inferenceResult).transformedPredictedValue();\n+                } else if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    return ((SingleValueInferenceResults)inferenceResult).value();\n+                } else {\n+                    return null;\n+                }\n+            } else if (SingleValueInferenceResults.FEATURE_IMPORTANCE.equals(field)) {\n+                if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    SingleValueInferenceResults valueResult = (SingleValueInferenceResults) inferenceResult;\n+                    return valueResult.getFeatureImportance();\n+                } else {\n+                    return null;\n+                }", "originalCommit": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY0NjY0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445646647", "bodyText": "The original implementation I had converted the result to a map which the code would navigate through using the split path so it was more generic as you suggested. I actually changed that so I could take a few lines of code out of this PR.\nThe only use of this method I could find is in a Bucket Selector which will only work with Doubles so I though this was sufficient in the first case.\nI'm open to the change though, maybe in the follow up PR?", "author": "davidkyle", "createdAt": "2020-06-25T15:29:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzNDY5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY1NTQyOA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445655428", "bodyText": "Follow up PR would be good.\nBucket select for when the most important feature is > X or when top class probability/score is > X would be neat!", "author": "benwtrent", "createdAt": "2020-06-25T15:43:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzNDY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTYzNTM3OA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445635378", "bodyText": "similar to the feature_importance path. Could we support getting the top class probability?", "author": "benwtrent", "createdAt": "2020-06-25T15:14:14Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -48,24 +51,44 @@ public InternalAggregation reduce(List<InternalAggregation> aggregations, Reduce\n     }\n \n     @Override\n-    @SuppressWarnings(\"unchecked\")\n     public Object getProperty(List<String> path) {\n-        Map<String, Object> resultMap = this.inferenceResult.writeResultToMap();\n-\n-        for (int i=0; i<path.size() -1; i++) {\n-            Object value = resultMap.get(path.get(i));\n-            if (value == null) {\n-                throw new InvalidAggregationPathException(\"Cannot find an key [\" + path.get(i) + \"] in \" + path);\n-            }\n-\n-            if (value instanceof Map<?, ?>) {\n-                resultMap = (Map<String, Object>)value;\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {\n+            String field = path.get(0);\n+            if (CommonFields.VALUE.getPreferredName().equals(field)) {\n+                if (inferenceResult instanceof ClassificationInferenceResults) {\n+                    return ((ClassificationInferenceResults)inferenceResult).transformedPredictedValue();\n+                } else if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    return ((SingleValueInferenceResults)inferenceResult).value();\n+                } else {\n+                    return null;\n+                }\n+            } else if (SingleValueInferenceResults.FEATURE_IMPORTANCE.equals(field)) {\n+                if (inferenceResult instanceof SingleValueInferenceResults) {\n+                    SingleValueInferenceResults valueResult = (SingleValueInferenceResults) inferenceResult;\n+                    return valueResult.getFeatureImportance();\n+                } else {\n+                    return null;\n+                }\n+            } else if (ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD.equals(field)) {", "originalCommit": "ab6e17850c7623cfec2ee7f0cdc51f8864d5dcb5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY2NzAzMg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445667032", "bodyText": "Since it only accepts skip, you could just remove the option from the parser and hardcode it.", "author": "polyfractal", "createdAt": "2020-06-25T16:00:15Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MTg2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446281861", "bodyText": "GapPolicy is a common setting, it feels a little underhand to silently ignore the setting and rewrite it to skip.", "author": "davidkyle", "createdAt": "2020-06-26T16:19:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY2NzAzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMwMzY4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446303681", "bodyText": "I think we'd just make it not something the parser recognized at all. Lots of aggs don't have a gap policy.", "author": "nik9000", "createdAt": "2020-06-26T17:01:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY2NzAzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA5MjY0MA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r447092640", "bodyText": "Yep, I meant removing it from the parser, so it'd throw an exception if the user tried to set it.  Most pipeline aggs have a gap_policy but it's not a required feature of pipelines by any means.", "author": "polyfractal", "createdAt": "2020-06-29T16:17:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY2NzAzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4OTc1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r447289752", "bodyText": "\ud83d\udc4d  I assumed pipeline aggs must accept a gap_policy but that doesn't make sense in every case.\nThe code was silly anyway I was parsing the field validating it then never reading the setting again. It's gone now.", "author": "davidkyle", "createdAt": "2020-06-29T22:18:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY2NzAzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY3NjQwMw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445676403", "bodyText": "I don't know what any of this is doing :), but I wanted to mention that by time this is called a lot of work will have been done on the data nodes.  So if any of this validation can be done earlier (parsing, etc) it would save some work.", "author": "polyfractal", "createdAt": "2020-06-25T16:14:10Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }\n+\n+        Exception e = error.get();\n+        if (e != null) {\n+            if (e instanceof RuntimeException) {\n+                throw (RuntimeException)e;\n+            } else {\n+                throw new RuntimeException(error.get());\n+            }\n+        }\n+\n+        InferenceConfigUpdate update = adaptForAggregation(inferenceConfig);\n+\n+        return new InferencePipelineAggregator(name, bucketPathMap, metaData, update, model.get());\n+    }\n+\n+    static InferenceConfigUpdate adaptForAggregation(InferenceConfigUpdate originalUpdate) {", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MjEzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446282131", "bodyText": "Very good point I've moved these checks to the validate() method", "author": "davidkyle", "createdAt": "2020-06-26T16:20:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY3NjQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY3ODYzOQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445678639", "bodyText": "Can these all be final?", "author": "polyfractal", "createdAt": "2020-06-25T16:17:32Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4OTQ5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445689497", "bodyText": "I think we can do bucket.getAggregations().asList().stream()... instead of StreamSupport.  The older pipelines have that laying around because they predated stream on the iterator itself iirc.", "author": "polyfractal", "createdAt": "2020-06-25T16:34:11Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof InternalNumericMetricsAggregation.SingleValue) {\n+                    double doubleVal = ((InternalNumericMetricsAggregation.SingleValue) propertyValue).value();\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof StringTerms.Bucket) {\n+                    StringTerms.Bucket b = (StringTerms.Bucket) propertyValue;\n+                    inputFields.put(aggName, b.getKeyAsString());\n+                } else if (propertyValue instanceof String) {\n+                    inputFields.put(aggName, propertyValue);\n+                } else if (propertyValue != null) {\n+                    // Doubles, String terms or null are valid, any other type is an error\n+                    throw invalidAggTypeError(bucketPath, propertyValue);\n+                }\n+            }\n+\n+\n+            InferenceResults inference;\n+            try {\n+                 inference = model.infer(inputFields, configUpdate);\n+            } catch (Exception e) {\n+                inference = new WarningInferenceResults(e.getMessage());\n+            }\n+\n+            final List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false).map(", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MjMyNw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446282327", "bodyText": "\ud83d\udc4d Guilty of copy-and-paste here.", "author": "davidkyle", "createdAt": "2020-06-26T16:20:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4OTQ5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY5ODI4Nw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445698287", "bodyText": "Just an FYI if you care: there's currently no validation that the user doesn't enter the same bucket path multiple times with different keys.  It's not generally a problem elsewhere (and sometimes useful), but since your requirements might be different, wanted to mention it :)", "author": "polyfractal", "createdAt": "2020-06-25T16:48:33Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcwMTA2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445701061", "bodyText": "Another Note: NaN or infinite could also mean the agg actually generate a NaN/Infinite.  Unfortunately there's not a good way of knowing without inspecting the underlying agg itself and seeing if it's using its placeholder value.\nOther pipelines don't particularly care, all NaN/Infinite are bad, so it's never been a problem.  But YMMV :)", "author": "polyfractal", "createdAt": "2020-06-25T16:53:00Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4ODMzNw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445688337", "bodyText": "Seems to be only accessed in tests.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public PredictionFieldType getPredictionFieldType() {\n          \n          \n            \n                PredictionFieldType getPredictionFieldType() {", "author": "benwtrent", "createdAt": "2020-06-25T16:32:17Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/results/ClassificationInferenceResults.java", "diffHunk": "@@ -85,6 +84,10 @@ public String getClassificationLabel() {\n         return topClasses;\n     }\n \n+    public PredictionFieldType getPredictionFieldType() {", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY5NjY2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445696662", "bodyText": "Suggested change", "author": "benwtrent", "createdAt": "2020-06-25T16:46:04Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/trainedmodel/ClassificationConfigUpdate.java", "diffHunk": "@@ -96,6 +96,8 @@ public ClassificationConfigUpdate(Integer numTopClasses,\n         }\n         this.numTopFeatureImportanceValues = featureImportance;\n         this.predictionFieldType = predictionFieldType;\n+", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcxMTYzMw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445711633", "bodyText": "I know it will mean more LoC, but this method is not extensible and only exists for a single thing/path.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                InferenceConfigUpdate duplicateWithResultsField(String resultsField);\n          \n          \n            \n                Builder newBuilder();\n          \n          \n            \n            \n          \n          \n            \n                interface Builder<T extends Builder, U extends InferenceConfigUpdate> {\n          \n          \n            \n            \n          \n          \n            \n                    U build();\n          \n          \n            \n            \n          \n          \n            \n                    T setResultsField(String resultsField);\n          \n          \n            \n            \n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nThen each builder satisfies this interface and needs a copy ctor accepting their appropriate update object.\nThe code to replace the resultsField name is now\nInferenceConfigUpdate newUpdate = update.newBuilder().setResultsField(\"foo\").build();\n\nThis seems more general + more clean than a one off method.", "author": "benwtrent", "createdAt": "2020-06-25T17:11:09Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/trainedmodel/InferenceConfigUpdate.java", "diffHunk": "@@ -6,14 +6,48 @@\n package org.elasticsearch.xpack.core.ml.inference.trainedmodel;\n \n import org.elasticsearch.common.io.stream.NamedWriteable;\n+import org.elasticsearch.xpack.core.ml.inference.TrainedModelConfig;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n import org.elasticsearch.xpack.core.ml.utils.NamedXContentObject;\n \n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n \n public interface InferenceConfigUpdate extends NamedXContentObject, NamedWriteable {\n+    Set<String> RESERVED_ML_FIELD_NAMES = new HashSet<>(Arrays.asList(\n+        WarningInferenceResults.WARNING.getPreferredName(),\n+        TrainedModelConfig.MODEL_ID.getPreferredName()));\n \n     InferenceConfig apply(InferenceConfig originalConfig);\n \n     InferenceConfig toConfig();\n \n     boolean isSupported(InferenceConfig config);\n+\n+    String getResultsField();\n+\n+    InferenceConfigUpdate duplicateWithResultsField(String resultsField);", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MjY0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446282649", "bodyText": "Good suggestion I've implemented the builder", "author": "davidkyle", "createdAt": "2020-06-26T16:21:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcxMTYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcxOTA3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445719073", "bodyText": "These types of validations seem like they belong in the agg parser.\nMaybe we should always parse the named objects into a ResultsFieldUpdate object or something.\nAt a minimum, these validations should occur before we attempt to load the model.", "author": "benwtrent", "createdAt": "2020-06-25T17:24:43Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.GAP_POLICY;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+        PARSER.declareField(InferencePipelineAggregationBuilder::setGapPolicy, p -> {\n+            if (p.currentToken() == XContentParser.Token.VALUE_STRING) {\n+                return BucketHelpers.GapPolicy.parse(p.text().toLowerCase(Locale.ROOT), p.getTokenLocation());\n+            }\n+            throw new IllegalArgumentException(\n+                \"Unsupported token [\" + p.currentToken() + \"] parsing inference aggregation \" + GAP_POLICY.getPreferredName());\n+        }, GAP_POLICY, ObjectParser.ValueType.STRING);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+    private BucketHelpers.GapPolicy gapPolicy = BucketHelpers.GapPolicy.SKIP;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        gapPolicy = BucketHelpers.GapPolicy.readFrom(in);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    void setGapPolicy(BucketHelpers.GapPolicy gapPolicy) {\n+        this.gapPolicy = gapPolicy;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"Model Id must be set\");\n+        }\n+        if (gapPolicy != BucketHelpers.GapPolicy.SKIP) {\n+            context.addValidationError(\"gap policy [\" + gapPolicy + \"] in not valid for [\" + NAME + \"] aggregation\");\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+        gapPolicy.writeTo(out);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<Model> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<Model> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            throw new RuntimeException(\"Inference aggregation interrupted loading model\", e);\n+        }\n+\n+        Exception e = error.get();\n+        if (e != null) {\n+            if (e instanceof RuntimeException) {\n+                throw (RuntimeException)e;\n+            } else {\n+                throw new RuntimeException(error.get());\n+            }\n+        }\n+\n+        InferenceConfigUpdate update = adaptForAggregation(inferenceConfig);\n+\n+        return new InferencePipelineAggregator(name, bucketPathMap, metaData, update, model.get());\n+    }\n+\n+    static InferenceConfigUpdate adaptForAggregation(InferenceConfigUpdate originalUpdate) {\n+        InferenceConfigUpdate updated;\n+        if (originalUpdate == null) {\n+            updated = new ResultsFieldUpdate(AGGREGATIONS_RESULTS_FIELD);\n+        } else {\n+            if (originalUpdate instanceof ClassificationConfigUpdate) {\n+                ClassificationConfigUpdate classUpdate = (ClassificationConfigUpdate)originalUpdate;\n+\n+                // error if the top classes result field is set and not equal to the only acceptable value\n+                String topClassesField = classUpdate.getTopClassesResultsField();\n+                if (Strings.isNullOrEmpty(topClassesField) == false &&\n+                    ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD.equals(topClassesField) == false) {\n+                    throw ExceptionsHelper.badRequestException(\"setting option [{}] to [{}] is not valid for inference aggregations\",\n+                        ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD, topClassesField);\n+                }\n+            }\n+\n+            // error if the results field is set and not equal to the only acceptable value\n+            String resultsField = originalUpdate.getResultsField();\n+            if (Strings.isNullOrEmpty(resultsField) == false && AGGREGATIONS_RESULTS_FIELD.equals(resultsField) == false) {\n+                throw ExceptionsHelper.badRequestException(\"setting option [{}] to [{}] is not valid for inference aggregations\",\n+                    ClassificationConfig.RESULTS_FIELD.getPreferredName(), resultsField);\n+            }", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcxOTcxNg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445719716", "bodyText": "Suggested change", "author": "benwtrent", "createdAt": "2020-06-25T17:25:47Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof InternalNumericMetricsAggregation.SingleValue) {\n+                    double doubleVal = ((InternalNumericMetricsAggregation.SingleValue) propertyValue).value();\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof StringTerms.Bucket) {\n+                    StringTerms.Bucket b = (StringTerms.Bucket) propertyValue;\n+                    inputFields.put(aggName, b.getKeyAsString());\n+                } else if (propertyValue instanceof String) {\n+                    inputFields.put(aggName, propertyValue);\n+                } else if (propertyValue != null) {\n+                    // Doubles, String terms or null are valid, any other type is an error\n+                    throw invalidAggTypeError(bucketPath, propertyValue);\n+                }\n+            }\n+", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcyMDEwNg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445720106", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                             inference = model.infer(inputFields, configUpdate);\n          \n          \n            \n                            inference = model.infer(inputFields, configUpdate);", "author": "benwtrent", "createdAt": "2020-06-25T17:26:29Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.search.aggregations.AggregationExecutionException;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;\n+import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\n+import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.search.aggregations.support.AggregationPath;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.Model;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+\n+public class InferencePipelineAggregator extends PipelineAggregator {\n+\n+    private Map<String, String> bucketPathMap;\n+    private InferenceConfigUpdate configUpdate;\n+    private Model model;\n+\n+    public InferencePipelineAggregator(String name, Map<String,\n+                                       String> bucketPathMap,\n+                                       Map<String, Object> metaData,\n+                                       InferenceConfigUpdate configUpdate,\n+                                       Model model) {\n+        super(name, bucketPathMap.values().toArray(new String[] {}), metaData);\n+        this.bucketPathMap = bucketPathMap;\n+        this.configUpdate = configUpdate;\n+        this.model = model;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, InternalAggregation.ReduceContext reduceContext) {\n+\n+        InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg =\n+            (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets();\n+\n+        List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>();\n+        for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) {\n+            Map<String, Object> inputFields = new HashMap<>();\n+\n+            if (bucket.getDocCount() == 0) {\n+                // ignore this empty bucket unless the doc count is used\n+                if (bucketPathMap.containsKey(\"_count\") == false) {\n+                    newBuckets.add(bucket);\n+                    continue;\n+                }\n+            }\n+\n+            for (Map.Entry<String, String> entry : bucketPathMap.entrySet()) {\n+                String aggName = entry.getKey();\n+                String bucketPath = entry.getValue();\n+                Object propertyValue = resolveBucketValue(originalAgg, bucket, bucketPath);\n+\n+                if (propertyValue instanceof Number) {\n+                    double doubleVal = ((Number) propertyValue).doubleValue();\n+                    // NaN or infinite values indicate a missing value\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof InternalNumericMetricsAggregation.SingleValue) {\n+                    double doubleVal = ((InternalNumericMetricsAggregation.SingleValue) propertyValue).value();\n+                    if (Double.isFinite(doubleVal)) {\n+                        inputFields.put(aggName, doubleVal);\n+                    }\n+                } else if (propertyValue instanceof StringTerms.Bucket) {\n+                    StringTerms.Bucket b = (StringTerms.Bucket) propertyValue;\n+                    inputFields.put(aggName, b.getKeyAsString());\n+                } else if (propertyValue instanceof String) {\n+                    inputFields.put(aggName, propertyValue);\n+                } else if (propertyValue != null) {\n+                    // Doubles, String terms or null are valid, any other type is an error\n+                    throw invalidAggTypeError(bucketPath, propertyValue);\n+                }\n+            }\n+\n+\n+            InferenceResults inference;\n+            try {\n+                 inference = model.infer(inputFields, configUpdate);", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcyNzMyMg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445727322", "bodyText": "Its difficult to fully grok with all these branches. Any way to use the fact that these branches are all returning out of the function?\nSomething like\nif (predicate) {\n   return \"foo\";\n}\nif (predicate2) {\n   return \"bar\";\n}\nreturn null;", "author": "benwtrent", "createdAt": "2020-06-25T17:39:02Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InvalidAggregationPathException;\n+import org.elasticsearch.xpack.core.ml.inference.results.ClassificationInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.InferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.SingleValueInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+public class InternalInferenceAggregation extends InternalAggregation {\n+\n+    private final InferenceResults inferenceResult;\n+\n+    protected InternalInferenceAggregation(String name, Map<String, Object> metadata,\n+                                           InferenceResults inferenceResult) {\n+        super(name, metadata);\n+        this.inferenceResult = inferenceResult;\n+    }\n+\n+    public InternalInferenceAggregation(StreamInput in) throws IOException {\n+        super(in);\n+        inferenceResult = in.readNamedWriteable(InferenceResults.class);\n+    }\n+\n+    public InferenceResults getInferenceResult() {\n+        return inferenceResult;\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeNamedWriteable(inferenceResult);\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) {\n+        throw new UnsupportedOperationException(\"Reducing an inference aggregation is not supported\");\n+    }\n+\n+    @Override\n+    public Object getProperty(List<String> path) {\n+        if (path.isEmpty()) {\n+            return this;\n+        } else if (path.size() == 1) {", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4Mzg4OA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446283888", "bodyText": "I've changed this again \ud83d\ude1e  now it only respects the value property.\nI've deleted an awful lot of branching code because as we discussed above we can come up with a better solution in a later PR", "author": "davidkyle", "createdAt": "2020-06-26T16:23:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcyNzMyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTczMTg3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445731872", "bodyText": "I do not see where this is used or referenced.", "author": "benwtrent", "createdAt": "2020-06-25T17:46:32Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/ParsedInference.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParseException;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.ParsedAggregation;\n+import org.elasticsearch.xpack.core.ml.inference.results.FeatureImportance;\n+import org.elasticsearch.xpack.core.ml.inference.results.SingleValueInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.results.TopClassEntry;\n+import org.elasticsearch.xpack.core.ml.inference.results.WarningInferenceResults;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.xpack.core.ml.inference.results.SingleValueInferenceResults.FEATURE_IMPORTANCE;\n+\n+\n+/**\n+ * There isn't enough information in toXContent representation of the\n+ * {@link org.elasticsearch.xpack.core.ml.inference.results.InferenceResults}\n+ * objects to fully reconstruct them. In particular, depending on which\n+ * fields are written (result value, feature importance) it is not possible to\n+ * distinguish between a Regression result and a Classification result.\n+ *\n+ * This class parses the union all possible fields that may be written by\n+ * InferenceResults.\n+ *\n+ * The warning field is mutually exclusive with all the other fields.\n+ */\n+public class ParsedInference extends ParsedAggregation {", "originalCommit": "285ae40134507395bb022490ab4e09bd24acd439", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc1ODE1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445758156", "bodyText": "These are usually in the HLRC.", "author": "nik9000", "createdAt": "2020-06-25T18:34:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTczMTg3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2MjQ1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r445762452", "bodyText": "But in that case it should be in the HLRC itself.", "author": "nik9000", "createdAt": "2020-06-25T18:41:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTczMTg3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5MTMxOA==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446191318", "bodyText": "This class is used in InternalInferenceAggregationTests which extends InternalAggregationTestCase. It is the base test that requires an implementation of ParsedAggregation.\nI think the reason the class is in the src directory rather than test is that as Nik said it is used by the HLRC or at least that is the convention I've followed.", "author": "davidkyle", "createdAt": "2020-06-26T13:41:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTczMTg3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5NzE3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r446197171", "bodyText": "\ud83d\udc4d", "author": "benwtrent", "createdAt": "2020-06-26T13:51:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTczMTg3Mg=="}], "type": "inlineReview"}, {"oid": "8b5cbb0268d18996ea2153e29499e116f3d71a20", "url": "https://github.com/elastic/elasticsearch/commit/8b5cbb0268d18996ea2153e29499e116f3d71a20", "message": "Remove the GapPolicy", "committedDate": "2020-06-30T08:04:03Z", "type": "forcePushed"}, {"oid": "5e8c63896d7ed268c0bbdd74d7a084badac1ae76", "url": "https://github.com/elastic/elasticsearch/commit/5e8c63896d7ed268c0bbdd74d7a084badac1ae76", "message": "First pass at docs", "committedDate": "2020-06-30T15:33:14Z", "type": "forcePushed"}, {"oid": "5c1c43a3ff92453719b7232a30160ece39064b4e", "url": "https://github.com/elastic/elasticsearch/commit/5c1c43a3ff92453719b7232a30160ece39064b4e", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>", "committedDate": "2020-07-01T14:07:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNDIzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448534235", "bodyText": "I'm ok to merge this, but I'll work immediately on replacing it with a rewrite. I don't think we can release this code.", "author": "nik9000", "createdAt": "2020-07-01T18:09:50Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.inference.aggs;\n+\n+import org.apache.lucene.util.SetOnce;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.LatchedActionListener;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfig;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ClassificationConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.InferenceConfigUpdate;\n+import org.elasticsearch.xpack.core.ml.inference.trainedmodel.ResultsFieldUpdate;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.LocalModel;\n+import org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+\n+public class InferencePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<InferencePipelineAggregationBuilder> {\n+\n+    public static String NAME = \"inference\";\n+\n+    public static final ParseField MODEL_ID = new ParseField(\"model_id\");\n+    private static final ParseField INFERENCE_CONFIG = new ParseField(\"inference_config\");\n+\n+    static String AGGREGATIONS_RESULTS_FIELD = \"value\";\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static final ConstructingObjectParser<InferencePipelineAggregationBuilder,\n+        Tuple<SetOnce<ModelLoadingService>, String>> PARSER = new ConstructingObjectParser<>(\n+        NAME, false,\n+        (args, context) -> new InferencePipelineAggregationBuilder(context.v2(), context.v1(), (Map<String, String>) args[0])\n+    );\n+\n+    static {\n+        PARSER.declareObject(constructorArg(), (p, c) -> p.mapStrings(), BUCKETS_PATH_FIELD);\n+        PARSER.declareString(InferencePipelineAggregationBuilder::setModelId, MODEL_ID);\n+        PARSER.declareNamedObject(InferencePipelineAggregationBuilder::setInferenceConfig,\n+            (p, c, n) -> p.namedObject(InferenceConfigUpdate.class, n, c), INFERENCE_CONFIG);\n+    }\n+\n+    private final Map<String, String> bucketPathMap;\n+    private String modelId;\n+    private InferenceConfigUpdate inferenceConfig;\n+    private final SetOnce<ModelLoadingService> modelLoadingService;\n+\n+    public static InferencePipelineAggregationBuilder parse(SetOnce<ModelLoadingService> modelLoadingService,\n+                                                            String pipelineAggregatorName,\n+                                                            XContentParser parser) {\n+        Tuple<SetOnce<ModelLoadingService>, String> context = new Tuple<>(modelLoadingService, pipelineAggregatorName);\n+        return PARSER.apply(parser, context);\n+    }\n+\n+    public InferencePipelineAggregationBuilder(String name, SetOnce<ModelLoadingService> modelLoadingService,\n+                                               Map<String, String> bucketsPath) {\n+        super(name, NAME, new TreeMap<>(bucketsPath).values().toArray(new String[] {}));\n+        this.modelLoadingService = modelLoadingService;\n+        this.bucketPathMap = bucketsPath;\n+    }\n+\n+    public InferencePipelineAggregationBuilder(StreamInput in, SetOnce<ModelLoadingService> modelLoadingService) throws IOException {\n+        super(in, NAME);\n+        modelId = in.readString();\n+        bucketPathMap = in.readMap(StreamInput::readString, StreamInput::readString);\n+        inferenceConfig = in.readOptionalNamedWriteable(InferenceConfigUpdate.class);\n+        this.modelLoadingService = modelLoadingService;\n+    }\n+\n+    void setModelId(String modelId) {\n+        this.modelId = modelId;\n+    }\n+\n+    void setInferenceConfig(InferenceConfigUpdate inferenceConfig) {\n+        this.inferenceConfig = inferenceConfig;\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        context.validateHasParent(NAME, name);\n+        if (modelId == null) {\n+            context.addValidationError(\"[model_id] must be set\");\n+        }\n+\n+        if (inferenceConfig != null) {\n+            // error if the results field is set and not equal to the only acceptable value\n+            String resultsField = inferenceConfig.getResultsField();\n+            if (Strings.isNullOrEmpty(resultsField) == false && AGGREGATIONS_RESULTS_FIELD.equals(resultsField) == false) {\n+                context.addValidationError(\"setting option [\" + ClassificationConfig.RESULTS_FIELD.getPreferredName()\n+                    + \"] to [\" + resultsField + \"] is not valid for inference aggregations\");\n+            }\n+\n+            if (inferenceConfig instanceof ClassificationConfigUpdate) {\n+                ClassificationConfigUpdate classUpdate = (ClassificationConfigUpdate)inferenceConfig;\n+\n+                // error if the top classes result field is set and not equal to the only acceptable value\n+                String topClassesField = classUpdate.getTopClassesResultsField();\n+                if (Strings.isNullOrEmpty(topClassesField) == false &&\n+                    ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD.equals(topClassesField) == false) {\n+                    context.addValidationError(\"setting option [\" + ClassificationConfig.DEFAULT_TOP_CLASSES_RESULTS_FIELD\n+                        + \"] to [\" + topClassesField + \"] is not valid for inference aggregations\");\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeString(modelId);\n+        out.writeMap(bucketPathMap, StreamOutput::writeString, StreamOutput::writeString);\n+        out.writeOptionalNamedWriteable(inferenceConfig);\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metaData) {\n+\n+        SetOnce<LocalModel> model = new SetOnce<>();\n+        SetOnce<Exception> error = new SetOnce<>();\n+        CountDownLatch latch = new CountDownLatch(1);\n+        ActionListener<LocalModel> listener = new LatchedActionListener<>(\n+            ActionListener.wrap(model::set, error::set), latch);\n+\n+        modelLoadingService.get().getModelForSearch(modelId, listener);\n+        try {\n+            // TODO Avoid the blocking wait\n+            latch.await();\n+        } catch (InterruptedException e) {", "originalCommit": "a474b2e90b4ea1940bd717ad7e446251b3b829fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODIwMw==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448598203", "bodyText": "I opened #58878 which should make this a thing we don't need to do any more. I've not made sure the tests pass with that yet, but I think it'll work soon enough.", "author": "nik9000", "createdAt": "2020-07-01T20:26:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNDIzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg0MzkxNg==", "url": "https://github.com/elastic/elasticsearch/pull/58193#discussion_r448843916", "bodyText": "Thanks for the review", "author": "davidkyle", "createdAt": "2020-07-02T08:41:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNDIzNQ=="}], "type": "inlineReview"}, {"oid": "dda58a2b05e9760534b9ba380868668ee2675929", "url": "https://github.com/elastic/elasticsearch/commit/dda58a2b05e9760534b9ba380868668ee2675929", "message": "inference pipeline agg classes", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "a89424875a135cf32953121962e0f1dc7d6313dd", "url": "https://github.com/elastic/elasticsearch/commit/a89424875a135cf32953121962e0f1dc7d6313dd", "message": "Add empty config update\n\n# Conflicts:\n#\tx-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InferencePipelineAggregationBuilder.java", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "a0fbf7538693f56db3ab77ae43d8e0d1d5c67033", "url": "https://github.com/elastic/elasticsearch/commit/a0fbf7538693f56db3ab77ae43d8e0d1d5c67033", "message": "Add yml test", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "4682eab2e15e47875e5e004b7fd52512559dbed1", "url": "https://github.com/elastic/elasticsearch/commit/4682eab2e15e47875e5e004b7fd52512559dbed1", "message": "Add InternalInferenceAggregation", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "32f49091b2bef1835246676aa80030c577b3c13a", "url": "https://github.com/elastic/elasticsearch/commit/32f49091b2bef1835246676aa80030c577b3c13a", "message": "Fixes for types that aren\u2019t a double", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "1eb6f5feda6941bb8bf7cab29fc30df88c0fa6e4", "url": "https://github.com/elastic/elasticsearch/commit/1eb6f5feda6941bb8bf7cab29fc30df88c0fa6e4", "message": "Implement getProperty", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "c26c08b8acbaec2cd5f1b5fbd1656b695bb36b78", "url": "https://github.com/elastic/elasticsearch/commit/c26c08b8acbaec2cd5f1b5fbd1656b695bb36b78", "message": "Some tidying up", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "1196d46d5e89eeee47bd9a399cd7793201635f0c", "url": "https://github.com/elastic/elasticsearch/commit/1196d46d5e89eeee47bd9a399cd7793201635f0c", "message": "Insert zeros gap policy is not valid", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "d494d268555b5e139073c059bfcb6c3b092275b0", "url": "https://github.com/elastic/elasticsearch/commit/d494d268555b5e139073c059bfcb6c3b092275b0", "message": "rework skip bucket", "committedDate": "2020-07-02T10:42:38Z", "type": "commit"}, {"oid": "b3a235cf1c2ca6ab6c2f5553c2b1704604c0d26d", "url": "https://github.com/elastic/elasticsearch/commit/b3a235cf1c2ca6ab6c2f5553c2b1704604c0d26d", "message": "Make inference results implement toxcontent\n\n# Conflicts:\n#\tx-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/results/SingleValueInferenceResults.java\n#\tx-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/ml/inference/results/WarningInferenceResultsTests.java\n#\tx-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/aggs/InternalInferenceAggregation.java", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "507f3878656239e8f8d8f3fe87cd549ed9989285", "url": "https://github.com/elastic/elasticsearch/commit/507f3878656239e8f8d8f3fe87cd549ed9989285", "message": "Add Internal Inference agg tests", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "f322c972d5bc4a1d1f4985750d6cc06b87488048", "url": "https://github.com/elastic/elasticsearch/commit/f322c972d5bc4a1d1f4985750d6cc06b87488048", "message": "Handle bucket terms", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "440d7c6453d632179a9c1afeae6f2eca70a35a6d", "url": "https://github.com/elastic/elasticsearch/commit/440d7c6453d632179a9c1afeae6f2eca70a35a6d", "message": "Delete broken test", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "4b609e811feedf16a1f439aafb35877311109c2b", "url": "https://github.com/elastic/elasticsearch/commit/4b609e811feedf16a1f439aafb35877311109c2b", "message": "precommit", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "c3e943a0e37a4eede8396b7c4bf5018b0bfaf40d", "url": "https://github.com/elastic/elasticsearch/commit/c3e943a0e37a4eede8396b7c4bf5018b0bfaf40d", "message": "Fix hashcode and tidy up", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "de4f8e6e0a2c18f2f48096798c4881356292f70f", "url": "https://github.com/elastic/elasticsearch/commit/de4f8e6e0a2c18f2f48096798c4881356292f70f", "message": "checkstyle", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "e67cae3f68e6dece4a0a5ccf9a22a80c555dcb35", "url": "https://github.com/elastic/elasticsearch/commit/e67cae3f68e6dece4a0a5ccf9a22a80c555dcb35", "message": "black list rest tests from security tests", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "c4848043c2dc2bfda4c943038e183c8f1c8312d5", "url": "https://github.com/elastic/elasticsearch/commit/c4848043c2dc2bfda4c943038e183c8f1c8312d5", "message": "Fix ml with security yml tests\n\nCheck an ml endpoint was called", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "158b99a674a4232b9c039e4c924ad4ab418b8583", "url": "https://github.com/elastic/elasticsearch/commit/158b99a674a4232b9c039e4c924ad4ab418b8583", "message": "un block ml with security tests", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "011e6f5a525c4f460c7ea625552992160452bfd5", "url": "https://github.com/elastic/elasticsearch/commit/011e6f5a525c4f460c7ea625552992160452bfd5", "message": "Don't swallow interrupt", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "1c793d0cbeac14922aa1d6ac127aab39e776db30", "url": "https://github.com/elastic/elasticsearch/commit/1c793d0cbeac14922aa1d6ac127aab39e776db30", "message": "Move config unique field name check", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "db8133a8dda7ae573810df32fd895615784b4f2e", "url": "https://github.com/elastic/elasticsearch/commit/db8133a8dda7ae573810df32fd895615784b4f2e", "message": "Do not allow setting the results_field in agg", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "c44b1e4873ed38a316e8483443f44343ad4107b5", "url": "https://github.com/elastic/elasticsearch/commit/c44b1e4873ed38a316e8483443f44343ad4107b5", "message": "Results field update WIP", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "8f3d57e81e5f3e3d71c351f95a8cefbda732a581", "url": "https://github.com/elastic/elasticsearch/commit/8f3d57e81e5f3e3d71c351f95a8cefbda732a581", "message": "Revert \"Results field update WIP\"\n\nThis reverts commit 2b00456f18105fec0f8d670468f12df5d72d63f0.", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "da5c99ebb8e4661f73a076cd478fc92d1ead0dd9", "url": "https://github.com/elastic/elasticsearch/commit/da5c99ebb8e4661f73a076cd478fc92d1ead0dd9", "message": "Add results field update", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "e3c33a61f30c7afed0decf71dc1c0bb108fe7847", "url": "https://github.com/elastic/elasticsearch/commit/e3c33a61f30c7afed0decf71dc1c0bb108fe7847", "message": "delete unused update class", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "e1119d01ad1e3f403a662050ac965a95e509d3c6", "url": "https://github.com/elastic/elasticsearch/commit/e1119d01ad1e3f403a662050ac965a95e509d3c6", "message": "Extract TopClassEntry", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "b7631cb47b821b96db6db4178e2ae079b318967e", "url": "https://github.com/elastic/elasticsearch/commit/b7631cb47b821b96db6db4178e2ae079b318967e", "message": "Fix serialisation test", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "f9b136ff8fddc0dd11ae16cab35478a95044dc69", "url": "https://github.com/elastic/elasticsearch/commit/f9b136ff8fddc0dd11ae16cab35478a95044dc69", "message": "Feature importance parser", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "1033b48bba31663b8b26b0222bc52857bd13f2f4", "url": "https://github.com/elastic/elasticsearch/commit/1033b48bba31663b8b26b0222bc52857bd13f2f4", "message": "Parsed Agg", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "4bbc76125747121c101e684a0b259987c0e0f45d", "url": "https://github.com/elastic/elasticsearch/commit/4bbc76125747121c101e684a0b259987c0e0f45d", "message": "Remove defaults in config update", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "ac2b2ac66cc531e1adc1e1902ec26f14f124b430", "url": "https://github.com/elastic/elasticsearch/commit/ac2b2ac66cc531e1adc1e1902ec26f14f124b430", "message": "Remove getProperty method on results", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "fad3e1484f0d58341ea7239267895ef901ec91a0", "url": "https://github.com/elastic/elasticsearch/commit/fad3e1484f0d58341ea7239267895ef901ec91a0", "message": "Reject setting the top_classes field", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "e6f8bf60e9bd045c2f283f516514a33beb5a0ee5", "url": "https://github.com/elastic/elasticsearch/commit/e6f8bf60e9bd045c2f283f516514a33beb5a0ee5", "message": "tidy up", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "9e5fafdda827fc1fe2f84107d241f267f20178dc", "url": "https://github.com/elastic/elasticsearch/commit/9e5fafdda827fc1fe2f84107d241f267f20178dc", "message": "checkstyle", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "756e8a08724d9d07032514c46739c79cce2f2ff5", "url": "https://github.com/elastic/elasticsearch/commit/756e8a08724d9d07032514c46739c79cce2f2ff5", "message": "Add predictedValue method", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "1c499d503d46772cbc76ad4bcbe581cc8114db0f", "url": "https://github.com/elastic/elasticsearch/commit/1c499d503d46772cbc76ad4bcbe581cc8114db0f", "message": "re work the awful duplicate method", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "27e2f19a512bf60a6351b8339fb431240f455fcd", "url": "https://github.com/elastic/elasticsearch/commit/27e2f19a512bf60a6351b8339fb431240f455fcd", "message": "Move validation to the validate method", "committedDate": "2020-07-02T10:42:39Z", "type": "commit"}, {"oid": "ab80263c26896eb07d5556f30b5efecd8c2a70f6", "url": "https://github.com/elastic/elasticsearch/commit/ab80263c26896eb07d5556f30b5efecd8c2a70f6", "message": "tidy up", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "14184fa9cdab966ed0757a2aaa14aa10283a053d", "url": "https://github.com/elastic/elasticsearch/commit/14184fa9cdab966ed0757a2aaa14aa10283a053d", "message": "Remove the GapPolicy", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "04444b7812db207e0c910aa01dd2877cb968fe91", "url": "https://github.com/elastic/elasticsearch/commit/04444b7812db207e0c910aa01dd2877cb968fe91", "message": "Fix compilation after rebase", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "113b0dddef698964b7911a0c748c216f09f19b27", "url": "https://github.com/elastic/elasticsearch/commit/113b0dddef698964b7911a0c748c216f09f19b27", "message": "First pass at docs", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "4c08a66536aa2657bca24695224432f2c154839c", "url": "https://github.com/elastic/elasticsearch/commit/4c08a66536aa2657bca24695224432f2c154839c", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "153d3997be94c25a9fef0d2aa8699fe1f8ed65e1", "url": "https://github.com/elastic/elasticsearch/commit/153d3997be94c25a9fef0d2aa8699fe1f8ed65e1", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "a9b637f4afeae1d99e83c7cf07e0cbe19cd7ad6b", "url": "https://github.com/elastic/elasticsearch/commit/a9b637f4afeae1d99e83c7cf07e0cbe19cd7ad6b", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "e32457a6453a6f01b6b3aa99151ee5681e74fa52", "url": "https://github.com/elastic/elasticsearch/commit/e32457a6453a6f01b6b3aa99151ee5681e74fa52", "message": "Update docs/reference/aggregations/pipeline/inference-bucket-aggregation.asciidoc\n\nCo-authored-by: Istv\u00e1n Zolt\u00e1n Szab\u00f3 <istvan.szabo@elastic.co>", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "8cdd8ae61e9ba55106aa429f780497be9a8f638d", "url": "https://github.com/elastic/elasticsearch/commit/8cdd8ae61e9ba55106aa429f780497be9a8f638d", "message": "Resolve conflict", "committedDate": "2020-07-02T10:42:40Z", "type": "commit"}, {"oid": "8cdd8ae61e9ba55106aa429f780497be9a8f638d", "url": "https://github.com/elastic/elasticsearch/commit/8cdd8ae61e9ba55106aa429f780497be9a8f638d", "message": "Resolve conflict", "committedDate": "2020-07-02T10:42:40Z", "type": "forcePushed"}]}