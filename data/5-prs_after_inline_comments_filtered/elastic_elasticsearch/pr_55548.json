{"pr_number": 55548, "pr_title": "Add regex query support to wildcard field (approach 2)", "pr_createdAt": "2020-04-21T17:01:36Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/55548", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjgzMzc0Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412833746", "bodyText": "I think it would help to have explicit checks on the clauses that we create, not just the fact that it's a boolean query. I know we have random tests that check the validity of these queries but that's generally useful to also have simple tests that test the logic more carefully.", "author": "jimczi", "createdAt": "2020-04-22T09:43:57Z", "path": "x-pack/plugin/wildcard/src/test/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapperTests.java", "diffHunk": "@@ -221,8 +255,115 @@ public void testSearchResultsVersusKeywordField() throws IOException {\n         reader.close();\n         dir.close();\n     }\n+    \n+    public void testRegexAcceleration() throws IOException {\n+        // All of these regexes should be accelerated. Previously we kept the \".*\" prefix while creating\n+        // automatons and this would cause many transitions. Some of the expressions below would just throw \n+        // in the towel and revert to \"match all\".\n+        String regexes[] = { \".*/etc/passw.*\", \".*etc/passwd HTTP.*\", \".*/etc/passwd.*\", \"/etc/passwd.*\", \n+            \".*ietcipasswd.*\", \".*jetcjpasswd.*\", \".*\\\\.c\", \".a\", \"b\", \"a.*bcgcgc\", \"a.*bcg(cg|ab)c\",\n+            \"\\\"[Ee][Nn][Cc][Oo][Dd][Ee][Dd]\\\"\"};\n+        for (String regex : regexes) {\n+            Query wildcardFieldQuery = wildcardFieldType.fieldType().regexpQuery(regex, 0, 20000, null, MOCK_QSC);\n+            assertTrue(regex+\" pattern not accelerated\", wildcardFieldQuery instanceof BooleanQuery);\n+        }\n+    }    \n+    \n+    public void testWildcardAcceleration() throws IOException {\n+        // All of these patterns should be accelerated. \n+        String patterns[] = { \"*foobar\", \"foobar*\", \"foo*bar\", \"foo?bar\", \"?foo*bar?\", \"*c\"};\n+        for (String pattern : patterns) {\n+            Query wildcardFieldQuery = wildcardFieldType.fieldType().wildcardQuery(pattern, null, MOCK_QSC);\n+            assertTrue(wildcardFieldQuery instanceof BooleanQuery);", "originalCommit": "f7ff69c03a325c19370bb1a5a5c8a765fc3eee72", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM5Nzc3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419397773", "bodyText": "Thanks for adding the tests, can you do the same to test the logic of fuzzy query ?", "author": "jimczi", "createdAt": "2020-05-04T12:26:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjgzMzc0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjgzNjc4OA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412836788", "bodyText": "I am not sure we should differentiate wildcard and regex, I think we should just ignore this setting for wildcard field.", "author": "jimczi", "createdAt": "2020-04-22T09:48:21Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,187 +229,174 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n-                    case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n-                        break;\n-                    case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n \n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n-                        }\n-                        break;\n-                    case WildcardQuery.WILDCARD_ESCAPE:\n-                        // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n-                            length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n-                    default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n-                }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n-            }\n-\n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n-            }\n-\n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n-            }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n-            }\n-\n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+            \n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            \n+            // Parse the wildcard, marking ? and * chars with a single invalid char\n+            // to create a simple finite automaton we can parse\n+            List<Automaton> automata = new ArrayList<>();            \n+            for (int i = 0; i < wildcardPattern.length();) {\n+              final int c = wildcardPattern.codePointAt(i);\n+              int length = Character.charCount(c);\n+              switch(c) {\n+                case WildcardQuery.WILDCARD_STRING: \n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_CHAR:\n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_ESCAPE:\n+                  // add the next codepoint instead, if it exists\n+                  if (i + length < wildcardPattern.length()) {\n+                    final int nextChar = wildcardPattern.codePointAt(i + length);\n+                    length += Character.charCount(nextChar);\n+                    automata.add(Automata.makeChar(nextChar));\n+                  } else {\n+                      automata.add(Automata.makeChar(c));                      \n+                  }\n+                  break;\n+                      \n+                default:\n+                  automata.add(Automata.makeChar(c));\n+              }\n+              i += length;\n             }\n+            \n+            Automaton approxAutomata = Operations.concatenate(automata);                \n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern);\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (context.allowExpensiveQueries() == false) {", "originalCommit": "f7ff69c03a325c19370bb1a5a5c8a765fc3eee72", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjk4NTkxMA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412985910", "bodyText": "Is it worth reserving it for patterns that don't produce an approximation query?", "author": "markharwood", "createdAt": "2020-04-22T13:33:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjgzNjc4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg0MDA4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412840085", "bodyText": "this is not needed, the field is always indexed ?", "author": "jimczi", "createdAt": "2020-04-22T09:52:58Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,187 +229,174 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n-                    case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n-                        break;\n-                    case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n \n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n-                        }\n-                        break;\n-                    case WildcardQuery.WILDCARD_ESCAPE:\n-                        // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n-                            length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n-                    default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n-                }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n-            }\n-\n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n-            }\n-\n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n-            }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n-            }\n-\n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+            \n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            \n+            // Parse the wildcard, marking ? and * chars with a single invalid char\n+            // to create a simple finite automaton we can parse\n+            List<Automaton> automata = new ArrayList<>();            \n+            for (int i = 0; i < wildcardPattern.length();) {\n+              final int c = wildcardPattern.codePointAt(i);\n+              int length = Character.charCount(c);\n+              switch(c) {\n+                case WildcardQuery.WILDCARD_STRING: \n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_CHAR:\n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_ESCAPE:\n+                  // add the next codepoint instead, if it exists\n+                  if (i + length < wildcardPattern.length()) {\n+                    final int nextChar = wildcardPattern.codePointAt(i + length);\n+                    length += Character.charCount(nextChar);\n+                    automata.add(Automata.makeChar(nextChar));\n+                  } else {\n+                      automata.add(Automata.makeChar(c));                      \n+                  }\n+                  break;\n+                      \n+                default:\n+                  automata.add(Automata.makeChar(c));\n+              }\n+              i += length;\n             }\n+            \n+            Automaton approxAutomata = Operations.concatenate(automata);                \n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern);\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            failIfNotIndexed();", "originalCommit": "f7ff69c03a325c19370bb1a5a5c8a765fc3eee72", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg1NzA4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412857085", "bodyText": "I don't think we should lowercase eagerly here. Lowercasing can happen in the ApproximateRegExp directly but I wonder if this should be tackled in a follow up ?", "author": "jimczi", "createdAt": "2020-04-22T10:18:06Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,187 +229,175 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n-                    case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n-                        break;\n-                    case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n \n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n-                        }\n-                        break;\n-                    case WildcardQuery.WILDCARD_ESCAPE:\n-                        // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n-                            length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n-                    default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n-                }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n-            }\n-\n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n-            }\n-\n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n-            }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n-            }\n-\n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+            \n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            \n+            String ngramIndexPattern = toLowerCase(wildcardPattern);\n+            // Parse the wildcard, marking ? and * chars with a single invalid char\n+            // to create a simple finite automaton we can parse\n+            List<Automaton> automata = new ArrayList<>();            \n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+              final int c = ngramIndexPattern.codePointAt(i);\n+              int length = Character.charCount(c);\n+              switch(c) {\n+                case WildcardQuery.WILDCARD_STRING: \n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_CHAR:\n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_ESCAPE:\n+                  // add the next codepoint instead, if it exists\n+                  if (i + length < wildcardPattern.length()) {\n+                    final int nextChar = wildcardPattern.codePointAt(i + length);\n+                    length += Character.charCount(nextChar);\n+                    automata.add(Automata.makeChar(nextChar));\n+                  } else {\n+                      automata.add(Automata.makeChar(c));                      \n+                  }\n+                  break;\n+                      \n+                default:\n+                  automata.add(Automata.makeChar(c));\n+              }\n+              i += length;\n             }\n+            \n+            Automaton approxAutomata = Operations.concatenate(automata);                \n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern);\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            failIfNotIndexed();\n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(toLowerCase(value), flags);", "originalCommit": "4b4d24ef17e503515581059ee79d418516ca2109", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2MTYwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412861609", "bodyText": "I'd like to see the solution without lowercasing the ngram index. We can try to optimize lowercased search in a follow up but for now we should ensure that we handle all types of automaton without relying on normalization ?", "author": "jimczi", "createdAt": "2020-04-22T10:25:19Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,187 +229,175 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n-                    case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n-                        break;\n-                    case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n \n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n-                        }\n-                        break;\n-                    case WildcardQuery.WILDCARD_ESCAPE:\n-                        // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n-                            length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n-                    default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n-                }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n-            }\n-\n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n-            }\n-\n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n-            }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n-            }\n-\n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+            \n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            \n+            String ngramIndexPattern = toLowerCase(wildcardPattern);\n+            // Parse the wildcard, marking ? and * chars with a single invalid char\n+            // to create a simple finite automaton we can parse\n+            List<Automaton> automata = new ArrayList<>();            \n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+              final int c = ngramIndexPattern.codePointAt(i);\n+              int length = Character.charCount(c);\n+              switch(c) {\n+                case WildcardQuery.WILDCARD_STRING: \n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_CHAR:\n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_ESCAPE:\n+                  // add the next codepoint instead, if it exists\n+                  if (i + length < wildcardPattern.length()) {\n+                    final int nextChar = wildcardPattern.codePointAt(i + length);\n+                    length += Character.charCount(nextChar);\n+                    automata.add(Automata.makeChar(nextChar));\n+                  } else {\n+                      automata.add(Automata.makeChar(c));                      \n+                  }\n+                  break;\n+                      \n+                default:\n+                  automata.add(Automata.makeChar(c));\n+              }\n+              i += length;\n             }\n+            \n+            Automaton approxAutomata = Operations.concatenate(automata);                \n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern);\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            failIfNotIndexed();\n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(toLowerCase(value), flags);\n+            Automaton ngramAutomaton = ngramRegex.toAutomaton(maxDeterminizedStates);\n \n-\n+            return createApproxAndVerifyQuery(ngramAutomaton, automaton, value);\n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n         }\n+        \n+        \n+        Query createApproxAndVerifyQuery(Automaton simplifiedAutomaton, Automaton verifyAutomaton, String expression) {\n \n+            Query ngramApproximation = createApproximationQueryFromAutomaton(simplifiedAutomaton);\n \n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), expression, verifyAutomaton);\n+            if (ngramApproximation != null && !(ngramApproximation instanceof MatchAllDocsQuery)) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(ngramApproximation, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            }\n+            return verifyingQuery;\n+        }\n+        \n+\n+        private Query createApproximationQueryFromAutomaton(Automaton simplifiedAutomaton) {\n+            FiniteStringsIterator iterator = new FiniteStringsIterator(simplifiedAutomaton);\n+            BooleanQuery.Builder result = new BooleanQuery.Builder();\n+            int numAlternativePathsConsidered = 0;\n+            for (IntsRef finiteString; (finiteString = iterator.next()) != null;) {\n+                // Efficient simplification is tough - we need to create a query that\n+                // covers ALL of the possible regex permutations.\n+                // However, FiniteStringsIterator is a depth-first iteration\n+                // so EVERY alternative path would have to be considered before we could\n+                // determine a common section all the paths shared in order to simplify.\n+                // So, we simplify up-front by:\n+                // 1) replacing all repetitions eg (foo)* with a single invalid char in the regex string used to build the automaton\n+                // 2) lowercasing all concrete values so searches like [Ee][Nn][Cc][Oo][Dd][Ee][Dd] don't fork myriad paths", "originalCommit": "4b4d24ef17e503515581059ee79d418516ca2109", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjk1ODkxMw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412958913", "bodyText": "I think I need to understand better how an FSI approach can be made to work with an approach that tries to find articulation points in the automaton. The former relies on FSI to chase down all the possible paths while the latter takes direct control of exploring the graph paths to understand the branching (like Nik's code).\nIf we try to make life simple by using FSI only then some up-front case normalisation is useful to avoid searches like the [Pp][Oo][Ww][Ee][Rr][Ss][Hh][Ee][Ll][Ll] example from this blog blowing the BooleanQuery limits.", "author": "markharwood", "createdAt": "2020-04-22T12:58:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2MTYwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2MzUwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412863509", "bodyText": "Should we skip 2 positions instead of 1 when there are enough characters ? 123456 should produce 123 and 456 rather than 123, 345, 456 ?", "author": "jimczi", "createdAt": "2020-04-22T10:28:11Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,187 +229,175 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n-                    case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n-                        break;\n-                    case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n \n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n-                        }\n-                        break;\n-                    case WildcardQuery.WILDCARD_ESCAPE:\n-                        // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n-                            length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n-                    default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n-                }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n-            }\n-\n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n-            }\n-\n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n-            }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n-            }\n-\n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+            \n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            \n+            String ngramIndexPattern = toLowerCase(wildcardPattern);\n+            // Parse the wildcard, marking ? and * chars with a single invalid char\n+            // to create a simple finite automaton we can parse\n+            List<Automaton> automata = new ArrayList<>();            \n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+              final int c = ngramIndexPattern.codePointAt(i);\n+              int length = Character.charCount(c);\n+              switch(c) {\n+                case WildcardQuery.WILDCARD_STRING: \n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_CHAR:\n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_ESCAPE:\n+                  // add the next codepoint instead, if it exists\n+                  if (i + length < wildcardPattern.length()) {\n+                    final int nextChar = wildcardPattern.codePointAt(i + length);\n+                    length += Character.charCount(nextChar);\n+                    automata.add(Automata.makeChar(nextChar));\n+                  } else {\n+                      automata.add(Automata.makeChar(c));                      \n+                  }\n+                  break;\n+                      \n+                default:\n+                  automata.add(Automata.makeChar(c));\n+              }\n+              i += length;\n             }\n+            \n+            Automaton approxAutomata = Operations.concatenate(automata);                \n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern);\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            failIfNotIndexed();\n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(toLowerCase(value), flags);\n+            Automaton ngramAutomaton = ngramRegex.toAutomaton(maxDeterminizedStates);\n \n-\n+            return createApproxAndVerifyQuery(ngramAutomaton, automaton, value);\n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n         }\n+        \n+        \n+        Query createApproxAndVerifyQuery(Automaton simplifiedAutomaton, Automaton verifyAutomaton, String expression) {\n \n+            Query ngramApproximation = createApproximationQueryFromAutomaton(simplifiedAutomaton);\n \n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), expression, verifyAutomaton);\n+            if (ngramApproximation != null && !(ngramApproximation instanceof MatchAllDocsQuery)) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(ngramApproximation, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            }\n+            return verifyingQuery;\n+        }\n+        \n+\n+        private Query createApproximationQueryFromAutomaton(Automaton simplifiedAutomaton) {\n+            FiniteStringsIterator iterator = new FiniteStringsIterator(simplifiedAutomaton);\n+            BooleanQuery.Builder result = new BooleanQuery.Builder();\n+            int numAlternativePathsConsidered = 0;\n+            for (IntsRef finiteString; (finiteString = iterator.next()) != null;) {\n+                // Efficient simplification is tough - we need to create a query that\n+                // covers ALL of the possible regex permutations.\n+                // However, FiniteStringsIterator is a depth-first iteration\n+                // so EVERY alternative path would have to be considered before we could\n+                // determine a common section all the paths shared in order to simplify.\n+                // So, we simplify up-front by:\n+                // 1) replacing all repetitions eg (foo)* with a single invalid char in the regex string used to build the automaton\n+                // 2) lowercasing all concrete values so searches like [Ee][Nn][Cc][Oo][Dd][Ee][Dd] don't fork myriad paths\n+\n+                numAlternativePathsConsidered++;\n+                if (numAlternativePathsConsidered > BooleanQuery.getMaxClauseCount()) {\n+                    // Bail out - too complex to represent with a single query on the ngram index.\n+                    return null;\n                 }\n \n-                // Add any start/end of string character\n-                if (i == 0 && patternStructure.openStart == false) {\n-                    // Start-of-string anchored (is not a leading wildcard)\n-                    fragment = TOKEN_START_OR_END_CHAR + fragment;\n-                }\n-                if (patternStructure.openEnd == false && i == patternStructure.fragments.length - 1) {\n-                    // End-of-string anchored (is not a trailing wildcard)\n-                    fragment = fragment + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR;\n-                }\n-                if (fragment.codePointCount(0, fragment.length()) <= NGRAM_SIZE) {\n-                    tokens.add(fragment);\n-                } else {\n-                    // Break fragment into multiple Ngrams\n-                    TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n-                    CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n-                    String lastUnusedToken = null;\n-                    try {\n-                        tokenizer.reset();\n-                        boolean takeThis = true;\n-                        // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n-                        // `123` and `345` - no need to search for 234. We take every other ngram.\n-                        while (tokenizer.incrementToken()) {\n-                            String tokenValue = termAtt.toString();\n-                            if (takeThis) {\n-                                tokens.add(tokenValue);\n-                            } else {\n-                                lastUnusedToken = tokenValue;\n-                            }\n-                            // alternate\n-                            takeThis = !takeThis;\n+                ArrayList<String> thisPathTokens = new ArrayList<>();\n+                StringBuilder pathFragment = new StringBuilder();\n+                // Add all the valid chars to make a path fragment\n+                for (int i = 0; i < finiteString.length; i++) {\n+                    int codePoint = finiteString.ints[finiteString.offset + i];\n+                    if (codePoint == INVALID_AUTOMATON_CHAR) {\n+                        // End of a run of concrete characters\n+                        if (pathFragment.length() > 0) {\n+                            getNgramTokens(thisPathTokens, pathFragment.toString());\n                         }\n-                        if (lastUnusedToken != null) {\n-                            // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n-                            // `ake` to complete the logic.\n-                            tokens.add(lastUnusedToken);\n+                        pathFragment = new StringBuilder();\n+                    } else {\n+                        if (i == 0) {\n+                            // Prefix with start of string\n+                            pathFragment.append(TOKEN_START_OR_END_CHAR);\n                         }\n-                        tokenizer.end();\n-                        tokenizer.close();\n-                    } catch (IOException ioe) {\n-                        throw new ElasticsearchParseException(\"Error parsing wildcard query pattern fragment [\" + fragment + \"]\");\n+                        pathFragment.append(Character.toChars(codePoint));\n                     }\n                 }\n+                if (pathFragment.length() > 0) {\n+                    // Have a concrete end-of-string\n+                    pathFragment.append(TOKEN_START_OR_END_CHAR);\n+                    pathFragment.append(TOKEN_START_OR_END_CHAR);\n+                    getNgramTokens(thisPathTokens, pathFragment.toString());\n+                }\n+                // Add one of several possible paths\n+                if (thisPathTokens.size() > 0) {\n+                    BooleanQuery onePath = createApproximationQuery(thisPathTokens);\n+                    result.add(onePath, Occur.SHOULD);\n+                }\n             }\n-\n-            if (patternStructure.isMatchAll()) {\n-                return new MatchAllDocsQuery();\n+            BooleanQuery q = result.build();\n+            if (q.clauses().size() == 0) {\n+                return null;\n             }\n-            BooleanQuery approximation = createApproximationQuery(tokens);\n-            if (approximation.clauses().size() > 1 || patternStructure.needsVerification()) {\n-                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n-                verifyingBuilder.add(new BooleanClause(approximation, Occur.MUST));\n-                Automaton automaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n-                verifyingBuilder.add(new BooleanClause(new AutomatonQueryOnBinaryDv(name(), wildcardPattern, automaton), Occur.MUST));\n-                return verifyingBuilder.build();\n+            return q;\n+        }\n+        \n+        protected void getNgramTokens(ArrayList<String> tokens, String fragment) {\n+            // Break fragment into multiple Ngrams\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            String lastUnusedToken = null;\n+            try {\n+                tokenizer.reset();\n+                boolean takeThis = true;\n+                // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n+                // `123` and `345` - no need to search for 234. We take every other ngram.", "originalCommit": "4b4d24ef17e503515581059ee79d418516ca2109", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjkxMDE3OA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412910178", "bodyText": "The original thinking was this might help keep them more position-based by having the middle term act as a bridge", "author": "markharwood", "createdAt": "2020-04-22T11:46:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2MzUwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjk1OTYxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412959615", "bodyText": "I see, thanks.", "author": "jimczi", "createdAt": "2020-04-22T12:59:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2MzUwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2NTk5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412865995", "bodyText": "I think that's ok for a first iteration but we should look at dividing the boolean query around the automaton's articulation points. That could optimize a lot the boolean query that we produce on complex automaton.", "author": "jimczi", "createdAt": "2020-04-22T10:32:04Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,187 +229,175 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n-                    case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n-                        break;\n-                    case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n \n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n-                        }\n-                        break;\n-                    case WildcardQuery.WILDCARD_ESCAPE:\n-                        // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n-                            length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n-                    default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n-                }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n-            }\n-\n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n-            }\n-\n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n-            }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n-            }\n-\n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+            \n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            \n+            String ngramIndexPattern = toLowerCase(wildcardPattern);\n+            // Parse the wildcard, marking ? and * chars with a single invalid char\n+            // to create a simple finite automaton we can parse\n+            List<Automaton> automata = new ArrayList<>();            \n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+              final int c = ngramIndexPattern.codePointAt(i);\n+              int length = Character.charCount(c);\n+              switch(c) {\n+                case WildcardQuery.WILDCARD_STRING: \n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_CHAR:\n+                  automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR));\n+                  break;\n+                case WildcardQuery.WILDCARD_ESCAPE:\n+                  // add the next codepoint instead, if it exists\n+                  if (i + length < wildcardPattern.length()) {\n+                    final int nextChar = wildcardPattern.codePointAt(i + length);\n+                    length += Character.charCount(nextChar);\n+                    automata.add(Automata.makeChar(nextChar));\n+                  } else {\n+                      automata.add(Automata.makeChar(c));                      \n+                  }\n+                  break;\n+                      \n+                default:\n+                  automata.add(Automata.makeChar(c));\n+              }\n+              i += length;\n             }\n+            \n+            Automaton approxAutomata = Operations.concatenate(automata);                \n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern);\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            failIfNotIndexed();\n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(toLowerCase(value), flags);\n+            Automaton ngramAutomaton = ngramRegex.toAutomaton(maxDeterminizedStates);\n \n-\n+            return createApproxAndVerifyQuery(ngramAutomaton, automaton, value);\n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n         }\n+        \n+        \n+        Query createApproxAndVerifyQuery(Automaton simplifiedAutomaton, Automaton verifyAutomaton, String expression) {\n \n+            Query ngramApproximation = createApproximationQueryFromAutomaton(simplifiedAutomaton);\n \n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), expression, verifyAutomaton);\n+            if (ngramApproximation != null && !(ngramApproximation instanceof MatchAllDocsQuery)) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(ngramApproximation, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            }\n+            return verifyingQuery;\n+        }\n+        \n+\n+        private Query createApproximationQueryFromAutomaton(Automaton simplifiedAutomaton) {\n+            FiniteStringsIterator iterator = new FiniteStringsIterator(simplifiedAutomaton);", "originalCommit": "4b4d24ef17e503515581059ee79d418516ca2109", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjkwOTE0OA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412909148", "bodyText": "Doesn't that take us back to Nik's code and exploring the transition points in the automaton to understand the graph? I'm not sure how FiniteStringsIterator works in conjunction with that sort of logic.", "author": "markharwood", "createdAt": "2020-04-22T11:44:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2NTk5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjk5ODYwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r412998609", "bodyText": "We have an example in Lucene in QueryBuilder#analyzeGraphBoolean. I don't think we should do that in the first iteration though so let's assume that we have to visit all the paths for now.\nHowever I am thinking of one thing that we could try to optimize in this pr, when we have a few transitions that have the same target. For instance [aA][bB][cD] that would create 8 paths, we could instead allow each path to contain a few unicode points per positions (2 maybe 3 max) and fork FiniteStringIterator to return optimized IntsRef[][] ?", "author": "jimczi", "createdAt": "2020-04-22T13:48:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2NTk5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzAzOTQwNg==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r413039406", "bodyText": "I'm not sure why we don't normalize case given:\n\ncase is likely to be the biggest cause of logic-branching (based on the existing rules we've seen from the security space)\nnormalising reduces our code complexity and increases the number of expressions we can hope to accelerate\nthe ngram search just has to be fast, not 100% correct. Normalising will:\n\ndramatically reduce the speed of searches (up to 8 x fewer unique terms)\nnot increase false positives massively for most cases (gut feel on this)", "author": "markharwood", "createdAt": "2020-04-22T14:36:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2NTk5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzA1OTM5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r413059391", "bodyText": "Sure but I still think we should consider this change as a follow up. This pr should ensure that we can handle all types of automaton correctly no matter if we lowercase or not.\nThen we can discuss how to handle case insensitive search but that's a different discussions imo.", "author": "jimczi", "createdAt": "2020-04-22T14:59:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2NTk5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzA5NTY1NA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r413095654", "bodyText": "Then we can discuss how to handle case insensitive search but that's a different discussions imo.\n\nThe idea behind normalisation in this context isn't about enabling case insensitivity for end users - it's about optimising the search performance and minimising the complexity of automatons.\nThere will inevitably be limits on the number of permutations we can consider and  working with a lower-cased vocabulary will help reduce the numbers.", "author": "markharwood", "createdAt": "2020-04-22T15:43:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2NTk5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzExMDU5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r413110593", "bodyText": "That's assuming that lots of query will contains [aA][bB] variations but this could be avoided if we provide a true case-insensitive search. Again, I am not saying we shouldn't do this \"optimization\" but to me this change would deserve a separate discussion where we'd also see think about exposing the case insensitive search on this field.", "author": "jimczi", "createdAt": "2020-04-22T16:01:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjg2NTk5NQ=="}], "type": "inlineReview"}, {"oid": "a325e78edeede0f07417b47f44fc23de8af51d14", "url": "https://github.com/elastic/elasticsearch/commit/a325e78edeede0f07417b47f44fc23de8af51d14", "message": "Added functional equivalence to keyword field for fuzzy matching. Will work on beefing up regex logic next.", "committedDate": "2020-04-27T10:50:32Z", "type": "forcePushed"}, {"oid": "52a908cd42f1d6494a39c3b20f56c2ee5d4b3eb6", "url": "https://github.com/elastic/elasticsearch/commit/52a908cd42f1d6494a39c3b20f56c2ee5d4b3eb6", "message": "Adds full equivalence for keyword field to the wildcard field. Regex, fuzzy, wildcard and prefix queries are all supported.\nAll queries use an approximation query backed by an automaton-based verification queries.\n\nCloses #54275", "committedDate": "2020-04-28T10:32:48Z", "type": "forcePushed"}, {"oid": "d161fdcec624318a593e25a3e68df499080ca88a", "url": "https://github.com/elastic/elasticsearch/commit/d161fdcec624318a593e25a3e68df499080ca88a", "message": "Unused import", "committedDate": "2020-04-28T22:30:24Z", "type": "forcePushed"}, {"oid": "be10d0fb7cde593f5905019af477519aefb68e00", "url": "https://github.com/elastic/elasticsearch/commit/be10d0fb7cde593f5905019af477519aefb68e00", "message": "Simplified BooleanQueries and made a* ngram queries prefix rather than wildcard.\nUpdated tests with simpler syntax and documented regexes that we\u2019d like to improve on, showing current suboptimal queries and the future form we\u2019d like to see.", "committedDate": "2020-04-29T16:27:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM3MjgzMA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419372830", "bodyText": "Can you use an exists query instead of adding a new query ?\nnew BooleanQuery.Builder()\n  .add(fieldType.existsQuery(), Occur.SHOULD)\n  .add(result)\n\n?", "author": "jimczi", "createdAt": "2020-05-04T11:35:15Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/ApproximateRegExp.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * dk.brics.automaton\n+ * \n+ * Copyright (c) 2001-2009 Anders Moeller\n+ * All rights reserved.\n+ * \n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions\n+ * are met:\n+ * 1. Redistributions of source code must retain the above copyright\n+ *    notice, this list of conditions and the following disclaimer.\n+ * 2. Redistributions in binary form must reproduce the above copyright\n+ *    notice, this list of conditions and the following disclaimer in the\n+ *    documentation and/or other materials provided with the distribution.\n+ * 3. The name of the author may not be used to endorse or promote products\n+ *    derived from this software without specific prior written permission.\n+ * \n+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\n+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\n+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+ */\n+\n+package org.elasticsearch.xpack.wildcard.mapper;\n+\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.MatchAllDocsQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.TermQuery;\n+import org.elasticsearch.xpack.wildcard.mapper.WildcardFieldMapper.WildcardFieldType;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * \n+ * This class is a fork of Lucene's RegExp class and is used to create a simplified\n+ * Query for use in accelerating searches to find those documents likely to match the regex. \n+ *\n+ */\n+public class ApproximateRegExp {\n+\n+    enum Kind {\n+        REGEXP_UNION,\n+        REGEXP_CONCATENATION,\n+        REGEXP_INTERSECTION,\n+        REGEXP_OPTIONAL,\n+        REGEXP_REPEAT,\n+        REGEXP_REPEAT_MIN,\n+        REGEXP_REPEAT_MINMAX,\n+        REGEXP_COMPLEMENT,\n+        REGEXP_CHAR,\n+        REGEXP_CHAR_RANGE,\n+        REGEXP_ANYCHAR,\n+        REGEXP_EMPTY,\n+        REGEXP_STRING,\n+        REGEXP_ANYSTRING,\n+        REGEXP_AUTOMATON,\n+        REGEXP_INTERVAL\n+    }\n+\n+    /**\n+     * Syntax flag, enables intersection (<code>&amp;</code>).\n+     */\n+    public static final int INTERSECTION = 0x0001;\n+\n+    /**\n+     * Syntax flag, enables complement (<code>~</code>).\n+     */\n+    public static final int COMPLEMENT = 0x0002;\n+\n+    /**\n+     * Syntax flag, enables empty language (<code>#</code>).\n+     */\n+    public static final int EMPTY = 0x0004;\n+\n+    /**\n+     * Syntax flag, enables anystring (<code>@</code>).\n+     */\n+    public static final int ANYSTRING = 0x0008;\n+\n+    /**\n+     * Syntax flag, enables named automata (<code>&lt;</code>identifier<code>&gt;</code>).\n+     */\n+    public static final int AUTOMATON = 0x0010;\n+\n+    /**\n+     * Syntax flag, enables numerical intervals (\n+     * <code>&lt;<i>n</i>-<i>m</i>&gt;</code>).\n+     */\n+    public static final int INTERVAL = 0x0020;\n+\n+    /**\n+     * Syntax flag, enables all optional regexp syntax.\n+     */\n+    public static final int ALL = 0xffff;\n+\n+    /**\n+     * Syntax flag, enables no optional regexp syntax.\n+     */\n+    public static final int NONE = 0x0000;\n+\n+    private final String originalString;\n+    Kind kind;\n+    ApproximateRegExp exp1, exp2;\n+    String s;\n+    int c;\n+    int min, max, digits;\n+    int from, to;\n+\n+    int flags;\n+    int pos;\n+    \n+    // The string content in between operators can be normalised using this hook\n+    public interface StringNormalizer {\n+        String normalize(String s);\n+    }\n+\n+    ApproximateRegExp() {\n+        this.originalString = null;\n+    }\n+\n+    /**\n+     * Constructs new <code>RegExp</code> from a string. Same as\n+     * <code>RegExp(s, ALL)</code>.\n+     * \n+     * @param s regexp string\n+     * @exception IllegalArgumentException if an error occurred while parsing the\n+     *              regular expression\n+     */\n+    public ApproximateRegExp(String s) throws IllegalArgumentException {\n+        this(s, ALL);\n+    }\n+\n+    /**\n+     * Constructs new <code>RegExp</code> from a string.\n+     * \n+     * @param s regexp string\n+     * @param syntax_flags boolean 'or' of optional syntax constructs to be\n+     *          enabled\n+     * @exception IllegalArgumentException if an error occurred while parsing the\n+     *              regular expression\n+     */\n+    public ApproximateRegExp(String s, int syntax_flags) throws IllegalArgumentException {\n+        originalString = s;\n+        flags = syntax_flags;\n+        ApproximateRegExp e;\n+        if (s.length() == 0) e = makeString(\"\");\n+        else {\n+            e = parseUnionExp();\n+            if (pos < originalString.length()) throw new IllegalArgumentException(\"end-of-string expected at position \" + pos);\n+        }\n+        kind = e.kind;\n+        exp1 = e.exp1;\n+        exp2 = e.exp2;\n+        this.s = e.s;\n+        c = e.c;\n+        min = e.min;\n+        max = e.max;\n+        digits = e.digits;\n+        from = e.from;\n+        to = e.to;\n+    }\n+\n+    // Convert a regular expression to a simplified query consisting of BooleanQuery and TermQuery objects\n+    // which captures as much of the logic as possible. Query can produce some false positives but shouldn't\n+    // produce any false negatives.\n+    // In addition to Term and BooleanQuery clauses there are MatchAllDocsQuery objects (e.g for .*) and\n+    // an equivalent MatchAllButRequireVerificationQuery. \n+    // *  If an expression resolves to a single MatchAllDocsQuery eg .* then a match all shortcut is possible with \n+    //    no verification needed.     \n+    // * If an expression resolves to a MatchAllButRequireVerificationQuery eg ?? then only the verification \n+    //   query is run.\n+    // * Anything else is a concrete query that should be run on the ngram index.\n+    public Query toApproximationQuery(StringNormalizer normalizer) throws IllegalArgumentException {\n+        Query result = null;\n+        switch (kind) {\n+            case REGEXP_UNION:\n+                result = createUnionQuery(normalizer);\n+                break;\n+            case REGEXP_CONCATENATION:\n+                result = createConcatenationQuery(normalizer);\n+                break;\n+            case REGEXP_STRING:\n+                String normalizedString = normalizer == null ? s : normalizer.normalize(s);\n+                result = new TermQuery(new Term(\"\", normalizedString));\n+                break;\n+            case REGEXP_CHAR:\n+                String cs =Character.toString(c);\n+                String normalizedChar = normalizer == null ? cs : normalizer.normalize(cs);\n+                result = new TermQuery(new Term(\"\", normalizedChar));\n+                break;\n+            case REGEXP_REPEAT:\n+                // Repeat is zero or more times so zero matches = match all\n+                result = new MatchAllDocsQuery();\n+                break;\n+                \n+            case REGEXP_REPEAT_MIN:\n+            case REGEXP_REPEAT_MINMAX:\n+                if (min > 0) {\n+                    result = exp1.toApproximationQuery(normalizer);\n+                    if(result instanceof TermQuery) {\n+                        // Wrap the repeating expression so that it is not concatenated by a parent which concatenates\n+                        // plain TermQuery objects together. Boolean queries are interpreted as a black box and not\n+                        // concatenated.\n+                        BooleanQuery.Builder wrapper = new BooleanQuery.Builder();\n+                        wrapper.add(result, Occur.MUST);\n+                        result = wrapper.build();\n+                    }\n+                } else {\n+                    // TODO match all was a nice assumption here for optimising .* but breaks \n+                    // for (a){0,3} which isn't a logical match all but empty string or up to 3 a's. \n+//                    result = new MatchAllDocsQuery();\n+                    result = new MatchAllButRequireVerificationQuery();", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ3NDYxNA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419474614", "bodyText": "I thought this class made the intention much clearer. Without it we'd have to interpret any FieldExistsQuery to mean \"DO run a verification\" query  (e.g. for regex ..) and for a MatchAllDocsQuery to mean don't run a verification query (e.g. for regex .*).\nThere are certain queries like .* where we know we can satisfy all criteria using the ngram index only.  However I thought today that we could extend this support by making MatchAllButRequireVerificationQuery a base class or marker interface. The regex a for example could also be satisfied using the ngram index only so returning a form of term query which tested for true in instanceof MatchAllButRequireVerificationQuery would help the WildcardField avoid running pointless verification for more than one regex type.", "author": "markharwood", "createdAt": "2020-05-04T14:23:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM3MjgzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ4NjU1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419486557", "bodyText": "I see but is it still the case if the ngram index is lowercased ? The search is case-sensitive so unless we differentiate characters eligible for case folding, we have to check every match ?", "author": "jimczi", "createdAt": "2020-05-04T14:38:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM3MjgzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTU1MzczOA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419553738", "bodyText": "We can see if the search value changes after normalisation and only optimise the query if there's no change. Perhaps a rare example of a user regex but illustrates the need to communicate where we think we can handle queries with the ngram index only.\nAnother example might be if we later index field lengths and can accelerate things like ??", "author": "markharwood", "createdAt": "2020-05-04T16:12:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM3MjgzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzAwMjU4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r423002589", "bodyText": "We can see if the search value changes after normalisation and only optimise the query if there's no change.\nPerhaps a rare example of a user regex but illustrates the need to communicate where we think we can handle queries with the ngram index only.\n\nI don't think this works since values in documents would be lowercased too so you'd need a verification for any character that can upper and lower cased ? We can think about optimizations in the future but as you said in previous comments we should aim for correctness first.", "author": "jimczi", "createdAt": "2020-05-11T12:27:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM3MjgzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM4MjAzMg==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419382032", "bodyText": "I think we should ignore short sequence, or at least require two characters ?", "author": "jimczi", "createdAt": "2020-05-04T11:55:18Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,218 +231,383 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n+\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+\n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+\n+            String ngramIndexPattern = addLineEndChars(toLowerCase(wildcardPattern));\n+\n+            // Break search term into tokens\n+            Set<String> tokens = new LinkedHashSet<>();\n+            StringBuilder sequence = new StringBuilder();\n+            int numWildcardChars = 0;\n+            int numWildcardStrings = 0;\n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+                final int c = ngramIndexPattern.codePointAt(i);\n+                int length = Character.charCount(c);\n+                switch (c) {\n                     case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n+                        numWildcardStrings++;\n                         break;\n                     case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n-\n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n+                        numWildcardChars++;\n                         break;\n                     case WildcardQuery.WILDCARD_ESCAPE:\n                         // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n+                        if (i + length < ngramIndexPattern.length()) {\n+                            final int nextChar = ngramIndexPattern.codePointAt(i + length);\n                             length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n+                            sequence.append(Character.toChars(nextChar));\n+                        } else {\n+                            sequence.append(Character.toChars(c));\n+                        }\n+                        break;\n+\n                     default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n+                        sequence.append(Character.toChars(c));\n                 }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n+                i += length;\n             }\n \n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n+            if (sequence.length() > 0) {\n+                getNgramTokens(tokens, sequence.toString());\n             }\n \n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n+            BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+            int clauseCount = 0;\n+            for (String string : tokens) {\n+                if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                    break;\n+                }\n+                addClause(string, rewritten, Occur.MUST);\n+                clauseCount++;\n             }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), wildcardPattern, dvAutomaton);\n+            if (clauseCount > 0) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery approxQuery = rewritten.build();\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            } else if (numWildcardChars == 0 || numWildcardStrings > 0) {\n+                // We have no concrete characters and we're not a pure length query e.g. ??? \n+                return new DocValuesFieldExistsQuery(name());\n             }\n+            return verifyingQuery;\n \n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (value.length() == 0) {\n+                return new MatchNoDocsQuery();\n             }\n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            \n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(addLineEndChars(toLowerCase(value)), flags);\n \n-\n-        }\n-\n-\n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            Query approxBooleanQuery = ngramRegex.toApproximationQuery(new ApproximateRegExp.StringNormalizer() {\n+                @Override\n+                public String normalize(String token) {\n+                    return toLowerCase(token);\n                 }\n-\n-                // Add any start/end of string character\n-                if (i == 0 && patternStructure.openStart == false) {\n-                    // Start-of-string anchored (is not a leading wildcard)\n-                    fragment = TOKEN_START_OR_END_CHAR + fragment;\n+            });\n+            Query approxNgramQuery = rewriteBoolToNgramQuery(approxBooleanQuery);\n+            \n+            // MatchAll is a special case meaning the regex is known to match everything .* and \n+            // there is no need for verification.\n+            if (approxNgramQuery instanceof MatchAllDocsQuery) {\n+                return new DocValuesFieldExistsQuery(name());\n+            }\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), value, automaton);\n+            \n+            // MatchAllButRequireVerificationQuery is a special case meaning the regex is reduced to a single\n+            // clause which we can't accelerate at all and needs verification. Example would be \"..\" \n+            if (approxNgramQuery instanceof MatchAllButRequireVerificationQuery) {\n+                return verifyingQuery;\n+            }\n+            \n+            if (approxNgramQuery != null) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxNgramQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            }\n+            return verifyingQuery;                        \n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n+        }\n+        \n+        // Takes a BooleanQuery + TermQuery tree representing query logic and rewrites using ngrams of appropriate size.\n+        private Query rewriteBoolToNgramQuery(Query approxQuery) {\n+            //TODO optimise more intelligently so we: \n+            // 1) favour full-length term queries eg abc over short eg a* when pruning too many clauses.\n+            // 2) make MAX_CLAUSES_IN_APPROXIMATION_QUERY a global cap rather than per-boolean clause.\n+            if (approxQuery == null) {\n+                return null;\n+            }\n+            if (approxQuery instanceof BooleanQuery) {\n+                BooleanQuery bq = (BooleanQuery) approxQuery;\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                int clauseCount = 0;\n+                for (BooleanClause clause : bq) {\n+                    Query q = rewriteBoolToNgramQuery(clause.getQuery());\n+                    if (q != null) {\n+                        if (clause.getOccur().equals(Occur.MUST)) {\n+                            // Can't drop \"should\" clauses because it can elevate a sibling optional item\n+                            // to mandatory (shoulds with 1 clause) causing false negatives\n+                            // Dropping MUSTs increase false positives which are OK because are verified anyway.\n+                            clauseCount++;\n+                            if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                                break;\n+                            }\n+                        }\n+                        rewritten.add(q, clause.getOccur());\n+                    }\n                 }\n-                if (patternStructure.openEnd == false && i == patternStructure.fragments.length - 1) {\n-                    // End-of-string anchored (is not a trailing wildcard)\n-                    fragment = fragment + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR;\n+                return simplify(rewritten.build());\n+            }\n+            if (approxQuery instanceof TermQuery) {\n+                TermQuery tq = (TermQuery) approxQuery;\n+                // Break term into tokens\n+                Set<String> tokens = new LinkedHashSet<>();\n+                getNgramTokens(tokens, tq.getTerm().text());\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (String string : tokens) {\n+                    addClause(string, rewritten, Occur.MUST);\n                 }\n-                if (fragment.codePointCount(0, fragment.length()) <= NGRAM_SIZE) {\n-                    tokens.add(fragment);\n+                return simplify(rewritten.build());\n+            }\n+            if (isMatchAll(approxQuery)) {\n+                return approxQuery;\n+            }\n+            throw new IllegalStateException(\"Invalid query type found parsing regex query:\" + approxQuery);\n+        }    \n+        \n+        static Query simplify(Query input) {\n+            if (input instanceof BooleanQuery == false) {\n+                return input;\n+            }\n+            BooleanQuery result = (BooleanQuery) input;\n+            if (result.clauses().size() == 0) {\n+                // A \".*\" clause can produce zero clauses in which case we return MatchAll\n+                return new MatchAllDocsQuery();\n+            }\n+            if (result.clauses().size() == 1) {\n+                return simplify(result.clauses().get(0).getQuery());\n+            }\n+\n+            // We may have a mix of MatchAll and concrete queries - assess if we can simplify\n+            int matchAllCount = 0;\n+            int verifyCount = 0;\n+            boolean allConcretesAreOptional = true;\n+            for (BooleanClause booleanClause : result.clauses()) {\n+                Query q = booleanClause.getQuery();\n+                if (q instanceof MatchAllDocsQuery) {\n+                    matchAllCount++;\n+                } else if (q instanceof MatchAllButRequireVerificationQuery) {\n+                    verifyCount++;\n                 } else {\n-                    // Break fragment into multiple Ngrams\n-                    TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n-                    CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n-                    String lastUnusedToken = null;\n-                    try {\n-                        tokenizer.reset();\n-                        boolean takeThis = true;\n-                        // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n-                        // `123` and `345` - no need to search for 234. We take every other ngram.\n-                        while (tokenizer.incrementToken()) {\n-                            String tokenValue = termAtt.toString();\n-                            if (takeThis) {\n-                                tokens.add(tokenValue);\n-                            } else {\n-                                lastUnusedToken = tokenValue;\n-                            }\n-                            // alternate\n-                            takeThis = !takeThis;\n-                        }\n-                        if (lastUnusedToken != null) {\n-                            // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n-                            // `ake` to complete the logic.\n-                            tokens.add(lastUnusedToken);\n-                        }\n-                        tokenizer.end();\n-                        tokenizer.close();\n-                    } catch (IOException ioe) {\n-                        throw new ElasticsearchParseException(\"Error parsing wildcard query pattern fragment [\" + fragment + \"]\");\n+                    // Concrete query\n+                    if (booleanClause.getOccur() != Occur.SHOULD) {\n+                        allConcretesAreOptional = false;\n                     }\n                 }\n             }\n \n-            if (patternStructure.isMatchAll()) {\n+            if ((allConcretesAreOptional && matchAllCount > 0)) {\n+                // Any match all expression takes precedence over all optional concrete queries.\n                 return new MatchAllDocsQuery();\n             }\n-            BooleanQuery approximation = createApproximationQuery(tokens);\n-            if (approximation.clauses().size() > 1 || patternStructure.needsVerification()) {\n-                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n-                verifyingBuilder.add(new BooleanClause(approximation, Occur.MUST));\n-                Automaton automaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n-                verifyingBuilder.add(new BooleanClause(new AutomatonQueryOnBinaryDv(name(), wildcardPattern, automaton), Occur.MUST));\n-                return verifyingBuilder.build();\n+\n+            if ((allConcretesAreOptional && verifyCount > 0)) {\n+                // Any match all expression that needs verification takes precedence over all optional concrete queries.\n+                return new MatchAllButRequireVerificationQuery();\n             }\n-            return approximation;\n-        }\n \n-        private BooleanQuery createApproximationQuery(ArrayList<String> tokens) {\n-            BooleanQuery.Builder bqBuilder = new BooleanQuery.Builder();\n-            if (tokens.size() <= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n-                for (String token : tokens) {\n-                    addClause(token, bqBuilder);\n+            // We have some mandatory concrete queries - strip out the superfluous match all expressions\n+            if (allConcretesAreOptional == false && matchAllCount + verifyCount > 0) {\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (BooleanClause booleanClause : result.clauses()) {\n+                    if (isMatchAll(booleanClause.getQuery()) == false) {\n+                        rewritten.add(booleanClause);\n+                    }\n                 }\n-                return bqBuilder.build();\n-            }\n-            // Thin out the number of clauses using a selection spread evenly across the range\n-            float step = (float) (tokens.size() - 1) / (float) (MAX_CLAUSES_IN_APPROXIMATION_QUERY - 1); // set step size\n-            for (int i = 0; i < MAX_CLAUSES_IN_APPROXIMATION_QUERY; i++) {\n-                addClause(tokens.get(Math.round(step * i)), bqBuilder); // add each element of a position which is a multiple of step\n+                return simplify(rewritten.build());\n             }\n-            // TODO we can be smarter about pruning here. e.g.\n-            // * Avoid wildcard queries if there are sufficient numbers of other terms that are full 3grams that are cheaper term queries\n-            // * We can select terms on their scarcity rather than even spreads across the search string.\n+            return result;\n+        }\n+        \n+        \n+        static boolean isMatchAll(Query q) {\n+            return q instanceof MatchAllDocsQuery || q instanceof MatchAllButRequireVerificationQuery;\n+        }\n \n-            return bqBuilder.build();\n+        protected void getNgramTokens(Set<String> tokens, String fragment) {\n+            if (fragment.equals(TOKEN_START_STRING) || fragment.equals(TOKEN_END_STRING)) {\n+                // If a regex is a form of match-all e.g. \".*\" we only produce the token start/end markers as search\n+                // terms which can be ignored.\n+                return;\n+            }\n+            // Break fragment into multiple Ngrams\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            // If fragment length < NGRAM_SIZE then it is not emitted by token stream so need\n+            // to initialise with the value here\n+            String lastUnusedToken = fragment;\n+            try {\n+                tokenizer.reset();\n+                boolean takeThis = true;\n+                // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n+                // `123` and `345` - no need to search for 234. We take every other ngram.\n+                while (tokenizer.incrementToken()) {\n+                    String tokenValue = termAtt.toString();\n+                    if (takeThis) {\n+                        tokens.add(tokenValue);\n+                    } else {\n+                        lastUnusedToken = tokenValue;\n+                    }\n+                    // alternate\n+                    takeThis = !takeThis;\n+                    if (tokens.size() >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                        lastUnusedToken = null;\n+                        break;\n+                    }\n+                }\n+                if (lastUnusedToken != null) {\n+                    // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n+                    // `ake` to complete the logic.\n+                    tokens.add(lastUnusedToken);\n+                }\n+                tokenizer.end();\n+                tokenizer.close();\n+            } catch (IOException ioe) {\n+                throw new ElasticsearchParseException(\"Error parsing wildcard regex pattern fragment [\" + fragment + \"]\");\n+            }\n         }\n \n-        private void addClause(String token, BooleanQuery.Builder bqBuilder) {\n+        private void addClause(String token, BooleanQuery.Builder bqBuilder, Occur occur) {\n             assert token.codePointCount(0, token.length()) <= NGRAM_SIZE;\n             if (token.codePointCount(0, token.length()) == NGRAM_SIZE) {\n                 TermQuery tq = new TermQuery(new Term(name(), token));\n-                bqBuilder.add(new BooleanClause(tq, Occur.MUST));\n+                bqBuilder.add(new BooleanClause(tq, occur));\n             } else {\n-                WildcardQuery wq = new WildcardQuery(new Term(name(), token + \"*\"));\n-                wq.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_REWRITE);\n-                bqBuilder.add(new BooleanClause(wq, Occur.MUST));\n+                // Ignore tokens that are just string start or end markers", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM4NDQ5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419384491", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            String cs =Character.toString(c);\n          \n          \n            \n                            String cs = Character.toString(c);", "author": "jimczi", "createdAt": "2020-05-04T12:00:24Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/ApproximateRegExp.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * dk.brics.automaton\n+ * \n+ * Copyright (c) 2001-2009 Anders Moeller\n+ * All rights reserved.\n+ * \n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions\n+ * are met:\n+ * 1. Redistributions of source code must retain the above copyright\n+ *    notice, this list of conditions and the following disclaimer.\n+ * 2. Redistributions in binary form must reproduce the above copyright\n+ *    notice, this list of conditions and the following disclaimer in the\n+ *    documentation and/or other materials provided with the distribution.\n+ * 3. The name of the author may not be used to endorse or promote products\n+ *    derived from this software without specific prior written permission.\n+ * \n+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\n+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\n+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+ */\n+\n+package org.elasticsearch.xpack.wildcard.mapper;\n+\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.MatchAllDocsQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.TermQuery;\n+import org.elasticsearch.xpack.wildcard.mapper.WildcardFieldMapper.WildcardFieldType;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * \n+ * This class is a fork of Lucene's RegExp class and is used to create a simplified\n+ * Query for use in accelerating searches to find those documents likely to match the regex. \n+ *\n+ */\n+public class ApproximateRegExp {\n+\n+    enum Kind {\n+        REGEXP_UNION,\n+        REGEXP_CONCATENATION,\n+        REGEXP_INTERSECTION,\n+        REGEXP_OPTIONAL,\n+        REGEXP_REPEAT,\n+        REGEXP_REPEAT_MIN,\n+        REGEXP_REPEAT_MINMAX,\n+        REGEXP_COMPLEMENT,\n+        REGEXP_CHAR,\n+        REGEXP_CHAR_RANGE,\n+        REGEXP_ANYCHAR,\n+        REGEXP_EMPTY,\n+        REGEXP_STRING,\n+        REGEXP_ANYSTRING,\n+        REGEXP_AUTOMATON,\n+        REGEXP_INTERVAL\n+    }\n+\n+    /**\n+     * Syntax flag, enables intersection (<code>&amp;</code>).\n+     */\n+    public static final int INTERSECTION = 0x0001;\n+\n+    /**\n+     * Syntax flag, enables complement (<code>~</code>).\n+     */\n+    public static final int COMPLEMENT = 0x0002;\n+\n+    /**\n+     * Syntax flag, enables empty language (<code>#</code>).\n+     */\n+    public static final int EMPTY = 0x0004;\n+\n+    /**\n+     * Syntax flag, enables anystring (<code>@</code>).\n+     */\n+    public static final int ANYSTRING = 0x0008;\n+\n+    /**\n+     * Syntax flag, enables named automata (<code>&lt;</code>identifier<code>&gt;</code>).\n+     */\n+    public static final int AUTOMATON = 0x0010;\n+\n+    /**\n+     * Syntax flag, enables numerical intervals (\n+     * <code>&lt;<i>n</i>-<i>m</i>&gt;</code>).\n+     */\n+    public static final int INTERVAL = 0x0020;\n+\n+    /**\n+     * Syntax flag, enables all optional regexp syntax.\n+     */\n+    public static final int ALL = 0xffff;\n+\n+    /**\n+     * Syntax flag, enables no optional regexp syntax.\n+     */\n+    public static final int NONE = 0x0000;\n+\n+    private final String originalString;\n+    Kind kind;\n+    ApproximateRegExp exp1, exp2;\n+    String s;\n+    int c;\n+    int min, max, digits;\n+    int from, to;\n+\n+    int flags;\n+    int pos;\n+    \n+    // The string content in between operators can be normalised using this hook\n+    public interface StringNormalizer {\n+        String normalize(String s);\n+    }\n+\n+    ApproximateRegExp() {\n+        this.originalString = null;\n+    }\n+\n+    /**\n+     * Constructs new <code>RegExp</code> from a string. Same as\n+     * <code>RegExp(s, ALL)</code>.\n+     * \n+     * @param s regexp string\n+     * @exception IllegalArgumentException if an error occurred while parsing the\n+     *              regular expression\n+     */\n+    public ApproximateRegExp(String s) throws IllegalArgumentException {\n+        this(s, ALL);\n+    }\n+\n+    /**\n+     * Constructs new <code>RegExp</code> from a string.\n+     * \n+     * @param s regexp string\n+     * @param syntax_flags boolean 'or' of optional syntax constructs to be\n+     *          enabled\n+     * @exception IllegalArgumentException if an error occurred while parsing the\n+     *              regular expression\n+     */\n+    public ApproximateRegExp(String s, int syntax_flags) throws IllegalArgumentException {\n+        originalString = s;\n+        flags = syntax_flags;\n+        ApproximateRegExp e;\n+        if (s.length() == 0) e = makeString(\"\");\n+        else {\n+            e = parseUnionExp();\n+            if (pos < originalString.length()) throw new IllegalArgumentException(\"end-of-string expected at position \" + pos);\n+        }\n+        kind = e.kind;\n+        exp1 = e.exp1;\n+        exp2 = e.exp2;\n+        this.s = e.s;\n+        c = e.c;\n+        min = e.min;\n+        max = e.max;\n+        digits = e.digits;\n+        from = e.from;\n+        to = e.to;\n+    }\n+\n+    // Convert a regular expression to a simplified query consisting of BooleanQuery and TermQuery objects\n+    // which captures as much of the logic as possible. Query can produce some false positives but shouldn't\n+    // produce any false negatives.\n+    // In addition to Term and BooleanQuery clauses there are MatchAllDocsQuery objects (e.g for .*) and\n+    // an equivalent MatchAllButRequireVerificationQuery. \n+    // *  If an expression resolves to a single MatchAllDocsQuery eg .* then a match all shortcut is possible with \n+    //    no verification needed.     \n+    // * If an expression resolves to a MatchAllButRequireVerificationQuery eg ?? then only the verification \n+    //   query is run.\n+    // * Anything else is a concrete query that should be run on the ngram index.\n+    public Query toApproximationQuery(StringNormalizer normalizer) throws IllegalArgumentException {\n+        Query result = null;\n+        switch (kind) {\n+            case REGEXP_UNION:\n+                result = createUnionQuery(normalizer);\n+                break;\n+            case REGEXP_CONCATENATION:\n+                result = createConcatenationQuery(normalizer);\n+                break;\n+            case REGEXP_STRING:\n+                String normalizedString = normalizer == null ? s : normalizer.normalize(s);\n+                result = new TermQuery(new Term(\"\", normalizedString));\n+                break;\n+            case REGEXP_CHAR:\n+                String cs =Character.toString(c);", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM4NjMyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419386325", "bodyText": "Can we use a marker rather than a full Lucene query ?", "author": "jimczi", "createdAt": "2020-05-04T12:04:18Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/ApproximateRegExp.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * dk.brics.automaton\n+ * \n+ * Copyright (c) 2001-2009 Anders Moeller\n+ * All rights reserved.\n+ * \n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions\n+ * are met:\n+ * 1. Redistributions of source code must retain the above copyright\n+ *    notice, this list of conditions and the following disclaimer.\n+ * 2. Redistributions in binary form must reproduce the above copyright\n+ *    notice, this list of conditions and the following disclaimer in the\n+ *    documentation and/or other materials provided with the distribution.\n+ * 3. The name of the author may not be used to endorse or promote products\n+ *    derived from this software without specific prior written permission.\n+ * \n+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\n+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\n+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+ */\n+\n+package org.elasticsearch.xpack.wildcard.mapper;\n+\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.MatchAllDocsQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.TermQuery;\n+import org.elasticsearch.xpack.wildcard.mapper.WildcardFieldMapper.WildcardFieldType;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * \n+ * This class is a fork of Lucene's RegExp class and is used to create a simplified\n+ * Query for use in accelerating searches to find those documents likely to match the regex. \n+ *\n+ */\n+public class ApproximateRegExp {\n+\n+    enum Kind {\n+        REGEXP_UNION,\n+        REGEXP_CONCATENATION,\n+        REGEXP_INTERSECTION,\n+        REGEXP_OPTIONAL,\n+        REGEXP_REPEAT,\n+        REGEXP_REPEAT_MIN,\n+        REGEXP_REPEAT_MINMAX,\n+        REGEXP_COMPLEMENT,\n+        REGEXP_CHAR,\n+        REGEXP_CHAR_RANGE,\n+        REGEXP_ANYCHAR,\n+        REGEXP_EMPTY,\n+        REGEXP_STRING,\n+        REGEXP_ANYSTRING,\n+        REGEXP_AUTOMATON,\n+        REGEXP_INTERVAL\n+    }\n+\n+    /**\n+     * Syntax flag, enables intersection (<code>&amp;</code>).\n+     */\n+    public static final int INTERSECTION = 0x0001;\n+\n+    /**\n+     * Syntax flag, enables complement (<code>~</code>).\n+     */\n+    public static final int COMPLEMENT = 0x0002;\n+\n+    /**\n+     * Syntax flag, enables empty language (<code>#</code>).\n+     */\n+    public static final int EMPTY = 0x0004;\n+\n+    /**\n+     * Syntax flag, enables anystring (<code>@</code>).\n+     */\n+    public static final int ANYSTRING = 0x0008;\n+\n+    /**\n+     * Syntax flag, enables named automata (<code>&lt;</code>identifier<code>&gt;</code>).\n+     */\n+    public static final int AUTOMATON = 0x0010;\n+\n+    /**\n+     * Syntax flag, enables numerical intervals (\n+     * <code>&lt;<i>n</i>-<i>m</i>&gt;</code>).\n+     */\n+    public static final int INTERVAL = 0x0020;\n+\n+    /**\n+     * Syntax flag, enables all optional regexp syntax.\n+     */\n+    public static final int ALL = 0xffff;\n+\n+    /**\n+     * Syntax flag, enables no optional regexp syntax.\n+     */\n+    public static final int NONE = 0x0000;\n+\n+    private final String originalString;\n+    Kind kind;\n+    ApproximateRegExp exp1, exp2;\n+    String s;\n+    int c;\n+    int min, max, digits;\n+    int from, to;\n+\n+    int flags;\n+    int pos;\n+    \n+    // The string content in between operators can be normalised using this hook\n+    public interface StringNormalizer {\n+        String normalize(String s);\n+    }\n+\n+    ApproximateRegExp() {\n+        this.originalString = null;\n+    }\n+\n+    /**\n+     * Constructs new <code>RegExp</code> from a string. Same as\n+     * <code>RegExp(s, ALL)</code>.\n+     * \n+     * @param s regexp string\n+     * @exception IllegalArgumentException if an error occurred while parsing the\n+     *              regular expression\n+     */\n+    public ApproximateRegExp(String s) throws IllegalArgumentException {\n+        this(s, ALL);\n+    }\n+\n+    /**\n+     * Constructs new <code>RegExp</code> from a string.\n+     * \n+     * @param s regexp string\n+     * @param syntax_flags boolean 'or' of optional syntax constructs to be\n+     *          enabled\n+     * @exception IllegalArgumentException if an error occurred while parsing the\n+     *              regular expression\n+     */\n+    public ApproximateRegExp(String s, int syntax_flags) throws IllegalArgumentException {\n+        originalString = s;\n+        flags = syntax_flags;\n+        ApproximateRegExp e;\n+        if (s.length() == 0) e = makeString(\"\");\n+        else {\n+            e = parseUnionExp();\n+            if (pos < originalString.length()) throw new IllegalArgumentException(\"end-of-string expected at position \" + pos);\n+        }\n+        kind = e.kind;\n+        exp1 = e.exp1;\n+        exp2 = e.exp2;\n+        this.s = e.s;\n+        c = e.c;\n+        min = e.min;\n+        max = e.max;\n+        digits = e.digits;\n+        from = e.from;\n+        to = e.to;\n+    }\n+\n+    // Convert a regular expression to a simplified query consisting of BooleanQuery and TermQuery objects\n+    // which captures as much of the logic as possible. Query can produce some false positives but shouldn't\n+    // produce any false negatives.\n+    // In addition to Term and BooleanQuery clauses there are MatchAllDocsQuery objects (e.g for .*) and\n+    // an equivalent MatchAllButRequireVerificationQuery. \n+    // *  If an expression resolves to a single MatchAllDocsQuery eg .* then a match all shortcut is possible with \n+    //    no verification needed.     \n+    // * If an expression resolves to a MatchAllButRequireVerificationQuery eg ?? then only the verification \n+    //   query is run.\n+    // * Anything else is a concrete query that should be run on the ngram index.\n+    public Query toApproximationQuery(StringNormalizer normalizer) throws IllegalArgumentException {\n+        Query result = null;\n+        switch (kind) {\n+            case REGEXP_UNION:\n+                result = createUnionQuery(normalizer);\n+                break;\n+            case REGEXP_CONCATENATION:\n+                result = createConcatenationQuery(normalizer);\n+                break;\n+            case REGEXP_STRING:\n+                String normalizedString = normalizer == null ? s : normalizer.normalize(s);\n+                result = new TermQuery(new Term(\"\", normalizedString));\n+                break;\n+            case REGEXP_CHAR:\n+                String cs =Character.toString(c);\n+                String normalizedChar = normalizer == null ? cs : normalizer.normalize(cs);\n+                result = new TermQuery(new Term(\"\", normalizedChar));\n+                break;\n+            case REGEXP_REPEAT:\n+                // Repeat is zero or more times so zero matches = match all\n+                result = new MatchAllDocsQuery();\n+                break;\n+                \n+            case REGEXP_REPEAT_MIN:\n+            case REGEXP_REPEAT_MINMAX:\n+                if (min > 0) {\n+                    result = exp1.toApproximationQuery(normalizer);\n+                    if(result instanceof TermQuery) {\n+                        // Wrap the repeating expression so that it is not concatenated by a parent which concatenates\n+                        // plain TermQuery objects together. Boolean queries are interpreted as a black box and not\n+                        // concatenated.\n+                        BooleanQuery.Builder wrapper = new BooleanQuery.Builder();\n+                        wrapper.add(result, Occur.MUST);\n+                        result = wrapper.build();\n+                    }\n+                } else {\n+                    // TODO match all was a nice assumption here for optimising .* but breaks \n+                    // for (a){0,3} which isn't a logical match all but empty string or up to 3 a's. \n+//                    result = new MatchAllDocsQuery();\n+                    result = new MatchAllButRequireVerificationQuery();\n+                }\n+                break;\n+            case REGEXP_ANYSTRING:\n+                // optimisation for .* queries - match all and no verification stage required.\n+                result = new MatchAllDocsQuery();\n+                break;\n+            // All other kinds of expression cannot be represented as a boolean or term query so return an object \n+            // that indicates verification is required\n+            case REGEXP_OPTIONAL:\n+            case REGEXP_INTERSECTION:\n+            case REGEXP_COMPLEMENT:\n+            case REGEXP_CHAR_RANGE:\n+            case REGEXP_ANYCHAR:\n+            case REGEXP_INTERVAL:\n+            case REGEXP_EMPTY: \n+            case REGEXP_AUTOMATON:\n+                result = new MatchAllButRequireVerificationQuery();", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ4NTIzOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419485239", "bodyText": "See my earlier comment re introducing a base class or marker interface for denoting query clauses that require no verification step. Thoughts?", "author": "markharwood", "createdAt": "2020-05-04T14:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM4NjMyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM4NzQwMg==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419387402", "bodyText": "I don't know if it's a bad thing or not but creating boolean queries upfront removes the simplification of the automaton.", "author": "jimczi", "createdAt": "2020-05-04T12:06:41Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/ApproximateRegExp.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * dk.brics.automaton\n+ * \n+ * Copyright (c) 2001-2009 Anders Moeller\n+ * All rights reserved.\n+ * \n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions\n+ * are met:\n+ * 1. Redistributions of source code must retain the above copyright\n+ *    notice, this list of conditions and the following disclaimer.\n+ * 2. Redistributions in binary form must reproduce the above copyright\n+ *    notice, this list of conditions and the following disclaimer in the\n+ *    documentation and/or other materials provided with the distribution.\n+ * 3. The name of the author may not be used to endorse or promote products\n+ *    derived from this software without specific prior written permission.\n+ * \n+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\n+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\n+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+ */\n+\n+package org.elasticsearch.xpack.wildcard.mapper;\n+\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.MatchAllDocsQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.TermQuery;\n+import org.elasticsearch.xpack.wildcard.mapper.WildcardFieldMapper.WildcardFieldType;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * \n+ * This class is a fork of Lucene's RegExp class and is used to create a simplified\n+ * Query for use in accelerating searches to find those documents likely to match the regex. \n+ *\n+ */", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM5MTY0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419391647", "bodyText": "When are we expecting null queries ? Should we also ignore MatchAllQuery since it's a valid query to return from rewriteBoolToNgramQuery ?", "author": "jimczi", "createdAt": "2020-05-04T12:14:52Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,218 +231,383 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n+\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+\n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+\n+            String ngramIndexPattern = addLineEndChars(toLowerCase(wildcardPattern));\n+\n+            // Break search term into tokens\n+            Set<String> tokens = new LinkedHashSet<>();\n+            StringBuilder sequence = new StringBuilder();\n+            int numWildcardChars = 0;\n+            int numWildcardStrings = 0;\n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+                final int c = ngramIndexPattern.codePointAt(i);\n+                int length = Character.charCount(c);\n+                switch (c) {\n                     case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n+                        numWildcardStrings++;\n                         break;\n                     case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n-\n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n+                        numWildcardChars++;\n                         break;\n                     case WildcardQuery.WILDCARD_ESCAPE:\n                         // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n+                        if (i + length < ngramIndexPattern.length()) {\n+                            final int nextChar = ngramIndexPattern.codePointAt(i + length);\n                             length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n+                            sequence.append(Character.toChars(nextChar));\n+                        } else {\n+                            sequence.append(Character.toChars(c));\n+                        }\n+                        break;\n+\n                     default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n+                        sequence.append(Character.toChars(c));\n                 }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n+                i += length;\n             }\n \n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n+            if (sequence.length() > 0) {\n+                getNgramTokens(tokens, sequence.toString());\n             }\n \n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n+            BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+            int clauseCount = 0;\n+            for (String string : tokens) {\n+                if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                    break;\n+                }\n+                addClause(string, rewritten, Occur.MUST);\n+                clauseCount++;\n             }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), wildcardPattern, dvAutomaton);\n+            if (clauseCount > 0) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery approxQuery = rewritten.build();\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            } else if (numWildcardChars == 0 || numWildcardStrings > 0) {\n+                // We have no concrete characters and we're not a pure length query e.g. ??? \n+                return new DocValuesFieldExistsQuery(name());\n             }\n+            return verifyingQuery;\n \n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (value.length() == 0) {\n+                return new MatchNoDocsQuery();\n             }\n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            \n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(addLineEndChars(toLowerCase(value)), flags);\n \n-\n-        }\n-\n-\n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            Query approxBooleanQuery = ngramRegex.toApproximationQuery(new ApproximateRegExp.StringNormalizer() {\n+                @Override\n+                public String normalize(String token) {\n+                    return toLowerCase(token);\n                 }\n-\n-                // Add any start/end of string character\n-                if (i == 0 && patternStructure.openStart == false) {\n-                    // Start-of-string anchored (is not a leading wildcard)\n-                    fragment = TOKEN_START_OR_END_CHAR + fragment;\n+            });\n+            Query approxNgramQuery = rewriteBoolToNgramQuery(approxBooleanQuery);\n+            \n+            // MatchAll is a special case meaning the regex is known to match everything .* and \n+            // there is no need for verification.\n+            if (approxNgramQuery instanceof MatchAllDocsQuery) {\n+                return new DocValuesFieldExistsQuery(name());\n+            }\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), value, automaton);\n+            \n+            // MatchAllButRequireVerificationQuery is a special case meaning the regex is reduced to a single\n+            // clause which we can't accelerate at all and needs verification. Example would be \"..\" \n+            if (approxNgramQuery instanceof MatchAllButRequireVerificationQuery) {\n+                return verifyingQuery;\n+            }\n+            \n+            if (approxNgramQuery != null) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxNgramQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            }\n+            return verifyingQuery;                        \n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n+        }\n+        \n+        // Takes a BooleanQuery + TermQuery tree representing query logic and rewrites using ngrams of appropriate size.\n+        private Query rewriteBoolToNgramQuery(Query approxQuery) {\n+            //TODO optimise more intelligently so we: \n+            // 1) favour full-length term queries eg abc over short eg a* when pruning too many clauses.\n+            // 2) make MAX_CLAUSES_IN_APPROXIMATION_QUERY a global cap rather than per-boolean clause.\n+            if (approxQuery == null) {\n+                return null;\n+            }\n+            if (approxQuery instanceof BooleanQuery) {\n+                BooleanQuery bq = (BooleanQuery) approxQuery;\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                int clauseCount = 0;\n+                for (BooleanClause clause : bq) {\n+                    Query q = rewriteBoolToNgramQuery(clause.getQuery());\n+                    if (q != null) {", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTUwMTI2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419501266", "bodyText": "I think we can lose the null check.\nMatchAllDocsQuery is intended to indicate that we have a .* type regex that needs no verification other than field exists (unlike a .. type regex where a MatchAllButRequireVerificationQuery indicates we need to just run the verification query).", "author": "markharwood", "createdAt": "2020-05-04T14:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM5MTY0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM5NDkzNA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419394934", "bodyText": "The automaton can be big so we should try to store/create it in weights rather than queries (see https://issues.apache.org/jira/browse/LUCENE-9350).", "author": "jimczi", "createdAt": "2020-05-04T12:21:35Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,218 +231,383 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n+\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+\n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+\n+            String ngramIndexPattern = addLineEndChars(toLowerCase(wildcardPattern));\n+\n+            // Break search term into tokens\n+            Set<String> tokens = new LinkedHashSet<>();\n+            StringBuilder sequence = new StringBuilder();\n+            int numWildcardChars = 0;\n+            int numWildcardStrings = 0;\n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+                final int c = ngramIndexPattern.codePointAt(i);\n+                int length = Character.charCount(c);\n+                switch (c) {\n                     case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n+                        numWildcardStrings++;\n                         break;\n                     case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n-\n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n+                        numWildcardChars++;\n                         break;\n                     case WildcardQuery.WILDCARD_ESCAPE:\n                         // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n+                        if (i + length < ngramIndexPattern.length()) {\n+                            final int nextChar = ngramIndexPattern.codePointAt(i + length);\n                             length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n+                            sequence.append(Character.toChars(nextChar));\n+                        } else {\n+                            sequence.append(Character.toChars(c));\n+                        }\n+                        break;\n+\n                     default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n+                        sequence.append(Character.toChars(c));\n                 }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n+                i += length;\n             }\n \n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n+            if (sequence.length() > 0) {\n+                getNgramTokens(tokens, sequence.toString());\n             }\n \n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n+            BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+            int clauseCount = 0;\n+            for (String string : tokens) {\n+                if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                    break;\n+                }\n+                addClause(string, rewritten, Occur.MUST);\n+                clauseCount++;\n             }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), wildcardPattern, dvAutomaton);\n+            if (clauseCount > 0) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery approxQuery = rewritten.build();\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            } else if (numWildcardChars == 0 || numWildcardStrings > 0) {\n+                // We have no concrete characters and we're not a pure length query e.g. ??? \n+                return new DocValuesFieldExistsQuery(name());\n             }\n+            return verifyingQuery;\n \n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (value.length() == 0) {\n+                return new MatchNoDocsQuery();\n             }\n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            \n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(addLineEndChars(toLowerCase(value)), flags);\n \n-\n-        }\n-\n-\n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            Query approxBooleanQuery = ngramRegex.toApproximationQuery(new ApproximateRegExp.StringNormalizer() {\n+                @Override\n+                public String normalize(String token) {\n+                    return toLowerCase(token);\n                 }\n-\n-                // Add any start/end of string character\n-                if (i == 0 && patternStructure.openStart == false) {\n-                    // Start-of-string anchored (is not a leading wildcard)\n-                    fragment = TOKEN_START_OR_END_CHAR + fragment;\n+            });\n+            Query approxNgramQuery = rewriteBoolToNgramQuery(approxBooleanQuery);\n+            \n+            // MatchAll is a special case meaning the regex is known to match everything .* and \n+            // there is no need for verification.\n+            if (approxNgramQuery instanceof MatchAllDocsQuery) {\n+                return new DocValuesFieldExistsQuery(name());\n+            }\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), value, automaton);\n+            \n+            // MatchAllButRequireVerificationQuery is a special case meaning the regex is reduced to a single\n+            // clause which we can't accelerate at all and needs verification. Example would be \"..\" \n+            if (approxNgramQuery instanceof MatchAllButRequireVerificationQuery) {\n+                return verifyingQuery;\n+            }\n+            \n+            if (approxNgramQuery != null) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxNgramQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            }\n+            return verifyingQuery;                        \n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n+        }\n+        \n+        // Takes a BooleanQuery + TermQuery tree representing query logic and rewrites using ngrams of appropriate size.\n+        private Query rewriteBoolToNgramQuery(Query approxQuery) {\n+            //TODO optimise more intelligently so we: \n+            // 1) favour full-length term queries eg abc over short eg a* when pruning too many clauses.\n+            // 2) make MAX_CLAUSES_IN_APPROXIMATION_QUERY a global cap rather than per-boolean clause.\n+            if (approxQuery == null) {\n+                return null;\n+            }\n+            if (approxQuery instanceof BooleanQuery) {\n+                BooleanQuery bq = (BooleanQuery) approxQuery;\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                int clauseCount = 0;\n+                for (BooleanClause clause : bq) {\n+                    Query q = rewriteBoolToNgramQuery(clause.getQuery());\n+                    if (q != null) {\n+                        if (clause.getOccur().equals(Occur.MUST)) {\n+                            // Can't drop \"should\" clauses because it can elevate a sibling optional item\n+                            // to mandatory (shoulds with 1 clause) causing false negatives\n+                            // Dropping MUSTs increase false positives which are OK because are verified anyway.\n+                            clauseCount++;\n+                            if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                                break;\n+                            }\n+                        }\n+                        rewritten.add(q, clause.getOccur());\n+                    }\n                 }\n-                if (patternStructure.openEnd == false && i == patternStructure.fragments.length - 1) {\n-                    // End-of-string anchored (is not a trailing wildcard)\n-                    fragment = fragment + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR;\n+                return simplify(rewritten.build());\n+            }\n+            if (approxQuery instanceof TermQuery) {\n+                TermQuery tq = (TermQuery) approxQuery;\n+                // Break term into tokens\n+                Set<String> tokens = new LinkedHashSet<>();\n+                getNgramTokens(tokens, tq.getTerm().text());\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (String string : tokens) {\n+                    addClause(string, rewritten, Occur.MUST);\n                 }\n-                if (fragment.codePointCount(0, fragment.length()) <= NGRAM_SIZE) {\n-                    tokens.add(fragment);\n+                return simplify(rewritten.build());\n+            }\n+            if (isMatchAll(approxQuery)) {\n+                return approxQuery;\n+            }\n+            throw new IllegalStateException(\"Invalid query type found parsing regex query:\" + approxQuery);\n+        }    \n+        \n+        static Query simplify(Query input) {\n+            if (input instanceof BooleanQuery == false) {\n+                return input;\n+            }\n+            BooleanQuery result = (BooleanQuery) input;\n+            if (result.clauses().size() == 0) {\n+                // A \".*\" clause can produce zero clauses in which case we return MatchAll\n+                return new MatchAllDocsQuery();\n+            }\n+            if (result.clauses().size() == 1) {\n+                return simplify(result.clauses().get(0).getQuery());\n+            }\n+\n+            // We may have a mix of MatchAll and concrete queries - assess if we can simplify\n+            int matchAllCount = 0;\n+            int verifyCount = 0;\n+            boolean allConcretesAreOptional = true;\n+            for (BooleanClause booleanClause : result.clauses()) {\n+                Query q = booleanClause.getQuery();\n+                if (q instanceof MatchAllDocsQuery) {\n+                    matchAllCount++;\n+                } else if (q instanceof MatchAllButRequireVerificationQuery) {\n+                    verifyCount++;\n                 } else {\n-                    // Break fragment into multiple Ngrams\n-                    TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n-                    CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n-                    String lastUnusedToken = null;\n-                    try {\n-                        tokenizer.reset();\n-                        boolean takeThis = true;\n-                        // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n-                        // `123` and `345` - no need to search for 234. We take every other ngram.\n-                        while (tokenizer.incrementToken()) {\n-                            String tokenValue = termAtt.toString();\n-                            if (takeThis) {\n-                                tokens.add(tokenValue);\n-                            } else {\n-                                lastUnusedToken = tokenValue;\n-                            }\n-                            // alternate\n-                            takeThis = !takeThis;\n-                        }\n-                        if (lastUnusedToken != null) {\n-                            // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n-                            // `ake` to complete the logic.\n-                            tokens.add(lastUnusedToken);\n-                        }\n-                        tokenizer.end();\n-                        tokenizer.close();\n-                    } catch (IOException ioe) {\n-                        throw new ElasticsearchParseException(\"Error parsing wildcard query pattern fragment [\" + fragment + \"]\");\n+                    // Concrete query\n+                    if (booleanClause.getOccur() != Occur.SHOULD) {\n+                        allConcretesAreOptional = false;\n                     }\n                 }\n             }\n \n-            if (patternStructure.isMatchAll()) {\n+            if ((allConcretesAreOptional && matchAllCount > 0)) {\n+                // Any match all expression takes precedence over all optional concrete queries.\n                 return new MatchAllDocsQuery();\n             }\n-            BooleanQuery approximation = createApproximationQuery(tokens);\n-            if (approximation.clauses().size() > 1 || patternStructure.needsVerification()) {\n-                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n-                verifyingBuilder.add(new BooleanClause(approximation, Occur.MUST));\n-                Automaton automaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n-                verifyingBuilder.add(new BooleanClause(new AutomatonQueryOnBinaryDv(name(), wildcardPattern, automaton), Occur.MUST));\n-                return verifyingBuilder.build();\n+\n+            if ((allConcretesAreOptional && verifyCount > 0)) {\n+                // Any match all expression that needs verification takes precedence over all optional concrete queries.\n+                return new MatchAllButRequireVerificationQuery();\n             }\n-            return approximation;\n-        }\n \n-        private BooleanQuery createApproximationQuery(ArrayList<String> tokens) {\n-            BooleanQuery.Builder bqBuilder = new BooleanQuery.Builder();\n-            if (tokens.size() <= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n-                for (String token : tokens) {\n-                    addClause(token, bqBuilder);\n+            // We have some mandatory concrete queries - strip out the superfluous match all expressions\n+            if (allConcretesAreOptional == false && matchAllCount + verifyCount > 0) {\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (BooleanClause booleanClause : result.clauses()) {\n+                    if (isMatchAll(booleanClause.getQuery()) == false) {\n+                        rewritten.add(booleanClause);\n+                    }\n                 }\n-                return bqBuilder.build();\n-            }\n-            // Thin out the number of clauses using a selection spread evenly across the range\n-            float step = (float) (tokens.size() - 1) / (float) (MAX_CLAUSES_IN_APPROXIMATION_QUERY - 1); // set step size\n-            for (int i = 0; i < MAX_CLAUSES_IN_APPROXIMATION_QUERY; i++) {\n-                addClause(tokens.get(Math.round(step * i)), bqBuilder); // add each element of a position which is a multiple of step\n+                return simplify(rewritten.build());\n             }\n-            // TODO we can be smarter about pruning here. e.g.\n-            // * Avoid wildcard queries if there are sufficient numbers of other terms that are full 3grams that are cheaper term queries\n-            // * We can select terms on their scarcity rather than even spreads across the search string.\n+            return result;\n+        }\n+        \n+        \n+        static boolean isMatchAll(Query q) {\n+            return q instanceof MatchAllDocsQuery || q instanceof MatchAllButRequireVerificationQuery;\n+        }\n \n-            return bqBuilder.build();\n+        protected void getNgramTokens(Set<String> tokens, String fragment) {\n+            if (fragment.equals(TOKEN_START_STRING) || fragment.equals(TOKEN_END_STRING)) {\n+                // If a regex is a form of match-all e.g. \".*\" we only produce the token start/end markers as search\n+                // terms which can be ignored.\n+                return;\n+            }\n+            // Break fragment into multiple Ngrams\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            // If fragment length < NGRAM_SIZE then it is not emitted by token stream so need\n+            // to initialise with the value here\n+            String lastUnusedToken = fragment;\n+            try {\n+                tokenizer.reset();\n+                boolean takeThis = true;\n+                // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n+                // `123` and `345` - no need to search for 234. We take every other ngram.\n+                while (tokenizer.incrementToken()) {\n+                    String tokenValue = termAtt.toString();\n+                    if (takeThis) {\n+                        tokens.add(tokenValue);\n+                    } else {\n+                        lastUnusedToken = tokenValue;\n+                    }\n+                    // alternate\n+                    takeThis = !takeThis;\n+                    if (tokens.size() >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                        lastUnusedToken = null;\n+                        break;\n+                    }\n+                }\n+                if (lastUnusedToken != null) {\n+                    // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n+                    // `ake` to complete the logic.\n+                    tokens.add(lastUnusedToken);\n+                }\n+                tokenizer.end();\n+                tokenizer.close();\n+            } catch (IOException ioe) {\n+                throw new ElasticsearchParseException(\"Error parsing wildcard regex pattern fragment [\" + fragment + \"]\");\n+            }\n         }\n \n-        private void addClause(String token, BooleanQuery.Builder bqBuilder) {\n+        private void addClause(String token, BooleanQuery.Builder bqBuilder, Occur occur) {\n             assert token.codePointCount(0, token.length()) <= NGRAM_SIZE;\n             if (token.codePointCount(0, token.length()) == NGRAM_SIZE) {\n                 TermQuery tq = new TermQuery(new Term(name(), token));\n-                bqBuilder.add(new BooleanClause(tq, Occur.MUST));\n+                bqBuilder.add(new BooleanClause(tq, occur));\n             } else {\n-                WildcardQuery wq = new WildcardQuery(new Term(name(), token + \"*\"));\n-                wq.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_REWRITE);\n-                bqBuilder.add(new BooleanClause(wq, Occur.MUST));\n+                // Ignore tokens that are just string start or end markers\n+                if (token.charAt(token.length() - 1) != TOKEN_START_OR_END_CHAR) {\n+                    PrefixQuery wq = new PrefixQuery(new Term(name(), token));\n+                    wq.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_REWRITE);\n+                    bqBuilder.add(new BooleanClause(wq, occur));\n+                }\n             }\n+        }\n+\n+        @Override\n+        public Query fuzzyQuery(\n+            Object value,\n+            Fuzziness fuzziness,\n+            int prefixLength,\n+            int maxExpansions,\n+            boolean transpositions,\n+            QueryShardContext context\n+        ) {\n+            String searchTerm = BytesRefs.toString(value);\n+            String lowerSearchTerm = toLowerCase(searchTerm);\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), lowerSearchTerm);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            ArrayList<String> tokens = new ArrayList<>();\n+            String firstToken = null;\n+            try {\n+                tokenizer.reset();\n+                int tokenNumber = 0;\n+                while (tokenizer.incrementToken()) {\n+                    if (tokenNumber == 0) {\n+                        String token = termAtt.toString();\n+                        if (firstToken == null) {\n+                            firstToken = token;\n+                        }\n+                        tokens.add(token);\n+                    }\n+                    // Take every 3rd ngram so they are all disjoint. Our calculation for min_should_match\n+                    // number relies on there being no overlaps\n+                    tokenNumber++;\n+                    if (tokenNumber == 3) {\n+                        tokenNumber = 0;\n+                    }\n+                }\n+                tokenizer.end();\n+                tokenizer.close();\n+\n+                BooleanQuery.Builder bqBuilder = new BooleanQuery.Builder();\n \n+                // Add any prefixLength as a MUST clause to the main BooleanQuery\n+                if (prefixLength > 0) {\n+                    if (firstToken == null) {\n+                        // Search term was too small to be tokenized\n+                        assert lowerSearchTerm.codePointCount(0, lowerSearchTerm.length()) <= NGRAM_SIZE;\n+                        firstToken = lowerSearchTerm;\n+                    }\n+                    String prefix = TOKEN_START_OR_END_CHAR + firstToken;\n+                    addClause(prefix.substring(0, Math.min(NGRAM_SIZE, prefixLength)), bqBuilder, Occur.MUST);\n+                }\n+\n+                BooleanQuery.Builder approxBuilder = new BooleanQuery.Builder();\n+                int numClauses = 0;\n+                for (String token : tokens) {\n+                    addClause(token, approxBuilder, Occur.SHOULD);\n+                    numClauses++;\n+                }\n+\n+                // Approximation query\n+                BooleanQuery approxQ = approxBuilder.build();\n+                if (numClauses > fuzziness.asDistance(searchTerm)) {\n+                    // Useful accelerant - set min should match based on number of permitted edits.\n+                    approxBuilder.setMinimumNumberShouldMatch(numClauses - fuzziness.asDistance(searchTerm));\n+                    bqBuilder.add(approxQ, Occur.MUST);\n+                }\n+\n+                // Verification query\n+                FuzzyQuery fq = new FuzzyQuery(\n+                    new Term(name(), searchTerm),\n+                    fuzziness.asDistance(searchTerm),\n+                    prefixLength,\n+                    maxExpansions,\n+                    transpositions\n+                );\n+                CompiledAutomaton[] automata = fq.getAutomata();\n+                // Multiple automata are produced - one for each edit distance (e.g. 0, 1 or 2), presumably so scoring\n+                // can be related to whichever automaton matches most closely.\n+                // For now we run only the automata with the biggest edit distance - closer matches should get higher boosts\n+                // anyway from the number of ngrams matched in the acceleration query.\n+                // TODO we can revisit this approach at a later point if we want to introduce more like-for-like scoring with\n+                // keyword field which will require tweaks to the verification query logic.\n+                Automaton biggestEditDistanceAutomaton = automata[automata.length - 1].automaton;\n+                bqBuilder.add(new AutomatonQueryOnBinaryDv(name(), searchTerm, biggestEditDistanceAutomaton), Occur.MUST);", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQwMDQ0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r419400443", "bodyText": "we could consider the entire prefix length instead of limiting to the first 3 characters ?", "author": "jimczi", "createdAt": "2020-05-04T12:31:51Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,218 +231,383 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n+\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+\n+            Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+\n+            String ngramIndexPattern = addLineEndChars(toLowerCase(wildcardPattern));\n+\n+            // Break search term into tokens\n+            Set<String> tokens = new LinkedHashSet<>();\n+            StringBuilder sequence = new StringBuilder();\n+            int numWildcardChars = 0;\n+            int numWildcardStrings = 0;\n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+                final int c = ngramIndexPattern.codePointAt(i);\n+                int length = Character.charCount(c);\n+                switch (c) {\n                     case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                        }\n-                        lastGap = Integer.MAX_VALUE;\n+                        numWildcardStrings++;\n                         break;\n                     case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n-\n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n+                        numWildcardChars++;\n                         break;\n                     case WildcardQuery.WILDCARD_ESCAPE:\n                         // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n+                        if (i + length < ngramIndexPattern.length()) {\n+                            final int nextChar = ngramIndexPattern.codePointAt(i + length);\n                             length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n+                            sequence.append(Character.toChars(nextChar));\n+                        } else {\n+                            sequence.append(Character.toChars(c));\n+                        }\n+                        break;\n+\n                     default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n+                        sequence.append(Character.toChars(c));\n                 }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n+                i += length;\n             }\n \n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n+            if (sequence.length() > 0) {\n+                getNgramTokens(tokens, sequence.toString());\n             }\n \n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n+            BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+            int clauseCount = 0;\n+            for (String string : tokens) {\n+                if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                    break;\n+                }\n+                addClause(string, rewritten, Occur.MUST);\n+                clauseCount++;\n             }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), wildcardPattern, dvAutomaton);\n+            if (clauseCount > 0) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery approxQuery = rewritten.build();\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            } else if (numWildcardChars == 0 || numWildcardStrings > 0) {\n+                // We have no concrete characters and we're not a pure length query e.g. ??? \n+                return new DocValuesFieldExistsQuery(name());\n             }\n+            return verifyingQuery;\n \n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (value.length() == 0) {\n+                return new MatchNoDocsQuery();\n             }\n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            \n+            RegExp regex = new RegExp(value, flags);\n+            Automaton automaton = regex.toAutomaton(maxDeterminizedStates);\n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(addLineEndChars(toLowerCase(value)), flags);\n \n-\n-        }\n-\n-\n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            Query approxBooleanQuery = ngramRegex.toApproximationQuery(new ApproximateRegExp.StringNormalizer() {\n+                @Override\n+                public String normalize(String token) {\n+                    return toLowerCase(token);\n                 }\n-\n-                // Add any start/end of string character\n-                if (i == 0 && patternStructure.openStart == false) {\n-                    // Start-of-string anchored (is not a leading wildcard)\n-                    fragment = TOKEN_START_OR_END_CHAR + fragment;\n+            });\n+            Query approxNgramQuery = rewriteBoolToNgramQuery(approxBooleanQuery);\n+            \n+            // MatchAll is a special case meaning the regex is known to match everything .* and \n+            // there is no need for verification.\n+            if (approxNgramQuery instanceof MatchAllDocsQuery) {\n+                return new DocValuesFieldExistsQuery(name());\n+            }\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), value, automaton);\n+            \n+            // MatchAllButRequireVerificationQuery is a special case meaning the regex is reduced to a single\n+            // clause which we can't accelerate at all and needs verification. Example would be \"..\" \n+            if (approxNgramQuery instanceof MatchAllButRequireVerificationQuery) {\n+                return verifyingQuery;\n+            }\n+            \n+            if (approxNgramQuery != null) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxNgramQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            }\n+            return verifyingQuery;                        \n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n+        }\n+        \n+        // Takes a BooleanQuery + TermQuery tree representing query logic and rewrites using ngrams of appropriate size.\n+        private Query rewriteBoolToNgramQuery(Query approxQuery) {\n+            //TODO optimise more intelligently so we: \n+            // 1) favour full-length term queries eg abc over short eg a* when pruning too many clauses.\n+            // 2) make MAX_CLAUSES_IN_APPROXIMATION_QUERY a global cap rather than per-boolean clause.\n+            if (approxQuery == null) {\n+                return null;\n+            }\n+            if (approxQuery instanceof BooleanQuery) {\n+                BooleanQuery bq = (BooleanQuery) approxQuery;\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                int clauseCount = 0;\n+                for (BooleanClause clause : bq) {\n+                    Query q = rewriteBoolToNgramQuery(clause.getQuery());\n+                    if (q != null) {\n+                        if (clause.getOccur().equals(Occur.MUST)) {\n+                            // Can't drop \"should\" clauses because it can elevate a sibling optional item\n+                            // to mandatory (shoulds with 1 clause) causing false negatives\n+                            // Dropping MUSTs increase false positives which are OK because are verified anyway.\n+                            clauseCount++;\n+                            if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                                break;\n+                            }\n+                        }\n+                        rewritten.add(q, clause.getOccur());\n+                    }\n                 }\n-                if (patternStructure.openEnd == false && i == patternStructure.fragments.length - 1) {\n-                    // End-of-string anchored (is not a trailing wildcard)\n-                    fragment = fragment + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR;\n+                return simplify(rewritten.build());\n+            }\n+            if (approxQuery instanceof TermQuery) {\n+                TermQuery tq = (TermQuery) approxQuery;\n+                // Break term into tokens\n+                Set<String> tokens = new LinkedHashSet<>();\n+                getNgramTokens(tokens, tq.getTerm().text());\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (String string : tokens) {\n+                    addClause(string, rewritten, Occur.MUST);\n                 }\n-                if (fragment.codePointCount(0, fragment.length()) <= NGRAM_SIZE) {\n-                    tokens.add(fragment);\n+                return simplify(rewritten.build());\n+            }\n+            if (isMatchAll(approxQuery)) {\n+                return approxQuery;\n+            }\n+            throw new IllegalStateException(\"Invalid query type found parsing regex query:\" + approxQuery);\n+        }    \n+        \n+        static Query simplify(Query input) {\n+            if (input instanceof BooleanQuery == false) {\n+                return input;\n+            }\n+            BooleanQuery result = (BooleanQuery) input;\n+            if (result.clauses().size() == 0) {\n+                // A \".*\" clause can produce zero clauses in which case we return MatchAll\n+                return new MatchAllDocsQuery();\n+            }\n+            if (result.clauses().size() == 1) {\n+                return simplify(result.clauses().get(0).getQuery());\n+            }\n+\n+            // We may have a mix of MatchAll and concrete queries - assess if we can simplify\n+            int matchAllCount = 0;\n+            int verifyCount = 0;\n+            boolean allConcretesAreOptional = true;\n+            for (BooleanClause booleanClause : result.clauses()) {\n+                Query q = booleanClause.getQuery();\n+                if (q instanceof MatchAllDocsQuery) {\n+                    matchAllCount++;\n+                } else if (q instanceof MatchAllButRequireVerificationQuery) {\n+                    verifyCount++;\n                 } else {\n-                    // Break fragment into multiple Ngrams\n-                    TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n-                    CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n-                    String lastUnusedToken = null;\n-                    try {\n-                        tokenizer.reset();\n-                        boolean takeThis = true;\n-                        // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n-                        // `123` and `345` - no need to search for 234. We take every other ngram.\n-                        while (tokenizer.incrementToken()) {\n-                            String tokenValue = termAtt.toString();\n-                            if (takeThis) {\n-                                tokens.add(tokenValue);\n-                            } else {\n-                                lastUnusedToken = tokenValue;\n-                            }\n-                            // alternate\n-                            takeThis = !takeThis;\n-                        }\n-                        if (lastUnusedToken != null) {\n-                            // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n-                            // `ake` to complete the logic.\n-                            tokens.add(lastUnusedToken);\n-                        }\n-                        tokenizer.end();\n-                        tokenizer.close();\n-                    } catch (IOException ioe) {\n-                        throw new ElasticsearchParseException(\"Error parsing wildcard query pattern fragment [\" + fragment + \"]\");\n+                    // Concrete query\n+                    if (booleanClause.getOccur() != Occur.SHOULD) {\n+                        allConcretesAreOptional = false;\n                     }\n                 }\n             }\n \n-            if (patternStructure.isMatchAll()) {\n+            if ((allConcretesAreOptional && matchAllCount > 0)) {\n+                // Any match all expression takes precedence over all optional concrete queries.\n                 return new MatchAllDocsQuery();\n             }\n-            BooleanQuery approximation = createApproximationQuery(tokens);\n-            if (approximation.clauses().size() > 1 || patternStructure.needsVerification()) {\n-                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n-                verifyingBuilder.add(new BooleanClause(approximation, Occur.MUST));\n-                Automaton automaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n-                verifyingBuilder.add(new BooleanClause(new AutomatonQueryOnBinaryDv(name(), wildcardPattern, automaton), Occur.MUST));\n-                return verifyingBuilder.build();\n+\n+            if ((allConcretesAreOptional && verifyCount > 0)) {\n+                // Any match all expression that needs verification takes precedence over all optional concrete queries.\n+                return new MatchAllButRequireVerificationQuery();\n             }\n-            return approximation;\n-        }\n \n-        private BooleanQuery createApproximationQuery(ArrayList<String> tokens) {\n-            BooleanQuery.Builder bqBuilder = new BooleanQuery.Builder();\n-            if (tokens.size() <= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n-                for (String token : tokens) {\n-                    addClause(token, bqBuilder);\n+            // We have some mandatory concrete queries - strip out the superfluous match all expressions\n+            if (allConcretesAreOptional == false && matchAllCount + verifyCount > 0) {\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (BooleanClause booleanClause : result.clauses()) {\n+                    if (isMatchAll(booleanClause.getQuery()) == false) {\n+                        rewritten.add(booleanClause);\n+                    }\n                 }\n-                return bqBuilder.build();\n-            }\n-            // Thin out the number of clauses using a selection spread evenly across the range\n-            float step = (float) (tokens.size() - 1) / (float) (MAX_CLAUSES_IN_APPROXIMATION_QUERY - 1); // set step size\n-            for (int i = 0; i < MAX_CLAUSES_IN_APPROXIMATION_QUERY; i++) {\n-                addClause(tokens.get(Math.round(step * i)), bqBuilder); // add each element of a position which is a multiple of step\n+                return simplify(rewritten.build());\n             }\n-            // TODO we can be smarter about pruning here. e.g.\n-            // * Avoid wildcard queries if there are sufficient numbers of other terms that are full 3grams that are cheaper term queries\n-            // * We can select terms on their scarcity rather than even spreads across the search string.\n+            return result;\n+        }\n+        \n+        \n+        static boolean isMatchAll(Query q) {\n+            return q instanceof MatchAllDocsQuery || q instanceof MatchAllButRequireVerificationQuery;\n+        }\n \n-            return bqBuilder.build();\n+        protected void getNgramTokens(Set<String> tokens, String fragment) {\n+            if (fragment.equals(TOKEN_START_STRING) || fragment.equals(TOKEN_END_STRING)) {\n+                // If a regex is a form of match-all e.g. \".*\" we only produce the token start/end markers as search\n+                // terms which can be ignored.\n+                return;\n+            }\n+            // Break fragment into multiple Ngrams\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            // If fragment length < NGRAM_SIZE then it is not emitted by token stream so need\n+            // to initialise with the value here\n+            String lastUnusedToken = fragment;\n+            try {\n+                tokenizer.reset();\n+                boolean takeThis = true;\n+                // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n+                // `123` and `345` - no need to search for 234. We take every other ngram.\n+                while (tokenizer.incrementToken()) {\n+                    String tokenValue = termAtt.toString();\n+                    if (takeThis) {\n+                        tokens.add(tokenValue);\n+                    } else {\n+                        lastUnusedToken = tokenValue;\n+                    }\n+                    // alternate\n+                    takeThis = !takeThis;\n+                    if (tokens.size() >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                        lastUnusedToken = null;\n+                        break;\n+                    }\n+                }\n+                if (lastUnusedToken != null) {\n+                    // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n+                    // `ake` to complete the logic.\n+                    tokens.add(lastUnusedToken);\n+                }\n+                tokenizer.end();\n+                tokenizer.close();\n+            } catch (IOException ioe) {\n+                throw new ElasticsearchParseException(\"Error parsing wildcard regex pattern fragment [\" + fragment + \"]\");\n+            }\n         }\n \n-        private void addClause(String token, BooleanQuery.Builder bqBuilder) {\n+        private void addClause(String token, BooleanQuery.Builder bqBuilder, Occur occur) {\n             assert token.codePointCount(0, token.length()) <= NGRAM_SIZE;\n             if (token.codePointCount(0, token.length()) == NGRAM_SIZE) {\n                 TermQuery tq = new TermQuery(new Term(name(), token));\n-                bqBuilder.add(new BooleanClause(tq, Occur.MUST));\n+                bqBuilder.add(new BooleanClause(tq, occur));\n             } else {\n-                WildcardQuery wq = new WildcardQuery(new Term(name(), token + \"*\"));\n-                wq.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_REWRITE);\n-                bqBuilder.add(new BooleanClause(wq, Occur.MUST));\n+                // Ignore tokens that are just string start or end markers\n+                if (token.charAt(token.length() - 1) != TOKEN_START_OR_END_CHAR) {\n+                    PrefixQuery wq = new PrefixQuery(new Term(name(), token));\n+                    wq.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_REWRITE);\n+                    bqBuilder.add(new BooleanClause(wq, occur));\n+                }\n             }\n+        }\n+\n+        @Override\n+        public Query fuzzyQuery(\n+            Object value,\n+            Fuzziness fuzziness,\n+            int prefixLength,\n+            int maxExpansions,\n+            boolean transpositions,\n+            QueryShardContext context\n+        ) {\n+            String searchTerm = BytesRefs.toString(value);\n+            String lowerSearchTerm = toLowerCase(searchTerm);\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), lowerSearchTerm);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            ArrayList<String> tokens = new ArrayList<>();\n+            String firstToken = null;\n+            try {\n+                tokenizer.reset();\n+                int tokenNumber = 0;\n+                while (tokenizer.incrementToken()) {\n+                    if (tokenNumber == 0) {\n+                        String token = termAtt.toString();\n+                        if (firstToken == null) {\n+                            firstToken = token;\n+                        }\n+                        tokens.add(token);\n+                    }\n+                    // Take every 3rd ngram so they are all disjoint. Our calculation for min_should_match\n+                    // number relies on there being no overlaps\n+                    tokenNumber++;\n+                    if (tokenNumber == 3) {\n+                        tokenNumber = 0;\n+                    }\n+                }\n+                tokenizer.end();\n+                tokenizer.close();\n+\n+                BooleanQuery.Builder bqBuilder = new BooleanQuery.Builder();\n \n+                // Add any prefixLength as a MUST clause to the main BooleanQuery", "originalCommit": "50565773354c3827106d27d23fc498ee04afdc35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "459d76dda53f965c280143b71254ae083e2d5c47", "url": "https://github.com/elastic/elasticsearch/commit/459d76dda53f965c280143b71254ae083e2d5c47", "message": "Addressing latest review comments by adding\n* Unlimited prefix length.\n* Delayed Autotmaton creation\n* FuzzyQuery tests", "committedDate": "2020-05-05T12:22:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzAwNTE1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r423005157", "bodyText": "This shouldn't be possible ? Maybe add an assert that tokenSize should be less than or equals to NGRAM_SIZE at this point ?", "author": "jimczi", "createdAt": "2020-05-11T12:31:54Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,218 +232,407 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n+\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+\n+            String ngramIndexPattern = addLineEndChars(toLowerCase(wildcardPattern));\n+\n+            // Break search term into tokens\n+            Set<String> tokens = new LinkedHashSet<>();\n+            StringBuilder sequence = new StringBuilder();\n+            int numWildcardChars = 0;\n+            int numWildcardStrings = 0;\n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+                final int c = ngramIndexPattern.codePointAt(i);\n+                int length = Character.charCount(c);\n+                switch (c) {\n                     case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n-                        lastGap = Integer.MAX_VALUE;\n+                        numWildcardStrings++;\n                         break;\n                     case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n-\n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n+                        numWildcardChars++;\n                         break;\n                     case WildcardQuery.WILDCARD_ESCAPE:\n                         // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n+                        if (i + length < ngramIndexPattern.length()) {\n+                            final int nextChar = ngramIndexPattern.codePointAt(i + length);\n                             length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n+                            sequence.append(Character.toChars(nextChar));\n+                        } else {\n+                            sequence.append(Character.toChars(c));\n+                        }\n+                        break;\n+\n                     default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n-                }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n+                        sequence.append(Character.toChars(c));\n                 }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n-\n+                i += length;\n             }\n \n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n-                }\n-                return true;\n+            if (sequence.length() > 0) {\n+                getNgramTokens(tokens, sequence.toString());\n             }\n \n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n+            BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+            int clauseCount = 0;\n+            for (String string : tokens) {\n+                if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                    break;\n+                }\n+                addClause(string, rewritten, Occur.MUST);\n+                clauseCount++;\n             }\n-\n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n+            Supplier<Automaton> deferredAutomatonSupplier = () -> {\n+                return WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            };\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), wildcardPattern, deferredAutomatonSupplier);\n+            if (clauseCount > 0) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery approxQuery = rewritten.build();\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            } else if (numWildcardChars == 0 || numWildcardStrings > 0) {\n+                // We have no concrete characters and we're not a pure length query e.g. ??? \n+                return new DocValuesFieldExistsQuery(name());\n             }\n+            return verifyingQuery;\n \n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (value.length() == 0) {\n+                return new MatchNoDocsQuery();\n             }\n \n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n+            \n+            ApproximateRegExp ngramRegex = new ApproximateRegExp(addLineEndChars(toLowerCase(value)), flags);\n \n-\n-        }\n-\n-\n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n+            Query approxBooleanQuery = ngramRegex.toApproximationQuery(new ApproximateRegExp.StringNormalizer() {\n+                @Override\n+                public String normalize(String token) {\n+                    return toLowerCase(token);\n                 }\n-\n-                // Add any start/end of string character\n-                if (i == 0 && patternStructure.openStart == false) {\n-                    // Start-of-string anchored (is not a leading wildcard)\n-                    fragment = TOKEN_START_OR_END_CHAR + fragment;\n+            });\n+            Query approxNgramQuery = rewriteBoolToNgramQuery(approxBooleanQuery);\n+            \n+            // MatchAll is a special case meaning the regex is known to match everything .* and \n+            // there is no need for verification.\n+            if (approxNgramQuery instanceof MatchAllDocsQuery) {\n+                return existsQuery(context);\n+            }\n+            Supplier<Automaton> deferredAutomatonSupplier = ()-> {\n+                RegExp regex = new RegExp(value, flags);\n+                return regex.toAutomaton(maxDeterminizedStates);\n+            };\n+\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), value, deferredAutomatonSupplier);\n+            \n+            // MatchAllButRequireVerificationQuery is a special case meaning the regex is reduced to a single\n+            // clause which we can't accelerate at all and needs verification. Example would be \"..\" \n+            if (approxNgramQuery instanceof MatchAllButRequireVerificationQuery) {\n+                return verifyingQuery;\n+            }\n+            \n+            // We can accelerate execution with the ngram query\n+            BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+            verifyingBuilder.add(new BooleanClause(approxNgramQuery, Occur.MUST));\n+            verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+            return verifyingBuilder.build();\n+        }\n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n+        }\n+        \n+        // Takes a BooleanQuery + TermQuery tree representing query logic and rewrites using ngrams of appropriate size.\n+        private Query rewriteBoolToNgramQuery(Query approxQuery) {\n+            //TODO optimise more intelligently so we: \n+            // 1) favour full-length term queries eg abc over short eg a* when pruning too many clauses.\n+            // 2) make MAX_CLAUSES_IN_APPROXIMATION_QUERY a global cap rather than per-boolean clause.\n+            if (approxQuery == null) {\n+                return null;\n+            }\n+            if (approxQuery instanceof BooleanQuery) {\n+                BooleanQuery bq = (BooleanQuery) approxQuery;\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                int clauseCount = 0;\n+                for (BooleanClause clause : bq) {\n+                    Query q = rewriteBoolToNgramQuery(clause.getQuery());\n+                    if (q != null) {\n+                        if (clause.getOccur().equals(Occur.MUST)) {\n+                            // Can't drop \"should\" clauses because it can elevate a sibling optional item\n+                            // to mandatory (shoulds with 1 clause) causing false negatives\n+                            // Dropping MUSTs increase false positives which are OK because are verified anyway.\n+                            clauseCount++;\n+                            if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                                break;\n+                            }\n+                        }\n+                        rewritten.add(q, clause.getOccur());\n+                    }\n                 }\n-                if (patternStructure.openEnd == false && i == patternStructure.fragments.length - 1) {\n-                    // End-of-string anchored (is not a trailing wildcard)\n-                    fragment = fragment + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR;\n+                return simplify(rewritten.build());\n+            }\n+            if (approxQuery instanceof TermQuery) {\n+                TermQuery tq = (TermQuery) approxQuery;\n+                // Break term into tokens\n+                Set<String> tokens = new LinkedHashSet<>();\n+                getNgramTokens(tokens, tq.getTerm().text());\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (String string : tokens) {\n+                    addClause(string, rewritten, Occur.MUST);\n                 }\n-                if (fragment.codePointCount(0, fragment.length()) <= NGRAM_SIZE) {\n-                    tokens.add(fragment);\n+                return simplify(rewritten.build());\n+            }\n+            if (isMatchAll(approxQuery)) {\n+                return approxQuery;\n+            }\n+            throw new IllegalStateException(\"Invalid query type found parsing regex query:\" + approxQuery);\n+        }    \n+        \n+        static Query simplify(Query input) {\n+            if (input instanceof BooleanQuery == false) {\n+                return input;\n+            }\n+            BooleanQuery result = (BooleanQuery) input;\n+            if (result.clauses().size() == 0) {\n+                // A \".*\" clause can produce zero clauses in which case we return MatchAll\n+                return new MatchAllDocsQuery();\n+            }\n+            if (result.clauses().size() == 1) {\n+                return simplify(result.clauses().get(0).getQuery());\n+            }\n+\n+            // We may have a mix of MatchAll and concrete queries - assess if we can simplify\n+            int matchAllCount = 0;\n+            int verifyCount = 0;\n+            boolean allConcretesAreOptional = true;\n+            for (BooleanClause booleanClause : result.clauses()) {\n+                Query q = booleanClause.getQuery();\n+                if (q instanceof MatchAllDocsQuery) {\n+                    matchAllCount++;\n+                } else if (q instanceof MatchAllButRequireVerificationQuery) {\n+                    verifyCount++;\n                 } else {\n-                    // Break fragment into multiple Ngrams\n-                    TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n-                    CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n-                    String lastUnusedToken = null;\n-                    try {\n-                        tokenizer.reset();\n-                        boolean takeThis = true;\n-                        // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n-                        // `123` and `345` - no need to search for 234. We take every other ngram.\n-                        while (tokenizer.incrementToken()) {\n-                            String tokenValue = termAtt.toString();\n-                            if (takeThis) {\n-                                tokens.add(tokenValue);\n-                            } else {\n-                                lastUnusedToken = tokenValue;\n-                            }\n-                            // alternate\n-                            takeThis = !takeThis;\n-                        }\n-                        if (lastUnusedToken != null) {\n-                            // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n-                            // `ake` to complete the logic.\n-                            tokens.add(lastUnusedToken);\n-                        }\n-                        tokenizer.end();\n-                        tokenizer.close();\n-                    } catch (IOException ioe) {\n-                        throw new ElasticsearchParseException(\"Error parsing wildcard query pattern fragment [\" + fragment + \"]\");\n+                    // Concrete query\n+                    if (booleanClause.getOccur() != Occur.SHOULD) {\n+                        allConcretesAreOptional = false;\n                     }\n                 }\n             }\n \n-            if (patternStructure.isMatchAll()) {\n+            if ((allConcretesAreOptional && matchAllCount > 0)) {\n+                // Any match all expression takes precedence over all optional concrete queries.\n                 return new MatchAllDocsQuery();\n             }\n-            BooleanQuery approximation = createApproximationQuery(tokens);\n-            if (approximation.clauses().size() > 1 || patternStructure.needsVerification()) {\n-                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n-                verifyingBuilder.add(new BooleanClause(approximation, Occur.MUST));\n-                Automaton automaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n-                verifyingBuilder.add(new BooleanClause(new AutomatonQueryOnBinaryDv(name(), wildcardPattern, automaton), Occur.MUST));\n-                return verifyingBuilder.build();\n+\n+            if ((allConcretesAreOptional && verifyCount > 0)) {\n+                // Any match all expression that needs verification takes precedence over all optional concrete queries.\n+                return new MatchAllButRequireVerificationQuery();\n             }\n-            return approximation;\n-        }\n \n-        private BooleanQuery createApproximationQuery(ArrayList<String> tokens) {\n-            BooleanQuery.Builder bqBuilder = new BooleanQuery.Builder();\n-            if (tokens.size() <= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n-                for (String token : tokens) {\n-                    addClause(token, bqBuilder);\n+            // We have some mandatory concrete queries - strip out the superfluous match all expressions\n+            if (allConcretesAreOptional == false && matchAllCount + verifyCount > 0) {\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (BooleanClause booleanClause : result.clauses()) {\n+                    if (isMatchAll(booleanClause.getQuery()) == false) {\n+                        rewritten.add(booleanClause);\n+                    }\n                 }\n-                return bqBuilder.build();\n-            }\n-            // Thin out the number of clauses using a selection spread evenly across the range\n-            float step = (float) (tokens.size() - 1) / (float) (MAX_CLAUSES_IN_APPROXIMATION_QUERY - 1); // set step size\n-            for (int i = 0; i < MAX_CLAUSES_IN_APPROXIMATION_QUERY; i++) {\n-                addClause(tokens.get(Math.round(step * i)), bqBuilder); // add each element of a position which is a multiple of step\n+                return simplify(rewritten.build());\n             }\n-            // TODO we can be smarter about pruning here. e.g.\n-            // * Avoid wildcard queries if there are sufficient numbers of other terms that are full 3grams that are cheaper term queries\n-            // * We can select terms on their scarcity rather than even spreads across the search string.\n+            return result;\n+        }\n+        \n+        \n+        static boolean isMatchAll(Query q) {\n+            return q instanceof MatchAllDocsQuery || q instanceof MatchAllButRequireVerificationQuery;\n+        }\n \n-            return bqBuilder.build();\n+        protected void getNgramTokens(Set<String> tokens, String fragment) {\n+            if (fragment.equals(TOKEN_START_STRING) || fragment.equals(TOKEN_END_STRING)) {\n+                // If a regex is a form of match-all e.g. \".*\" we only produce the token start/end markers as search\n+                // terms which can be ignored.\n+                return;\n+            }\n+            // Break fragment into multiple Ngrams\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            // If fragment length < NGRAM_SIZE then it is not emitted by token stream so need\n+            // to initialise with the value here\n+            String lastUnusedToken = fragment;\n+            try {\n+                tokenizer.reset();\n+                boolean takeThis = true;\n+                // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n+                // `123` and `345` - no need to search for 234. We take every other ngram.\n+                while (tokenizer.incrementToken()) {\n+                    String tokenValue = termAtt.toString();\n+                    if (takeThis) {\n+                        tokens.add(tokenValue);\n+                        lastUnusedToken = null;\n+                    } else {\n+                        lastUnusedToken = tokenValue;\n+                    }\n+                    // alternate\n+                    takeThis = !takeThis;\n+                    if (tokens.size() >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                        lastUnusedToken = null;\n+                        break;\n+                    }\n+                }\n+                if (lastUnusedToken != null) {\n+                    // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n+                    // `ake` to complete the logic.\n+                    tokens.add(lastUnusedToken);\n+                }\n+                tokenizer.end();\n+                tokenizer.close();\n+            } catch (IOException ioe) {\n+                throw new ElasticsearchParseException(\"Error parsing wildcard regex pattern fragment [\" + fragment + \"]\");\n+            }\n         }\n+        \n \n-        private void addClause(String token, BooleanQuery.Builder bqBuilder) {\n+        private void addClause(String token, BooleanQuery.Builder bqBuilder, Occur occur) {\n             assert token.codePointCount(0, token.length()) <= NGRAM_SIZE;\n-            if (token.codePointCount(0, token.length()) == NGRAM_SIZE) {\n+            int tokenSize = token.codePointCount(0, token.length());\n+            if (tokenSize < 2 || token.equals(WildcardFieldMapper.TOKEN_END_STRING)) {\n+                // there's something concrete to be searched but it's too short\n+                // Require verification.\n+                bqBuilder.add(new BooleanClause(new MatchAllButRequireVerificationQuery(), occur));\n+                return;\n+            }\n+            if (tokenSize == NGRAM_SIZE) {\n                 TermQuery tq = new TermQuery(new Term(name(), token));\n-                bqBuilder.add(new BooleanClause(tq, Occur.MUST));\n+                bqBuilder.add(new BooleanClause(tq, occur));\n             } else {", "originalCommit": "c8a85c9cad26a9560b018316d999e40f9e6e5af0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzAzODY4Nw==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r423038687", "bodyText": "Not sure I understand - do you mean the else is not possible? It should catch when tokenSize ==2. I think we agreed that ab* was worth running but not a*\n\nMaybe add an assert that tokenSize should be less than or equals to NGRAM_SIZE at this point ?\n\nline 515 has this assert?", "author": "markharwood", "createdAt": "2020-05-11T13:28:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzAwNTE1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzA0NDA1OA==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r423044058", "bodyText": "Sorry I missed the too short condition.", "author": "jimczi", "createdAt": "2020-05-11T13:36:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzAwNTE1Nw=="}], "type": "inlineReview"}, {"oid": "ef33b62c0ed27a0f9ed136d125dd1d0b79d924a3", "url": "https://github.com/elastic/elasticsearch/commit/ef33b62c0ed27a0f9ed136d125dd1d0b79d924a3", "message": "Simplified ApproximateRegExp, removing dependency on WildcardFieldMapper class. All query simplification logic is now consolidated in the WildcardFieldMapper class.", "committedDate": "2020-05-12T10:25:43Z", "type": "forcePushed"}, {"oid": "99c9f29346abc5f38f57ce6f08c58f846089b01b", "url": "https://github.com/elastic/elasticsearch/commit/99c9f29346abc5f38f57ce6f08c58f846089b01b", "message": "Removed fork of Lucene\u2019s RegEx now we have access to its internals", "committedDate": "2020-05-21T09:15:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwOTUyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55548#discussion_r429909529", "bodyText": "nit: Can you remove the empty lines ?", "author": "jimczi", "createdAt": "2020-05-25T12:29:28Z", "path": "x-pack/plugin/wildcard/src/main/java/org/elasticsearch/xpack/wildcard/mapper/WildcardFieldMapper.java", "diffHunk": "@@ -216,218 +233,538 @@ public WildcardFieldType clone() {\n             return result;\n         }\n \n-        // Holds parsed information about the wildcard pattern\n-        static class PatternStructure {\n-            boolean openStart, openEnd, hasSymbols;\n-            int lastGap =0;\n-            int wildcardCharCount, wildcardStringCount;\n-            String[] fragments;\n-            Integer []  precedingGapSizes;\n-            final String pattern;\n-\n-            @SuppressWarnings(\"fallthrough\") // Intentionally uses fallthrough mirroring implementation in Lucene's WildcardQuery\n-            PatternStructure (String wildcardText) {\n-                this.pattern = wildcardText;\n-                ArrayList<String> fragmentList = new ArrayList<>();\n-                ArrayList<Integer> precedingGapSizeList = new ArrayList<>();\n-                StringBuilder sb = new StringBuilder();\n-                for (int i = 0; i < wildcardText.length();) {\n-                    final int c = wildcardText.codePointAt(i);\n-                    int length = Character.charCount(c);\n-                    switch (c) {\n+\n+        @Override\n+        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n+\n+            String ngramIndexPattern = addLineEndChars(toLowerCase(wildcardPattern));\n+\n+            // Break search term into tokens\n+            Set<String> tokens = new LinkedHashSet<>();\n+            StringBuilder sequence = new StringBuilder();\n+            int numWildcardChars = 0;\n+            int numWildcardStrings = 0;\n+            for (int i = 0; i < ngramIndexPattern.length();) {\n+                final int c = ngramIndexPattern.codePointAt(i);\n+                int length = Character.charCount(c);\n+                switch (c) {\n                     case WildcardQuery.WILDCARD_STRING:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        openEnd = true;\n-                        hasSymbols = true;\n-                        wildcardStringCount++;\n-\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n-                        lastGap = Integer.MAX_VALUE;\n+                        numWildcardStrings++;\n                         break;\n                     case WildcardQuery.WILDCARD_CHAR:\n-                        if (i == 0) {\n-                            openStart = true;\n-                        }\n-                        hasSymbols = true;\n-                        wildcardCharCount++;\n-                        openEnd = true;\n-                        if (sb.length() > 0) {\n-                            precedingGapSizeList.add(lastGap);\n-                            fragmentList.add(sb.toString());\n-                            sb = new StringBuilder();\n-                            lastGap = 0;\n-                        }\n-\n-                        if (lastGap != Integer.MAX_VALUE) {\n-                            lastGap++;\n+                        if (sequence.length() > 0) {\n+                            getNgramTokens(tokens, sequence.toString());\n+                            sequence = new StringBuilder();\n                         }\n+                        numWildcardChars++;\n                         break;\n                     case WildcardQuery.WILDCARD_ESCAPE:\n                         // add the next codepoint instead, if it exists\n-                        if (i + length < wildcardText.length()) {\n-                            final int nextChar = wildcardText.codePointAt(i + length);\n+                        if (i + length < ngramIndexPattern.length()) {\n+                            final int nextChar = ngramIndexPattern.codePointAt(i + length);\n                             length += Character.charCount(nextChar);\n-                            sb.append(Character.toChars(nextChar));\n-                            openEnd = false;\n-                            break;\n-                        } // else fallthru, lenient parsing with a trailing \\\n+                            sequence.append(Character.toChars(nextChar));\n+                        } else {\n+                            sequence.append(Character.toChars(c));\n+                        }\n+                        break;\n+\n                     default:\n-                        openEnd = false;\n-                        sb.append(Character.toChars(c));\n-                    }\n-                    i += length;\n+                        sequence.append(Character.toChars(c));\n                 }\n-                if (sb.length() > 0) {\n-                    precedingGapSizeList.add(lastGap);\n-                    fragmentList.add(sb.toString());\n-                    lastGap = 0;\n-                }\n-                fragments = fragmentList.toArray(new String[0]);\n-                precedingGapSizes = precedingGapSizeList.toArray(new Integer[0]);\n+                i += length;\n+            }\n \n+            if (sequence.length() > 0) {\n+                getNgramTokens(tokens, sequence.toString());\n             }\n \n-            public boolean needsVerification() {\n-                // Return true if term queries are not enough evidence\n-                if (fragments.length == 1 && wildcardCharCount == 0) {\n-                    // The one case where we don't need verification is when\n-                    // we have a single fragment and no ? characters\n-                    return false;\n+            BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+            int clauseCount = 0;\n+            for (String string : tokens) {\n+                if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                    break;\n                 }\n-                return true;\n+                addClause(string, rewritten, Occur.MUST);\n+                clauseCount++;\n             }\n-\n-            // Returns number of positions for last gap (Integer.MAX means unlimited gap)\n-            public int getPrecedingGapSize(int fragmentNum) {\n-                return precedingGapSizes[fragmentNum];\n+            Supplier<Automaton> deferredAutomatonSupplier = () -> {\n+                return WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n+            };\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), wildcardPattern, deferredAutomatonSupplier);\n+            if (clauseCount > 0) {\n+                // We can accelerate execution with the ngram query\n+                BooleanQuery approxQuery = rewritten.build();\n+                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+                verifyingBuilder.add(new BooleanClause(approxQuery, Occur.MUST));\n+                verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+                return verifyingBuilder.build();\n+            } else if (numWildcardChars == 0 || numWildcardStrings > 0) {\n+                // We have no concrete characters and we're not a pure length query e.g. ??? \n+                return new DocValuesFieldExistsQuery(name());\n             }\n+            return verifyingQuery;\n \n-            public boolean isMatchAll() {\n-                return fragments.length == 0 && wildcardStringCount >0 && wildcardCharCount ==0;\n+        }\n+        \n+        @Override\n+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context) {\n+            if (value.length() == 0) {\n+                return new MatchNoDocsQuery();\n             }\n \n-            @Override\n-            public int hashCode() {\n-                return pattern.hashCode();\n+            if (context.allowExpensiveQueries() == false) {\n+                throw new ElasticsearchException(\n+                    \"[regexp] queries cannot be executed when '\" + ALLOW_EXPENSIVE_QUERIES.getKey() + \"' is set to false.\"\n+                );\n             }\n-\n-            @Override\n-            public boolean equals(Object obj) {\n-                PatternStructure other = (PatternStructure) obj;\n-                return pattern.equals(other.pattern);\n+            \n+            RegExp ngramRegex = new RegExp(addLineEndChars(toLowerCase(value)), flags);\n+\n+            Query approxBooleanQuery = toApproximationQuery(ngramRegex);\n+            Query approxNgramQuery = rewriteBoolToNgramQuery(approxBooleanQuery);\n+            \n+            // MatchAll is a special case meaning the regex is known to match everything .* and \n+            // there is no need for verification.\n+            if (approxNgramQuery instanceof MatchAllDocsQuery) {\n+                return existsQuery(context);\n             }\n-\n-\n+            Supplier<Automaton> deferredAutomatonSupplier = ()-> {\n+                RegExp regex = new RegExp(value, flags);\n+                return regex.toAutomaton(maxDeterminizedStates);\n+            };\n+\n+            AutomatonQueryOnBinaryDv verifyingQuery = new AutomatonQueryOnBinaryDv(name(), value, deferredAutomatonSupplier);\n+            \n+            // MatchAllButRequireVerificationQuery is a special case meaning the regex is reduced to a single\n+            // clause which we can't accelerate at all and needs verification. Example would be \"..\" \n+            if (approxNgramQuery instanceof MatchAllButRequireVerificationQuery) {\n+                return verifyingQuery;\n+            }\n+            \n+            // We can accelerate execution with the ngram query\n+            BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n+            verifyingBuilder.add(new BooleanClause(approxNgramQuery, Occur.MUST));\n+            verifyingBuilder.add(new BooleanClause(verifyingQuery, Occur.MUST));\n+            return verifyingBuilder.build();\n+        }\n+        \n+        // Convert a regular expression to a simplified query consisting of BooleanQuery and TermQuery objects\n+        // which captures as much of the logic as possible. Query can produce some false positives but shouldn't\n+        // produce any false negatives.\n+        // In addition to Term and BooleanQuery clauses there are MatchAllDocsQuery objects (e.g for .*) and\n+        // a RegExpQuery if we can't resolve to any of the above. \n+        // *  If an expression resolves to a single MatchAllDocsQuery eg .* then a match all shortcut is possible with \n+        //    no verification needed.     \n+        // * If an expression resolves to a RegExpQuery eg ?? then only the verification \n+        //   query is run.\n+        // * Anything else is a concrete query that should be run on the ngram index.\n+        public static Query toApproximationQuery(RegExp r) throws IllegalArgumentException {\n+            Query result = null;\n+            switch (r.kind) {\n+                case REGEXP_UNION:\n+                    result = createUnionQuery(r);\n+                    break;\n+                case REGEXP_CONCATENATION:\n+                    result = createConcatenationQuery(r);\n+                    break;\n+                case REGEXP_STRING:\n+                    String normalizedString = toLowerCase(r.s);\n+                    result = new TermQuery(new Term(\"\", normalizedString));\n+                    break;\n+                case REGEXP_CHAR:\n+                    String cs = Character.toString(r.c);\n+                    String normalizedChar = toLowerCase(cs);\n+                    result = new TermQuery(new Term(\"\", normalizedChar));\n+                    break;\n+                case REGEXP_REPEAT:\n+                    // Repeat is zero or more times so zero matches = match all\n+                    result = new MatchAllDocsQuery();\n+                    break;\n+                    \n+                case REGEXP_REPEAT_MIN:\n+                case REGEXP_REPEAT_MINMAX:\n+                    if (r.min > 0) {\n+                        result = toApproximationQuery(r.exp1);\n+                        if(result instanceof TermQuery) {\n+                            // Wrap the repeating expression so that it is not concatenated by a parent which concatenates\n+                            // plain TermQuery objects together. Boolean queries are interpreted as a black box and not\n+                            // concatenated.\n+                            BooleanQuery.Builder wrapper = new BooleanQuery.Builder();\n+                            wrapper.add(result, Occur.MUST);\n+                            result = wrapper.build();\n+                        }\n+                    } else {\n+                        // Expressions like (a){0,3} match empty string or up to 3 a's.\n+                        result = new MatchAllButRequireVerificationQuery();\n+                    }\n+                    break;\n+                case REGEXP_ANYSTRING:\n+                    // optimisation for .* queries - match all and no verification stage required.\n+                    result = new MatchAllDocsQuery();\n+                    break;\n+                // All other kinds of expression cannot be represented as a boolean or term query so return an object \n+                // that indicates verification is required\n+                case REGEXP_OPTIONAL:\n+                case REGEXP_INTERSECTION:\n+                case REGEXP_COMPLEMENT:\n+                case REGEXP_CHAR_RANGE:\n+                case REGEXP_ANYCHAR:\n+                case REGEXP_INTERVAL:\n+                case REGEXP_EMPTY: \n+                case REGEXP_AUTOMATON:\n+                    result = new MatchAllButRequireVerificationQuery();\n+                    break;\n+            }\n+            assert result != null; // All regex types are understood and translated to a query.\n+            return result;\n         }\n-\n-\n-        @Override\n-        public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) {\n-            PatternStructure patternStructure = new PatternStructure(wildcardPattern);\n-            ArrayList<String> tokens = new ArrayList<>();\n-\n-            for (int i = 0; i < patternStructure.fragments.length; i++) {\n-                String fragment = patternStructure.fragments[i];\n-                int fLength = fragment.length();\n-                if (fLength == 0) {\n-                    continue;\n-                }\n-\n-                // Add any start/end of string character\n-                if (i == 0 && patternStructure.openStart == false) {\n-                    // Start-of-string anchored (is not a leading wildcard)\n-                    fragment = TOKEN_START_OR_END_CHAR + fragment;\n+        \n+        private static Query createConcatenationQuery(RegExp r) {\n+            // Create ANDs of expressions plus collapse consecutive TermQuerys into single longer ones\n+            ArrayList<Query> queries = new ArrayList<>();\n+            findLeaves(r.exp1, Kind.REGEXP_CONCATENATION, queries);\n+            findLeaves(r.exp2, Kind.REGEXP_CONCATENATION, queries);\n+            BooleanQuery.Builder bAnd = new BooleanQuery.Builder();\n+            StringBuilder sequence = new StringBuilder();\n+            for (Query query : queries) {\n+                if (query instanceof TermQuery) {\n+                    TermQuery tq = (TermQuery) query;\n+                    sequence.append(tq.getTerm().text());\n+                } else {\n+                    if (sequence.length() > 0) {\n+                        bAnd.add(new TermQuery(new Term(\"\", sequence.toString())), Occur.MUST);\n+                        sequence = new StringBuilder();\n+                    }\n+                    bAnd.add(query, Occur.MUST);                    \n                 }\n-                if (patternStructure.openEnd == false && i == patternStructure.fragments.length - 1) {\n-                    // End-of-string anchored (is not a trailing wildcard)\n-                    fragment = fragment + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR;\n+            }\n+            if (sequence.length() > 0) {\n+                bAnd.add(new TermQuery(new Term(\"\", sequence.toString())), Occur.MUST);\n+            }\n+            BooleanQuery combined = bAnd.build();\n+            if (combined.clauses().size() > 0) {\n+                return combined;\n+            }\n+            // There's something in the regex we couldn't represent as a query - resort to a match all with verification \n+            return new MatchAllButRequireVerificationQuery();\n+            \n+        }\n+\n+        private static Query createUnionQuery(RegExp r) {\n+            // Create an OR of clauses\n+            ArrayList<Query> queries = new ArrayList<>();\n+            findLeaves(r.exp1, Kind.REGEXP_UNION, queries);\n+            findLeaves(r.exp2, Kind.REGEXP_UNION, queries);\n+            BooleanQuery.Builder bOr = new BooleanQuery.Builder();\n+            HashSet<Query> uniqueClauses = new HashSet<>();\n+            for (Query query : queries) {\n+                if (uniqueClauses.add(query)) {\n+                    bOr.add(query, Occur.SHOULD);\n                 }\n-                if (fragment.codePointCount(0, fragment.length()) <= NGRAM_SIZE) {\n-                    tokens.add(fragment);\n+            }\n+            if (uniqueClauses.size() > 0) {\n+                if (uniqueClauses.size() == 1) {\n+                    // Fully-understood ORs that collapse to a single term should be returned minus\n+                    // the BooleanQuery wrapper so that they might be concatenated.\n+                    // Helps turn [Pp][Oo][Ww][Ee][Rr][Ss][Hh][Ee][Ll][Ll] into \"powershell\"\n+                    // Each char pair eg (P OR p) can be normalized to (p) which can be a single term\n+                    return uniqueClauses.iterator().next();\n                 } else {\n-                    // Break fragment into multiple Ngrams\n-                    TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n-                    CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n-                    String lastUnusedToken = null;\n-                    try {\n-                        tokenizer.reset();\n-                        boolean takeThis = true;\n-                        // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n-                        // `123` and `345` - no need to search for 234. We take every other ngram.\n-                        while (tokenizer.incrementToken()) {\n-                            String tokenValue = termAtt.toString();\n-                            if (takeThis) {\n-                                tokens.add(tokenValue);\n-                            } else {\n-                                lastUnusedToken = tokenValue;\n+                    return bOr.build();\n+                }\n+            }\n+            // There's something in the regex we couldn't represent as a query - resort to a match all with verification \n+            return new MatchAllButRequireVerificationQuery();\n+        }\n+\n+        private static void findLeaves(RegExp exp, Kind kind, List<Query> queries) {\n+            if (exp.kind == kind) {\n+                findLeaves(exp.exp1, kind, queries);\n+                findLeaves( exp.exp2, kind, queries);\n+            } else {\n+                queries.add(toApproximationQuery(exp));\n+            }\n+        }        \n+        \n+        private static String toLowerCase(String string) {\n+            return lowercaseNormalizer.normalize(null, string).utf8ToString();\n+        }\n+        \n+        // Takes a BooleanQuery + TermQuery tree representing query logic and rewrites using ngrams of appropriate size.\n+        private Query rewriteBoolToNgramQuery(Query approxQuery) {\n+            //TODO optimise more intelligently so we: \n+            // 1) favour full-length term queries eg abc over short eg a* when pruning too many clauses.\n+            // 2) make MAX_CLAUSES_IN_APPROXIMATION_QUERY a global cap rather than per-boolean clause.\n+            if (approxQuery == null) {\n+                return null;\n+            }\n+            if (approxQuery instanceof BooleanQuery) {\n+                BooleanQuery bq = (BooleanQuery) approxQuery;\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                int clauseCount = 0;\n+                for (BooleanClause clause : bq) {\n+                    Query q = rewriteBoolToNgramQuery(clause.getQuery());\n+                    if (q != null) {\n+                        if (clause.getOccur().equals(Occur.MUST)) {\n+                            // Can't drop \"should\" clauses because it can elevate a sibling optional item\n+                            // to mandatory (shoulds with 1 clause) causing false negatives\n+                            // Dropping MUSTs increase false positives which are OK because are verified anyway.\n+                            clauseCount++;\n+                            if (clauseCount >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                                break;\n                             }\n-                            // alternate\n-                            takeThis = !takeThis;\n-                        }\n-                        if (lastUnusedToken != null) {\n-                            // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n-                            // `ake` to complete the logic.\n-                            tokens.add(lastUnusedToken);\n                         }\n-                        tokenizer.end();\n-                        tokenizer.close();\n-                    } catch (IOException ioe) {\n-                        throw new ElasticsearchParseException(\"Error parsing wildcard query pattern fragment [\" + fragment + \"]\");\n+                        rewritten.add(q, clause.getOccur());\n                     }\n                 }\n+                return simplify(rewritten.build());\n             }\n-\n-            if (patternStructure.isMatchAll()) {\n+            if (approxQuery instanceof TermQuery) {\n+                TermQuery tq = (TermQuery) approxQuery;\n+               \n+                //Remove simple terms that are only string beginnings or ends.\n+                String s = tq.getTerm().text();\n+                if (s.equals(WildcardFieldMapper.TOKEN_START_STRING) || s.equals(WildcardFieldMapper.TOKEN_END_STRING)) {\n+                    return new MatchAllButRequireVerificationQuery();\n+                }\n+                \n+                // Break term into tokens\n+                Set<String> tokens = new LinkedHashSet<>();\n+                getNgramTokens(tokens, s);\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (String string : tokens) {\n+                    addClause(string, rewritten, Occur.MUST);\n+                }\n+                return simplify(rewritten.build());\n+            }\n+            if (isMatchAll(approxQuery)) {\n+                return approxQuery;\n+            }\n+            throw new IllegalStateException(\"Invalid query type found parsing regex query:\" + approxQuery);\n+        }    \n+        \n+        static Query simplify(Query input) {\n+            if (input instanceof BooleanQuery == false) {\n+                return input;\n+            }\n+            BooleanQuery result = (BooleanQuery) input;\n+            if (result.clauses().size() == 0) {\n+                // A \".*\" clause can produce zero clauses in which case we return MatchAll\n                 return new MatchAllDocsQuery();\n             }\n-            BooleanQuery approximation = createApproximationQuery(tokens);\n-            if (approximation.clauses().size() > 1 || patternStructure.needsVerification()) {\n-                BooleanQuery.Builder verifyingBuilder = new BooleanQuery.Builder();\n-                verifyingBuilder.add(new BooleanClause(approximation, Occur.MUST));\n-                Automaton automaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern));\n-                verifyingBuilder.add(new BooleanClause(new AutomatonQueryOnBinaryDv(name(), wildcardPattern, automaton), Occur.MUST));\n-                return verifyingBuilder.build();\n+            if (result.clauses().size() == 1) {\n+                return simplify(result.clauses().get(0).getQuery());\n             }\n-            return approximation;\n-        }\n \n-        private BooleanQuery createApproximationQuery(ArrayList<String> tokens) {\n-            BooleanQuery.Builder bqBuilder = new BooleanQuery.Builder();\n-            if (tokens.size() <= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n-                for (String token : tokens) {\n-                    addClause(token, bqBuilder);\n+            // We may have a mix of MatchAll and concrete queries - assess if we can simplify\n+            int matchAllCount = 0;\n+            int verifyCount = 0;\n+            boolean allConcretesAreOptional = true;\n+            for (BooleanClause booleanClause : result.clauses()) {\n+                Query q = booleanClause.getQuery();\n+                if (q instanceof MatchAllDocsQuery) {\n+                    matchAllCount++;\n+                } else if (q instanceof MatchAllButRequireVerificationQuery) {\n+                    verifyCount++;\n+                } else {\n+                    // Concrete query\n+                    if (booleanClause.getOccur() != Occur.SHOULD) {\n+                        allConcretesAreOptional = false;\n+                    }\n                 }\n-                return bqBuilder.build();\n             }\n-            // Thin out the number of clauses using a selection spread evenly across the range\n-            float step = (float) (tokens.size() - 1) / (float) (MAX_CLAUSES_IN_APPROXIMATION_QUERY - 1); // set step size\n-            for (int i = 0; i < MAX_CLAUSES_IN_APPROXIMATION_QUERY; i++) {\n-                addClause(tokens.get(Math.round(step * i)), bqBuilder); // add each element of a position which is a multiple of step\n+\n+            if ((allConcretesAreOptional && matchAllCount > 0)) {\n+                // Any match all expression takes precedence over all optional concrete queries.\n+                return new MatchAllDocsQuery();\n+            }\n+\n+            if ((allConcretesAreOptional && verifyCount > 0)) {\n+                // Any match all expression that needs verification takes precedence over all optional concrete queries.\n+                return new MatchAllButRequireVerificationQuery();\n             }\n-            // TODO we can be smarter about pruning here. e.g.\n-            // * Avoid wildcard queries if there are sufficient numbers of other terms that are full 3grams that are cheaper term queries\n-            // * We can select terms on their scarcity rather than even spreads across the search string.\n \n-            return bqBuilder.build();\n+            // We have some mandatory concrete queries - strip out the superfluous match all expressions\n+            if (allConcretesAreOptional == false && matchAllCount + verifyCount > 0) {\n+                BooleanQuery.Builder rewritten = new BooleanQuery.Builder();\n+                for (BooleanClause booleanClause : result.clauses()) {\n+                    if (isMatchAll(booleanClause.getQuery()) == false) {\n+                        rewritten.add(booleanClause);\n+                    }\n+                }\n+                return simplify(rewritten.build());\n+            }\n+            return result;\n+        }\n+        \n+        \n+        static boolean isMatchAll(Query q) {\n+            return q instanceof MatchAllDocsQuery || q instanceof MatchAllButRequireVerificationQuery;\n         }\n \n-        private void addClause(String token, BooleanQuery.Builder bqBuilder) {\n+        protected void getNgramTokens(Set<String> tokens, String fragment) {\n+            if (fragment.equals(TOKEN_START_STRING) || fragment.equals(TOKEN_END_STRING)) {\n+                // If a regex is a form of match-all e.g. \".*\" we only produce the token start/end markers as search\n+                // terms which can be ignored.\n+                return;\n+            }\n+            // Break fragment into multiple Ngrams\n+            TokenStream tokenizer = WILDCARD_ANALYZER.tokenStream(name(), fragment);\n+            CharTermAttribute termAtt = tokenizer.addAttribute(CharTermAttribute.class);\n+            // If fragment length < NGRAM_SIZE then it is not emitted by token stream so need\n+            // to initialise with the value here\n+            String lastUnusedToken = fragment;\n+            try {\n+                tokenizer.reset();\n+                boolean takeThis = true;\n+                // minimise number of terms searched - eg for \"12345\" and 3grams we only need terms\n+                // `123` and `345` - no need to search for 234. We take every other ngram.\n+                while (tokenizer.incrementToken()) {\n+                    String tokenValue = termAtt.toString();\n+                    if (takeThis) {\n+                        tokens.add(tokenValue);\n+                        lastUnusedToken = null;\n+                    } else {\n+                        lastUnusedToken = tokenValue;\n+                    }\n+                    // alternate\n+                    takeThis = !takeThis;\n+                    if (tokens.size() >= MAX_CLAUSES_IN_APPROXIMATION_QUERY) {\n+                        lastUnusedToken = null;\n+                        break;\n+                    }\n+                }\n+                if (lastUnusedToken != null) {\n+                    // given `cake` and 3 grams the loop above would output only `cak` and we need to add trailing\n+                    // `ake` to complete the logic.\n+                    tokens.add(lastUnusedToken);\n+                }\n+                tokenizer.end();\n+                tokenizer.close();\n+            } catch (IOException ioe) {\n+                throw new ElasticsearchParseException(\"Error parsing wildcard regex pattern fragment [\" + fragment + \"]\");\n+            }\n+        }\n+        \n+\n+        private void addClause(String token, BooleanQuery.Builder bqBuilder, Occur occur) {\n             assert token.codePointCount(0, token.length()) <= NGRAM_SIZE;\n-            if (token.codePointCount(0, token.length()) == NGRAM_SIZE) {\n+            int tokenSize = token.codePointCount(0, token.length());\n+            if (tokenSize < 2 || token.equals(WildcardFieldMapper.TOKEN_END_STRING)) {\n+                // there's something concrete to be searched but it's too short\n+                // Require verification.\n+                bqBuilder.add(new BooleanClause(new MatchAllButRequireVerificationQuery(), occur));\n+                return;\n+            }\n+            if (tokenSize == NGRAM_SIZE) {\n                 TermQuery tq = new TermQuery(new Term(name(), token));\n-                bqBuilder.add(new BooleanClause(tq, Occur.MUST));\n+                bqBuilder.add(new BooleanClause(tq, occur));\n             } else {\n-                WildcardQuery wq = new WildcardQuery(new Term(name(), token + \"*\"));\n+                PrefixQuery wq = new PrefixQuery(new Term(name(), token));\n                 wq.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_REWRITE);\n-                bqBuilder.add(new BooleanClause(wq, Occur.MUST));\n+                bqBuilder.add(new BooleanClause(wq, occur));\n             }\n+        }\n \n+        @Override\n+        public Query fuzzyQuery(\n+            Object value,\n+            Fuzziness fuzziness,\n+            int prefixLength,\n+            int maxExpansions,\n+            boolean transpositions,\n+            QueryShardContext context\n+        ) {\n+            String searchTerm = BytesRefs.toString(value);\n+            String lowerSearchTerm = toLowerCase(searchTerm);\n+            \n+            try {", "originalCommit": "99c9f29346abc5f38f57ce6f08c58f846089b01b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9d2de29326d28435b0663db61ad3fd4d1161b147", "url": "https://github.com/elastic/elasticsearch/commit/9d2de29326d28435b0663db61ad3fd4d1161b147", "message": "Adds full equivalence for keyword field to the wildcard field. Regex, fuzzy, wildcard and prefix queries are all supported.\nAll queries use an approximation query backed by an automaton-based verification queries.\n\nCloses #54275", "committedDate": "2020-05-26T10:03:03Z", "type": "commit"}, {"oid": "5b4eb9ef1b15f8cb2bf76943dd8b538b8edc2861", "url": "https://github.com/elastic/elasticsearch/commit/5b4eb9ef1b15f8cb2bf76943dd8b538b8edc2861", "message": "Formatting tidy and tightened expectations in test results for acceleration queries", "committedDate": "2020-05-26T10:03:03Z", "type": "commit"}, {"oid": "11ea83ef354e3cdcd5787ed3cc905169f1ddb6d0", "url": "https://github.com/elastic/elasticsearch/commit/11ea83ef354e3cdcd5787ed3cc905169f1ddb6d0", "message": "Unused import", "committedDate": "2020-05-26T10:03:03Z", "type": "commit"}, {"oid": "893f960410bd4ae81484ff3ed5cbbfbf349e232d", "url": "https://github.com/elastic/elasticsearch/commit/893f960410bd4ae81484ff3ed5cbbfbf349e232d", "message": "Simplified BooleanQueries and made a* ngram queries prefix rather than wildcard.\nUpdated tests with simpler syntax and documented regexes that we\u2019d like to improve on, showing current suboptimal queries and the future form we\u2019d like to see.", "committedDate": "2020-05-26T10:03:03Z", "type": "commit"}, {"oid": "e8c2a7109ede3b75e27329d5f841d00cf1615310", "url": "https://github.com/elastic/elasticsearch/commit/e8c2a7109ede3b75e27329d5f841d00cf1615310", "message": "Add options to rewrite to match none, match all or match all with verification.", "committedDate": "2020-05-26T10:03:03Z", "type": "commit"}, {"oid": "fcc0527d30ee4c4bae82efc47753ae33539194df", "url": "https://github.com/elastic/elasticsearch/commit/fcc0527d30ee4c4bae82efc47753ae33539194df", "message": "Remove system.out call", "committedDate": "2020-05-26T10:03:03Z", "type": "commit"}, {"oid": "ad3f1b48f792810bd374d9bb98642b308e479367", "url": "https://github.com/elastic/elasticsearch/commit/ad3f1b48f792810bd374d9bb98642b308e479367", "message": "Addressing latest review comments by adding\n* Unlimited prefix length.\n* Delayed Autotmaton creation\n* FuzzyQuery tests", "committedDate": "2020-05-26T10:03:04Z", "type": "commit"}, {"oid": "2c3ce869c68efec22f5a649f05e58d764ee2540c", "url": "https://github.com/elastic/elasticsearch/commit/2c3ce869c68efec22f5a649f05e58d764ee2540c", "message": "Checkstyle fix", "committedDate": "2020-05-26T10:03:04Z", "type": "commit"}, {"oid": "31dc81f588cbb30eca08bfe2bc2ecf45546c3f2e", "url": "https://github.com/elastic/elasticsearch/commit/31dc81f588cbb30eca08bfe2bc2ecf45546c3f2e", "message": "Checkstyle", "committedDate": "2020-05-26T10:03:04Z", "type": "commit"}, {"oid": "458cdffc0c2a48a716378ec536eef5422218e81a", "url": "https://github.com/elastic/elasticsearch/commit/458cdffc0c2a48a716378ec536eef5422218e81a", "message": "Simplified ApproximateRegExp, removing dependency on WildcardFieldMapper class. All query simplification logic is now consolidated in the WildcardFieldMapper class.", "committedDate": "2020-05-26T10:03:04Z", "type": "commit"}, {"oid": "2b705dbe59c662e23c5f78c690e4103e5e514076", "url": "https://github.com/elastic/elasticsearch/commit/2b705dbe59c662e23c5f78c690e4103e5e514076", "message": "Add test logic for @ syntax", "committedDate": "2020-05-26T10:03:04Z", "type": "commit"}, {"oid": "59191217f49e7238a9695a0d60b744ace5480cee", "url": "https://github.com/elastic/elasticsearch/commit/59191217f49e7238a9695a0d60b744ace5480cee", "message": "Removed fork of Lucene\u2019s RegEx now we have access to its internals", "committedDate": "2020-05-26T10:03:04Z", "type": "commit"}, {"oid": "fc1266b59288177e6ea5ee15dc53a8d1b3e642f0", "url": "https://github.com/elastic/elasticsearch/commit/fc1266b59288177e6ea5ee15dc53a8d1b3e642f0", "message": "Enabled integration tests, removed redundant license declaration and whitespace", "committedDate": "2020-05-26T10:03:04Z", "type": "commit"}, {"oid": "fc1266b59288177e6ea5ee15dc53a8d1b3e642f0", "url": "https://github.com/elastic/elasticsearch/commit/fc1266b59288177e6ea5ee15dc53a8d1b3e642f0", "message": "Enabled integration tests, removed redundant license declaration and whitespace", "committedDate": "2020-05-26T10:03:04Z", "type": "forcePushed"}, {"oid": "177c7f81c99e011be418eea30de2600ed935d41c", "url": "https://github.com/elastic/elasticsearch/commit/177c7f81c99e011be418eea30de2600ed935d41c", "message": "Reverse last gradle change - can\u2019t safely enable integration tests until this PR is backported to 7x", "committedDate": "2020-05-26T10:19:32Z", "type": "commit"}]}