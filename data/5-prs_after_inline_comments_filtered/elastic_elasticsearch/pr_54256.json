{"pr_number": 54256, "pr_title": "Improve Snapshot Abort Behavior", "pr_createdAt": "2020-03-26T09:42:14Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/54256", "timeline": [{"oid": "f4c1419398d7271cf5836583443210375dab1551", "url": "https://github.com/elastic/elasticsearch/commit/f4c1419398d7271cf5836583443210375dab1551", "message": "Improve Snapshot Abort Behavior\n\nThis commit improves the behavior of aborting snapshots and by that fixes\nsome extremely rare test failures.\n\nImprovements:\n1. When aborting a snapshot while it is in the `INIT` stage we do not need\nto ever delete anything from the repository because nothing is written to the\nrepo during INIT any more (in the past running deletes for these snapshots made\nsense because we were writing `snap-` and `meta-` blobs during the `INIT` step).\n2. Do not try to finalize snapshots that never moved past `INIT`. Same reason as\nwith the first step. If we never moved past `INIT` no data was written to the repo\nso no need to now write a useless entry for the aborted snapshot to `index-N`.\nThis is especially true, since the reason the snapshot was aborted during `INIT` was\na delete call so the useless empty snapshot just added to `index-N` would be removed\nby the subsequent delete that is still waiting anyway.\n3. if after aborting a snapshot we wait for it to finish we should not try deleting it\nif it failed. If the snapshot failed it means it did not become part of the most recent\n`RepositoryData` so a delete for it will needlessly fail with a confusing message about\nthat snapshot being missing or concurrent repository modification.", "committedDate": "2020-03-26T09:31:25Z", "type": "commit"}, {"oid": "56f11059f053608b5926499176236ba1304ec4c3", "url": "https://github.com/elastic/elasticsearch/commit/56f11059f053608b5926499176236ba1304ec4c3", "message": "bck", "committedDate": "2020-03-26T09:41:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODQzODI3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r398438279", "bodyText": "This whole step is kind of stupid now in 7.6+ because we don't write anything during INIT. Ideally (and I'd do that in a follow-up), we shouldn't move the snapshot to ABORTED here but instead just drop it from the cluster state right away and resolve the listener in beginSnapshot to not have the redundant CS updates from moving to ABORTED and then removing the snapshot from the CS in beginSnapshot.", "author": "original-brownbear", "createdAt": "2020-03-26T09:45:54Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1292,6 +1300,7 @@ public ClusterState execute(ClusterState currentState) {\n                         shards = snapshotEntry.shards();\n                         assert shards.isEmpty();\n                         failure = \"Snapshot was aborted during initialization\";\n+                        abortedDuringInit = true;", "originalCommit": "56f11059f053608b5926499176236ba1304ec4c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYyNjA2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r398626065", "bodyText": "The JavaDocs on beginSnapshot should be updated as well, as it claims that the snapshot is created in the repo.", "author": "ywelsch", "createdAt": "2020-03-26T14:41:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODQzODI3OQ=="}], "type": "inlineReview"}, {"oid": "53e9ae10e3572bfd634e6375e9cfbc88e78ed021", "url": "https://github.com/elastic/elasticsearch/commit/53e9ae10e3572bfd634e6375e9cfbc88e78ed021", "message": "nope", "committedDate": "2020-03-26T10:48:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODQ5OTc3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r398499779", "bodyText": "This change will help debug future issues+test-failures more easily. If we're aborting we get a log sequence as below now (without the newly added information it was impossible to tell if the deletes were due to rerunning the delete after finishing the snapshot or due to REST client retries):\n[2020-03-26T11:45:02,542][INFO ][o.e.s.SnapshotsService   ] [asyncIntegTest-0] deleting snapshot [test_repository:test_snapshot/P5-mkax-TwupQvH2i4i6Kw] assuming repository generation [-1] and with priory [NORMAL]\n[2020-03-26T11:45:02,640][INFO ][o.e.s.SnapshotsService   ] [asyncIntegTest-0] snapshot [test_repository:test_snapshot/P5-mkax-TwupQvH2i4i6Kw] completed with state [SUCCESS]\n[2020-03-26T11:45:02,656][INFO ][o.e.s.SnapshotsService   ] [asyncIntegTest-0] deleting snapshot [test_repository:test_snapshot/P5-mkax-TwupQvH2i4i6Kw] assuming repository generation [0] and with priory [IMMEDIATE]", "author": "original-brownbear", "createdAt": "2020-03-26T11:27:50Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1227,12 +1233,15 @@ public void deleteSnapshot(final String repositoryName, final String snapshotNam\n      */\n     private void deleteSnapshot(final Snapshot snapshot, final ActionListener<Void> listener, final long repositoryStateId,\n                                 final boolean immediatePriority) {\n-        logger.info(\"deleting snapshot [{}]\", snapshot);\n         Priority priority = immediatePriority ? Priority.IMMEDIATE : Priority.NORMAL;\n+        logger.info(\"deleting snapshot [{}] assuming repository generation [{}] and with priory [{}]\",", "originalCommit": "53e9ae10e3572bfd634e6375e9cfbc88e78ed021", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYyMDA4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r398620086", "bodyText": "s/priory/priority", "author": "ywelsch", "createdAt": "2020-03-26T14:33:25Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1227,12 +1233,15 @@ public void deleteSnapshot(final String repositoryName, final String snapshotNam\n      */\n     private void deleteSnapshot(final Snapshot snapshot, final ActionListener<Void> listener, final long repositoryStateId,\n                                 final boolean immediatePriority) {\n-        logger.info(\"deleting snapshot [{}]\", snapshot);\n         Priority priority = immediatePriority ? Priority.IMMEDIATE : Priority.NORMAL;\n+        logger.info(\"deleting snapshot [{}] assuming repository generation [{}] and with priory [{}]\",", "originalCommit": "53e9ae10e3572bfd634e6375e9cfbc88e78ed021", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYzNzE5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r398637193", "bodyText": "I'm not sure what cases this is supposed to cover. In particular, I'm wondering about the case where the current node failed (e.g. got disconnected from the rest of the cluster) and another master completed the snapshot. How are the listeners in snapshotCompletionListeners informed?", "author": "ywelsch", "createdAt": "2020-03-26T14:55:13Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1356,19 +1366,14 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n                             );\n                         },\n                         e -> {\n-                            logger.warn(\"deleted snapshot failed - deleting files\", e);\n-                            threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(() -> {\n-                                try {\n-                                    deleteSnapshot(snapshot.getRepository(), snapshot.getSnapshotId().getName(), listener, true);\n-                                } catch (SnapshotMissingException smex) {\n-                                    logger.info(() -> new ParameterizedMessage(\n-                                        \"Tried deleting in-progress snapshot [{}], but it could not be found after failing to abort.\",\n-                                        smex.getSnapshotName()), e);\n-                                    listener.onFailure(new SnapshotException(snapshot,\n-                                        \"Tried deleting in-progress snapshot [\" + smex.getSnapshotName() + \"], but it \" +\n-                                            \"could not be found after failing to abort.\", smex));\n-                                }\n-                            });\n+                            if (abortedDuringInit) {\n+                                logger.debug(() -> new ParameterizedMessage(\"Snapshot [{}] was aborted during INIT\", snapshot), e);\n+                                listener.onResponse(null);\n+                            } else {\n+                                logger.warn(\"deleted snapshot failed\", e);\n+                                listener.onFailure(\n+                                    new SnapshotMissingException(snapshot.getRepository(), snapshot.getSnapshotId(), e));", "originalCommit": "53e9ae10e3572bfd634e6375e9cfbc88e78ed021", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODY1NzEzMA==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r398657130", "bodyText": "I'm not sure what cases this is supposed to cover.\n\nThe only way I see of getting here is the one in the linked test failure.\nMaster tried to finalize the snapshot and ran into an IOException (or other but I don't see which one).\nThat said ... you're right, on master fail-over we can leak the snapshotCompletionListeners  (urgh ... I wonder if that explain the odd test failure of a hanging snapshot once a month) I'll open another PR for that?", "author": "original-brownbear", "createdAt": "2020-03-26T15:19:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYzNzE5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODcxNTIxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r398715219", "bodyText": "#54286 should do it here but I'd like a few hours of SnapshotsResiliencyTests to be sure :)", "author": "original-brownbear", "createdAt": "2020-03-26T16:32:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYzNzE5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTEyOTQ2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r399129461", "bodyText": "By wrapping the original exception here, I wonder if we potentially turn a failing master (FailedToCommitClusterStateException / NotMasterException) into a SnapshotMissingException", "author": "ywelsch", "createdAt": "2020-03-27T09:19:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYzNzE5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTEzNzE1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r399137152", "bodyText": "That's a good point ... I wonder if we should just pass those two exceptions (failed to commit/ not master) as they come without wrapping. At this point, the delete has not in fact put anything into the cluster state aside from aborting the snapshot. So if we get here and run into one of those master fail-over exceptions, then retrying the delete request (master transport action will do that here) seems what we would actually want to happen right?", "author": "original-brownbear", "createdAt": "2020-03-27T09:32:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYzNzE5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE2MTAwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r399161001", "bodyText": "Pushed 2a2422b for the above", "author": "original-brownbear", "createdAt": "2020-03-27T10:15:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODYzNzE5Mw=="}], "type": "inlineReview"}, {"oid": "7155cc72d9e22cc46a8a925e867a6b844844d389", "url": "https://github.com/elastic/elasticsearch/commit/7155cc72d9e22cc46a8a925e867a6b844844d389", "message": "fixes", "committedDate": "2020-03-26T15:17:35Z", "type": "commit"}, {"oid": "026dda41ed0b6680ffee565682fce9a0726f744e", "url": "https://github.com/elastic/elasticsearch/commit/026dda41ed0b6680ffee565682fce9a0726f744e", "message": "Merge remote-tracking branch 'elastic/master' into repo-abort-snapshot-bug", "committedDate": "2020-03-27T10:09:46Z", "type": "commit"}, {"oid": "2a2422b86923b496eb3d0e63bcb9c7aa2600d32c", "url": "https://github.com/elastic/elasticsearch/commit/2a2422b86923b496eb3d0e63bcb9c7aa2600d32c", "message": "retry on master failover", "committedDate": "2020-03-27T10:14:29Z", "type": "commit"}, {"oid": "92e0db0ec35f708d5aabdd27de2544fe92243cbb", "url": "https://github.com/elastic/elasticsearch/commit/92e0db0ec35f708d5aabdd27de2544fe92243cbb", "message": "Merge remote-tracking branch 'elastic/master' into repo-abort-snapshot-bug", "committedDate": "2020-03-27T10:14:40Z", "type": "commit"}, {"oid": "e373a8834f6d0675ff0c6c99479ba3b08820920e", "url": "https://github.com/elastic/elasticsearch/commit/e373a8834f6d0675ff0c6c99479ba3b08820920e", "message": "CS", "committedDate": "2020-03-27T10:23:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4NjE3OA==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r399986178", "bodyText": "I think I would rather always bubble up the original exception, marking the deletion as failed (and have the client retry). This listener here can be called in a range of situations, and I don't think that in all cases it denotes that the snapshot has been deleted or fully aborted (especially because with waitForSnapshot we are supposed to wait until the snapshot has truly completed, whether exceptional or not).", "author": "ywelsch", "createdAt": "2020-03-30T07:42:34Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1356,19 +1365,21 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n                             );\n                         },\n                         e -> {\n-                            logger.warn(\"deleted snapshot failed - deleting files\", e);\n-                            threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(() -> {\n-                                try {\n-                                    deleteSnapshot(snapshot.getRepository(), snapshot.getSnapshotId().getName(), listener, true);\n-                                } catch (SnapshotMissingException smex) {\n-                                    logger.info(() -> new ParameterizedMessage(\n-                                        \"Tried deleting in-progress snapshot [{}], but it could not be found after failing to abort.\",\n-                                        smex.getSnapshotName()), e);\n-                                    listener.onFailure(new SnapshotException(snapshot,\n-                                        \"Tried deleting in-progress snapshot [\" + smex.getSnapshotName() + \"], but it \" +\n-                                            \"could not be found after failing to abort.\", smex));\n+                            if (abortedDuringInit) {\n+                                logger.debug(() -> new ParameterizedMessage(\"Snapshot [{}] was aborted during INIT\", snapshot), e);\n+                                listener.onResponse(null);\n+                            } else {\n+                                if (ExceptionsHelper.unwrap(e, NotMasterException.class, FailedToCommitClusterStateException.class)\n+                                    != null) {\n+                                    logger.warn(\"master failover before deleted snapshot could complete\", e);\n+                                    // Just pass the exception to the transport handler as is so it is retried on the new master\n+                                    listener.onFailure(e);\n+                                } else {\n+                                    logger.warn(\"deleted snapshot failed\", e);\n+                                    listener.onFailure(\n+                                        new SnapshotMissingException(snapshot.getRepository(), snapshot.getSnapshotId(), e));", "originalCommit": "e373a8834f6d0675ff0c6c99479ba3b08820920e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk5NzYyMg==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r399997622", "bodyText": "and I don't think that in all cases it denotes that the snapshot has been deleted or fully aborted\n\nI don't think that's true. With the exception of the master failover exceptions now handled above, all other exceptions method that snapshot finalization failed. Since we never retry the snapshot finalization except for on master fail-over we can be sure that the snapshot will never be created at this point.\nIf snapshot finalization failed, then the snapshot has not been finalized in the repo (i.e. is not part of the latest index-N) and hence will always throw SnapshotMissingException in deleteSnapshot`.\nWithout this change, the situation of a failed finalization will behave differently based on timing:\nIf the finalization fails before the delete comes in, then we get the SnapshotMissingException / 404.\nIf it fails after the delete comes in, we throw some other SnapshotException wrapping the SnapshotMissingException and needlessly try to find the snapshot in the repo.\n=> I think we can cleanly leverage the fact that recent changes made things deterministic here and not run deletes that we know will end up in a 404?\nNote: The reason I'm adding these simplifications is (outside of fixing some tests) so that the changes for concurrent snapshots become more obvious. For concurrent snapshot operations we will have to leverage the now very deterministic behavior around snapshot finalizations (and them failing) as well in exactly this way.", "author": "original-brownbear", "createdAt": "2020-03-30T08:03:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4NjE3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAxNjc1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/54256#discussion_r400016757", "bodyText": "OK, I tried to follow all paths through the code, and couldn't find an issue. The whole listener notification logic seems super brittle though.", "author": "ywelsch", "createdAt": "2020-03-30T08:35:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4NjE3OA=="}], "type": "inlineReview"}]}