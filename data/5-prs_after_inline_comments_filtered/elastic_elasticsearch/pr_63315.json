{"pr_number": 63315, "pr_title": "[Transform] improve continuous transform date_histogram group_by with ingest timestamps", "pr_createdAt": "2020-10-06T10:44:00Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/63315", "timeline": [{"oid": "544ef6d5515473f316813f9f739a5ed1972ac815", "url": "https://github.com/elastic/elasticsearch/commit/544ef6d5515473f316813f9f739a5ed1972ac815", "message": "optimize continuous data histogram group_by for other time fields\nindependent of sync, this allows the usage of ingest timestamp in continuous\nmode", "committedDate": "2020-10-06T12:33:53Z", "type": "forcePushed"}, {"oid": "02ba4cc766a391fa8ced0f65ca130603d8fefcd9", "url": "https://github.com/elastic/elasticsearch/commit/02ba4cc766a391fa8ced0f65ca130603d8fefcd9", "message": "remove workaround", "committedDate": "2020-10-06T15:27:23Z", "type": "forcePushed"}, {"oid": "e1a6d6132e1ab2cac66136c1025fb869cf258f85", "url": "https://github.com/elastic/elasticsearch/commit/e1a6d6132e1ab2cac66136c1025fb869cf258f85", "message": "optimize continuous data histogram group_by for other time fields\nindependent of sync, this allows the usage of ingest timestamp in continuous\nmode", "committedDate": "2020-10-07T12:11:00Z", "type": "commit"}, {"oid": "82983c0e5ec87ba1b31c5b564e912263ccdda000", "url": "https://github.com/elastic/elasticsearch/commit/82983c0e5ec87ba1b31c5b564e912263ccdda000", "message": "remove workaround", "committedDate": "2020-10-07T12:11:00Z", "type": "commit"}, {"oid": "d7c6de5366d5aac72426db167fd9fd6c66d9b17d", "url": "https://github.com/elastic/elasticsearch/commit/d7c6de5366d5aac72426db167fd9fd6c66d9b17d", "message": "improve comment", "committedDate": "2020-10-07T12:11:00Z", "type": "commit"}, {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b", "url": "https://github.com/elastic/elasticsearch/commit/7ccf8367b08b1645940012d238d983b2691f9a3b", "message": "extend test case to randomly add an additional group_by on terms", "committedDate": "2020-10-07T12:11:00Z", "type": "commit"}, {"oid": "7ccf8367b08b1645940012d238d983b2691f9a3b", "url": "https://github.com/elastic/elasticsearch/commit/7ccf8367b08b1645940012d238d983b2691f9a3b", "message": "extend test case to randomly add an additional group_by on terms", "committedDate": "2020-10-07T12:11:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1NTMwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501255309", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);\n          \n          \n            \n                        logger.debug(() -> new ParameterizedMessage(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder));\n          \n      \n    \n    \n  \n\nIt is probably wise to not needlessly create strings and such, especially calling toString on the sourceBuilder", "author": "benwtrent", "createdAt": "2020-10-07T19:23:43Z", "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/TransformIndexer.java", "diffHunk": "@@ -863,7 +863,7 @@ private SearchSourceBuilder buildUpdateQuery(SearchSourceBuilder sourceBuilder)\n         // if its either the 1st run or not continuous, do not apply extra filters\n         if (nextCheckpoint.getCheckpoint() == 1 || isContinuous() == false) {\n             sourceBuilder.query(queryBuilder);\n-            logger.trace(\"running query: {}\", sourceBuilder);\n+            logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);", "originalCommit": "7ccf8367b08b1645940012d238d983b2691f9a3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1NTUxMw==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501255513", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);\n          \n          \n            \n                    logger.debug(() -> new ParameterizedMessage(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder));", "author": "benwtrent", "createdAt": "2020-10-07T19:24:06Z", "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/TransformIndexer.java", "diffHunk": "@@ -880,7 +880,7 @@ private SearchSourceBuilder buildUpdateQuery(SearchSourceBuilder sourceBuilder)\n         }\n \n         sourceBuilder.query(filteredQuery);\n-        logger.trace(\"running query: {}\", sourceBuilder);\n+        logger.debug(\"[{}] Querying for data: {}\", getJobId(), sourceBuilder);", "originalCommit": "7ccf8367b08b1645940012d238d983b2691f9a3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MzY4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501273689", "bodyText": "I don't know why this is necessary. If compositeAggregation == null, that means it was never set.\nConsequently, a composite aggregation should never be executed for this changes check.\nWhy can't afterKey just stay null?", "author": "benwtrent", "createdAt": "2020-10-07T19:58:22Z", "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -492,20 +670,26 @@ public boolean processSearchResponse(final SearchResponse searchResponse) {\n             return true;\n         }\n \n-        final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n-\n-        Collection<? extends Bucket> buckets = agg.getBuckets();\n-        afterKey = agg.afterKey();\n+        boolean isDone = true;\n \n-        if (buckets.isEmpty()) {\n-            return true;\n+        // collect changes from composite buckets\n+        if (compositeAggregation != null) {\n+            final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n+            Collection<? extends Bucket> buckets = agg.getBuckets();\n+            afterKey = agg.afterKey();\n+            for (FieldCollector fieldCollector : fieldCollectors.values()) {\n+                isDone &= fieldCollector.collectChangesFromCompositeBuckets(buckets);\n+            }\n+        } else {\n+            afterKey = AFTER_KEY_MAGIC;", "originalCommit": "7ccf8367b08b1645940012d238d983b2691f9a3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NjQxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501276419", "bodyText": "If the changes detector is not using a composite agg, the caller shouldn't attempt to iterate on it. Consequently a null after key would be acceptable as it would only be called once.", "author": "benwtrent", "createdAt": "2020-10-07T20:03:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MzY4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTQ5Njg1NA==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501496854", "bodyText": "This is basically a state problem. I need to know whether the \"changes query\" has been executed or not, that means this is conceptually a boolean hasRun. However, this state is managed by the indexer, not by the change collector. E.g. what if something fails, the change collector doesn't know how to handle failures, but the indexer does. With that magic I let the indexer control this state object instead of having a boolean in the collector.\nHaving that said, I fully agree that this looks hacky. I will look at it again, suggestions welcome.", "author": "hendrikmuhs", "createdAt": "2020-10-08T07:15:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MzY4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMwNjMxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r503306311", "bodyText": "Could these methods return IterationResult instead of a map?\nIt seems we always need two pieces of info:\n\nAre we done?\nWhat is the position?\n\nThat might prove a nicer abstraction than having a special bucket position.\nAlso, I am not sure in what case the bucket processor (when not utilizing composite aggs), should ever be called twice. It seems weird to me as it should indicate that it is done :/.", "author": "benwtrent", "createdAt": "2020-10-12T13:44:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MzY4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTUzNzcxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r505537719", "bodyText": "by moving the afterKey to the indexer - where it actually belongs - I was able to remove the bool and the method to get the key. Note that the key can not be persisted right after the response, but only after the real changes got indexed. This is necessary in the failure scenario.\nThe \"special bucket position\" is part of making change collection scalable, it's required if more than 65k (10k in older versions) changes have been found.\nIn case you have 1 field collector that uses min/max and 1 that uses the composite agg(terms), the collector only adds the min/max aggs on the 1st run, but not on following runs in case the terms collector requires it.\nI think the challenge is that we combine 2 different ways of change collection in 1 call.", "author": "hendrikmuhs", "createdAt": "2020-10-15T13:23:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MzY4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NDA2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501274069", "bodyText": "Is this the purpose of the AFTER_KEY_MAGIC ?", "author": "benwtrent", "createdAt": "2020-10-07T19:59:05Z", "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -449,10 +625,12 @@ private CompositeBucketsChangeCollector(\n     public SearchSourceBuilder buildChangesQuery(SearchSourceBuilder sourceBuilder, Map<String, Object> position, int pageSize) {\n         sourceBuilder.size(0);\n         for (FieldCollector fieldCollector : fieldCollectors.values()) {\n-            AggregationBuilder aggregationForField = fieldCollector.aggregateChanges();\n \n-            if (aggregationForField != null) {\n-                sourceBuilder.aggregation(aggregationForField);\n+            // add aggregations, but only for the 1st run\n+            if (position == null || position.isEmpty()) {", "originalCommit": "7ccf8367b08b1645940012d238d983b2691f9a3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTUwNTc3NA==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501505774", "bodyText": "yes\nNote that AFTER_KEY_MAGIC is only used if there is only a field collector for date_histogram using aggs, but not if you combine date_histogram with terms.", "author": "hendrikmuhs", "createdAt": "2020-10-08T07:32:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NDA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NDc3OA==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501274778", "bodyText": "This is a strange optimization to me. If you only want to optionally construct the compositeAggregationBuilder make it a CachedSupplier that is only called once or something.", "author": "benwtrent", "createdAt": "2020-10-07T20:00:25Z", "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -523,13 +707,22 @@ public boolean isOptimized() {\n         return fieldCollectors.values().stream().anyMatch(FieldCollector::isOptimized);\n     }\n \n+    @Override\n+    public boolean queryForChanges() {\n+        return fieldCollectors.values().stream().anyMatch(FieldCollector::queryForChanges);\n+    }\n+\n     public static ChangeCollector buildChangeCollector(\n-        @Nullable CompositeAggregationBuilder compositeAggregationBuilder,\n+        CompositeAggregationBuilder compositeAggregationBuilder,\n         Map<String, SingleGroupSource> groups,\n         String synchronizationField\n     ) {\n         Map<String, FieldCollector> fieldCollectors = createFieldCollectors(groups, synchronizationField);\n-        return new CompositeBucketsChangeCollector(compositeAggregationBuilder, fieldCollectors);\n+\n+        // after building the field collectors we know whether we need the composite aggregation\n+        boolean keepCompositeAggregatonBuilder = fieldCollectors.values().stream().anyMatch(FieldCollector::requiresCompositeSources);\n+\n+        return new CompositeBucketsChangeCollector(keepCompositeAggregatonBuilder ? compositeAggregationBuilder : null, fieldCollectors);", "originalCommit": "7ccf8367b08b1645940012d238d983b2691f9a3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTUwMTU0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501501549", "bodyText": "true, however I am not convinced a CachedSupplier is necessary. It makes it more complicated than necessary. Note that this is constructed exactly once after _start, so performance does not matter.", "author": "hendrikmuhs", "createdAt": "2020-10-08T07:24:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NDc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NTMzNg==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501275336", "bodyText": "Why are you doing this null check? Wouldn't it be better to rely on if any of fieldCollector items requiresCompositeSources ?", "author": "benwtrent", "createdAt": "2020-10-07T20:01:27Z", "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -492,20 +670,26 @@ public boolean processSearchResponse(final SearchResponse searchResponse) {\n             return true;\n         }\n \n-        final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n-\n-        Collection<? extends Bucket> buckets = agg.getBuckets();\n-        afterKey = agg.afterKey();\n+        boolean isDone = true;\n \n-        if (buckets.isEmpty()) {\n-            return true;\n+        // collect changes from composite buckets\n+        if (compositeAggregation != null) {", "originalCommit": "7ccf8367b08b1645940012d238d983b2691f9a3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTUwMjc0OA==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501502748", "bodyText": "yes, that's an alternative, however the null check is more robust regarding bugs.", "author": "hendrikmuhs", "createdAt": "2020-10-08T07:26:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NTMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY2OTYwMA==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r501669600", "bodyText": "The only reason it is more robust in regarding bugs is because it is set to null explicitly earlier.\nI am not sure that is even necessary. It seems to me that some external thing is knowledgable of the internal logic of this system and is making changes.\nThe separation of concerns seem to be bleeding across a bit.", "author": "benwtrent", "createdAt": "2020-10-08T12:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NTMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTU0MDQ0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r505540447", "bodyText": "I moved the creation of the composite agg part into the change collector, that way I also addressed your concern regarding creating the composite but than throwing it away. The also made the checks a bit easier, although we still need some as you can have any combination of aggs, depending on which collectors are used.", "author": "hendrikmuhs", "createdAt": "2020-10-15T13:26:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NTMzNg=="}], "type": "inlineReview"}, {"oid": "c4535c13d6aa7fb390a7a7c184426957bb09979a", "url": "https://github.com/elastic/elasticsearch/commit/c4535c13d6aa7fb390a7a7c184426957bb09979a", "message": "Apply suggestions from code review\n\nCo-authored-by: Benjamin Trent <ben.w.trent@gmail.com>", "committedDate": "2020-10-08T07:04:08Z", "type": "commit"}, {"oid": "e73645fc9a9337fff7ec34328e65b6fcaf4176ae", "url": "https://github.com/elastic/elasticsearch/commit/e73645fc9a9337fff7ec34328e65b6fcaf4176ae", "message": "Merge branch 'master' of github.com:elastic/elasticsearch into transform-dh-opt2", "committedDate": "2020-10-08T09:59:26Z", "type": "commit"}, {"oid": "67ecae55243004aceafa8ada55cfe169075e1c45", "url": "https://github.com/elastic/elasticsearch/commit/67ecae55243004aceafa8ada55cfe169075e1c45", "message": "improve separation between composite bucket change collector and pivot", "committedDate": "2020-10-14T11:59:30Z", "type": "commit"}, {"oid": "3689b71849b32c324833b5f0fa42e0860b180579", "url": "https://github.com/elastic/elasticsearch/commit/3689b71849b32c324833b5f0fa42e0860b180579", "message": "simplify keeping track of the change collector bucket position by moving it to the indexer", "committedDate": "2020-10-15T11:48:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjEzODE3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r506138177", "bodyText": "[optional] Alternatively, you could express it this way:\n        assert (compositeAggregation == null) == (compositeAggResults == null);\n\nbut I'm not sure which one is more readable.", "author": "przemekwitek", "createdAt": "2020-10-16T07:51:26Z", "path": "x-pack/plugin/transform/src/main/java/org/elasticsearch/xpack/transform/transforms/pivot/CompositeBucketsChangeCollector.java", "diffHunk": "@@ -486,26 +672,42 @@ public QueryBuilder buildFilterQuery(long lastCheckpointTimestamp, long nextchec\n     }\n \n     @Override\n-    public boolean processSearchResponse(final SearchResponse searchResponse) {\n+    public Map<String, Object> processSearchResponse(final SearchResponse searchResponse) {\n         final Aggregations aggregations = searchResponse.getAggregations();\n         if (aggregations == null) {\n-            return true;\n+            return null;\n         }\n \n-        final CompositeAggregation agg = aggregations.get(compositeAggregation.getName());\n-\n-        Collection<? extends Bucket> buckets = agg.getBuckets();\n-        afterKey = agg.afterKey();\n+        // every collector reports if the collection of changes is done after processing, if all are done, the indexer\n+        // will not run a query for change collection again\n+        boolean isDone = true;\n \n-        if (buckets.isEmpty()) {\n-            return true;\n+        // collect changes from aggregations added by field collectors\n+        for (FieldCollector fieldCollector : fieldCollectors.values()) {\n+            isDone &= fieldCollector.collectChangesFromAggregations(aggregations);\n         }\n \n-        for (FieldCollector fieldCollector : fieldCollectors.values()) {\n-            fieldCollector.collectChanges(buckets);\n+        // collect changes from composite buckets\n+        final CompositeAggregation compositeAggResults = aggregations.get(COMPOSITE_AGGREGATION_NAME);\n+\n+        // xor: either both must exist or not exist\n+        assert (compositeAggregation == null ^ compositeAggResults == null) == false;", "originalCommit": "3689b71849b32c324833b5f0fa42e0860b180579", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjEzOTU1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/63315#discussion_r506139556", "bodyText": "static", "author": "przemekwitek", "createdAt": "2020-10-16T07:53:19Z", "path": "x-pack/plugin/transform/qa/single-node-tests/src/javaRestTest/java/org/elasticsearch/xpack/transform/integration/continuous/DateHistogramGroupByOtherTimeFieldIT.java", "diffHunk": "@@ -0,0 +1,234 @@\n+package org.elasticsearch.xpack.transform.integration.continuous;\n+\n+import org.elasticsearch.action.search.SearchRequest;\n+import org.elasticsearch.action.search.SearchResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.client.transform.transforms.DestConfig;\n+import org.elasticsearch.client.transform.transforms.SourceConfig;\n+import org.elasticsearch.client.transform.transforms.TransformConfig;\n+import org.elasticsearch.client.transform.transforms.pivot.DateHistogramGroupSource;\n+import org.elasticsearch.client.transform.transforms.pivot.GroupConfig;\n+import org.elasticsearch.client.transform.transforms.pivot.PivotConfig;\n+import org.elasticsearch.client.transform.transforms.pivot.TermsGroupSource;\n+import org.elasticsearch.common.xcontent.support.XContentMapValues;\n+import org.elasticsearch.search.SearchHit;\n+import org.elasticsearch.search.aggregations.AggregatorFactories;\n+import org.elasticsearch.search.aggregations.BucketOrder;\n+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregationBuilder;\n+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;\n+import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;\n+import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.terms.Terms;\n+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+\n+import java.io.IOException;\n+import java.time.ZonedDateTime;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.lessThanOrEqualTo;\n+\n+/**\n+ * Testcase for date histogram group_by on _different_ fields than used for sync\n+ */\n+public class DateHistogramGroupByOtherTimeFieldIT extends ContinuousTestCase {\n+    private static final String NAME = \"continuous-date-histogram-pivot-other-timefield-test\";\n+\n+    private final boolean addGroupByTerms;\n+\n+    public DateHistogramGroupByOtherTimeFieldIT() {\n+        addGroupByTerms = randomBoolean();\n+    }\n+\n+    @Override\n+    public TransformConfig createConfig() {\n+        TransformConfig.Builder transformConfigBuilder = new TransformConfig.Builder();\n+        addCommonBuilderParameters(transformConfigBuilder);\n+        transformConfigBuilder.setSource(new SourceConfig(CONTINUOUS_EVENTS_SOURCE_INDEX));\n+        transformConfigBuilder.setDest(new DestConfig(NAME, INGEST_PIPELINE));\n+        transformConfigBuilder.setId(NAME);\n+        PivotConfig.Builder pivotConfigBuilder = new PivotConfig.Builder();\n+        GroupConfig.Builder groups = new GroupConfig.Builder().groupBy(\n+            \"second\",\n+            new DateHistogramGroupSource.Builder().setField(\"metric-timestamp\")\n+                .setInterval(new DateHistogramGroupSource.FixedInterval(DateHistogramInterval.SECOND))\n+                .build()\n+        );\n+        if (addGroupByTerms) {\n+            groups.groupBy(\"event\", new TermsGroupSource.Builder().setField(\"event\").build());\n+        }\n+        pivotConfigBuilder.setGroups(groups.build());\n+        AggregatorFactories.Builder aggregations = new AggregatorFactories.Builder();\n+        addCommonAggregations(aggregations);\n+\n+        pivotConfigBuilder.setAggregations(aggregations);\n+        transformConfigBuilder.setPivotConfig(pivotConfigBuilder.build());\n+        return transformConfigBuilder.build();\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return NAME;\n+    }\n+\n+    @Override\n+    public void testIteration(int iteration) throws IOException {\n+        SearchRequest searchRequestSource = new SearchRequest(CONTINUOUS_EVENTS_SOURCE_INDEX).allowPartialSearchResults(false)\n+            .indicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN);\n+        SearchSourceBuilder sourceBuilderSource = new SearchSourceBuilder().size(0);\n+        DateHistogramAggregationBuilder bySecond = new DateHistogramAggregationBuilder(\"second\").field(\"metric-timestamp\")\n+            .fixedInterval(DateHistogramInterval.SECOND)\n+            .order(BucketOrder.key(true));\n+\n+        if (addGroupByTerms) {\n+            TermsAggregationBuilder terms = new TermsAggregationBuilder(\"event\").size(1000).field(\"event\").order(BucketOrder.key(true));\n+            bySecond.subAggregation(terms);\n+        }\n+        sourceBuilderSource.aggregation(bySecond);\n+        searchRequestSource.source(sourceBuilderSource);\n+        SearchResponse responseSource = search(searchRequestSource);\n+\n+        SearchRequest searchRequestDest = new SearchRequest(NAME).allowPartialSearchResults(false)\n+            .indicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN);\n+        SearchSourceBuilder sourceBuilderDest = new SearchSourceBuilder().size(10000).sort(\"second\");\n+        if (addGroupByTerms) {\n+            sourceBuilderDest.sort(\"event\");\n+        }\n+\n+        searchRequestDest.source(sourceBuilderDest);\n+        SearchResponse responseDest = search(searchRequestDest);\n+\n+        if (addGroupByTerms) {\n+            assertResultsGroupByDateHistogramAndTerms(iteration, responseSource, responseDest);\n+        } else {\n+            assertResultsGroupByDateHistogram(iteration, responseSource, responseDest);\n+        }\n+    }\n+\n+    private void assertResultsGroupByDateHistogram(int iteration, SearchResponse responseSource, SearchResponse responseDest) {\n+        List<? extends Bucket> buckets = ((Histogram) responseSource.getAggregations().get(\"second\")).getBuckets();\n+        Iterator<? extends Bucket> sourceIterator = buckets.iterator();\n+        Iterator<SearchHit> destIterator = responseDest.getHits().iterator();\n+\n+        while (sourceIterator.hasNext() && destIterator.hasNext()) {\n+            Bucket bucket = sourceIterator.next();\n+            SearchHit searchHit = destIterator.next();\n+            Map<String, Object> source = searchHit.getSourceAsMap();\n+\n+            Long transformBucketKey = (Long) XContentMapValues.extractValue(\"second\", source);\n+\n+            // aggs return buckets with 0 doc_count while composite aggs skip over them\n+            while (bucket.getDocCount() == 0L) {\n+                assertTrue(sourceIterator.hasNext());\n+                bucket = sourceIterator.next();\n+            }\n+            long bucketKey = ((ZonedDateTime) bucket.getKey()).toEpochSecond() * 1000;\n+\n+            // test correctness, the results from the aggregation and the results from the transform should be the same\n+            assertThat(\n+                \"Buckets did not match, source: \" + source + \", expected: \" + bucketKey + \", iteration: \" + iteration,\n+                transformBucketKey,\n+                equalTo(bucketKey)\n+            );\n+            assertThat(\n+                \"Doc count did not match, source: \" + source + \", expected: \" + bucket.getDocCount() + \", iteration: \" + iteration,\n+                XContentMapValues.extractValue(\"count\", source),\n+                equalTo(Double.valueOf(bucket.getDocCount()))\n+            );\n+\n+            // transform should only rewrite documents that require it\n+            assertThat(\n+                \"Ingest run: \"\n+                    + XContentMapValues.extractValue(INGEST_RUN_FIELD, source)\n+                    + \" did not match max run: \"\n+                    + XContentMapValues.extractValue(MAX_RUN_FIELD, source)\n+                    + \", iteration: \"\n+                    + iteration,\n+                // we use a fixed_interval of `1s`, the transform runs every `1s`, a bucket might be recalculated at the next run\n+                // but should NOT be recalculated for the 2nd/3rd/... run\n+                Double.valueOf((Integer) XContentMapValues.extractValue(INGEST_RUN_FIELD, source)) - (Double) XContentMapValues\n+                    .extractValue(MAX_RUN_FIELD, source),\n+                is(lessThanOrEqualTo(1.0))\n+            );\n+\n+        }\n+        assertFalse(sourceIterator.hasNext());\n+        assertFalse(destIterator.hasNext());\n+    }\n+\n+    private void assertResultsGroupByDateHistogramAndTerms(int iteration, SearchResponse responseSource, SearchResponse responseDest) {\n+        List<? extends Bucket> buckets = ((Histogram) responseSource.getAggregations().get(\"second\")).getBuckets();\n+\n+        List<Map<String, Object>> flattenedBuckets = new ArrayList<>();\n+        for (Bucket b : buckets) {\n+            if (b.getDocCount() == 0) {\n+                continue;\n+            }\n+            long second = ((ZonedDateTime) b.getKey()).toEpochSecond() * 1000;\n+            List<? extends Terms.Bucket> terms = ((Terms) b.getAggregations().get(\"event\")).getBuckets();\n+            for (Terms.Bucket t : terms) {\n+                flattenedBuckets.add(flattenedResult(second, t.getKeyAsString(), t.getDocCount()));\n+            }\n+        }\n+\n+        Iterator<Map<String, Object>> sourceIterator = flattenedBuckets.iterator();\n+        Iterator<SearchHit> destIterator = responseDest.getHits().iterator();\n+\n+        while (sourceIterator.hasNext() && destIterator.hasNext()) {\n+            Map<String, Object> bucket = sourceIterator.next();\n+\n+            SearchHit searchHit = destIterator.next();\n+            Map<String, Object> source = searchHit.getSourceAsMap();\n+\n+            Long transformBucketKey = (Long) XContentMapValues.extractValue(\"second\", source);\n+\n+            // test correctness, the results from the aggregation and the results from the transform should be the same\n+            assertThat(\n+                \"Buckets did not match, source: \" + source + \", expected: \" + bucket.get(\"second\") + \", iteration: \" + iteration,\n+                transformBucketKey,\n+                equalTo(bucket.get(\"second\"))\n+            );\n+            assertThat(\n+                \"Doc count did not match, source: \" + source + \", expected: \" + bucket.get(\"count\") + \", iteration: \" + iteration,\n+                XContentMapValues.extractValue(\"count\", source),\n+                equalTo(Double.valueOf(((Long) bucket.get(\"count\"))))\n+            );\n+            assertThat(\n+                \"Term did not match, source: \" + source + \", expected: \" + bucket.get(\"event\") + \", iteration: \" + iteration,\n+                XContentMapValues.extractValue(\"event\", source),\n+                equalTo(bucket.get(\"event\"))\n+            );\n+\n+            // transform should only rewrite documents that require it\n+            assertThat(\n+                \"Ingest run: \"\n+                    + XContentMapValues.extractValue(INGEST_RUN_FIELD, source)\n+                    + \" did not match max run: \"\n+                    + XContentMapValues.extractValue(MAX_RUN_FIELD, source)\n+                    + \", iteration: \"\n+                    + iteration,\n+                // we use a fixed_interval of `1s`, the transform runs every `1s`, a bucket might be recalculated at the next run\n+                // but should NOT be recalculated for the 2nd/3rd/... run\n+                Double.valueOf((Integer) XContentMapValues.extractValue(INGEST_RUN_FIELD, source)) - (Double) XContentMapValues\n+                    .extractValue(MAX_RUN_FIELD, source),\n+                is(lessThanOrEqualTo(1.0))\n+            );\n+        }\n+        assertFalse(sourceIterator.hasNext());\n+        assertFalse(destIterator.hasNext());\n+    }\n+\n+    private Map<String, Object> flattenedResult(long second, String event, long count) {", "originalCommit": "3689b71849b32c324833b5f0fa42e0860b180579", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "23bfac4e91546ebce1a79f74174f29c8c1225f1a", "url": "https://github.com/elastic/elasticsearch/commit/23bfac4e91546ebce1a79f74174f29c8c1225f1a", "message": "apply review suggestion", "committedDate": "2020-10-16T09:11:53Z", "type": "commit"}]}