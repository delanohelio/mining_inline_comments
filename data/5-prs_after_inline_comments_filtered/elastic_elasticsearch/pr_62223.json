{"pr_number": 62223, "pr_title": "Request-level circuit breaker support on coordinating nodes", "pr_createdAt": "2020-09-10T13:29:35Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62223", "timeline": [{"oid": "934a3d8315d07ec14f1fe6cbb0c35b930c05165c", "url": "https://github.com/elastic/elasticsearch/commit/934a3d8315d07ec14f1fe6cbb0c35b930c05165c", "message": "Request-level circuit breaker support on coordinating nodes\n\nThis commit allows coordinating node to account the memory used to perform partial and final reduce of\naggregations in the request circuit breaker. The search coordinator adds the memory that it used to save\nand reduce the results of shard aggregations in the request circuit breaker. Before any partial or final\nreduce, the memory needed to reduce the aggregations is estimated and a CircuitBreakingException} is thrown\nif exceeds the maximum memory allowed in this breaker.\nThis size is estimated as roughly 1.5 times the size of the serialized aggregations that need to be reduced.\nThis estimation can be completely off for some aggregations but it is corrected with the real size after\nthe reduce completes.\nIf the reduce is successful, we update the circuit breaker to remove the size of the source aggregations\nand replace the estimation with the serialized size of the newly reduced result.\n\nAs a follow up we could trigger partial reduces based on the memory accounted in the circuit breaker instead\nof relying on a static number of shard responses. A simpler follow up that could be done in the mean time is\nto [reduce the default batch reduce size](https://github.com/elastic/elasticsearch/issues/51857) of blocking\nsearch request to a more sane number.\n\nCloses #37182", "committedDate": "2020-09-10T13:27:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjMzODE1OQ==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r486338159", "bodyText": "Note for reviewer: I moved these tests in https://github.com/elastic/elasticsearch/pull/62223/files#diff-0721d29fdc234c0f88a9019057ea55bd", "author": "jimczi", "createdAt": "2020-09-10T13:31:33Z", "path": "server/src/test/java/org/elasticsearch/action/search/TransportSearchActionSingleNodeTests.java", "diffHunk": "@@ -1,177 +0,0 @@\n-/*\n- * Licensed to Elasticsearch under one or more contributor", "originalCommit": "934a3d8315d07ec14f1fe6cbb0c35b930c05165c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwNDkwNw==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493404907", "bodyText": "it is a bit of a shame that these go from single node tests to full blown IT tests, what is the reasoning behind this choice?", "author": "javanna", "createdAt": "2020-09-23T10:08:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjMzODE1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQzNTA4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493435086", "bodyText": "I regrouped the search action tests in a single IT class. I agree that these tests may not require the full IT but they are grouped with other tests that require it so I thought that it makes sense to move them here.", "author": "jimczi", "createdAt": "2020-09-23T10:41:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjMzODE1OQ=="}], "type": "inlineReview"}, {"oid": "2a719fa7b44c62cab64018a9668d93d925dcbc14", "url": "https://github.com/elastic/elasticsearch/commit/2a719fa7b44c62cab64018a9668d93d925dcbc14", "message": "style", "committedDate": "2020-09-10T13:32:41Z", "type": "commit"}, {"oid": "74671a36a292295a1f684eca0dc1c68ccf32c057", "url": "https://github.com/elastic/elasticsearch/commit/74671a36a292295a1f684eca0dc1c68ccf32c057", "message": "Merge branch 'master' into enhancements/reduce_aggs_circuit_breaker", "committedDate": "2020-09-10T13:35:21Z", "type": "commit"}, {"oid": "62decd38a510b50a5e0d8ff2756fab5b6bbb09ff", "url": "https://github.com/elastic/elasticsearch/commit/62decd38a510b50a5e0d8ff2756fab5b6bbb09ff", "message": "style", "committedDate": "2020-09-10T14:00:27Z", "type": "commit"}, {"oid": "e8831994fe54b756e801cb9bfea19f52d5ce78a8", "url": "https://github.com/elastic/elasticsearch/commit/e8831994fe54b756e801cb9bfea19f52d5ce78a8", "message": "fix unit test", "committedDate": "2020-09-10T14:49:56Z", "type": "commit"}, {"oid": "021f89c4f4af7a0fa8a0ba7ae07f78318b753750", "url": "https://github.com/elastic/elasticsearch/commit/021f89c4f4af7a0fa8a0ba7ae07f78318b753750", "message": "Merge branch 'master' into enhancements/reduce_aggs_circuit_breaker", "committedDate": "2020-09-15T07:49:59Z", "type": "commit"}, {"oid": "603de6048600252f58cdd1ddc1eafa4253773dce", "url": "https://github.com/elastic/elasticsearch/commit/603de6048600252f58cdd1ddc1eafa4253773dce", "message": "Don't serialize partial reduce eagerly\n\nThis commit removes the serialization of partial reduce in order to speed up the merges when\nthe batch reduce size is smaller than the number of shards in the request.\nThe estimation of the size of partial reduce is still based on the binary size (serialized form) but we\nkeep the full java object and estimate the size with a counting stream output.\nFinally this change adds a benchmark for the reduce of nested terms aggs. This benchmark was used to\noptimize the code in this PR.", "committedDate": "2020-09-15T11:45:28Z", "type": "commit"}, {"oid": "8c19f4820d42c1c45991b16c167e8bb1c1266780", "url": "https://github.com/elastic/elasticsearch/commit/8c19f4820d42c1c45991b16c167e8bb1c1266780", "message": "add a seed for the aggs benchmark", "committedDate": "2020-09-15T12:25:18Z", "type": "commit"}, {"oid": "24e708f57bec44901ac425a9c49c9d5dc56288a2", "url": "https://github.com/elastic/elasticsearch/commit/24e708f57bec44901ac425a9c49c9d5dc56288a2", "message": "fix ut", "committedDate": "2020-09-15T19:19:38Z", "type": "commit"}, {"oid": "4ae09cbc849276365342df3d7deb68bc7be50100", "url": "https://github.com/elastic/elasticsearch/commit/4ae09cbc849276365342df3d7deb68bc7be50100", "message": "Merge branch 'master' into enhancements/reduce_aggs_circuit_breaker", "committedDate": "2020-09-15T20:02:10Z", "type": "commit"}, {"oid": "599f01d19d723370fb53133a7427a8af3575e62b", "url": "https://github.com/elastic/elasticsearch/commit/599f01d19d723370fb53133a7427a8af3575e62b", "message": "adapt new code after master merge", "committedDate": "2020-09-15T20:45:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMzNzg5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490337891", "bodyText": "I believe plugins don't need to take a Settings as an argument.", "author": "nik9000", "createdAt": "2020-09-17T15:20:48Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/action/search/TransportSearchIT.java", "diffHunk": "@@ -19,22 +19,242 @@\n \n package org.elasticsearch.action.search;\n \n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.search.CollectionTerminatedException;\n+import org.apache.lucene.search.ScoreMode;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;\n+import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.action.support.WriteRequest;\n+import org.elasticsearch.client.Client;\n import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.breaker.CircuitBreaker;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.AtomicArray;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.QueryShardContext;\n import org.elasticsearch.index.query.RangeQueryBuilder;\n import org.elasticsearch.index.shard.IndexShard;\n import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.SearchPlugin;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.SearchHit;\n+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;\n+import org.elasticsearch.search.aggregations.AggregationBuilder;\n+import org.elasticsearch.search.aggregations.Aggregations;\n+import org.elasticsearch.search.aggregations.Aggregator;\n+import org.elasticsearch.search.aggregations.AggregatorBase;\n+import org.elasticsearch.search.aggregations.AggregatorFactories;\n+import org.elasticsearch.search.aggregations.AggregatorFactory;\n+import org.elasticsearch.search.aggregations.CardinalityUpperBound;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.LeafBucketCollector;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongTerms;\n+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder;\n+import org.elasticsearch.search.aggregations.metrics.InternalMax;\n+import org.elasticsearch.search.aggregations.support.ValueType;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.fetch.FetchSubPhase;\n+import org.elasticsearch.search.fetch.FetchSubPhaseProcessor;\n+import org.elasticsearch.search.internal.SearchContext;\n import org.elasticsearch.test.ESIntegTestCase;\n \n+import java.io.IOException;\n+import java.util.Collection;\n import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n \n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n import static org.hamcrest.Matchers.containsString;\n import static org.hamcrest.Matchers.equalTo;\n \n public class TransportSearchIT extends ESIntegTestCase {\n+    public static class TestPlugin extends Plugin implements SearchPlugin {\n+        public TestPlugin(Settings settings) {}", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQwMTE1MA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490401150", "bodyText": "++, thanks", "author": "jimczi", "createdAt": "2020-09-17T16:33:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMzNzg5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMzOTE5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490339197", "bodyText": "I love the name of this method. It makes me think \"I absolutely don't care about the contents of these docs.\" They are just \"some docs\".", "author": "nik9000", "createdAt": "2020-09-17T15:22:43Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/action/search/TransportSearchIT.java", "diffHunk": "@@ -104,4 +324,274 @@ public void testSearchIdle() throws Exception {\n             assertThat(resp.getHits().getTotalHits().value, equalTo(2L));\n         });\n     }\n+\n+    public void testCircuitBreakerReduceFail() throws Exception {\n+        int numShards = randomIntBetween(1, 10);\n+        indexSomeDocs(\"test\", numShards, numShards*3);", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0MDAyMA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490340020", "bodyText": "All the new tests make me happy!", "author": "nik9000", "createdAt": "2020-09-17T15:23:45Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/action/search/TransportSearchIT.java", "diffHunk": "@@ -104,4 +324,274 @@ public void testSearchIdle() throws Exception {\n             assertThat(resp.getHits().getTotalHits().value, equalTo(2L));\n         });\n     }\n+\n+    public void testCircuitBreakerReduceFail() throws Exception {\n+        int numShards = randomIntBetween(1, 10);\n+        indexSomeDocs(\"test\", numShards, numShards*3);\n+\n+        {\n+            final AtomicArray<Boolean> responses = new AtomicArray<>(10);\n+            final CountDownLatch latch = new CountDownLatch(10);\n+            for (int i = 0; i < 10; i++) {\n+                int batchReduceSize = randomIntBetween(2, Math.max(numShards + 1, 3));\n+                SearchRequest request = client().prepareSearch(\"test\")\n+                    .addAggregation(new TestAggregationBuilder(\"test\"))\n+                    .setBatchedReduceSize(batchReduceSize)\n+                    .request();\n+                final int index = i;\n+                client().search(request, new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(SearchResponse response) {\n+                        responses.set(index, true);\n+                        latch.countDown();\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        responses.set(index, false);\n+                        latch.countDown();\n+                    }\n+                });\n+            }\n+            latch.await();\n+            assertThat(responses.asList().size(), equalTo(10));\n+            for (boolean resp : responses.asList()) {\n+                assertTrue(resp);\n+            }\n+            assertBusy(() -> assertThat(requestBreakerUsed(), equalTo(0L)));\n+        }\n+\n+        try {\n+            Settings settings = Settings.builder()\n+                .put(\"indices.breaker.request.limit\", \"1b\")\n+                .build();\n+            assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings));\n+            final Client client = client();\n+            assertBusy(() -> {\n+                Exception exc = expectThrows(Exception.class, () -> client.prepareSearch(\"test\")\n+                    .addAggregation(new TestAggregationBuilder(\"test\"))\n+                    .get());\n+                assertThat(exc.getCause().getMessage(), containsString(\"<reduce_aggs>\"));\n+            });\n+\n+            final AtomicArray<Exception> exceptions = new AtomicArray<>(10);\n+            final CountDownLatch latch = new CountDownLatch(10);\n+            for (int i = 0; i < 10; i++) {\n+                int batchReduceSize = randomIntBetween(2, Math.max(numShards + 1, 3));\n+                SearchRequest request = client().prepareSearch(\"test\")\n+                    .addAggregation(new TestAggregationBuilder(\"test\"))\n+                    .setBatchedReduceSize(batchReduceSize)\n+                    .request();\n+                final int index = i;\n+                client().search(request, new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(SearchResponse response) {\n+                        latch.countDown();\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception exc) {\n+                        exceptions.set(index, exc);\n+                        latch.countDown();\n+                    }\n+                });\n+            }\n+            latch.await();\n+            assertThat(exceptions.asList().size(), equalTo(10));\n+            for (Exception exc : exceptions.asList()) {\n+                assertThat(exc.getCause().getMessage(), containsString(\"<reduce_aggs>\"));\n+            }\n+            assertBusy(() -> assertThat(requestBreakerUsed(), equalTo(0L)));\n+        } finally {\n+            Settings settings = Settings.builder()\n+                .putNull(\"indices.breaker.request.limit\")\n+                .build();\n+            assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings));\n+        }\n+    }\n+\n+    public void testCircuitBreakerFetchFail() throws Exception {\n+        int numShards = randomIntBetween(1, 10);\n+        int numDocs = numShards*10;\n+        indexSomeDocs(\"boom\", numShards, numDocs);\n+\n+        final AtomicArray<Exception> exceptions = new AtomicArray<>(10);\n+        final CountDownLatch latch = new CountDownLatch(10);\n+        for (int i = 0; i < 10; i++) {\n+            int batchReduceSize = randomIntBetween(2, Math.max(numShards + 1, 3));\n+            SearchRequest request = client().prepareSearch(\"boom\")\n+                .setBatchedReduceSize(batchReduceSize)\n+                .setAllowPartialSearchResults(false)\n+                .request();\n+            final int index = i;\n+            client().search(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(SearchResponse response) {\n+                    latch.countDown();\n+                }\n+\n+                @Override\n+                public void onFailure(Exception exc) {\n+                    exceptions.set(index, exc);\n+                    latch.countDown();\n+                }\n+            });\n+        }\n+        latch.await();\n+        assertThat(exceptions.asList().size(), equalTo(10));\n+        for (Exception exc : exceptions.asList()) {\n+            assertThat(exc.getCause().getMessage(), containsString(\"boom\"));\n+        }\n+        assertBusy(() -> assertThat(requestBreakerUsed(), equalTo(0L)));\n+    }\n+\n+    private void indexSomeDocs(String indexName, int numberOfShards, int numberOfDocs) {\n+        createIndex(indexName, Settings.builder().put(\"index.number_of_shards\", numberOfShards).build());\n+\n+        for (int i = 0; i < numberOfDocs; i++) {\n+            IndexResponse indexResponse  = client().prepareIndex(indexName)\n+                .setSource(\"number\", randomInt())\n+                .get();\n+            assertEquals(RestStatus.CREATED, indexResponse.status());\n+        }\n+        client().admin().indices().prepareRefresh(indexName).get();\n+    }\n+\n+    private long requestBreakerUsed() {\n+        NodesStatsResponse stats = client().admin().cluster().prepareNodesStats().setBreaker(true).get();\n+        long estimated = 0;\n+        for (NodeStats nodeStats : stats.getNodes()) {\n+            estimated += nodeStats.getBreaker().getStats(CircuitBreaker.REQUEST).getEstimated();\n+        }\n+        return estimated;\n+    }\n+\n+    /**\n+     * A test aggregation that doesn't consume circuit breaker memory when running on shards.\n+     * It is used to test the behavior of the circuit breaker when reducing multiple aggregations\n+     * together (coordinator node).\n+     */\n+    private static class TestAggregationBuilder extends AbstractAggregationBuilder<TestAggregationBuilder> {\n+        static final String NAME = \"test\";\n+\n+        private static final ObjectParser<TestAggregationBuilder, String> PARSER =\n+            ObjectParser.fromBuilder(NAME, TestAggregationBuilder::new);\n+\n+        TestAggregationBuilder(String name) {\n+            super(name);\n+        }\n+\n+        TestAggregationBuilder(StreamInput input) throws IOException {\n+            super(input);\n+        }\n+\n+\n+        @Override\n+        protected void doWriteTo(StreamOutput out) throws IOException {\n+            // noop\n+        }\n+\n+        @Override\n+        protected AggregatorFactory doBuild(QueryShardContext queryShardContext,\n+                                            AggregatorFactory parent,\n+                                            AggregatorFactories.Builder subFactoriesBuilder) throws IOException {\n+            return new AggregatorFactory(name, queryShardContext, parent, subFactoriesBuilder, metadata) {\n+                @Override\n+                protected Aggregator createInternal(SearchContext searchContext,\n+                                                    Aggregator parent,\n+                                                    CardinalityUpperBound cardinality,\n+                                                    Map<String, Object> metadata) throws IOException {\n+                    return new TestAggregator(name, parent, searchContext);\n+                }\n+            };\n+        }\n+\n+        @Override\n+        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {\n+            return builder;\n+        }\n+\n+        @Override\n+        protected AggregationBuilder shallowCopy(AggregatorFactories.Builder factoriesBuilder, Map<String, Object> metadata) {\n+            return new TestAggregationBuilder(name);\n+        }\n+\n+        @Override\n+        public BucketCardinality bucketCardinality() {\n+            return BucketCardinality.NONE;\n+        }\n+\n+        @Override\n+        public String getType() {\n+            return \"test\";\n+        }\n+    }\n+\n+    /**\n+     * A test aggregator that extends {@link Aggregator} instead of {@link AggregatorBase}\n+     * to avoid tripping the circuit breaker when executing on a shard.\n+     */\n+    private static class TestAggregator extends Aggregator {\n+        private final String name;\n+        private final Aggregator parent;\n+        private final SearchContext context;\n+\n+        private TestAggregator(String name, Aggregator parent, SearchContext context) {\n+            this.name = name;\n+            this.parent = parent;\n+            this.context = context;\n+        }\n+\n+\n+        @Override\n+        public String name() {\n+            return name;\n+        }\n+\n+        @Override\n+        public SearchContext context() {\n+            return context;\n+        }\n+\n+        @Override\n+        public Aggregator parent() {\n+            return parent;\n+        }\n+\n+        @Override\n+        public Aggregator subAggregator(String name) {\n+            return null;\n+        }\n+\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            return new InternalAggregation[] {\n+                new InternalMax(name(), Double.NaN, DocValueFormat.RAW, Collections.emptyMap())\n+            };\n+        }\n+\n+        @Override\n+        public InternalAggregation buildEmptyAggregation() {\n+            return new InternalMax(name(), Double.NaN, DocValueFormat.RAW, Collections.emptyMap());\n+        }\n+\n+        @Override\n+        public void close() {}\n+\n+        @Override\n+        public LeafBucketCollector getLeafCollector(LeafReaderContext ctx) throws IOException {\n+            throw new CollectionTerminatedException();\n+        }\n+\n+        @Override\n+        public ScoreMode scoreMode() {\n+            return ScoreMode.COMPLETE_NO_SCORES;\n+        }\n+\n+        @Override\n+        public void preCollection() throws IOException {}\n+\n+        @Override\n+        public void postCollection() throws IOException {}\n+    }", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0NDExNg==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490344116", "bodyText": "The aggregations should always be serialized here, right? I wonder if it is worth asserting that or something.", "author": "nik9000", "createdAt": "2020-09-17T15:29:14Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -193,49 +206,106 @@ private MergeResult partialReduce(MergeTask task,\n             processedShards.add(new SearchShard(target.getClusterAlias(), target.getShardId()));\n         }\n         progressListener.notifyPartialReduce(processedShards, topDocsStats.getTotalHits(), newAggs, numReducePhases);\n-        return new MergeResult(processedShards, newTopDocs, newAggs);\n+        return new MergeResult(processedShards, newTopDocs, newAggs, hasAggs ? newAggs.getBinarySize() : 0);\n     }\n \n     public int getNumReducePhases() {\n         return pendingMerges.numReducePhases;\n     }\n \n-    private class PendingMerges {\n-        private final int bufferSize;\n-\n-        private int index;\n-        private final QuerySearchResult[] buffer;\n+    private class PendingMerges implements Releasable {\n+        private final int batchReduceSize;\n+        private final List<QuerySearchResult> buffer = new ArrayList<>();\n         private final List<SearchShard> emptyResults = new ArrayList<>();\n+        // the memory that is accounted in the circuit breaker for this consumer\n+        private volatile long circuitBreakerBytes;\n+        // the memory that is currently used in the buffer\n+        private volatile long aggsCurrentBufferSize;\n+        private volatile long maxAggsCurrentBufferSize = 0;\n \n-        private final TopDocsStats topDocsStats;\n-        private MergeResult mergeResult;\n         private final ArrayDeque<MergeTask> queue = new ArrayDeque<>();\n         private final AtomicReference<MergeTask> runningTask = new AtomicReference<>();\n         private final AtomicReference<Exception> failure = new AtomicReference<>();\n \n-        private boolean hasPartialReduce;\n-        private int numReducePhases;\n+        private final TopDocsStats topDocsStats;\n+        private volatile MergeResult mergeResult;\n+        private volatile boolean hasPartialReduce;\n+        private volatile int numReducePhases;\n \n-        PendingMerges(int bufferSize, int trackTotalHitsUpTo) {\n-            this.bufferSize = bufferSize;\n+        PendingMerges(int batchReduceSize, int trackTotalHitsUpTo) {\n+            this.batchReduceSize = batchReduceSize;\n             this.topDocsStats = new TopDocsStats(trackTotalHitsUpTo);\n-            this.buffer = new QuerySearchResult[bufferSize];\n         }\n \n-        public boolean hasFailure() {\n+        @Override\n+        public synchronized void close() {\n+            assert hasPendingMerges() == false : \"cannot close with partial reduce in-flight\";\n+            if (hasFailure()) {\n+                assert circuitBreakerBytes == 0;\n+                return;\n+            }\n+            assert circuitBreakerBytes >= 0;\n+            circuitBreaker.addWithoutBreaking(-circuitBreakerBytes);\n+            circuitBreakerBytes = 0;\n+        }\n+\n+        synchronized Exception getFailure() {\n+            return failure.get();\n+        }\n+\n+        boolean hasFailure() {\n             return failure.get() != null;\n         }\n \n-        public synchronized boolean hasPendingMerges() {\n+        boolean hasPendingMerges() {\n             return queue.isEmpty() == false || runningTask.get() != null;\n         }\n \n-        public synchronized void sortBuffer() {\n-            if (index > 0) {\n-                Arrays.sort(buffer, 0, index, Comparator.comparingInt(QuerySearchResult::getShardIndex));\n+        void sortBuffer() {\n+            if (buffer.size() > 0) {\n+                Collections.sort(buffer, Comparator.comparingInt(QuerySearchResult::getShardIndex));\n             }\n         }\n \n+        synchronized long addWithoutBreaking(long size) {\n+            circuitBreaker.addWithoutBreaking(size);\n+            circuitBreakerBytes += size;\n+            maxAggsCurrentBufferSize = Math.max(maxAggsCurrentBufferSize, circuitBreakerBytes);\n+            return circuitBreakerBytes;\n+        }\n+\n+        synchronized long addEstimateAndMaybeBreak(long estimatedSize) {\n+            circuitBreaker.addEstimateBytesAndMaybeBreak(estimatedSize, \"<reduce_aggs>\");\n+            circuitBreakerBytes += estimatedSize;\n+            maxAggsCurrentBufferSize = Math.max(maxAggsCurrentBufferSize, circuitBreakerBytes);\n+            return circuitBreakerBytes;\n+        }\n+\n+        /**\n+         * Returns the size of the serialized aggregation that is contained in the\n+         * provided {@link QuerySearchResult}.\n+         */\n+        long ramBytesUsedQueryResult(QuerySearchResult result) {\n+            if (hasAggs == false) {\n+                return 0;\n+            }\n+            return result.aggregations()\n+                .asSerialized(InternalAggregations::readFrom, namedWriteableRegistry)", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0OTA5OA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490349098", "bodyText": "I think you can skip the result variable now.", "author": "nik9000", "createdAt": "2020-09-17T15:36:11Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -164,27 +182,22 @@ private MergeResult partialReduce(MergeTask task,\n             newTopDocs = null;\n         }\n \n-        final DelayableWriteable.Serialized<InternalAggregations> newAggs;\n+        final InternalAggregations newAggs;\n         if (hasAggs) {\n             List<InternalAggregations> aggsList = new ArrayList<>();\n             if (lastMerge != null) {\n-                aggsList.add(lastMerge.reducedAggs.expand());\n+                aggsList.add(lastMerge.reducedAggs);\n             }\n             for (QuerySearchResult result : toConsume) {\n                 aggsList.add(result.consumeAggs().expand());\n             }\n             InternalAggregations result = InternalAggregations.topLevelReduce(aggsList,\n                 aggReduceContextBuilder.forPartialReduction());\n-            newAggs = DelayableWriteable.referencing(result).asSerialized(InternalAggregations::readFrom, namedWriteableRegistry);\n-            long previousBufferSize = aggsCurrentBufferSize;\n-            aggsCurrentBufferSize = newAggs.ramBytesUsed();\n-            aggsMaxBufferSize = Math.max(aggsCurrentBufferSize, aggsMaxBufferSize);\n-            logger.trace(\"aggs partial reduction [{}->{}] max [{}]\",\n-                previousBufferSize, aggsCurrentBufferSize, aggsMaxBufferSize);\n+            newAggs = result;", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQwMTY2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490401663", "bodyText": "++, 7178dab", "author": "jimczi", "createdAt": "2020-09-17T16:34:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0OTA5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0OTcwMg==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490349702", "bodyText": "Its probably worth commenting about how we leave the results un-serialized but we send their serialized size because serializing is slow.", "author": "nik9000", "createdAt": "2020-09-17T15:36:59Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -193,49 +206,106 @@ private MergeResult partialReduce(MergeTask task,\n             processedShards.add(new SearchShard(target.getClusterAlias(), target.getShardId()));\n         }\n         progressListener.notifyPartialReduce(processedShards, topDocsStats.getTotalHits(), newAggs, numReducePhases);\n-        return new MergeResult(processedShards, newTopDocs, newAggs);\n+        return new MergeResult(processedShards, newTopDocs, newAggs, hasAggs ? newAggs.getBinarySize() : 0);", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQwMTc2OA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490401768", "bodyText": "7178dab", "author": "jimczi", "createdAt": "2020-09-17T16:34:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0OTcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0OTg3NA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490349874", "bodyText": "getSerializedSize?", "author": "nik9000", "createdAt": "2020-09-17T15:37:14Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java", "diffHunk": "@@ -166,4 +167,47 @@ public static InternalAggregations reduce(List<InternalAggregations> aggregation\n \n         return from(reducedAggregations);\n     }\n+\n+    /**\n+     * Returns the number of bytes required to serialize these aggregations in binary form.\n+     */\n+    public long getBinarySize() {", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQwMTg1OA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490401858", "bodyText": "7178dab", "author": "jimczi", "createdAt": "2020-09-17T16:34:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0OTg3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM1MDU4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490350583", "bodyText": "Are these loops just to make sure we get a whole bunch of random cases every run?", "author": "nik9000", "createdAt": "2020-09-17T15:38:18Z", "path": "server/src/test/java/org/elasticsearch/action/search/SearchPhaseControllerTests.java", "diffHunk": "@@ -931,19 +935,36 @@ public void onFinalReduce(List<SearchShard> shards, TotalHits totalHits, Interna\n         }\n     }\n \n-    public void testPartialMergeFailure() throws InterruptedException {\n+    public void testPartialReduce() throws Exception {\n+        for (int i = 0; i < 10; i++) {", "originalCommit": "599f01d19d723370fb53133a7427a8af3575e62b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM2MzEyNg==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r490363126", "bodyText": "yes,  mainly to exercice the batch reduced size", "author": "jimczi", "createdAt": "2020-09-17T15:56:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM1MDU4Mw=="}], "type": "inlineReview"}, {"oid": "0a78ac8e41f94482aa8fa3fc56755e3ab7a09af1", "url": "https://github.com/elastic/elasticsearch/commit/0a78ac8e41f94482aa8fa3fc56755e3ab7a09af1", "message": "Merge branch 'master' into enhancements/reduce_aggs_circuit_breaker", "committedDate": "2020-09-17T15:57:38Z", "type": "commit"}, {"oid": "7178dab4566971b0b0daee19f2b22a8f597f61b1", "url": "https://github.com/elastic/elasticsearch/commit/7178dab4566971b0b0daee19f2b22a8f597f61b1", "message": "address review feedback", "committedDate": "2020-09-17T16:05:02Z", "type": "commit"}, {"oid": "8f69584e570ca39a5fb286e2f9cae44d131a45f7", "url": "https://github.com/elastic/elasticsearch/commit/8f69584e570ca39a5fb286e2f9cae44d131a45f7", "message": "asserting doesn't work with tests, revert. Fix tests after merge with master.", "committedDate": "2020-09-17T16:31:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjI1MA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493402250", "bodyText": "what is the rationale behind the choice of keeping the de-serialized partial aggs around rather then serializing them and expanding them later when needed?", "author": "javanna", "createdAt": "2020-09-23T10:05:17Z", "path": "x-pack/plugin/async-search/src/main/java/org/elasticsearch/xpack/search/AsyncSearchTask.java", "diffHunk": "@@ -401,16 +400,15 @@ public void onPartialReduce(List<SearchShard> shards, TotalHits totalHits,\n                 reducedAggs = () -> null;\n             } else {\n                 /*\n-                 * Keep a reference to the serialized form of the partially\n-                 * reduced aggs and reduce it on the fly when someone asks\n+                 * Keep a reference to the partially reduced aggs and reduce it on the fly when someone asks\n                  * for it. It's important that we wait until someone needs\n                  * the result so we don't perform the final reduce only to\n                  * throw it away. And it is important that we keep the reference\n-                 * to the serialized aggregations because SearchPhaseController\n+                 * to the aggregations because SearchPhaseController\n                  * *already* has that reference so we're not creating more garbage.\n                  */\n                 reducedAggs = () ->\n-                    InternalAggregations.topLevelReduce(singletonList(aggregations.expand()), aggReduceContextSupplier.get());", "originalCommit": "8f69584e570ca39a5fb286e2f9cae44d131a45f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQyOTk5NA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493429994", "bodyText": "Because serializing and de-serializing is slow and makes the memory consumption worst when expanding. We can maybe revise this later but I feel like the real difference would be to serialize on a temporary file or in an index in case of async_search.", "author": "jimczi", "createdAt": "2020-09-23T10:35:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjI1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQ0MzUyMA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493443520", "bodyText": "I see, so this particular change does not really have to do with circuit breaking, but more around the feeling that keeping aggs around as they are is better overall? Was this measured out of curiosity? I think I am asking because I wonder why we were serializing/deserializing before", "author": "javanna", "createdAt": "2020-09-23T10:50:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjI1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUwMjYxOA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493502618", "bodyText": "Was this measured out of curiosity?\n\nYes, see #62223 (comment)", "author": "jimczi", "createdAt": "2020-09-23T11:58:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjI1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUxMzQ5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493513496", "bodyText": "++ great thanks", "author": "javanna", "createdAt": "2020-09-23T12:10:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjI1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUxNTYxMg==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493515612", "bodyText": "one thing that could have made things clearer would be to have two separate PRs for the two changes, but it's fine as-is at this point, and I am maybe missing some reason why they were made as part of a single PR.", "author": "javanna", "createdAt": "2020-09-23T12:12:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjc4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493402781", "bodyText": "is this test no longer relevant?", "author": "javanna", "createdAt": "2020-09-23T10:05:50Z", "path": "x-pack/plugin/async-search/src/test/java/org/elasticsearch/xpack/search/AsyncSearchTaskTests.java", "diffHunk": "@@ -155,56 +152,14 @@ public void onFailure(Exception e) {\n         latch.await();\n     }\n \n-    public void testGetResponseFailureDuringReduction() throws InterruptedException {\n-        AsyncSearchTask task = createAsyncSearchTask();\n-        task.getSearchProgressActionListener().onListShards(Collections.emptyList(), Collections.emptyList(),\n-            SearchResponse.Clusters.EMPTY, false);\n-        InternalAggregations aggs = InternalAggregations.from(Collections.singletonList(new StringTerms(\"name\", BucketOrder.key(true),\n-            BucketOrder.key(true), 1, 1, Collections.emptyMap(), DocValueFormat.RAW, 1, false, 1, Collections.emptyList(), 0)));\n-        //providing an empty named writeable registry will make the expansion fail, hence the delayed reduction will fail too\n-        //causing an exception when executing getResponse as part of the completion listener callback\n-        DelayableWriteable.Serialized<InternalAggregations> serializedAggs = DelayableWriteable.referencing(aggs)\n-            .asSerialized(InternalAggregations::readFrom, new NamedWriteableRegistry(Collections.emptyList()));\n-        task.getSearchProgressActionListener().onPartialReduce(Collections.emptyList(), new TotalHits(0, TotalHits.Relation.EQUAL_TO),\n-            serializedAggs, 1);\n-        AtomicReference<AsyncSearchResponse> response = new AtomicReference<>();\n-        CountDownLatch latch = new CountDownLatch(1);\n-        task.addCompletionListener(new ActionListener<>() {\n-            @Override\n-            public void onResponse(AsyncSearchResponse asyncSearchResponse) {\n-                assertTrue(response.compareAndSet(null, asyncSearchResponse));\n-                latch.countDown();\n-            }\n-\n-            @Override\n-            public void onFailure(Exception e) {\n-                throw new AssertionError(\"onFailure should not be called\");\n-            }\n-        }, TimeValue.timeValueMillis(10L));\n-        assertTrue(latch.await(1, TimeUnit.SECONDS));\n-        assertNotNull(response.get().getSearchResponse());\n-        assertEquals(0, response.get().getSearchResponse().getTotalShards());\n-        assertEquals(0, response.get().getSearchResponse().getSuccessfulShards());\n-        assertEquals(0, response.get().getSearchResponse().getFailedShards());\n-        assertThat(response.get().getFailure(), instanceOf(ElasticsearchException.class));\n-        assertEquals(\"Async search: error while reducing partial results\", response.get().getFailure().getMessage());\n-        assertThat(response.get().getFailure().getCause(), instanceOf(IllegalArgumentException.class));\n-        assertEquals(\"Unknown NamedWriteable category [\" + InternalAggregation.class.getName() + \"]\",\n-            response.get().getFailure().getCause().getMessage());\n-    }", "originalCommit": "8f69584e570ca39a5fb286e2f9cae44d131a45f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQzMjgxOA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493432818", "bodyText": "it cannot work anymore since we don't need to serialize the aggs. I think it's ok since we have other tests that check that exception thrown during a partial/final reduce are handled correctly.", "author": "jimczi", "createdAt": "2020-09-23T10:38:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjc4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQ0NDM5NA==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r493444394", "bodyText": "I see, you mean the condition that the test had to trigger the failure, which was around serialization?", "author": "javanna", "createdAt": "2020-09-23T10:51:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjc4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4MTUyMw==", "url": "https://github.com/elastic/elasticsearch/pull/62223#discussion_r494081523", "bodyText": "yep", "author": "jimczi", "createdAt": "2020-09-24T06:59:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQwMjc4MQ=="}], "type": "inlineReview"}, {"oid": "ae905088c8f5665654c84f9b0e6e050767756162", "url": "https://github.com/elastic/elasticsearch/commit/ae905088c8f5665654c84f9b0e6e050767756162", "message": "Merge branch 'master' into enhancements/reduce_aggs_circuit_breaker", "committedDate": "2020-09-23T15:33:36Z", "type": "commit"}, {"oid": "400e77a9844ebbc8e6487fb1088ee3fc5b7df0fb", "url": "https://github.com/elastic/elasticsearch/commit/400e77a9844ebbc8e6487fb1088ee3fc5b7df0fb", "message": "fix compilation after merge with  master", "committedDate": "2020-09-23T18:45:32Z", "type": "commit"}]}