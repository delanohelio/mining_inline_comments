{"pr_number": 51325, "pr_title": "Fix TransportMasterNodeAction not Retrying NodeClosedException", "pr_createdAt": "2020-01-22T17:29:16Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/51325", "timeline": [{"oid": "6bee96195a77b7d7f9f70e857381bcb9d0cfe326", "url": "https://github.com/elastic/elasticsearch/commit/6bee96195a77b7d7f9f70e857381bcb9d0cfe326", "message": "Fix TransportMasterNodeAction not Retrying NodeClosedException\n\nAdded node closed exception to the retryable remote exceptions.", "committedDate": "2020-01-22T17:25:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0NTQ4OA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370045488", "bodyText": "Can you write an integration test that shows that this solves an issue?\nPerhaps a test that restarts the current master while concurrently sending some TransportMasterNodeReadAction (e.g. cluster health).\nI'm surprised that we have not seen this issue in any of our integration tests, where we sometimes restart nodes (but perhaps don't do this to concurrently issuing master-level requests).", "author": "ywelsch", "createdAt": "2020-01-23T10:42:08Z", "path": "server/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java", "diffHunk": "@@ -178,7 +178,7 @@ protected void doStart(ClusterState clusterState) {\n                                 @Override\n                                 public void handleException(final TransportException exp) {\n                                     Throwable cause = exp.unwrapCause();\n-                                    if (cause instanceof ConnectTransportException) {\n+                                    if (cause instanceof ConnectTransportException || cause instanceof NodeClosedException) {", "originalCommit": "6bee96195a77b7d7f9f70e857381bcb9d0cfe326", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDIxMDk2OA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370210968", "bodyText": "There we go: 3378d3d\nBit of a dirty test but reliably reproduces the issue without any other hacks :)\nI've researched this a little and the problem seems to be isolated to 7.x+. When shutting down a master there is a short window during which:\n\nWe will get the cluster state without master node from org.elasticsearch.cluster.coordination.Coordinator#clusterStateWithNoMasterBlock\nThat then causes the master action on the current master node to fail with NotMasterException and add the cluster state observer for the retry waiting for the new master. That new master never comes and instead the wait is shut-down with the NodeClosedException when the cluster service is shut down\nClient receives NodeClosedException back from the master\n\n=> this fix still seems fine, if we retry on NodeClosedException because we can interpret it as the master node going away we're good IMO.", "author": "original-brownbear", "createdAt": "2020-01-23T16:08:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0NTQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDU1NDU5OA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370554598", "bodyText": "the problem seems to be isolated to 7.x+\n\nDoes that also include 7.0, 7.1, ...?\nI would like to understand why this is not an issue in earlier versions.", "author": "ywelsch", "createdAt": "2020-01-24T10:03:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0NTQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDU4MDY3NA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370580674", "bodyText": "For snapshots this only started showing up because I inadvertently disabled the snapshot shard's services own retry in #50788\nI can also reproduce this failure back to 7.1, though interestingly enough it seems to be a lot less likely in 7.1 than in  7.6 (maybe that's due to some locking we removed from the internal test cluster operations, but I can't tell right now).\nI pushed 646823d to slow things down a little more and make the test repro better. Without this change it takes a pretty large number of iterations for it to fail on 7.1", "author": "original-brownbear", "createdAt": "2020-01-24T11:05:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0NTQ4OA=="}], "type": "inlineReview"}, {"oid": "323f0d5ee6417b724a9cc595201c735b78a5aa64", "url": "https://github.com/elastic/elasticsearch/commit/323f0d5ee6417b724a9cc595201c735b78a5aa64", "message": "Merge remote-tracking branch 'elastic/master' into retry-sending-snapshot-shard-status-updates", "committedDate": "2020-01-23T10:48:47Z", "type": "commit"}, {"oid": "3378d3de2a248e87e30c4f67315842322c90f432", "url": "https://github.com/elastic/elasticsearch/commit/3378d3de2a248e87e30c4f67315842322c90f432", "message": "test via healthcheck", "committedDate": "2020-01-23T15:55:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDUyMTI1NA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370521254", "bodyText": "I think we should separate out cases where the NodeClosedException originate on the local node (for isntance the case where transport stopped, which is also relayed as a NodeClosedException). There should be no need to retry those.\nAdding a check that the NodeClosedException is wrapped in a RemoteTransportException would ensure we only retry in true client cases.", "author": "henningandersen", "createdAt": "2020-01-24T08:40:24Z", "path": "server/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java", "diffHunk": "@@ -178,7 +178,7 @@ protected void doStart(ClusterState clusterState) {\n                                 @Override\n                                 public void handleException(final TransportException exp) {\n                                     Throwable cause = exp.unwrapCause();\n-                                    if (cause instanceof ConnectTransportException) {\n+                                    if (cause instanceof ConnectTransportException || cause instanceof NodeClosedException) {", "originalCommit": "3378d3de2a248e87e30c4f67315842322c90f432", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDUyODA2NA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370528064", "bodyText": "Thanks Henning, I pushed 1d07a26 :)", "author": "original-brownbear", "createdAt": "2020-01-24T08:59:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDUyMTI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDczODA0NA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370738044", "bodyText": "AFAICS, TransportException is not an ElasticsearchWrapperException (in contrast to RemoteTransportException), and therefore unwrapCause will not yield the NodeClosedException in case of a local transport exception.\nThis means that the code as before would have been fine I think", "author": "ywelsch", "createdAt": "2020-01-24T16:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDUyMTI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0MTg4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370741881", "bodyText": "I think we can run into SendRequestTransportException with NodeClosedException which is an ElasticsearchWrapperException?", "author": "original-brownbear", "createdAt": "2020-01-24T17:05:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDUyMTI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0NzAzMA==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370747030", "bodyText": "^^ seems to have happened here https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+pull-request-1/14196/consoleText\n  1> [2020-01-21T11:51:16,833][WARN ][o.e.s.SnapshotShardsService] [node_td4] [test-repo:test-snap/SQinm62PQkmhonsriaGPUw] [ShardSnapshotStatus[state=FAILED, nodeId=ULGA-lGmRfiN3aM53N3qqg, reason=[test-idx/6AougmeaSJCvxG4mFvfRZA][[test-idx][11]] org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: Aborted\n  1> \tat org.elasticsearch.repositories.blobstore.BlobStoreRepository.snapshotShard(BlobStoreRepository.java:1501)\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:341)\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService.lambda$startNewShards$1(SnapshotShardsService.java:287)\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:629)\n  1> \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n  1> \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n  1> \tat java.base/java.lang.Thread.run(Thread.java:834)\n  1> , generation=null]] failed to update snapshot state\n  1> org.elasticsearch.transport.SendRequestTransportException: [node_td4][127.0.0.1:32974][internal:cluster/snapshot/update_snapshot_status]\n  1> \tat org.elasticsearch.transport.TransportService.sendRequestInternal(TransportService.java:644) ~[main/:?]\n  1> \tat org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:545) ~[main/:?]\n  1> \tat org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:520) ~[main/:?]\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService.lambda$sendSnapshotShardUpdate$2(SnapshotShardsService.java:492) ~[main/:?]\n  1> \tat org.elasticsearch.transport.TransportRequestDeduplicator.executeOnce(TransportRequestDeduplicator.java:52) ~[main/:?]\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService.sendSnapshotShardUpdate(SnapshotShardsService.java:478) ~[main/:?]\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService.notifyFailedSnapshotShard(SnapshotShardsService.java:472) ~[main/:?]\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService$1.onFailure(SnapshotShardsService.java:305) ~[main/:?]\n  1> \tat org.elasticsearch.action.ActionListener$5.onFailure(ActionListener.java:256) ~[main/:?]\n  1> \tat org.elasticsearch.repositories.blobstore.BlobStoreRepository.snapshotShard(BlobStoreRepository.java:1624) ~[main/:?]\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:341) ~[main/:?]\n  1> \tat org.elasticsearch.snapshots.SnapshotShardsService.lambda$startNewShards$1(SnapshotShardsService.java:287) ~[main/:?]\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:629) ~[main/:?]\n  1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n  1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n  1> \tat java.lang.Thread.run(Thread.java:834) [?:?]\n  1> Caused by: org.elasticsearch.node.NodeClosedException: node closed {node_td4}{ULGA-lGmRfiN3aM53N3qqg}{MhSHeUC3SG2Qv9JTAFNk9Q}{127.0.0.1}{127.0.0.1:32974}{di}\n  1> \tat org.elasticsearch.transport.TransportService.sendRequestInternal(TransportService.java:626) ~[main/:?]\n  1> \t... 15 more", "author": "original-brownbear", "createdAt": "2020-01-24T17:16:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDUyMTI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0Nzc5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/51325#discussion_r370747799", "bodyText": "alright, the tests and you win", "author": "ywelsch", "createdAt": "2020-01-24T17:18:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDUyMTI1NA=="}], "type": "inlineReview"}, {"oid": "d3b05a1b0374b968b5cf55cce24fea62696f9c0a", "url": "https://github.com/elastic/elasticsearch/commit/d3b05a1b0374b968b5cf55cce24fea62696f9c0a", "message": "Merge remote-tracking branch 'elastic/master' into retry-sending-snapshot-shard-status-updates", "committedDate": "2020-01-24T08:52:20Z", "type": "commit"}, {"oid": "1d07a269a00d6899f129c79f9a9c63bbdfbdb5d7", "url": "https://github.com/elastic/elasticsearch/commit/1d07a269a00d6899f129c79f9a9c63bbdfbdb5d7", "message": "only retry remote node closed", "committedDate": "2020-01-24T08:58:56Z", "type": "commit"}, {"oid": "646823d17d0f1cd972d9dd2b6c316674ba23a572", "url": "https://github.com/elastic/elasticsearch/commit/646823d17d0f1cd972d9dd2b6c316674ba23a572", "message": "better repro", "committedDate": "2020-01-24T11:02:56Z", "type": "commit"}]}