{"pr_number": 66452, "pr_title": "New terms_enum API for discovering terms in the index.", "pr_createdAt": "2020-12-16T16:08:32Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/66452", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2MTUyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544661521", "bodyText": "since getDocCount() returns a primitive type, may be for comparison we can just do: getDocCount() == other.gertDocCount()?", "author": "mayya-sharipova", "createdAt": "2020-12-16T22:14:55Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {\n+        this.term = term;\n+        this.docCount = count;\n+    }\n+\n+    public String getTerm() {\n+        return this.term;\n+    }\n+\n+    public long getDocCount() {\n+        return this.docCount;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeString(term);\n+        out.writeLong(docCount);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.field(TERM_FIELD, getTerm());\n+        builder.field(DOC_COUNT_FIELD, getDocCount());\n+        return builder;\n+    }\n+\n+    public static TermCount fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TermCount other = (TermCount) o;\n+        return Objects.equals(getTerm(), other.getTerm()) && Objects.equals(getDocCount(), other.getDocCount());", "originalCommit": "acff4942d0f38be6ad922f957692abf10e017d09", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NDE4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544664181", "bodyText": "what does SKIPPED_FIELD is responsible for ?", "author": "mayya-sharipova", "createdAt": "2020-12-16T22:19:55Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumResponse.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+/**\n+ * The response of the termenum/list action.\n+ */\n+public class TermEnumResponse extends BroadcastResponse {\n+\n+    public static final String TERMS_FIELD = \"terms\";\n+    public static final String TIMED_OUT_FIELD = \"timed_out\";\n+    public static final String SKIPPED_FIELD = \"skipped\";", "originalCommit": "acff4942d0f38be6ad922f957692abf10e017d09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk3NjI1MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544976250", "bodyText": "Like the search API - it's what shards failed the canMatch phase.\nIf you supply an index_filter it does a canMatch on each shard before considering the terms in it. (Currently the test fails for use of index filters after I moved the code to xpack. Something about QueryBuilders not being available).", "author": "markharwood", "createdAt": "2020-12-17T10:24:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NDE4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA2NDMxMw==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621064313", "bodyText": "Removed", "author": "markharwood", "createdAt": "2021-04-27T10:07:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NDE4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544666387", "bodyText": "May be too early to talk about this, but when security is enabled, who can access to this endpoint (those who can read the index))?  what about when FLS and DLS is enabled?", "author": "mayya-sharipova", "createdAt": "2020-12-16T22:24:02Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "originalCommit": "acff4942d0f38be6ad922f957692abf10e017d09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk3NjYyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544976629", "bodyText": "DLS and FLS is not part of this PR currently. A TBD.", "author": "markharwood", "createdAt": "2020-12-17T10:25:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM2ODU1OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r545368558", "bodyText": "@markharwood Thanks for the answers, Mark. Is this PR is ready for a more thorough review? or you wanted to add more/ do more modifications?", "author": "mayya-sharipova", "createdAt": "2020-12-17T20:03:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwNzc2NA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r545707764", "bodyText": "Thanks for the review Mayya! There's still some stuff I want to add just yet. (Sorry, I should have added the WIP label).\nI need to\n\nadd logic and tests to filter slower indices based on tiers (we ignore frozen indices and searchable snapshots).\nUse a separate thread pool to the search one.\nAdd HLRC support.\n\nI'll leave DLS and FLS for a subsequent PR.", "author": "markharwood", "createdAt": "2020-12-18T09:29:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzODc2MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612338760", "bodyText": "Should it be indices:data/read/terms ? No need to add xpack reference here.", "author": "jimczi", "createdAt": "2021-04-13T10:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTMwMTUxOA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551301518", "bodyText": "We shouldn't return the doc count in the final response. It's an approximation that cannot be used without an error bound so I'd rather remove it from the response.", "author": "jimczi", "createdAt": "2021-01-04T12:57:00Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {", "originalCommit": "c0d98c2a5895775c1b75b689bca8edd9f4518de6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2a1ac2dd39787de3a6f150c6dbb04ba927645525", "url": "https://github.com/elastic/elasticsearch/commit/2a1ac2dd39787de3a6f150c6dbb04ba927645525", "message": "The way TermEnums are obtained has changed.", "committedDate": "2021-01-05T17:53:54Z", "type": "forcePushed"}, {"oid": "b48a4730e2ac8335766dc214ebfe8a5577f588cc", "url": "https://github.com/elastic/elasticsearch/commit/b48a4730e2ac8335766dc214ebfe8a5577f588cc", "message": "Removed counts from response and related tests", "committedDate": "2021-01-07T12:19:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1ODM5MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555758390", "bodyText": "You need to change the name here too (string)  ?", "author": "jimczi", "createdAt": "2021-01-12T13:11:02Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/ShardTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.broadcast.BroadcastShardRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.search.internal.AliasFilter;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+/**\n+ * Internal termenum request executed directly against a specific index shard.\n+ */\n+public class ShardTermEnumRequest extends BroadcastShardRequest {\n+\n+    private String field;\n+    private String pattern;\n+    private long taskStartedTimeMillis;\n+    private long shardStartedTimeMillis;\n+    private AliasFilter filteringAliases;\n+    private boolean caseInsensitive;\n+    private boolean sortByPopularity;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    \n+\n+    public ShardTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a shard.\n+        shardStartedTimeMillis = System.currentTimeMillis();\n+\n+        filteringAliases = new AliasFilter(in);\n+        field = in.readString();\n+        pattern = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        sortByPopularity = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+    }\n+\n+    public ShardTermEnumRequest(ShardId shardId, AliasFilter filteringAliases, TermEnumRequest request) {\n+        super(shardId, request);\n+        this.field = request.field();\n+        this.pattern = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.sortByPopularity = request.sortByPopularity();\n+        this.filteringAliases = Objects.requireNonNull(filteringAliases, \"filteringAliases must not be null\");\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String pattern() {", "originalCommit": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1OTc3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555759771", "bodyText": "Is the TODO still relevant ? The threadpool is there from what I can see.", "author": "jimczi", "createdAt": "2021-01-12T13:13:25Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool", "originalCommit": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2MzMyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555763321", "bodyText": "We should rely on node's data-tier when selecting indices and not care about index settings. The goal is to   restrict the API to some data-tiers, we can add more flexibility in the future but I'd prefer that we start simple. Only the hot and warm tier should be queried.", "author": "jimczi", "createdAt": "2021-01-12T13:19:24Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.", "originalCommit": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2NDQzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555764431", "bodyText": "we should never catch an AssertionError, did you mean Exception ?", "author": "jimczi", "createdAt": "2021-01-12T13:21:21Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);\n+        try {\n+            IndexReader reader = searchContext.getQueryShardContext().searcher().getTopReaderContext().reader();\n+            Terms terms = MultiTerms.getTerms(reader, request.field());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+            }\n+            Automaton a = request.caseInsensitive()\n+                    ? AutomatonQueries.caseInsensitivePrefix(request.pattern())\n+                    : Automata.makeString(request.pattern());\n+            a = Operations.concatenate(a, Automata.makeAnyString());\n+            a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+\n+            // TODO make this a param and scale up based on num shards like we do with terms aggs?\n+            int shard_size = request.size();\n+\n+            CompiledAutomaton automaton = new CompiledAutomaton(a);\n+            TermsEnum te = automaton.getTermsEnum(terms);\n+\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            if (request.sortByPopularity()) {\n+                // Collect most popular matches\n+                TermCountPriorityQueue pq = new TermCountPriorityQueue(shard_size);\n+                TermCount spare = null;\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            // Gather what we have collected so far\n+                            while (pq.size() > 0) {\n+                                termsList.add(pq.pop());\n+                            }\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+\n+                    if (spare == null) {\n+                        spare = new TermCount(bytes.utf8ToString(), df);\n+                    } else {\n+                        spare.setTerm(bytes.utf8ToString());\n+                        spare.setDocCount(df);\n+                    }\n+                    spare = pq.insertWithOverflow(spare);\n+                }\n+                while (pq.size() > 0) {\n+                    termsList.add(pq.pop());\n+                }\n+            } else {\n+                // Collect in alphabetical order\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+                    termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                    if (termsList.size() >= shard_size) {\n+                        break;\n+                    }\n+                }\n+            }\n+\n+        } catch (AssertionError e) {", "originalCommit": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc5OTIwNg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555799206", "bodyText": "Yep - a copy and paste from TransportValidateQueryAction", "author": "markharwood", "createdAt": "2021-01-12T14:14:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2NDQzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555772995", "bodyText": "In the interest of limiting the number of threads, I wonder if we can group the shard requests per node ?\nIf there are 30 shards to lookup on a single node, I think it would be better to do it in a single call and adapt the strategy globally. With the current configuration, running the request on these 30 shards would raise 18 errors out of 30 (2 can run and 10 are queued, the rest are refused).", "author": "jimczi", "createdAt": "2021-01-12T13:35:09Z", "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "originalCommit": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTgwMzA3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555803076", "bodyText": "Would it best to leave unbundled, then admins can play with degrees of parallelism they want to throw at the problem by tweaking num threads and queue size settings?", "author": "markharwood", "createdAt": "2021-01-12T14:18:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI4MDM2OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612280368", "bodyText": "I wonder if we should start with a bigger queue size ? I'd prefer that we start with a more conservative value like 100. We also said that we'll try to use the search thread pool when the queue is empty. Can you add the idea to the meta issue ?", "author": "jimczi", "createdAt": "2021-04-13T09:22:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQwMjI1MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612402251", "bodyText": "My gut feeling is that we should have a slightly larger queue size as well, 100 sounds better to me too.\nShould we also scale the number of threads based on the number of processors? 2 threads feels too much for a single-core machine, maybe something like max(allocatedProcessors/4, 1)?", "author": "jpountz", "createdAt": "2021-04-13T12:32:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3NzU5OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555777598", "bodyText": "You don't need a SearchContex here, you should use aQueryShardContext instead like we do in the can match phase. Also note that you could use the same QueryShardContext for the canMatchShard above, SearchService#queryStillMatchesAfterRewrite was added for this purpose.", "author": "jimczi", "createdAt": "2021-01-12T13:42:21Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);", "originalCommit": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3OTA0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555779041", "bodyText": "The timeout could be checked here to save some round trip and could avoid accumulating requests on the node's thread pool ?", "author": "jimczi", "createdAt": "2021-01-12T13:44:38Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);\n+        try {\n+            IndexReader reader = searchContext.getQueryShardContext().searcher().getTopReaderContext().reader();\n+            Terms terms = MultiTerms.getTerms(reader, request.field());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+            }\n+            Automaton a = request.caseInsensitive()\n+                    ? AutomatonQueries.caseInsensitivePrefix(request.pattern())\n+                    : Automata.makeString(request.pattern());\n+            a = Operations.concatenate(a, Automata.makeAnyString());\n+            a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+\n+            // TODO make this a param and scale up based on num shards like we do with terms aggs?\n+            int shard_size = request.size();\n+\n+            CompiledAutomaton automaton = new CompiledAutomaton(a);\n+            TermsEnum te = automaton.getTermsEnum(terms);\n+\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            if (request.sortByPopularity()) {\n+                // Collect most popular matches\n+                TermCountPriorityQueue pq = new TermCountPriorityQueue(shard_size);\n+                TermCount spare = null;\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            // Gather what we have collected so far\n+                            while (pq.size() > 0) {\n+                                termsList.add(pq.pop());\n+                            }\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+\n+                    if (spare == null) {\n+                        spare = new TermCount(bytes.utf8ToString(), df);\n+                    } else {\n+                        spare.setTerm(bytes.utf8ToString());\n+                        spare.setDocCount(df);\n+                    }\n+                    spare = pq.insertWithOverflow(spare);\n+                }\n+                while (pq.size() > 0) {\n+                    termsList.add(pq.pop());\n+                }\n+            } else {\n+                // Collect in alphabetical order\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+                    termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                    if (termsList.size() >= shard_size) {\n+                        break;\n+                    }\n+                }\n+            }\n+\n+        } catch (AssertionError e) {\n+            error = e.getMessage();\n+        } finally {\n+            Releasables.close(searchContext);\n+        }\n+\n+        return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+    }\n+    \n+    \n+    private boolean canMatchShard(ShardTermEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(req.shardId(), req.taskStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }    \n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermEnumRequest request;\n+        private ActionListener<TermEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final GroupShardsIterator<ShardIterator> shardsIts;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray shardsResponses;\n+\n+        protected AsyncBroadcastAction(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            shardsIts = shards(clusterState, request, concreteIndices);\n+            expectedOps = shardsIts.size();\n+\n+            shardsResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (shardsIts.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray(0), clusterState, false));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int shardIndex = -1;\n+            for (final ShardIterator shardIt : shardsIts) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                shardIndex++;\n+                final ShardRouting shard = shardIt.nextOrNull();\n+                if (shard != null) {\n+                    performOperation(shardIt, shard, shardIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onOperation(null, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+                }\n+            }\n+        }\n+        \n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ( (now - task.getStartTime()) > request.timeout().getMillis() ) {\n+                finishHim(true);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final ShardIterator shardIt, final ShardRouting shard, final int shardIndex) {\n+            if (shard == null) {\n+                // no more active shards... (we should not really get here, just safety)\n+                onOperation(null, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+            } else {\n+                try {\n+                    //TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final ShardTermEnumRequest shardRequest = newShardRequest(shardIt.size(), shard, request);\n+                    shardRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(shard.currentNodeId());\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onOperation(shard, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+                    } else {", "originalCommit": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e9fe715ff98eec5d1d529a2845b7c3b678bce71f", "url": "https://github.com/elastic/elasticsearch/commit/e9fe715ff98eec5d1d529a2845b7c3b678bce71f", "message": "Addressing some review comments - change to QueryShardContext, remove TODO, change pattern to string, stop catching assertion error, add timer check before calling each shard", "committedDate": "2021-01-12T16:26:44Z", "type": "forcePushed"}, {"oid": "3e2d905ca14cf29acf95e111ce9f3fa97ac32865", "url": "https://github.com/elastic/elasticsearch/commit/3e2d905ca14cf29acf95e111ce9f3fa97ac32865", "message": "Addressing some review comments - change to QueryShardContext, remove TODO, change pattern to string, stop catching assertion error, add timer check before calling each shard", "committedDate": "2021-01-12T17:16:24Z", "type": "forcePushed"}, {"oid": "9c78360a7fdcc49051ddc1bf08f0145f2eec67b8", "url": "https://github.com/elastic/elasticsearch/commit/9c78360a7fdcc49051ddc1bf08f0145f2eec67b8", "message": "Doh.", "committedDate": "2021-01-13T14:29:03Z", "type": "forcePushed"}, {"oid": "601ab8cfa90ed8f28dcbdbf0540b3261519dd1f3", "url": "https://github.com/elastic/elasticsearch/commit/601ab8cfa90ed8f28dcbdbf0540b3261519dd1f3", "message": "Doh.", "committedDate": "2021-01-13T17:12:20Z", "type": "forcePushed"}, {"oid": "29f92da0c426e2905ceadd3e5a584c60200b95aa", "url": "https://github.com/elastic/elasticsearch/commit/29f92da0c426e2905ceadd3e5a584c60200b95aa", "message": "Doh.", "committedDate": "2021-01-14T10:35:35Z", "type": "forcePushed"}, {"oid": "6916077afcff83997d1ed08f074af65401518a25", "url": "https://github.com/elastic/elasticsearch/commit/6916077afcff83997d1ed08f074af65401518a25", "message": "Doh.", "committedDate": "2021-01-18T11:56:01Z", "type": "forcePushed"}, {"oid": "7e2ce352f0ff69e79cc2189943fef4a153f981e2", "url": "https://github.com/elastic/elasticsearch/commit/7e2ce352f0ff69e79cc2189943fef4a153f981e2", "message": "Compile fix after QueryShardContext rename", "committedDate": "2021-01-18T14:31:39Z", "type": "forcePushed"}, {"oid": "9d955e1c15797f6a8f9f4863cc21478b983524b6", "url": "https://github.com/elastic/elasticsearch/commit/9d955e1c15797f6a8f9f4863cc21478b983524b6", "message": "Change IOUtils implies due to forbidden APIs", "committedDate": "2021-01-25T17:06:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r569886766", "bodyText": "I realize this is WIP, but had one high-level thought that might affect code structure/ naming. It'd be great to encapsulate this term loading logic into MappedFieldType instead of interacting with Lucene data structures directly here. We have been trying to do this consistently, and now delegate query creation, doc value loading, and other data fetching tasks to MappedFieldType. Some benefits:\n\nField types can decide to handle the call differently. For example constant_keyword could support this functionality (even though it doesn't write them in the index).\nField aliases will be handled correctly, since we resolve them when looking up MappedFieldType\nIf there's any change to logic because of an index version change, we can consolidate that in one place", "author": "jtibshirani", "createdAt": "2021-02-04T01:52:51Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "originalCommit": "242a84c404a48148e463746b861c689a0eb2c363", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDA4NDI1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570084257", "bodyText": "Discussed with @jimczi and we agreed to add a new option to MappedFieldType to retrieve a Terms enumerator given some search criteria e.g.\nTerms getTerms(String prefix, boolean caseSensitive, int timeoutMs)\n\nThe default impl in MappedFieldType could be to return null or some empty enumerator if not supported.\nThe caller (the autocomplete service) would have timer checks as it iterated across the result to make sure it hadn't timed out so the Terms implementation returned shouldn't block for lengthy periods on individual Terms.next() calls. The mapped field type shouldn't spend too long in constructing the getTerms(...) response either and the timeoutMs is provided as an advisory notice to maybe take some shortcuts e.g. using sampling to achieve the required response time. With the proposed interface above it's not clear to me how the MappedFieldType would report back that shortcuts were made in the Terms object construction. This would be important information given the autocomplete service ultimately aims to report back if the the results were exhaustive or not. Presumably we'd need to return something other than a Terms object in this API to convey this completeness?", "author": "markharwood", "createdAt": "2021-02-04T09:48:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE2MzkxNA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570163914", "bodyText": "+1 to add the logic to return Terms or TermsEnum in the MappedFiedlType. Mark we've discussed this already and I remember saying that it was important in order to handle constant_keyword and flattened correctly ;).\nIt could return an empty or null terms by default to make it clear that the API is not mandatory for each field type.", "author": "jimczi", "createdAt": "2021-02-04T11:52:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDI0Mjk3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570242975", "bodyText": "What's your feeling on the timer question in my (edited) comment above?", "author": "markharwood", "createdAt": "2021-02-04T13:58:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDI3MDQwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570270409", "bodyText": "I don't understand the relation with completeness ? We're talking of a terms enum that can only be used to search by prefix so the proposal is something like:\nTermsEnum getTerms(String prefix, boolean caseSensitive) or Terms getTerms() where we'd leave more room for the caller. So there's no need for each field type to implement anything regarding early termination or timeout. It's the terms enum API that wraps all these TermsEnum in a MultiTermsEnum and knows where to stop and if the result is complete or not.", "author": "jimczi", "createdAt": "2021-02-04T14:33:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDMyMDMyMg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570320322", "bodyText": "It's allowing for anything that might be expensive e.g. runtime fields or wildcard fields that choose to attempt something within the provided time constraints (timeout being something you said should be a parameter in this API).\nThey'd have to gather a sample of docs before returning a TermsEnum which enumerates over the discovered terms.", "author": "markharwood", "createdAt": "2021-02-04T15:34:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDMyMzY2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570323669", "bodyText": "I think runtime fields should just return an empty TermsEnum here? Probably the same with wildcard fields tbh, they don't have a sensible terms dictionary to respond from.  Or wildcard fields could return a MultiTerms over the term dictionaries for all their doc value leaf dictionaries, which will be fast to construct even if its slower to iterate over", "author": "romseygeek", "createdAt": "2021-02-04T15:38:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM1NzE3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570357175", "bodyText": "I don't think wildcard could do anything trustworthy whether it chooses to block on the construction or the logic that determines what to return first from terms.next();\nThat's kind of the point here - we want to communicate to users if there's the possibility of any false negatives in the results being presented. A false negative is bad because we don't want users to walk away without searching because the autocomplete results have (falsely) suggested to them there's nothing to be found.\nWe have several policy options:\n\nKibana teaches users to distrust all suggestions\nUsers generally trust results but Kibana flags any incomplete suggestions. Field types inform incompleteness by either:\na) returning all-or-nothing. A null response means users are told there's stuff missing. A non-null response means the set is assumed to be complete\nb) field types can return a subset of  terms but somehow communicate the completeness of this set.\n\nIf we assume communicating completeness is useful (and that was a design goal) we need to pick either 2a or 2b for this API contract", "author": "markharwood", "createdAt": "2021-02-04T16:19:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM2MTY3MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570361670", "bodyText": "It's allowing for anything that might be expensive e.g. runtime fields or wildcard fields that choose to attempt something within the provided time constraints (timeout being something you said should be a parameter in this API).\nThey'd have to gather a sample of docs before returning a TermsEnum which enumerates over the discovered terms.\n\nPlease no, we discussed that already and we said that it's not something we want to provide. If you have a wildcard or a runtime field then you're not eligible to the terms enum API and we return an empty enum or object. That's the part that should be clear in the javadocs of the method, return the terms enum only if you can provide the feature efficiently.", "author": "jimczi", "createdAt": "2021-02-04T16:25:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM4OTIyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570389229", "bodyText": "OK. What confused me was the idea that timeout should be a parameter passed to this field type API and how that would be interpreted/used", "author": "markharwood", "createdAt": "2021-02-04T17:00:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MTkzMjc3OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r571932778", "bodyText": "The MappedFieldType interface may need some more thought.\nFor efficiency's sake the current PR uses a MultiReader on a node to provide a single view across potentially many shards. While quick, there are several issues here:\n\nInteger doc count limit - if we ever want to sort by popularity, the TermsEnum.docFreq() returns an integer which will potentially overflow if we try use a single view across multiple shards. Using this Lucene-based abstraction over multiple Lucene indices is not going to scale (TODO - fix in Lucene or copy Lucene's MultiTermsEnum ?)\nResource management the current one-size-fits-all logic  has a try-finally block to close readers used while iterating across Terms - the proposed new MappedFieldType API above has not considered how to close any underlying resources accessed by the returned Terms object.\nSome, all or no fusion delegation? - an open question is if the underlying field types are expected to perform any fusion of data across shards in the view offered by the returned TermsEnum. How do they know which Lucene  index or indices they should access? Are they passed one shardID or multiple? How would the caller determine that 2 field types e.g. the keyword field for host in index logs08-02-2021 and the same field in index logs09-02-2021 were merge-able by one of the field types? We would, of course, assume that MappedFieldTypes of different classes would have different implementations and so their TermsEnums would have to be merged by the caller doing the collection on a node. How would the caller fuse multiple TermEnums? The Lucene MultiTermsEnum class is constructed using ReaderSlice objects.", "author": "markharwood", "createdAt": "2021-02-08T10:25:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MTk4MDY0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r571980649", "bodyText": "Discussed with Jim. We'll go with\nTermsEnum getTerms(String prefix, boolean caseSensitive, SearchExecutionContext context)\n\nThe caller calls this on the relevant field type for every shard and merges the TermsEnum objects using a new MultiTermsEnum class that differs from the Lucene one in that it:\n\nTakes a number of TermsEnum objects rather than ReaderSlice objects\nUses longs not ints for summing doc frequencies\nDoesn't implement TermsEnum, so doesn't offer all its methods like ord, postings etc Instead it has next(), term() and docFreq() only", "author": "markharwood", "createdAt": "2021-02-08T11:41:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}], "type": "inlineReview"}, {"oid": "1dc1510782ac00837d913586d1926aae5796501f", "url": "https://github.com/elastic/elasticsearch/commit/1dc1510782ac00837d913586d1926aae5796501f", "message": "A TermsEnum API for discovering terms in the index.\nTerms matching a given prefix can be returned in alphabetical or by popularity,\nA timeout can limit the amount of time spent looking for matches.\n\nExpected to be useful in auto-complete or regex debugging use cases.", "committedDate": "2021-02-10T10:15:25Z", "type": "forcePushed"}, {"oid": "051ffd0064db8d3c51d2f8a7ad0f353d298f9ef0", "url": "https://github.com/elastic/elasticsearch/commit/051ffd0064db8d3c51d2f8a7ad0f353d298f9ef0", "message": "Renamed to naming convention for tests", "committedDate": "2021-02-10T15:31:05Z", "type": "forcePushed"}, {"oid": "8d528a9d07783e162203771ec69c58e7681cec0f", "url": "https://github.com/elastic/elasticsearch/commit/8d528a9d07783e162203771ec69c58e7681cec0f", "message": "Fix bug where some indices don\u2019t have mapped field type", "committedDate": "2021-02-16T12:12:37Z", "type": "forcePushed"}, {"oid": "6869b7f5e430d651eb3e737d00cd351296e40dfc", "url": "https://github.com/elastic/elasticsearch/commit/6869b7f5e430d651eb3e737d00cd351296e40dfc", "message": "A TermsEnum API for discovering terms in the index.\nTerms matching a given prefix can be returned in alphabetical or by popularity,\nA timeout can limit the amount of time spent looking for matches.\n\nExpected to be useful in auto-complete or regex debugging use cases.", "committedDate": "2021-02-16T15:14:10Z", "type": "forcePushed"}, {"oid": "7bbf5b2abc391c333726a5c62022c3ca88821533", "url": "https://github.com/elastic/elasticsearch/commit/7bbf5b2abc391c333726a5c62022c3ca88821533", "message": "Remove HLRC code for now - requires less-than-ideal package names while we\u2019re unable to move main server implementation out of xpack (due to security dependency).\nDon\u2019t want clients to build dependencies on the wrong package names and the Java client changes are underway anyway.", "committedDate": "2021-02-18T10:51:15Z", "type": "forcePushed"}, {"oid": "e5c48c1b3d0d2cd615fd539c141533bd823ce136", "url": "https://github.com/elastic/elasticsearch/commit/e5c48c1b3d0d2cd615fd539c141533bd823ce136", "message": "Remove HLRC code for now - requires less-than-ideal package names while we\u2019re unable to move main server implementation out of xpack (due to security dependency).\nDon\u2019t want clients to build dependencies on the wrong package names and the Java client changes are underway anyway.", "committedDate": "2021-03-10T10:21:34Z", "type": "forcePushed"}, {"oid": "6d1d1b7d1ab5017ee9c77d79f55b04aaffadc2a4", "url": "https://github.com/elastic/elasticsearch/commit/6d1d1b7d1ab5017ee9c77d79f55b04aaffadc2a4", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards", "committedDate": "2021-03-11T16:53:23Z", "type": "forcePushed"}, {"oid": "de5615884977a6dadd3e3b5fc5e14190e13e31db", "url": "https://github.com/elastic/elasticsearch/commit/de5615884977a6dadd3e3b5fc5e14190e13e31db", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards", "committedDate": "2021-03-16T09:33:22Z", "type": "forcePushed"}, {"oid": "31a8844f55b4d3cc2e683c7b7714ee5a8eb60936", "url": "https://github.com/elastic/elasticsearch/commit/31a8844f55b4d3cc2e683c7b7714ee5a8eb60936", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards", "committedDate": "2021-03-23T11:51:28Z", "type": "forcePushed"}, {"oid": "b89016b61481ec9d8a9274db0c2d17ace1bd1c7f", "url": "https://github.com/elastic/elasticsearch/commit/b89016b61481ec9d8a9274db0c2d17ace1bd1c7f", "message": "Types warning", "committedDate": "2021-03-23T15:52:26Z", "type": "forcePushed"}, {"oid": "81c8a734d5e76bcd0a4982aa3f99c40bdc7bbcaa", "url": "https://github.com/elastic/elasticsearch/commit/81c8a734d5e76bcd0a4982aa3f99c40bdc7bbcaa", "message": "Removed hot/warm tier tests (in anticipation of new queryable _tier field)\nMove canMatch logic to run on network thread\nInjected searchService so we can use its canMatch method", "committedDate": "2021-04-06T17:34:54Z", "type": "forcePushed"}, {"oid": "15e2ca23ed5818e6bca66c58f7240be3b59a690e", "url": "https://github.com/elastic/elasticsearch/commit/15e2ca23ed5818e6bca66c58f7240be3b59a690e", "message": "Unused import", "committedDate": "2021-04-08T15:30:35Z", "type": "forcePushed"}, {"oid": "235af04eca795bc2957990335ceda6f69f275c51", "url": "https://github.com/elastic/elasticsearch/commit/235af04eca795bc2957990335ceda6f69f275c51", "message": "Removed sort by popularity option", "committedDate": "2021-04-12T14:49:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MDg4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612270881", "bodyText": "nit: remove", "author": "jimczi", "createdAt": "2021-04-13T09:08:45Z", "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RequestConverters.java", "diffHunk": "@@ -534,7 +534,7 @@ static Request rankEval(RankEvalRequest rankEvalRequest) throws IOException {\n         request.setEntity(createEntity(rankEvalRequest.getRankEvalSpec(), REQUEST_BODY_CONTENT_TYPE));\n         return request;\n     }\n-\n+    ", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MTIzNA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612271234", "bodyText": "These changes are not needed anymore", "author": "jimczi", "createdAt": "2021-04-13T09:09:14Z", "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RestHighLevelClient.java", "diffHunk": "@@ -120,18 +120,18 @@\n import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MjQwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612272409", "bodyText": "doc_freq is not required either ?", "author": "jimczi", "createdAt": "2021-04-13T09:10:52Z", "path": "server/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java", "diffHunk": "@@ -399,4 +400,20 @@ public TextSearchInfo getTextSearchInfo() {\n         KEYWORD,\n         NUMERIC\n     }\n+\n+    /**\n+     * This method is used to support auto-complete services and implementations\n+     * are expected to find terms beginning with the provided string very quickly.\n+     * If fields cannot look up matching terms quickly they should return null.  \n+     * The returned TermEnum should implement next(), term() and doc_freq() methods", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM0NzI0MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612347240", "bodyText": "I kept it in for now because 1) it's cheap to get from the index and 2) Even if we don't use it for popularity sorting we might want to use it later to filter low-frequency items e.g. user tags that occur only once", "author": "markharwood", "createdAt": "2021-04-13T11:06:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MjQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNTk5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612335991", "bodyText": "Is this still accurate. I thought that we would try to rewrite the role query and if it results in a match_all then we'd accept the request ?", "author": "jimczi", "createdAt": "2021-04-13T10:47:05Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - see https://github.com/elastic/elasticsearch/issues/70221", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ3MzA1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612473055", "bodyText": "The TODO should perhaps be about moving any security filtering logic like this to a custom \"Interceptor\" class which wraps the core implementation and rewrites requests rather than having security+impl in the same class.", "author": "markharwood", "createdAt": "2021-04-13T13:58:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNTk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNjc0MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612336740", "bodyText": "We could also use the search thread pool if the queue is empty. Happy to do it in a follow up.", "author": "jimczi", "createdAt": "2021-04-13T10:48:20Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - see https://github.com/elastic/elasticsearch/issues/70221\n+    private boolean canAccess(String indexName, String fieldName, XPackLicenseState frozenLicenseState, ThreadContext threadContext) {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(indexName);\n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.shardStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermEnumRequest request;\n+        private ActionListener<TermEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final NodeTermEnumRequest nodeRequest = newNodeRequest(nodeId, shardIds, request);\n+                    nodeRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(nodeId);\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onNoOperation(nodeId);\n+                    } else if (checkForEarlyFinish() == false) {\n+                        transportService.sendRequest(\n+                            node,\n+                            transportShardAction,\n+                            nodeRequest,\n+                            new TransportResponseHandler<NodeTermEnumResponse>() {\n+                                @Override\n+                                public NodeTermEnumResponse read(StreamInput in) throws IOException {\n+                                    return readShardResponse(in);\n+                                }\n+\n+                                @Override\n+                                public void handleResponse(NodeTermEnumResponse response) {\n+                                    onOperation(nodeId, nodeIndex, response);\n+                                }\n+\n+                                @Override\n+                                public void handleException(TransportException e) {\n+                                    onNoOperation(nodeId);\n+                                }\n+                            }\n+                        );\n+                    }\n+                } catch (Exception e) {\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        protected void onOperation(String nodeId, int nodeIndex, NodeTermEnumResponse response) {\n+            logger.trace(\"received response for node {}\", nodeId);\n+            nodesResponses.set(nodeIndex, response);\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            } else {\n+                checkForEarlyFinish();\n+            }\n+        }\n+\n+        void onNoOperation(String nodeId) {\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            }\n+        }\n+\n+        // Can be called multiple times - either for early time-outs or for fully-completed collections.\n+        protected synchronized void finishHim(boolean complete) {\n+            if (listener == null) {\n+                return;\n+            }\n+            try {\n+                listener.onResponse(newResponse(request, nodesResponses, complete, nodeBundles));\n+            } catch (Exception e) {\n+                listener.onFailure(e);\n+            } finally {\n+                listener = null;\n+            }\n+        }\n+    }\n+\n+    class NodeTransportHandler implements TransportRequestHandler<NodeTermEnumRequest> {\n+\n+        @Override\n+        public void messageReceived(NodeTermEnumRequest request, TransportChannel channel, Task task) throws Exception {\n+            asyncNodeOperation(request, task, ActionListener.wrap(channel::sendResponse, e -> {\n+                try {\n+                    channel.sendResponse(e);\n+                } catch (Exception e1) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\n+                            \"Failed to send error response for action [{}] and request [{}]\",\n+                            actionName,\n+                            request\n+                        ),\n+                        e1\n+                    );\n+                }\n+            }));\n+        }\n+    }\n+\n+    private void asyncNodeOperation(NodeTermEnumRequest request, Task task, ActionListener<NodeTermEnumResponse> listener)\n+        throws IOException {\n+        // DLS/FLS check copied from ResizeRequestInterceptor - check permissions and\n+        // any index_filter canMatch checks on network thread before allocating work\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+        for (ShardId shardId : request.shardIds().toArray(new ShardId[0])) {\n+            if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) == false || canMatchShard(\n+                shardId,\n+                request\n+            ) == false) {\n+                // Permission denied or can't match, remove shardID from request\n+                request.remove(shardId);\n+            }\n+        }\n+        if (request.shardIds().size() == 0) {\n+            listener.onResponse(new NodeTermEnumResponse(request.nodeId(), Collections.emptyList(), null, true));\n+        } else {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNjkxMw==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612336913", "bodyText": "nit: extra lines", "author": "jimczi", "createdAt": "2021-04-13T10:48:36Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/rest/RestTermEnumAction.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.rest;\n+\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.rest.BaseRestHandler;\n+import org.elasticsearch.rest.RestRequest;\n+import org.elasticsearch.rest.action.RestToXContentListener;\n+import org.elasticsearch.xpack.core.termenum.action.TermEnumAction;\n+import org.elasticsearch.xpack.core.termenum.action.TermEnumRequest;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.elasticsearch.rest.RestRequest.Method.GET;\n+import static org.elasticsearch.rest.RestRequest.Method.POST;\n+\n+public class RestTermEnumAction extends BaseRestHandler {\n+\n+    @Override\n+    public List<Route> routes() {\n+        return List.of(\n+            new Route(GET, \"/{index}/_terms\"),\n+            new Route(POST, \"/{index}/_terms\"));\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return \"term_enum_action\";\n+    }\n+\n+    @Override\n+    public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException {\n+        try (XContentParser parser = request.contentOrSourceParamParser()) {\n+            TermEnumRequest termEnumRequest = TermEnumAction.fromXContent(parser, \n+                Strings.splitStringByCommaToArray(request.param(\"index\")));\n+            return channel ->\n+            client.execute(TermEnumAction.INSTANCE, termEnumRequest, new RestToXContentListener<>(channel));\n+        }        \n+        ", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzOTI0NA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612339244", "bodyText": "Should we use the plural (Terms) instead ? That would be consistent with the name of the API (_terms). Same for all the other class names (TermEnumResponse, ...).", "author": "jimczi", "createdAt": "2021-04-13T10:52:34Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzA3OTA0OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r613079048", "bodyText": "I don't know if \"Terms\" will be too short. We already have a lot of o.e.Term* classes to navigate and users looking for help with the terms aggregation.\nA verb would be a good addition e.g. termFinder", "author": "markharwood", "createdAt": "2021-04-14T09:15:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzOTI0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTAyNDMyOA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621024328", "bodyText": "Now have REST endpoint _terms_enum and TermsEnumXxxx class names", "author": "markharwood", "createdAt": "2021-04-27T09:16:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzOTI0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MzcyMA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612393720", "bodyText": "can you add a newline between the two functions?", "author": "jpountz", "createdAt": "2021-04-13T12:20:16Z", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldParser.java", "diffHunk": "@@ -166,4 +166,14 @@ static BytesRef extractKey(BytesRef keyedValue) {\n         }\n         return new BytesRef(keyedValue.bytes, keyedValue.offset, length);\n     }\n+    static BytesRef extractValue(BytesRef keyedValue) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTcxMw==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395713", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(result != null) {\n          \n          \n            \n                        if (result != null) {", "author": "jpountz", "createdAt": "2021-04-13T12:23:14Z", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{\n+        TermsEnum delegate;\n+\n+        TranslatingTermsEnum(TermsEnum delegate) {\n+            this.delegate = delegate;\n+        }\n+        \n+        @Override\n+        public BytesRef next() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.next();\n+            if(result != null) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTc5OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395798", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(result != null) {\n          \n          \n            \n                        if (result != null) {", "author": "jpountz", "createdAt": "2021-04-13T12:23:21Z", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{\n+        TermsEnum delegate;\n+\n+        TranslatingTermsEnum(TermsEnum delegate) {\n+            this.delegate = delegate;\n+        }\n+        \n+        @Override\n+        public BytesRef next() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.next();\n+            if(result != null) {\n+                result = FlattenedFieldParser.extractValue(result);\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public BytesRef term() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.term();\n+            if(result != null) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTk2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395962", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                static class TranslatingTermsEnum extends TermsEnum{\n          \n          \n            \n                static class TranslatingTermsEnum extends TermsEnum {", "author": "jpountz", "createdAt": "2021-04-13T12:23:36Z", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5ODM0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612398349", "bodyText": "I don't think that this is correct as this will do case-insensitive search on the field name too. E.g. if the flattened object contains {\"foo\": \"bar\", \"Foo\": \"quux\"} we should only consider the right foo/Foo even when case insensitivity is enabled.", "author": "jpountz", "createdAt": "2021-04-13T12:26:45Z", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -239,6 +252,26 @@ public Query wildcardQuery(String value,\n         public Query termQueryCaseInsensitive(Object value, SearchExecutionContext context) {\n             return AutomatonQueries.caseInsensitiveTermQuery(new Term(name(), indexedValueForSearch(value)));\n         }\n+        \n+        @Override\n+        public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException {\n+            IndexReader reader = queryShardContext.searcher().getTopReaderContext().reader();\n+            String searchString = FlattenedFieldParser.createKeyedValue(key, string);\n+            Terms terms = MultiTerms.getTerms(reader, name());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return null;\n+            }\n+            Automaton a = caseInsensitive\n+                ? AutomatonQueries.caseInsensitivePrefix(searchString)\n+                : Automata.makeString(searchString);", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612430232", "bodyText": "it feels inconsistent to set it when deserializing, could we require callers to call shardStartedTimeMillis instead?", "author": "jpountz", "createdAt": "2021-04-13T13:08:44Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA1Mjk4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621052986", "bodyText": "Can we rely on caller and data nodes' clocks not drifting by the small amounts we measure here?\nWe subtract the time-already-spent on caller node from the available max time-to-spend on the data node to account for caller delays in dispatch but I wanted to avoid any clock-drift between caller and data node to be a part of the calculations on the data node", "author": "markharwood", "createdAt": "2021-04-27T09:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzgwMjI2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r623802269", "bodyText": "I think what Adrien meant is that we shouldn't set the timer when deserializing the data. That feels fragile and inconsistent. If that represents the time when the action started on a data node then we should be able to set it once in TransportTermsEnumAction#asyncNodeOperation ?", "author": "jimczi", "createdAt": "2021-04-30T11:19:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzg2MTE0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r623861142", "bodyText": "My goal was to start the timer at the earliest opportunity on the data node (ideally prior to any queuing etc) and given the NodeTermEnumRequest was passed to TransportTermsEnumAction#asyncNodeOperation then the deserialization would have occurred before this point?", "author": "markharwood", "createdAt": "2021-04-30T13:04:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzg2NDIzNA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r623864234", "bodyText": "I don't think this is needed. We call asyncNodeOperation early enough to not worry too much about what happens before.", "author": "jimczi", "createdAt": "2021-04-30T13:09:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMTA0MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612431040", "bodyText": "I'm confused why the method is called shardStartedXXX while the field is called nodeStartedXXX.", "author": "jpountz", "createdAt": "2021-04-13T13:09:47Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    ", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTAzMDg4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621030882", "bodyText": "A relic from when we did per-shard requests rather than bundling into a per-node request", "author": "markharwood", "createdAt": "2021-04-27T09:23:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMTA0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMjg1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612432852", "bodyText": "I don't think that this assert is legal. The delta could be negative if we spent a long time on the above line for instance.", "author": "jpountz", "createdAt": "2021-04-13T13:12:04Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    \n+    \n+    public Set<ShardId> shardIds() {\n+        return Collections.unmodifiableSet(shardIds);\n+    }\n+\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    public long timeout() {\n+        return timeout;\n+    }\n+    public String nodeId() {\n+        return nodeId;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        // Adjust the amount of permitted time the shard has remaining to gather terms. \n+        long timeSpentSoFarInCoordinatingNode = System.currentTimeMillis() - taskStartedTimeMillis;\n+        assert timeSpentSoFarInCoordinatingNode >= 0;", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMzUzNA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612433534", "bodyText": "why do we need to cast to an int?", "author": "jpountz", "createdAt": "2021-04-13T13:12:47Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    \n+    \n+    public Set<ShardId> shardIds() {\n+        return Collections.unmodifiableSet(shardIds);\n+    }\n+\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    public long timeout() {\n+        return timeout;\n+    }\n+    public String nodeId() {\n+        return nodeId;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        // Adjust the amount of permitted time the shard has remaining to gather terms. \n+        long timeSpentSoFarInCoordinatingNode = System.currentTimeMillis() - taskStartedTimeMillis;\n+        assert timeSpentSoFarInCoordinatingNode >= 0;\n+        int remainingTimeForShardToUse = (int) (timeout - timeSpentSoFarInCoordinatingNode);", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNjIyNA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612436224", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class SimpleTermCountEnum extends TermsEnum{\n          \n          \n            \n            public class SimpleTermCountEnum extends TermsEnum {", "author": "jpountz", "createdAt": "2021-04-13T13:16:08Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNjY5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612436695", "bodyText": "could you copy the input array instead of modifying it in-place?", "author": "jpountz", "createdAt": "2021-04-13T13:16:43Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{\n+    int index =-1;\n+    TermCount[] sortedTerms;\n+    TermCount current = null;\n+    \n+    public SimpleTermCountEnum(TermCount[] terms) {\n+        sortedTerms = terms;\n+        Arrays.sort(sortedTerms, Comparator.comparing(TermCount::getTerm));", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNzEyMw==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612437123", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if(index >= sortedTerms.length) {\n          \n          \n            \n                    if (index >= sortedTerms.length) {", "author": "jpountz", "createdAt": "2021-04-13T13:17:12Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{\n+    int index =-1;\n+    TermCount[] sortedTerms;\n+    TermCount current = null;\n+    \n+    public SimpleTermCountEnum(TermCount[] terms) {\n+        sortedTerms = terms;\n+        Arrays.sort(sortedTerms, Comparator.comparing(TermCount::getTerm));\n+    }\n+    \n+    public SimpleTermCountEnum(TermCount termCount) {\n+        sortedTerms = new TermCount[1];\n+        sortedTerms[0] = termCount;\n+    }\n+\n+    @Override\n+    public BytesRef term() throws IOException {\n+        if (current == null) {\n+            return null;\n+        }\n+        return new BytesRef(current.getTerm());\n+    }    \n+\n+    @Override\n+    public BytesRef next() throws IOException {\n+        index++;\n+        if(index >= sortedTerms.length) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzODM2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612438362", "bodyText": "Should we be using constructorArg() rather than optionalConstructorArg()?", "author": "jpountz", "createdAt": "2021-04-13T13:18:40Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzOTI0OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612439248", "bodyText": "I'm not seeing where these setters are called, are they needed? If not can we make term and docCount final?", "author": "jpountz", "createdAt": "2021-04-13T13:19:45Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {\n+        this.term = term;\n+        this.docCount = count;\n+    }\n+\n+    public String getTerm() {\n+        return this.term;\n+    }\n+\n+    public long getDocCount() {\n+        return this.docCount;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeString(term);\n+        out.writeLong(docCount);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.field(TERM_FIELD, getTerm());\n+        builder.field(DOC_COUNT_FIELD, getDocCount());\n+        return builder;\n+    }\n+\n+    public static TermCount fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TermCount other = (TermCount) o;\n+        return Objects.equals(getTerm(), other.getTerm()) && Objects.equals(getDocCount(), other.getDocCount());\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(getTerm(), getDocCount());\n+    }\n+\n+    void addToDocCount(long extra) {\n+        docCount += extra;\n+    }\n+\n+    void setTerm(String term) {\n+        this.term = term;\n+    }\n+\n+    void setDocCount(long docCount) {\n+        this.docCount = docCount;\n+    }", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA0MjY4OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621042688", "bodyText": "addToDocCount(int) is used to combine counts from shards but the setDocCount isn't. Removed.", "author": "markharwood", "createdAt": "2021-04-27T09:38:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzOTI0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MDgwMg==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612440802", "bodyText": "let's write it as a time value?", "author": "jpountz", "createdAt": "2021-04-13T13:21:41Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumRequest.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.ValidateActions;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.action.support.broadcast.BroadcastRequest;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+/**\n+ * A request to gather terms for a given field matching a string prefix\n+ */\n+public class TermEnumRequest extends BroadcastRequest<TermEnumRequest> implements ToXContentObject {\n+\n+    public static int DEFAULT_SIZE = 10;\n+    public static int DEFAULT_TIMEOUT_MILLIS = 1000;\n+\n+    private String field;\n+    private String string;\n+    private int size = DEFAULT_SIZE;\n+    private boolean caseInsensitive;\n+    long taskStartTimeMillis;\n+    private QueryBuilder indexFilter;\n+\n+    public TermEnumRequest() {\n+        this(Strings.EMPTY_ARRAY);\n+    }\n+\n+    public TermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+    }\n+\n+    /**\n+     * Constructs a new term enum request against the provided indices. No indices provided means it will\n+     * run against all indices.\n+     */\n+    public TermEnumRequest(String... indices) {\n+        super(indices);\n+        indicesOptions(IndicesOptions.fromOptions(false, false, true, false));\n+        timeout(new TimeValue(DEFAULT_TIMEOUT_MILLIS));\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        ActionRequestValidationException validationException = super.validate();\n+        if (field == null) {\n+            validationException = ValidateActions.addValidationError(\"field cannot be null\", validationException);\n+        }\n+        return validationException;\n+    }\n+\n+    /**\n+     * The field to look inside for values\n+     */\n+    public void field(String field) {\n+        this.field = field;\n+    }\n+\n+    /**\n+     * Indicates if detailed information about query is requested\n+     */\n+    public String field() {\n+        return field;\n+    }\n+\n+    /**\n+     * The string required in matching field values\n+     */\n+    public void string(String string) {\n+        this.string = string;\n+    }\n+\n+    /**\n+     * The string required in matching field values\n+     */\n+    public String string() {\n+        return string;\n+    }\n+\n+    /**\n+     *  The number of terms to return\n+     */\n+    public int size() {\n+        return size;\n+    }\n+\n+    /**\n+     * The number of terms to return\n+     */\n+    public void size(int size) {\n+        this.size = size;\n+    }\n+\n+    /**\n+     * TThe max time in milliseconds to spend gathering terms\n+     */\n+    public void timeoutInMillis(int timeout) {\n+        timeout(new TimeValue(timeout));\n+    }\n+\n+    /**\n+     * If case insensitive matching is required\n+     */\n+    public void caseInsensitive(boolean caseInsensitive) {\n+        this.caseInsensitive = caseInsensitive;\n+    }\n+\n+    /**\n+     * If case insensitive matching is required\n+     */\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    /**\n+     * Allows to filter shards if the provided {@link QueryBuilder} rewrites to `match_none`.\n+     */\n+    public void indexFilter(QueryBuilder indexFilter) {\n+        this.indexFilter = indexFilter;\n+    }    \n+    \n+    public QueryBuilder indexFilter() {\n+        return indexFilter;\n+    }    \n+    \n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        out.writeOptionalNamedWriteable(indexFilter);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"[\" + Arrays.toString(indices) + \"] field[\" + field + \"], string[\" + string + \"] \"  + \" size=\" + size + \" timeout=\"\n+            + timeout().getMillis() + \" case_insensitive=\"\n+            + caseInsensitive + \" indexFilter = \"+ indexFilter;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.field(\"field\", field);\n+        builder.field(\"string\", string);\n+        builder.field(\"size\", size);\n+        builder.field(\"timeout\", timeout().getMillis());", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjI2Nw==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442267", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if(randomBoolean()) {\n          \n          \n            \n                                if (randomBoolean()) {", "author": "jpountz", "createdAt": "2021-04-13T13:23:26Z", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjM4OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442388", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    while(te.next()!=null) {\n          \n          \n            \n                                    while (te.next()!=null) {", "author": "jpountz", "createdAt": "2021-04-13T13:23:35Z", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjU0MA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442540", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if(term.startsWith(searchPrefix)) {\n          \n          \n            \n                                if (term.startsWith(searchPrefix)) {", "author": "jpountz", "createdAt": "2021-04-13T13:23:45Z", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {\n+                            termCounts.add(new TermCount(te.term().utf8ToString(), te.docFreq()));\n+                        }\n+                        SimpleTermCountEnum simpleEnum = new SimpleTermCountEnum(termCounts.toArray(new TermCount[0]));\n+                        termsEnums.add(simpleEnum);\n+                    } else {\n+                        termsEnums.add(te);\n+                    }\n+                }\n+                MultiShardTermsEnum mte = new MultiShardTermsEnum(termsEnums.toArray(new TermsEnum[0]));\n+                HashMap<String, Integer> expecteds = new HashMap<>();\n+\n+                for (String term : globalTermCounts.keySet()) {\n+                    if(term.startsWith(searchPrefix)) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MzMyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612443321", "bodyText": "nit: can you iterate over entries rathen than keys since you need values too?", "author": "jpountz", "createdAt": "2021-04-13T13:24:33Z", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {\n+                            termCounts.add(new TermCount(te.term().utf8ToString(), te.docFreq()));\n+                        }\n+                        SimpleTermCountEnum simpleEnum = new SimpleTermCountEnum(termCounts.toArray(new TermCount[0]));\n+                        termsEnums.add(simpleEnum);\n+                    } else {\n+                        termsEnums.add(te);\n+                    }\n+                }\n+                MultiShardTermsEnum mte = new MultiShardTermsEnum(termsEnums.toArray(new TermsEnum[0]));\n+                HashMap<String, Integer> expecteds = new HashMap<>();\n+\n+                for (String term : globalTermCounts.keySet()) {", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0NDM4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612444386", "bodyText": "should we use maxDoc() to be consistent with keyword fields, which don't ignore deletes?", "author": "jpountz", "createdAt": "2021-04-13T13:25:41Z", "path": "x-pack/plugin/mapper-constant-keyword/src/main/java/org/elasticsearch/xpack/constantkeyword/mapper/ConstantKeywordFieldMapper.java", "diffHunk": "@@ -140,6 +144,20 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n                 ? lookup -> List.of()\n                 : lookup -> List.of(value);\n         }\n+        \n+        \n+\n+        @Override\n+        public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException {\n+            boolean matches = caseInsensitive ? \n+                value.toLowerCase(Locale.ROOT).startsWith(string.toLowerCase(Locale.ROOT)) : \n+                value.startsWith(string);\n+            if (matches == false) {\n+                return null;\n+            }\n+            int docCount = queryShardContext.searcher().getIndexReader().numDocs();", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0NzQ4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612447486", "bodyText": "I usually don't like losing stack traces, which can be very useful for debugging purposes.", "author": "jpountz", "createdAt": "2021-04-13T13:29:09Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();", "originalCommit": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "830f52b43a5ed1c94028a1b81ae726dfe6fb3f5c", "url": "https://github.com/elastic/elasticsearch/commit/830f52b43a5ed1c94028a1b81ae726dfe6fb3f5c", "message": "Docs tidy up", "committedDate": "2021-04-14T10:24:41Z", "type": "forcePushed"}, {"oid": "2afb47beca8753746c2770e32ce763c72f7c3451", "url": "https://github.com/elastic/elasticsearch/commit/2afb47beca8753746c2770e32ce763c72f7c3451", "message": "Provide full stack traces for errors, change TODO comment", "committedDate": "2021-04-14T16:49:29Z", "type": "forcePushed"}, {"oid": "ac097fece74dcfc02f3af6cacd1a64a50d785b8b", "url": "https://github.com/elastic/elasticsearch/commit/ac097fece74dcfc02f3af6cacd1a64a50d785b8b", "message": "Provide full stack traces for errors, change TODO comment", "committedDate": "2021-04-19T08:28:18Z", "type": "forcePushed"}, {"oid": "5821ccb3253168ad870c07f7a18ec8dbf2d18949", "url": "https://github.com/elastic/elasticsearch/commit/5821ccb3253168ad870c07f7a18ec8dbf2d18949", "message": "Move location of YAML test - was causing errors when seated alongside core/src/yamlRestTest", "committedDate": "2021-04-19T11:34:52Z", "type": "forcePushed"}, {"oid": "24d73065943096b7174b216e8ec4fed76538a63c", "url": "https://github.com/elastic/elasticsearch/commit/24d73065943096b7174b216e8ec4fed76538a63c", "message": "Remove acquisition of searcher from security check code", "committedDate": "2021-04-23T12:38:01Z", "type": "forcePushed"}, {"oid": "62782c3d62e5d967b32a13be373841297deac591", "url": "https://github.com/elastic/elasticsearch/commit/62782c3d62e5d967b32a13be373841297deac591", "message": "Remove acquisition of searcher from security check code", "committedDate": "2021-04-26T12:51:06Z", "type": "forcePushed"}, {"oid": "fafdcef55087945712582ed0a3661d6d417478bd", "url": "https://github.com/elastic/elasticsearch/commit/fafdcef55087945712582ed0a3661d6d417478bd", "message": "Remove acquisition of searcher from security check code", "committedDate": "2021-04-26T13:20:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMDk2MDk3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r620960971", "bodyText": "nit: extra lines ?", "author": "jimczi", "createdAt": "2021-04-27T08:00:46Z", "path": "server/src/main/java/org/elasticsearch/index/mapper/KeywordFieldMapper.java", "diffHunk": "@@ -247,6 +257,27 @@ public KeywordFieldType(String name, NamedAnalyzer analyzer) {\n             this.eagerGlobalOrdinals = false;\n             this.scriptValues = null;\n         }\n+        \n+        \n+", "originalCommit": "e43475f856f7dca4ff6bd92598f2d64dda5cf921", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA3ODUyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621078521", "bodyText": "We should use the search threadpool here if the queue is empty.\nSomething like:\n        assert transportService.getThreadPool().executor(ThreadPool.Names.SEARCH) instanceof EsThreadPoolExecutor\n            : \"SEARCH threadpool must be an instance of ThreadPoolExecutor\";\n        EsThreadPoolExecutor ex = (EsThreadPoolExecutor) transportService.getThreadPool().executor(ThreadPool.Names.SEARCH);\n        final String executorName = ex.getQueue().size() == 0 ? ThreadPool.Names.SEARCH : shardExecutor;", "author": "jimczi", "createdAt": "2021-04-27T10:27:52Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termsenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.query.Rewriteable;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.script.ScriptService;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.SecurityContext;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+import org.elasticsearch.xpack.core.security.authz.support.DLSRoleQueryValidator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermsEnumAction extends HandledTransportAction<TermsEnumRequest, TermsEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    private final ScriptService scriptService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermsEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ScriptService scriptService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermsEnumAction.NAME, transportService, actionFilters, TermsEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.scriptService = scriptService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermsEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermsEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermsEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermsEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermsEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermsEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermsEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermsEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermsEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermsEnumResponse newResponse(\n+        TermsEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermsEnumResponse str = (NodeTermsEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermsEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermsEnumResponse dataNodeOperation(NodeTermsEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = ExceptionsHelper.stackTrace(e);\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - write a separate Interceptor class to \n+    // rewrite requests according to security rules \n+    private boolean canAccess(\n+        ShardId shardId,\n+        NodeTermsEnumRequest request,\n+        XPackLicenseState frozenLicenseState,\n+        ThreadContext threadContext        \n+    ) throws IOException {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(shardId.getIndexName());\n+\n+         \n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    // Check to see if any of the roles defined for the current user rewrite to match_all \n+                    \n+                    SecurityContext securityContext = new SecurityContext(clusterService.getSettings(), threadContext);\n+                    final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        null,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+\n+                    // Current user has potentially many roles and therefore potentially many queries\n+                    // defining sets of docs accessible\n+                    Set<BytesReference> queries = indexAccessControl.getDocumentPermissions().getQueries();\n+                    for (BytesReference querySource : queries) {\n+                        QueryBuilder queryBuilder = DLSRoleQueryValidator.evaluateAndVerifyRoleQuery(\n+                            querySource,\n+                            scriptService,\n+                            queryShardContext.getXContentRegistry(),\n+                            securityContext.getUser()\n+                        );\n+                        QueryBuilder rewrittenQueryBuilder = Rewriteable.rewrite(queryBuilder, queryShardContext);\n+                        if (rewrittenQueryBuilder instanceof MatchAllQueryBuilder) {\n+                            // One of the roles assigned has \"all\" permissions - allow unfettered access to termsDict\n+                            return true;\n+                        }\n+                    }\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermsEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.shardStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermsEnumRequest request;\n+        private ActionListener<TermsEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermsEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final NodeTermsEnumRequest nodeRequest = newNodeRequest(nodeId, shardIds, request);\n+                    nodeRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(nodeId);\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onNoOperation(nodeId);\n+                    } else if (checkForEarlyFinish() == false) {\n+                        transportService.sendRequest(\n+                            node,\n+                            transportShardAction,\n+                            nodeRequest,\n+                            new TransportResponseHandler<NodeTermsEnumResponse>() {\n+                                @Override\n+                                public NodeTermsEnumResponse read(StreamInput in) throws IOException {\n+                                    return readShardResponse(in);\n+                                }\n+\n+                                @Override\n+                                public void handleResponse(NodeTermsEnumResponse response) {\n+                                    onOperation(nodeId, nodeIndex, response);\n+                                }\n+\n+                                @Override\n+                                public void handleException(TransportException e) {\n+                                    onNoOperation(nodeId);\n+                                }\n+                            }\n+                        );\n+                    }\n+                } catch (Exception e) {\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        protected void onOperation(String nodeId, int nodeIndex, NodeTermsEnumResponse response) {\n+            logger.trace(\"received response for node {}\", nodeId);\n+            nodesResponses.set(nodeIndex, response);\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            } else {\n+                checkForEarlyFinish();\n+            }\n+        }\n+\n+        void onNoOperation(String nodeId) {\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            }\n+        }\n+\n+        // Can be called multiple times - either for early time-outs or for fully-completed collections.\n+        protected synchronized void finishHim(boolean complete) {\n+            if (listener == null) {\n+                return;\n+            }\n+            try {\n+                listener.onResponse(newResponse(request, nodesResponses, complete, nodeBundles));\n+            } catch (Exception e) {\n+                listener.onFailure(e);\n+            } finally {\n+                listener = null;\n+            }\n+        }\n+    }\n+\n+    class NodeTransportHandler implements TransportRequestHandler<NodeTermsEnumRequest> {\n+\n+        @Override\n+        public void messageReceived(NodeTermsEnumRequest request, TransportChannel channel, Task task) throws Exception {\n+            asyncNodeOperation(request, task, ActionListener.wrap(channel::sendResponse, e -> {\n+                try {\n+                    channel.sendResponse(e);\n+                } catch (Exception e1) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\n+                            \"Failed to send error response for action [{}] and request [{}]\",\n+                            actionName,\n+                            request\n+                        ),\n+                        e1\n+                    );\n+                }\n+            }));\n+        }\n+    }\n+\n+    private void asyncNodeOperation(NodeTermsEnumRequest request, Task task, ActionListener<NodeTermsEnumResponse> listener)\n+        throws IOException {\n+        // DLS/FLS check copied from ResizeRequestInterceptor - check permissions and\n+        // any index_filter canMatch checks on network thread before allocating work\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+        for (ShardId shardId : request.shardIds().toArray(new ShardId[0])) {\n+            if (canAccess(shardId, request, frozenLicenseState, threadContext) == false || canMatchShard(\n+                shardId,\n+                request\n+            ) == false) {\n+                // Permission denied or can't match, remove shardID from request\n+                request.remove(shardId);\n+            }\n+        }\n+        if (request.shardIds().size() == 0) {\n+            listener.onResponse(new NodeTermsEnumResponse(request.nodeId(), Collections.emptyList(), null, true));\n+        } else {", "originalCommit": "e43475f856f7dca4ff6bd92598f2d64dda5cf921", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "73dda500944e011d198785cb0da7e444efb94527", "url": "https://github.com/elastic/elasticsearch/commit/73dda500944e011d198785cb0da7e444efb94527", "message": "Checkstyle fix", "committedDate": "2021-04-27T16:27:05Z", "type": "forcePushed"}, {"oid": "721b9315d36be5a43ae2ec272caa4315fb8339e5", "url": "https://github.com/elastic/elasticsearch/commit/721b9315d36be5a43ae2ec272caa4315fb8339e5", "message": "In flattened fields make only the value (not the field name) subject to case insensitive search", "committedDate": "2021-04-30T09:55:44Z", "type": "forcePushed"}, {"oid": "900732e5b142282fcf0a921348973d214a97371f", "url": "https://github.com/elastic/elasticsearch/commit/900732e5b142282fcf0a921348973d214a97371f", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation", "committedDate": "2021-04-30T13:48:08Z", "type": "forcePushed"}, {"oid": "bcf76a4c62533664747506bc1c876a5b2a1fbf2c", "url": "https://github.com/elastic/elasticsearch/commit/bcf76a4c62533664747506bc1c876a5b2a1fbf2c", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation", "committedDate": "2021-05-04T13:26:48Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNzE0MTQ5OA==", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r627141498", "bodyText": "Is the TODO still needed ?", "author": "jimczi", "createdAt": "2021-05-06T07:11:35Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termsenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.query.Rewriteable;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.script.ScriptService;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.SecurityContext;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+import org.elasticsearch.xpack.core.security.authz.support.DLSRoleQueryValidator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermsEnumAction extends HandledTransportAction<TermsEnumRequest, TermsEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    private final ScriptService scriptService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermsEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ScriptService scriptService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermsEnumAction.NAME, transportService, actionFilters, TermsEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.scriptService = scriptService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermsEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermsEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermsEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermsEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermsEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermsEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermsEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermsEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermsEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermsEnumResponse newResponse(\n+        TermsEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermsEnumResponse str = (NodeTermsEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermsEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermsEnumResponse dataNodeOperation(NodeTermsEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.nodeStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::nodeStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = ExceptionsHelper.stackTrace(e);\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - write a separate Interceptor class to \n+    // rewrite requests according to security rules \n+    private boolean canAccess(\n+        ShardId shardId,\n+        NodeTermsEnumRequest request,\n+        XPackLicenseState frozenLicenseState,\n+        ThreadContext threadContext        \n+    ) throws IOException {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(shardId.getIndexName());\n+\n+         \n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    // Check to see if any of the roles defined for the current user rewrite to match_all \n+                    \n+                    SecurityContext securityContext = new SecurityContext(clusterService.getSettings(), threadContext);\n+                    final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        null,\n+                        request::nodeStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+\n+                    // Current user has potentially many roles and therefore potentially many queries\n+                    // defining sets of docs accessible\n+                    Set<BytesReference> queries = indexAccessControl.getDocumentPermissions().getQueries();\n+                    for (BytesReference querySource : queries) {\n+                        QueryBuilder queryBuilder = DLSRoleQueryValidator.evaluateAndVerifyRoleQuery(\n+                            querySource,\n+                            scriptService,\n+                            queryShardContext.getXContentRegistry(),\n+                            securityContext.getUser()\n+                        );\n+                        QueryBuilder rewrittenQueryBuilder = Rewriteable.rewrite(queryBuilder, queryShardContext);\n+                        if (rewrittenQueryBuilder instanceof MatchAllQueryBuilder) {\n+                            // One of the roles assigned has \"all\" permissions - allow unfettered access to termsDict\n+                            return true;\n+                        }\n+                    }\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermsEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.nodeStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermsEnumRequest request;\n+        private ActionListener<TermsEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermsEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.", "originalCommit": "bcf76a4c62533664747506bc1c876a5b2a1fbf2c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9ffbb03e37184c44c256a22a45e227032f554723", "url": "https://github.com/elastic/elasticsearch/commit/9ffbb03e37184c44c256a22a45e227032f554723", "message": "A TermsEnum API for discovering terms in the index.\nTerms matching a given prefix can be returned in alphabetical or by popularity,\nA timeout can limit the amount of time spent looking for matches.\n\nExpected to be useful in auto-complete or regex debugging use cases.", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "b95cf815b49ce78d42b51440ad983c025bb9c254", "url": "https://github.com/elastic/elasticsearch/commit/b95cf815b49ce78d42b51440ad983c025bb9c254", "message": "Added HLRC support and related integration test", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "ab826b43205d0bfd257353fb46e234dc43103b3d", "url": "https://github.com/elastic/elasticsearch/commit/ab826b43205d0bfd257353fb46e234dc43103b3d", "message": "Added client classes for HLRC.\nAdded yaml test with index filter", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "7a5e654f83912d0a27e87c0dca0b2ea89ede51d2", "url": "https://github.com/elastic/elasticsearch/commit/7a5e654f83912d0a27e87c0dca0b2ea89ede51d2", "message": "License fix", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "93dfe30fca29bffc372feda1bc44d3e317e6e973", "url": "https://github.com/elastic/elasticsearch/commit/93dfe30fca29bffc372feda1bc44d3e317e6e973", "message": "Remove HLRC code for now - requires less-than-ideal package names while we\u2019re unable to move main server implementation out of xpack (due to security dependency).\nDon\u2019t want clients to build dependencies on the wrong package names and the Java client changes are underway anyway.", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "58bbf413044259897bd8c54e4013e6243c3eb630", "url": "https://github.com/elastic/elasticsearch/commit/58bbf413044259897bd8c54e4013e6243c3eb630", "message": "Return empty arrays when no results rather than no `terms` property at all", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "7fb781d5f55fdf345242ac73ca9165ee998e4768", "url": "https://github.com/elastic/elasticsearch/commit/7fb781d5f55fdf345242ac73ca9165ee998e4768", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "4e9da3918ad693375203246f2bb73303007156f3", "url": "https://github.com/elastic/elasticsearch/commit/4e9da3918ad693375203246f2bb73303007156f3", "message": "Type fixes", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "cac1bb32d38a4481dd6812adb80790dbfa8784e9", "url": "https://github.com/elastic/elasticsearch/commit/cac1bb32d38a4481dd6812adb80790dbfa8784e9", "message": "Types warning", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "810d638656e36752cbe3f7c97a4dbdc6e8eeff1b", "url": "https://github.com/elastic/elasticsearch/commit/810d638656e36752cbe3f7c97a4dbdc6e8eeff1b", "message": "Removed hot/warm tier tests (in anticipation of new queryable _tier field)\nMove canMatch logic to run on network thread\nInjected searchService so we can use its canMatch method", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "156302f3edfd1032c92b4e766eaacaaa3fe983a5", "url": "https://github.com/elastic/elasticsearch/commit/156302f3edfd1032c92b4e766eaacaaa3fe983a5", "message": "Move rest-api-spec and related YML test to new standard home for this stuff.", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "62af758dc220f2a25d825c4e809403be73dfcb4d", "url": "https://github.com/elastic/elasticsearch/commit/62af758dc220f2a25d825c4e809403be73dfcb4d", "message": "Unused import", "committedDate": "2021-05-06T08:40:31Z", "type": "commit"}, {"oid": "03f79deb3b10a9907699a8ca981a6b6c77b8bf75", "url": "https://github.com/elastic/elasticsearch/commit/03f79deb3b10a9907699a8ca981a6b6c77b8bf75", "message": "Move test to xpack", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "53e12cdbbd0a1d884da6b44d19eaabf59eaaa35f", "url": "https://github.com/elastic/elasticsearch/commit/53e12cdbbd0a1d884da6b44d19eaabf59eaaa35f", "message": "Return early on network thread if can\u2019t match any shards.\nRemove FLS logic now we use the right searcher.", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "b36f477f5901ed4f5eaeb1686fbe9b2676a88e3d", "url": "https://github.com/elastic/elasticsearch/commit/b36f477f5901ed4f5eaeb1686fbe9b2676a88e3d", "message": "Removed sort by popularity option", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "3b1b6d935a51a4940e6ea7d05d97e4b3d87b1614", "url": "https://github.com/elastic/elasticsearch/commit/3b1b6d935a51a4940e6ea7d05d97e4b3d87b1614", "message": "Unused import", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "5250cc7e524b1019121feadbde24b80167a8a0da", "url": "https://github.com/elastic/elasticsearch/commit/5250cc7e524b1019121feadbde24b80167a8a0da", "message": "Addressing some review comments (thanks Jim/Adrien!)", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "3288641ae857d9aae7096f704ade6fdd9eac53e8", "url": "https://github.com/elastic/elasticsearch/commit/3288641ae857d9aae7096f704ade6fdd9eac53e8", "message": "Docs tidy up", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "2c0096859600a7e8a8a3f36a92ab5a205dcd8dd2", "url": "https://github.com/elastic/elasticsearch/commit/2c0096859600a7e8a8a3f36a92ab5a205dcd8dd2", "message": "Provide full stack traces for errors, change TODO comment", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "6a68b70631cf226f2130069d4a369b50f3c7f01a", "url": "https://github.com/elastic/elasticsearch/commit/6a68b70631cf226f2130069d4a369b50f3c7f01a", "message": "Move location of YAML test - was causing errors when seated alongside core/src/yamlRestTest", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "1fe0a11b41924d69a2fbea9e2d1b19c0cd3ff8b6", "url": "https://github.com/elastic/elasticsearch/commit/1fe0a11b41924d69a2fbea9e2d1b19c0cd3ff8b6", "message": "Security enhancement - allow access where DLS rewrites to match_all. Added security tests", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "2f598607ac112be5158e0c8654810fbdff006045", "url": "https://github.com/elastic/elasticsearch/commit/2f598607ac112be5158e0c8654810fbdff006045", "message": "Remove acquisition of searcher from security check code", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "4c38b78005934cbd8c6b83e4ba65d74b98c6fd62", "url": "https://github.com/elastic/elasticsearch/commit/4c38b78005934cbd8c6b83e4ba65d74b98c6fd62", "message": "Changed termenum to termsenum. REST endpoint is now _terms_enum", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "c40a0dba4994b8146499d0e298eeca90e527ab04", "url": "https://github.com/elastic/elasticsearch/commit/c40a0dba4994b8146499d0e298eeca90e527ab04", "message": "Checkstyle fix", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "6b9f41c3e3c7fb613745361977d485fdbb456a19", "url": "https://github.com/elastic/elasticsearch/commit/6b9f41c3e3c7fb613745361977d485fdbb456a19", "message": "Addressing review comments - formatting, thread pool choices and more", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "814e45e9e0c5e4ae1d94c26aed5fdb091851690d", "url": "https://github.com/elastic/elasticsearch/commit/814e45e9e0c5e4ae1d94c26aed5fdb091851690d", "message": "Oops. Thought I\u2019d resolved this review comment but hadn\u2019t", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "989751883f134abeafdef6da3a747ac70203573a", "url": "https://github.com/elastic/elasticsearch/commit/989751883f134abeafdef6da3a747ac70203573a", "message": "Changed timeout setting to a TimeValue", "committedDate": "2021-05-06T08:40:32Z", "type": "commit"}, {"oid": "2cb91dfcc247cd1deb849fd8cf5403f7e7117f34", "url": "https://github.com/elastic/elasticsearch/commit/2cb91dfcc247cd1deb849fd8cf5403f7e7117f34", "message": "Checkstyle fix", "committedDate": "2021-05-06T08:40:33Z", "type": "commit"}, {"oid": "6d55f99c93e4f61b20a537d6c8ad1c507f95a314", "url": "https://github.com/elastic/elasticsearch/commit/6d55f99c93e4f61b20a537d6c8ad1c507f95a314", "message": "In flattened fields make only the value (not the field name) subject to case insensitive search", "committedDate": "2021-05-06T08:40:33Z", "type": "commit"}, {"oid": "cf7005361a9b3426685eb5911f36c6255c423f2e", "url": "https://github.com/elastic/elasticsearch/commit/cf7005361a9b3426685eb5911f36c6255c423f2e", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation", "committedDate": "2021-05-06T08:40:33Z", "type": "commit"}, {"oid": "22312cf82527d80e4b330d56fec8108c9cdfd513", "url": "https://github.com/elastic/elasticsearch/commit/22312cf82527d80e4b330d56fec8108c9cdfd513", "message": "Remove outdated TODOs", "committedDate": "2021-05-06T08:40:33Z", "type": "commit"}, {"oid": "22312cf82527d80e4b330d56fec8108c9cdfd513", "url": "https://github.com/elastic/elasticsearch/commit/22312cf82527d80e4b330d56fec8108c9cdfd513", "message": "Remove outdated TODOs", "committedDate": "2021-05-06T08:40:33Z", "type": "forcePushed"}]}