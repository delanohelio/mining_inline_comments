{"pr_number": 62441, "pr_title": "Also abort ongoing file restores when snapshot restore is aborted", "pr_createdAt": "2020-09-16T10:08:05Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62441", "timeline": [{"oid": "1683d75ab56959f3ebd8fce210fc9914fa9a2255", "url": "https://github.com/elastic/elasticsearch/commit/1683d75ab56959f3ebd8fce210fc9914fa9a2255", "message": "Also abort ongoing file restores when snapshot restore is aborted", "committedDate": "2020-09-16T09:45:08Z", "type": "commit"}, {"oid": "a7c76e0dcaa5a572a38ad721a33109979938210a", "url": "https://github.com/elastic/elasticsearch/commit/a7c76e0dcaa5a572a38ad721a33109979938210a", "message": "Merge branch 'master' into abort-file-restores-when-restore-is-aborted", "committedDate": "2020-09-16T12:16:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM4NjM4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489386386", "bodyText": "I'd reword this a little:\n/**\n * @return true if the {@link Store#close()} method has been called. This indicates that the current \n * store is either closed or being closed waiting for all references to it to be released. \n * You might prefer to use {@link Store#ensureOpen()} instead.\n */", "author": "original-brownbear", "createdAt": "2020-09-16T12:09:44Z", "path": "server/src/main/java/org/elasticsearch/index/store/Store.java", "diffHunk": "@@ -403,14 +403,21 @@ public final void decRef() {\n \n     @Override\n     public void close() {\n-\n         if (isClosed.compareAndSet(false, true)) {\n             // only do this once!\n             decRef();\n             logger.debug(\"store reference count on close: {}\", refCounter.refCount());\n         }\n     }\n \n+    /**\n+     * @return true if the {@link Store#close()} method has been called indicating that the current store is being closed but potentially\n+     * not yet fully closed and released. You might prefer to use {@link Store#ensureOpen()} instead.\n+     */\n+    public boolean isClosing() {", "originalCommit": "1683d75ab56959f3ebd8fce210fc9914fa9a2255", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDA0ODA4Nw==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r490048087", "bodyText": "It's better, thanks!", "author": "tlrx", "createdAt": "2020-09-17T08:01:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM4NjM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM4ODQ4NA==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489388484", "bodyText": "See below comment: I think you can just use createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE) if you check the store on every chunk read?", "author": "original-brownbear", "createdAt": "2020-09-16T12:13:34Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/AbortedRestoreIT.java", "diffHunk": "@@ -0,0 +1,352 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.RepositoriesService;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.AbortedRestoreIT.BlockingDataFileReadsRepository.BlockingInputStream;\n+import org.elasticsearch.snapshots.mockstore.BlobStoreWrapper;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.threadpool.ThreadPoolStats;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.allOf;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class AbortedRestoreIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final Collection<Class<? extends Plugin>> plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(BlockingDataFileReadsRepository.Plugin.class);\n+        return plugins;\n+    }\n+\n+    public void testAbortedRestoreAlsoAbortFileRestores() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+\n+        final String indexName = \"test-abort-restore\";\n+        final int numPrimaries = randomIntBetween(1, 3);\n+        createIndex(indexName, indexSettingsNoReplicas(numPrimaries).build());\n+        indexRandomDocs(indexName, scaledRandomIntBetween(10, 1_000));\n+        ensureGreen();\n+        forceMerge();\n+\n+        final String repositoryName = \"repository\";\n+        createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE, Settings.builder().put(\"location\", randomRepoPath()));", "originalCommit": "1683d75ab56959f3ebd8fce210fc9914fa9a2255", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM5MTgwMA==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489391800", "bodyText": "Maybe just override org.elasticsearch.snapshots.mockstore.MockRepository#blockOnDataFiles and add a method to verify no more interactions (could just add a flag that throws an AssertionError past a certain point)? Then you can just wait for the snapshot threads to all block, unblock and wait for the snapshot pool to be empty. Much less new test code to maintain and tests the same thing pretty much?", "author": "original-brownbear", "createdAt": "2020-09-16T12:19:16Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/AbortedRestoreIT.java", "diffHunk": "@@ -0,0 +1,352 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.RepositoriesService;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.AbortedRestoreIT.BlockingDataFileReadsRepository.BlockingInputStream;\n+import org.elasticsearch.snapshots.mockstore.BlobStoreWrapper;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.threadpool.ThreadPoolStats;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.allOf;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class AbortedRestoreIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final Collection<Class<? extends Plugin>> plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(BlockingDataFileReadsRepository.Plugin.class);\n+        return plugins;\n+    }\n+\n+    public void testAbortedRestoreAlsoAbortFileRestores() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+\n+        final String indexName = \"test-abort-restore\";\n+        final int numPrimaries = randomIntBetween(1, 3);\n+        createIndex(indexName, indexSettingsNoReplicas(numPrimaries).build());\n+        indexRandomDocs(indexName, scaledRandomIntBetween(10, 1_000));\n+        ensureGreen();\n+        forceMerge();\n+\n+        final String repositoryName = \"repository\";\n+        createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE, Settings.builder().put(\"location\", randomRepoPath()));\n+\n+        final String snapshotName = \"snapshot\";\n+        createFullSnapshot(repositoryName, snapshotName);\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        logger.info(\"--> blocking node on data files [{}] before restore\", dataNode);\n+        final BlockingDataFileReadsRepository blockRepository = ((BlockingDataFileReadsRepository) internalCluster()\n+            .getInstance(RepositoriesService.class, dataNode)\n+            .repository(repositoryName));\n+        blockRepository.block();\n+\n+        logger.info(\"--> starting restore\");\n+        final ActionFuture<RestoreSnapshotResponse> future = client().admin().cluster().prepareRestoreSnapshot(repositoryName, snapshotName)\n+            .setWaitForCompletion(true)\n+            .setIndices(indexName)\n+            .execute();\n+\n+        assertBusy(() -> {\n+            final RecoveryResponse recoveries = client().admin().indices().prepareRecoveries(indexName)\n+                .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN).setActiveOnly(true).get();\n+            assertThat(recoveries.hasRecoveries(), is(true));\n+            final List<RecoveryState> shardRecoveries = recoveries.shardRecoveryStates().get(indexName);\n+            assertThat(shardRecoveries, hasSize(numPrimaries));\n+            assertThat(future.isDone(), is(false));\n+\n+            for (RecoveryState shardRecovery : shardRecoveries) {\n+                assertThat(shardRecovery.getRecoverySource().getType(), equalTo(RecoverySource.Type.SNAPSHOT));\n+                assertThat(shardRecovery.getStage(), equalTo(RecoveryState.Stage.INDEX));\n+            }\n+        });\n+\n+        logger.info(\"--> waiting for snapshot thread pool to be full\");\n+        assertBusy(() -> {\n+            ThreadPool threadPool = internalCluster().getInstance(ClusterService.class, dataNode).getClusterApplierService().threadPool();\n+            int activeSnapshotThreads = -1;\n+            for (ThreadPoolStats.Stats threadPoolStats : threadPool.stats()) {\n+                if (threadPoolStats.getName().equals(ThreadPool.Names.SNAPSHOT)) {\n+                    activeSnapshotThreads = threadPoolStats.getActive();\n+                    break;\n+                }\n+            }\n+            final ThreadPool.Info threadPoolInfo = threadPool.info(ThreadPool.Names.SNAPSHOT);\n+            assertThat(activeSnapshotThreads, allOf(greaterThan(0), equalTo(threadPoolInfo.getMax())));\n+            assertThat(blockRepository.streams().filter(BlockingInputStream::isBlocked).count(), equalTo((long) activeSnapshotThreads));\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        logger.info(\"--> aborting restore by deleting the index\");\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // Total number of blobs that have been opened from the blob store\n+        final long totalBlobsRead = blockRepository.streams().count();\n+\n+        // Total number of bytes that have been read from the blob store\n+        final long totalBytesRead = blockRepository.streams().mapToLong(BlockingInputStream::getCount).sum();\n+\n+        logger.info(\"--> unblocking node [{}]\", dataNode);\n+        blockRepository.unblock();\n+        assertThat(blockRepository.streams().noneMatch(BlockingInputStream::isBlocked), is(true));\n+\n+        logger.info(\"--> restore should have failed\");\n+        final RestoreSnapshotResponse restoreSnapshotResponse = future.get();\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().failedShards(), equalTo(numPrimaries));\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().successfulShards(), equalTo(0));\n+\n+        logger.info(\"--> waiting for snapshot thread pool to be empty\");\n+        assertBusy(() -> {\n+            ThreadPool threadPool = internalCluster().getInstance(ClusterService.class, dataNode).getClusterApplierService().threadPool();\n+            int activeSnapshotThreads = -1;\n+            for (ThreadPoolStats.Stats threadPoolStats : threadPool.stats()) {\n+                if (threadPoolStats.getName().equals(ThreadPool.Names.SNAPSHOT)) {\n+                    activeSnapshotThreads = threadPoolStats.getActive();\n+                    break;\n+                }\n+            }\n+            assertThat(activeSnapshotThreads, equalTo(0));\n+            assertThat(blockRepository.streams().filter(BlockingInputStream::isBlocked).count(), equalTo(0L));\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        assertThat(\"No more blobs should have been opened from the blob store\",\n+            blockRepository.streams().count(), equalTo(totalBlobsRead));\n+        assertThat(\"No more bytes should have been read from the blob store\",\n+            blockRepository.streams().mapToLong(BlockingInputStream::getCount).sum(), equalTo(totalBytesRead));\n+    }\n+\n+    /**\n+     * A blob store repository that blocks read operations on {@link InputStream} when the {@code blockStreams} flag\n+     * is set to true. It also keep track of the number of bytes read from {@link InputStream} it opens.\n+     */\n+    public static class BlockingDataFileReadsRepository extends FsRepository {\n+\n+        static final String TYPE = \"block_on_data_file_reads\";\n+\n+        private final List<BlockingInputStream> streams = Collections.synchronizedList(new ArrayList<>());\n+        private volatile boolean blockStreams = false;\n+\n+        public BlockingDataFileReadsRepository(", "originalCommit": "1683d75ab56959f3ebd8fce210fc9914fa9a2255", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQwMzk5MA==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489403990", "bodyText": "I agree it would be less code to maintain, but MockRepository blocks before reading any bytes and I wanted something that allows to read few bytes then block. But now I'm thinking to it again it does not make more difference as InputStream opened by FS blob containers do not have any abort method. I'm going to take a look again at using MockRepository.", "author": "tlrx", "createdAt": "2020-09-16T12:39:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM5MTgwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM5NTM0NA==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r489395344", "bodyText": "Maybe put ensureNotClosing here as well? Then you can run your test with random chunk_size as well I think?", "author": "original-brownbear", "createdAt": "2020-09-16T12:25:16Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -2101,9 +2107,10 @@ protected InputStream openSlice(int slice) throws IOException {\n                                     return container.readBlob(fileInfo.partName(slice));", "originalCommit": "a7c76e0dcaa5a572a38ad721a33109979938210a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDA1MTY5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r490051699", "bodyText": "Sure, makes sense. Not sure why I missed it", "author": "tlrx", "createdAt": "2020-09-17T08:07:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM5NTM0NA=="}], "type": "inlineReview"}, {"oid": "7e350b94462da2652444cff70bcb6c21ba949cee", "url": "https://github.com/elastic/elasticsearch/commit/7e350b94462da2652444cff70bcb6c21ba949cee", "message": "feedback", "committedDate": "2020-09-17T10:36:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI1OTk5OA==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r490259998", "bodyText": "Do we still need this override (and associated changes)? It seems with recent changes we're not doing any more partial reading of data blobs? If so I think this whole class/plugin can go away?", "author": "original-brownbear", "createdAt": "2020-09-17T13:47:59Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/AbortedRestoreIT.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.threadpool.ThreadPoolStats;\n+import org.hamcrest.Matcher;\n+\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class AbortedRestoreIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final Collection<Class<? extends Plugin>> plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(BlockingDataFileReadsRepository.Plugin.class);\n+        return plugins;\n+    }\n+\n+    public void testAbortedRestoreAlsoAbortFileRestores() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+\n+        final String indexName = \"test-abort-restore\";\n+        createIndex(indexName, indexSettingsNoReplicas(1).build());\n+        indexRandomDocs(indexName, scaledRandomIntBetween(10, 1_000));\n+        ensureGreen();\n+        forceMerge();\n+\n+        final String repositoryName = \"repository\";\n+        createRepository(repositoryName, BlockingDataFileReadsRepository.TYPE,\n+            Settings.builder()\n+                .put(randomRepositorySettings().build())\n+                .put(\"fail_reads_after_unblock\", true));\n+\n+        final String snapshotName = \"snapshot\";\n+        createFullSnapshot(repositoryName, snapshotName);\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        logger.info(\"--> blocking all data nodes for repository [{}]\", repositoryName);\n+        blockAllDataNodes(repositoryName);\n+\n+        logger.info(\"--> starting restore\");\n+        final ActionFuture<RestoreSnapshotResponse> future = client().admin().cluster().prepareRestoreSnapshot(repositoryName, snapshotName)\n+            .setWaitForCompletion(true)\n+            .setIndices(indexName)\n+            .execute();\n+\n+        assertBusy(() -> {\n+            final RecoveryResponse recoveries = client().admin().indices().prepareRecoveries(indexName)\n+                .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN).setActiveOnly(true).get();\n+            assertThat(recoveries.hasRecoveries(), is(true));\n+            final List<RecoveryState> shardRecoveries = recoveries.shardRecoveryStates().get(indexName);\n+            assertThat(shardRecoveries, hasSize(1));\n+            assertThat(future.isDone(), is(false));\n+\n+            for (RecoveryState shardRecovery : shardRecoveries) {\n+                assertThat(shardRecovery.getRecoverySource().getType(), equalTo(RecoverySource.Type.SNAPSHOT));\n+                assertThat(shardRecovery.getStage(), equalTo(RecoveryState.Stage.INDEX));\n+            }\n+        });\n+\n+        final ThreadPool.Info snapshotThreadPoolInfo = threadPool(dataNode).info(ThreadPool.Names.SNAPSHOT);\n+        assertThat(snapshotThreadPoolInfo.getMax(), greaterThan(0));\n+\n+        logger.info(\"--> waiting for snapshot thread [max={}] pool to be full\", snapshotThreadPoolInfo.getMax());\n+        waitForMaxActiveSnapshotThreads(dataNode, equalTo(snapshotThreadPoolInfo.getMax()));\n+\n+        logger.info(\"--> aborting restore by deleting the index\");\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        logger.info(\"--> unblocking repository [{}]\", repositoryName);\n+        unblockAllDataNodes(repositoryName);\n+\n+        logger.info(\"--> restore should have failed\");\n+        final RestoreSnapshotResponse restoreSnapshotResponse = future.get();\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().failedShards(), equalTo(1));\n+        assertThat(restoreSnapshotResponse.getRestoreInfo().successfulShards(), equalTo(0));\n+\n+        logger.info(\"--> waiting for snapshot thread pool to be empty\");\n+        waitForMaxActiveSnapshotThreads(dataNode, equalTo(0));\n+    }\n+\n+    private static void waitForMaxActiveSnapshotThreads(final String node, final Matcher<Integer> matcher) throws Exception {\n+        assertBusy(() -> assertThat(threadPoolStats(node, ThreadPool.Names.SNAPSHOT).getActive(), matcher), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    private static ThreadPool threadPool(final String node) {\n+        return internalCluster().getInstance(ClusterService.class, node).getClusterApplierService().threadPool();\n+    }\n+\n+    private static ThreadPoolStats.Stats threadPoolStats(final String node, final String threadPoolName) {\n+        return StreamSupport.stream(threadPool(node).stats().spliterator(), false)\n+            .filter(threadPool -> threadPool.getName().equals(threadPoolName))\n+            .findFirst()\n+            .orElseThrow(() -> new AssertionError(\"Failed to find thread pool \" + threadPoolName));\n+    }\n+\n+    /**\n+     * A blob store repository that blocks read operations on {@link InputStream} when the {@code blockStreams} flag\n+     * is set to true. It also keep track of the number of bytes read from {@link InputStream} it opens.\n+     */\n+    public static class BlockingDataFileReadsRepository extends MockRepository {\n+\n+        static final String TYPE = \"block_on_data_file_reads\";\n+\n+        public BlockingDataFileReadsRepository(\n+            RepositoryMetadata metadata,\n+            Environment environment,\n+            NamedXContentRegistry namedXContentRegistry,\n+            ClusterService clusterService,\n+            RecoverySettings recoverySettings\n+        ) {\n+            super(metadata, environment, namedXContentRegistry, clusterService, recoverySettings);\n+        }\n+\n+        @Override\n+        protected int bufferSize() {", "originalCommit": "7e350b94462da2652444cff70bcb6c21ba949cee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI5MDM3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r490290372", "bodyText": "Agreed, I reverted this change", "author": "tlrx", "createdAt": "2020-09-17T14:26:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI1OTk5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI2NTA2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62441#discussion_r490265063", "bodyText": "Could we make this less general and more in line with existing usage of the MockRepository and maybe just add another setter for this field? Hiding this functionality behind a setting makes it much hard for others that might have use for it to find it (or for us to remove it once it's unused ... many of the settings we currently have here seem unused already ... I'll open a PR for dealing with those soon).", "author": "original-brownbear", "createdAt": "2020-09-17T13:54:41Z", "path": "test/framework/src/main/java/org/elasticsearch/snapshots/mockstore/MockRepository.java", "diffHunk": "@@ -138,6 +144,7 @@ public MockRepository(RepositoryMetadata metadata, Environment environment,\n         blockAndFailOnWriteSnapFile = metadata.settings().getAsBoolean(\"block_on_snap\", false);\n         randomPrefix = metadata.settings().get(\"random\", \"default\");\n         waitAfterUnblock = metadata.settings().getAsLong(\"wait_after_unblock\", 0L);\n+        failReadsAfterUnblock = metadata.settings().getAsBoolean(\"fail_reads_after_unblock\", false);", "originalCommit": "7e350b94462da2652444cff70bcb6c21ba949cee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0699acf09f766e660c44d382be05cbf28200b475", "url": "https://github.com/elastic/elasticsearch/commit/0699acf09f766e660c44d382be05cbf28200b475", "message": "Add test", "committedDate": "2020-09-17T14:21:55Z", "type": "commit"}, {"oid": "068e297b5d4a76499f6eaaae88191ef67ee7e6a1", "url": "https://github.com/elastic/elasticsearch/commit/068e297b5d4a76499f6eaaae88191ef67ee7e6a1", "message": "revert buffersize", "committedDate": "2020-09-17T14:25:09Z", "type": "commit"}, {"oid": "725ce72122b6c857cdacb718920d271e502d0c83", "url": "https://github.com/elastic/elasticsearch/commit/725ce72122b6c857cdacb718920d271e502d0c83", "message": "Merge branch 'master' into abort-file-restores-when-restore-is-aborted", "committedDate": "2020-09-18T07:38:22Z", "type": "commit"}, {"oid": "eeb399ef7a7fce7801447774227d2b6b7a34e6e2", "url": "https://github.com/elastic/elasticsearch/commit/eeb399ef7a7fce7801447774227d2b6b7a34e6e2", "message": "import", "committedDate": "2020-09-18T07:38:42Z", "type": "commit"}]}