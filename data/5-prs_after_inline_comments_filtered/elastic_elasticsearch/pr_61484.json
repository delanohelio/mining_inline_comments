{"pr_number": 61484, "pr_title": "Write deprecation logs to a data stream", "pr_createdAt": "2020-08-24T15:15:28Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/61484", "timeline": [{"oid": "126f0cabf7450a6f3cce7ac65a91059d91946328", "url": "https://github.com/elastic/elasticsearch/commit/126f0cabf7450a6f3cce7ac65a91059d91946328", "message": "Allow deprecation logs to be indexed", "committedDate": "2020-08-25T10:11:12Z", "type": "commit"}, {"oid": "126f0cabf7450a6f3cce7ac65a91059d91946328", "url": "https://github.com/elastic/elasticsearch/commit/126f0cabf7450a6f3cce7ac65a91059d91946328", "message": "Allow deprecation logs to be indexed", "committedDate": "2020-08-25T10:11:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNDkyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r478714929", "bodyText": "add can indeed block, but only when the max concurrent requests are met and when the number of documents or size of docs are are exceeded (as opposed to the timed flush). By default the number of concurrent requests is 1, document count is 1000 and size is 5MB... so if any of these conditions are met by default within the default flush interval of 10s it will block on add.\nRather forking here I would suggest to tweak the bulk processor to avoid blocking.\nThe rationale here is that if the for some reason the index is not writable for a very long period of time, you can end up with a large number of threads from the GENERIC pool. This is further compounded by the fact that some (409) write exceptions are retried by default, leaving the thread alive 5s+ by default.  I understand this is protected by the deprecation throttling..so maybe it is moot point once things are warmed up.\nI 100% agree with the motivation here to ensure that we avoid blocking here, however I think we can better address this by 1) increasing the concurrent requests for the bulk processor (maybe = core count with min 2 ?) 2) disable the bulk processor retry, or configure the retry to be less forgiving (as to consume a concurrent request slot for less time) 3) bump the document count and size default up high so that the flush scheduler thread will be much more likely to be thing that is blocked if anything is blocked. 4) (optionally) add a new configuration that will not try to execute a request if the max concurrent requests are met. eliminating the potential to even block the flush thread.", "author": "jakelandis", "createdAt": "2020-08-27T21:48:57Z", "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import co.elastic.logging.log4j2.EcsLayout;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.elasticsearch.action.bulk.BulkProcessor;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.logging.ECSJsonLayout;\n+import org.elasticsearch.common.logging.Loggers;\n+import org.elasticsearch.common.logging.RateLimitingFilter;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * This component manages the construction and lifecycle of the {@link DeprecationIndexingAppender}.\n+ * It also starts and stops the appender\n+ */\n+public class DeprecationIndexingComponent extends AbstractLifecycleComponent implements ClusterStateListener {\n+    private static final Logger logger = LogManager.getLogger(DeprecationIndexingComponent.class);\n+\n+    public static final Setting<Boolean> WRITE_DEPRECATION_LOGS_TO_INDEX = Setting.boolSetting(\n+        \"cluster.deprecation_indexing.enabled\",\n+        false,\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private final DeprecationIndexingAppender appender;\n+    private final BulkProcessor processor;\n+    private final RateLimitingFilter filter;\n+\n+    public DeprecationIndexingComponent(ThreadPool threadPool, Client client) {\n+        this.processor = getBulkProcessor(new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN));\n+        final Consumer<IndexRequest> consumer = buildIndexRequestConsumer(threadPool);\n+\n+        final LoggerContext context = (LoggerContext) LogManager.getContext(false);\n+        final Configuration configuration = context.getConfiguration();\n+\n+        final EcsLayout ecsLayout = ECSJsonLayout.newBuilder().setType(\"deprecation\").setConfiguration(configuration).build();\n+\n+        this.filter = new RateLimitingFilter();\n+        this.appender = new DeprecationIndexingAppender(\"deprecation_indexing_appender\", filter, ecsLayout, consumer);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.appender.start();\n+        Loggers.addAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        Loggers.removeAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+        this.appender.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        this.processor.close();\n+    }\n+\n+    /**\n+     * Listens for changes to the cluster state, in order to know whether to toggle indexing\n+     * and to set the cluster UUID and node ID. These can't be set in the constructor because\n+     * the initial cluster state won't be set yet.\n+     *\n+     * @param event the cluster state event to process\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        final ClusterState state = event.state();\n+        final boolean newEnabled = WRITE_DEPRECATION_LOGS_TO_INDEX.get(state.getMetadata().settings());\n+        if (appender.isEnabled() != newEnabled) {\n+            // We've flipped from disabled to enabled. Make sure we start with a clean cache of\n+            // previously-seen keys, otherwise we won't index anything.\n+            if (newEnabled) {\n+                this.filter.reset();\n+            }\n+            appender.setEnabled(newEnabled);\n+        }\n+    }\n+\n+    /**\n+     * Constructs a {@link Consumer} that knows what to do with the {@link IndexRequest} instances that the\n+     * {@link DeprecationIndexingAppender} creates. This logic is separated from the service in order to make\n+     * testing significantly easier, and to separate concerns.\n+     * <p>\n+     * Writes are done via {@link BulkProcessor}, which handles batching up writes and retries.\n+     *\n+     * @param threadPool due to <a href=\"https://github.com/elastic/elasticsearch/issues/50440\">#50440</a>,\n+     *                   extra care must be taken to avoid blocking the thread that writes a deprecation message.\n+     * @return           a consumer that accepts an index request and handles all the details of writing it\n+     *                   into the cluster\n+     */\n+    private Consumer<IndexRequest> buildIndexRequestConsumer(ThreadPool threadPool) {\n+        return indexRequest -> {\n+            try {\n+                // TODO: remove the threadpool wrapping when the .add call is non-blocking\n+                // (it can currently execute the bulk request occasionally)\n+                // see: https://github.com/elastic/elasticsearch/issues/50440\n+                threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> this.processor.add(indexRequest));", "originalCommit": "126f0cabf7450a6f3cce7ac65a91059d91946328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTA5MDU2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r481090561", "bodyText": "I've pushed some changes in e8429ab, please let me know if this is what you were intending. Since BulkProcessor apparently already supports disabling the size and doc count checks, I went ahead and did that, and tweaked the backoff times along the same lines as an ILM backoff I found, the total retry time should be less I think?", "author": "pugnascotia", "createdAt": "2020-09-01T12:12:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNDkyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NzM3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r481447375", "bodyText": "yes, this is what I was intending... an additional comment inline: #61484 (comment)", "author": "jakelandis", "createdAt": "2020-09-01T21:39:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNDkyOQ=="}], "type": "inlineReview"}, {"oid": "c9da4af18bfa96496fe447e66fc730aabff97f2a", "url": "https://github.com/elastic/elasticsearch/commit/c9da4af18bfa96496fe447e66fc730aabff97f2a", "message": "Merge remote-tracking branch 'upstream/master' into 46106-index-deprecation-logs", "committedDate": "2020-09-01T11:12:43Z", "type": "commit"}, {"oid": "e8429ab200034bf6b55a6a78909e22668900e7cd", "url": "https://github.com/elastic/elasticsearch/commit/e8429ab200034bf6b55a6a78909e22668900e7cd", "message": "Reconfigure bulk processor after review", "committedDate": "2020-09-01T12:10:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzMTQxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r481431419", "bodyText": "we actually want the bulk actions and bulk size to be large values with a reasonably short flush interval. setting them to -1 will cause the execution to happen one at a time (removing the bulk from the bulk processor) and more likely to happen on the calling thread. setting to large values allows the bulks to build up until the flush takes over and submits the request.\nAlso, even with flush it does block a little bit while mutating the bulk request, but that should be a very fast operation.", "author": "jakelandis", "createdAt": "2020-09-01T21:06:26Z", "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import co.elastic.logging.log4j2.EcsLayout;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.elasticsearch.action.bulk.BackoffPolicy;\n+import org.elasticsearch.action.bulk.BulkProcessor;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.logging.ECSJsonLayout;\n+import org.elasticsearch.common.logging.Loggers;\n+import org.elasticsearch.common.logging.RateLimitingFilter;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * This component manages the construction and lifecycle of the {@link DeprecationIndexingAppender}.\n+ * It also starts and stops the appender\n+ */\n+public class DeprecationIndexingComponent extends AbstractLifecycleComponent implements ClusterStateListener {\n+    private static final Logger logger = LogManager.getLogger(DeprecationIndexingComponent.class);\n+\n+    public static final Setting<Boolean> WRITE_DEPRECATION_LOGS_TO_INDEX = Setting.boolSetting(\n+        \"cluster.deprecation_indexing.enabled\",\n+        false,\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private final DeprecationIndexingAppender appender;\n+    private final BulkProcessor processor;\n+    private final RateLimitingFilter filter;\n+\n+    public DeprecationIndexingComponent(Client client, Settings settings) {\n+        this.processor = getBulkProcessor(new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN), settings);\n+        final Consumer<IndexRequest> consumer = this.processor::add;\n+\n+        final LoggerContext context = (LoggerContext) LogManager.getContext(false);\n+        final Configuration configuration = context.getConfiguration();\n+\n+        final EcsLayout ecsLayout = ECSJsonLayout.newBuilder().setType(\"deprecation\").setConfiguration(configuration).build();\n+\n+        this.filter = new RateLimitingFilter();\n+        this.appender = new DeprecationIndexingAppender(\"deprecation_indexing_appender\", filter, ecsLayout, consumer);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.appender.start();\n+        Loggers.addAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        Loggers.removeAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+        this.appender.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        this.processor.close();\n+    }\n+\n+    /**\n+     * Listens for changes to the cluster state, in order to know whether to toggle indexing\n+     * and to set the cluster UUID and node ID. These can't be set in the constructor because\n+     * the initial cluster state won't be set yet.\n+     *\n+     * @param event the cluster state event to process\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        final ClusterState state = event.state();\n+        final boolean newEnabled = WRITE_DEPRECATION_LOGS_TO_INDEX.get(state.getMetadata().settings());\n+        if (appender.isEnabled() != newEnabled) {\n+            // We've flipped from disabled to enabled. Make sure we start with a clean cache of\n+            // previously-seen keys, otherwise we won't index anything.\n+            if (newEnabled) {\n+                this.filter.reset();\n+            }\n+            appender.setEnabled(newEnabled);\n+        }\n+    }\n+\n+    /**\n+     * Constructs a bulk processor for writing documents\n+     * @param client the client to use\n+     * @param settings the settings to use\n+     * @return an initialised bulk processor\n+     */\n+    private BulkProcessor getBulkProcessor(Client client, Settings settings) {\n+        final OriginSettingClient originSettingClient = new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN);\n+        final BulkProcessor.Listener listener = new DeprecationBulkListener();\n+\n+        // This configuration disables the size count and size thresholds,\n+        // and instead uses a scheduled flush only. This means that calling\n+        // processor.add() will not block the calling thread.\n+        return BulkProcessor.builder(originSettingClient::bulk, listener)\n+            .setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(1000), 3))\n+            .setConcurrentRequests(Math.max(2, EsExecutors.allocatedProcessors(settings)))\n+            .setBulkActions(-1)\n+            .setBulkSize(new ByteSizeValue(-1, ByteSizeUnit.BYTES))", "originalCommit": "e8429ab200034bf6b55a6a78909e22668900e7cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjAyNDQ0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482024443", "bodyText": "I set them to -1 after reading the source of BulkProcessor, and the only place I found where bulkActions and bulkSize are used is in isOverTheLimit(), and in both cases, if the value is -1 then the check is skipped. That implies that the bulk request can grow in document count or request size indefinitely. What am I missing?\nAlso, what flush interval are you thinking of? Less than 5s?", "author": "pugnascotia", "createdAt": "2020-09-02T12:20:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzMTQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MzY0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482083643", "bodyText": "What am I missing?\n\nnothing ... it's me, sorry for the noise. (misremembered what that magic number does and a case of confirmation bias spot checking the code)\n5s should be fine", "author": "jakelandis", "createdAt": "2020-09-02T13:49:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzMTQxOQ=="}], "type": "inlineReview"}, {"oid": "e3c6c824fbe1b0ab4321a40b4d6dfabfb57a0601", "url": "https://github.com/elastic/elasticsearch/commit/e3c6c824fbe1b0ab4321a40b4d6dfabfb57a0601", "message": "update gradle for new testing plugins", "committedDate": "2020-09-01T21:27:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1NDg1OQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r481454859", "bodyText": "this is going to match the default logs-- template which defines the following mappings:\nhttps://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/core/src/main/resources/logs-mappings.json\nWe should make sure that we populate the predefined mappings for this template. (datastream, ecs, host, etc.)", "author": "jakelandis", "createdAt": "2020-09-01T21:57:06Z", "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingAppender.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import org.apache.logging.log4j.core.Appender;\n+import org.apache.logging.log4j.core.Core;\n+import org.apache.logging.log4j.core.Filter;\n+import org.apache.logging.log4j.core.Layout;\n+import org.apache.logging.log4j.core.LogEvent;\n+import org.apache.logging.log4j.core.appender.AbstractAppender;\n+import org.apache.logging.log4j.core.config.plugins.Plugin;\n+import org.elasticsearch.action.DocWriteRequest;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.common.xcontent.XContentType;\n+\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+/**\n+ * This log4j appender writes deprecation log messages to an index. It does not perform the actual\n+ * writes, but instead constructs an {@link IndexRequest} for the log message and passes that\n+ * to a callback.\n+ */\n+@Plugin(name = \"DeprecationIndexingAppender\", category = Core.CATEGORY_NAME, elementType = Appender.ELEMENT_TYPE)\n+public class DeprecationIndexingAppender extends AbstractAppender {\n+    public static final String DEPRECATION_MESSAGES_DATA_STREAM = \"logs-deprecation-elasticsearch\";", "originalCommit": "e8429ab200034bf6b55a6a78909e22668900e7cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkyMjc3OA==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r481922778", "bodyText": "this at the moment depends on the ECSJsonLayout where we use https://github.com/elastic/ecs-logging-java to generate ECS compliant JSON. The same JSON is written to our json logs.\nA sample is available here https://github.com/elastic/beats/blob/master/filebeat/module/elasticsearch/deprecation/test/es_deprecation-json.800.log\n(note this could utilise ECS even more by renaming node.name to service.node.name)\nAdding ecs version and host would probably be worthy to add to these json logs as well. I am not sure we should populate datastream to a JSON written to logs.\nMy point is, that this difference would mean we would need a separate layout for JSON logs and for indexed logs ?\nIs it worthy to make other ES logs (server, slow logs) following the logs-mappings.json ?", "author": "pgomulka", "createdAt": "2020-09-02T09:12:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1NDg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA5NzQ5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482097491", "bodyText": "Is it worthy to make other ES logs (server, slow logs) following the logs-mappings.json ?\n\nThe goal of the logs-mapping that ships by default is to have a basic baseline of mappings.  We can always introduce more specific ones as needed. w.r.t to these deprecation logs we should probably wait for some usage to add customizations beyond the baseline.\n\nI am not sure we should populate datastream to a JSON written to logs.\n\nI think it would be OK to include this just index request.", "author": "jakelandis", "createdAt": "2020-09-02T14:07:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1NDg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjEwMTM4OA==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482101388", "bodyText": "@jakelandis @pgomulka so do I need to make any changes here?", "author": "pugnascotia", "createdAt": "2020-09-02T14:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1NDg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjEyNzUzNA==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482127534", "bodyText": "I am +1 to add the datastream information and the host.ip, but don't think it is a blocker to this PR. (doing a last round of review right now)", "author": "jakelandis", "createdAt": "2020-09-02T14:46:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1NDg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjEzMzYxMA==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482133610", "bodyText": "@jakelandis so we do populate more fields at the moment (node.id, node.name, cluster.uuid, etc) Should we add them to the logs-mapping ?\n@pugnascotia I think we need to add datastream and ecs version fields to the log message.", "author": "pgomulka", "createdAt": "2020-09-02T14:54:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1NDg1OQ=="}], "type": "inlineReview"}, {"oid": "035bd6e92f879b1c0d4d3b0854022846ef2d7994", "url": "https://github.com/elastic/elasticsearch/commit/035bd6e92f879b1c0d4d3b0854022846ef2d7994", "message": "Merge pull request #3 from jakelandis/46106-index-deprecation-logs-jake\n\nupdate gradle config for new testing plugins", "committedDate": "2020-09-02T12:21:40Z", "type": "commit"}, {"oid": "e4ad3893efca5cd50c1944d376eec1a9c380b828", "url": "https://github.com/elastic/elasticsearch/commit/e4ad3893efca5cd50c1944d376eec1a9c380b828", "message": "Include more fields in DeprecatedMessage", "committedDate": "2020-09-02T16:01:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4Njk3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482186979", "bodyText": "@jakelandis how do these look to you?", "author": "pugnascotia", "createdAt": "2020-09-02T16:02:33Z", "path": "server/src/main/java/org/elasticsearch/common/logging/DeprecatedMessage.java", "diffHunk": "@@ -40,10 +41,14 @@ public static ESLogMessage of(String key, String xOpaqueId, String messagePatter\n             @Override\n             public String toString() {\n                 return ParameterizedMessage.format(messagePattern, args);\n-\n             }\n         };\n+\n         return new ESLogMessage(messagePattern, args)\n+            .field(\"data_stream.type\", \"logs\")\n+            .field(\"data_stream.datatype\", \"deprecation\")\n+            .field(\"data_stream.namespace\", \"elasticsearch\")\n+            .field(\"ecs.version\", ECS_VERSION)", "originalCommit": "e4ad3893efca5cd50c1944d376eec1a9c380b828", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk5MDUwNA==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r482990504", "bodyText": "LGTM", "author": "jakelandis", "createdAt": "2020-09-03T13:47:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4Njk3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEzMDMyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r494130329", "bodyText": "@pugnascotia This should be data_stream.dataset to be aligned with the indexing strategy.\nI would also propose to keep the namespace as default and use deprecation.elasticsearch as the dataset name. Only important thing is that the dataset does not contain a  -.", "author": "ruflin", "createdAt": "2020-09-24T08:24:47Z", "path": "server/src/main/java/org/elasticsearch/common/logging/DeprecatedMessage.java", "diffHunk": "@@ -40,10 +41,14 @@ public static ESLogMessage of(String key, String xOpaqueId, String messagePatter\n             @Override\n             public String toString() {\n                 return ParameterizedMessage.format(messagePattern, args);\n-\n             }\n         };\n+\n         return new ESLogMessage(messagePattern, args)\n+            .field(\"data_stream.type\", \"logs\")\n+            .field(\"data_stream.datatype\", \"deprecation\")", "originalCommit": "e4ad3893efca5cd50c1944d376eec1a9c380b828", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE1OTMxMA==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r494159310", "bodyText": "@ruflin so would we have the following, then?\n    .field(\"data_stream.dataset\", \"default\")\n    .field(\"data_stream.namespace\", \"deprecation.elasticsearch\")", "author": "pugnascotia", "createdAt": "2020-09-24T09:09:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEzMDMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE2MDkyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r494160925", "bodyText": "No, the other way around:\n.field(\"data_stream.dataset\", \"deprecation.elasticsearch\")\n.field(\"data_stream.namespace\", \"default\")", "author": "ruflin", "createdAt": "2020-09-24T09:11:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEzMDMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE2NDA1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/61484#discussion_r494164057", "bodyText": "Thanks, I'll get that changed \ud83d\udc4d", "author": "pugnascotia", "createdAt": "2020-09-24T09:17:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEzMDMyOQ=="}], "type": "inlineReview"}]}