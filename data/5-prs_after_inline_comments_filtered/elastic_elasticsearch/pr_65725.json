{"pr_number": 65725, "pr_title": "Make searchable snapshots cache persistent", "pr_createdAt": "2020-12-02T10:46:29Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/65725", "timeline": [{"oid": "102d9aad4b98c34bc2783a8020968083c2cce85e", "url": "https://github.com/elastic/elasticsearch/commit/102d9aad4b98c34bc2783a8020968083c2cce85e", "message": "Add persistent cache", "committedDate": "2020-12-02T10:27:07Z", "type": "commit"}, {"oid": "6ff96eb6d5c89c48cf09e6743b57f0d326f332ad", "url": "https://github.com/elastic/elasticsearch/commit/6ff96eb6d5c89c48cf09e6743b57f0d326f332ad", "message": "indexed cache file path should be relative", "committedDate": "2020-12-02T11:31:32Z", "type": "commit"}, {"oid": "4b697a8c7ef35de7f56e6478cddfa735259b8bd1", "url": "https://github.com/elastic/elasticsearch/commit/4b697a8c7ef35de7f56e6478cddfa735259b8bd1", "message": "mute test + remove suppress filesystems", "committedDate": "2020-12-02T11:31:50Z", "type": "commit"}, {"oid": "039e8eadb5d231352809f9da0eba18fdb3ae8cbb", "url": "https://github.com/elastic/elasticsearch/commit/039e8eadb5d231352809f9da0eba18fdb3ae8cbb", "message": "remove leftovers", "committedDate": "2020-12-02T11:47:29Z", "type": "commit"}, {"oid": "a864370f4de9ee6aabf82fa40271b19b0476395d", "url": "https://github.com/elastic/elasticsearch/commit/a864370f4de9ee6aabf82fa40271b19b0476395d", "message": "Merge branch 'master' into add-persistent-cache", "committedDate": "2020-12-02T12:18:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDIxMjczNA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r534212734", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(writer.nodePath())) {\n          \n          \n            \n                            for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(nodePath)) {", "author": "henningandersen", "createdAt": "2020-12-02T14:33:39Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,616 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                final NodeEnvironment.NodePath nodePath = writer.nodePath();\n+                logger.debug(\"loading persistent cache on data path [{}]\", nodePath);\n+\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(writer.nodePath())) {", "originalCommit": "a864370f4de9ee6aabf82fa40271b19b0476395d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEwNTA5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r535105091", "bodyText": "Thanks, I pushed 9bef60d", "author": "tlrx", "createdAt": "2020-12-03T11:02:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDIxMjczNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI0NDA5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r534244099", "bodyText": "Would it be possible to extract this to a separate method and pass the map directly to the CacheFileVisitor? It feels a bit backwards to pass it through the CacheIndexWriter, but there might be a valid reason for it?", "author": "henningandersen", "createdAt": "2020-12-02T15:12:29Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,616 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                final NodeEnvironment.NodePath nodePath = writer.nodePath();\n+                logger.debug(\"loading persistent cache on data path [{}]\", nodePath);\n+\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(writer.nodePath())) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = writer.nodePath().resolve(shardId);\n+                        final Path shardCachePath = getShardCachePath(new ShardPath(false, shardDataPath, shardDataPath, shardId));\n+\n+                        if (Files.isDirectory(shardCachePath)) {\n+                            logger.trace(\"found snapshot cache dir at [{}], loading cache files from disk and index\", shardCachePath);\n+                            Files.walkFileTree(shardCachePath, new CacheFileVisitor(cacheService, writer));\n+                        }\n+                    }\n+                }\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+            logger.info(\"persistent cache index loaded\");\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw new UncheckedIOException(\"Failed to load persistent cache\", e);\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    void commit() throws IOException {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index writer\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw e;\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    private void closeIfAnyIndexWriterHasTragedyOrIsClosed() {\n+        if (writers.stream().map(writer -> writer.indexWriter).anyMatch(iw -> iw.getTragicException() != null || iw.isOpen() == false)) {\n+            try {\n+                close();\n+            } catch (Exception e) {\n+                logger.warn(\"failed to close persistent cache index\", e);\n+            }\n+        }\n+    }\n+\n+    public boolean hasDeletions() {\n+        ensureOpen();\n+        for (CacheIndexWriter writer : writers) {\n+            if (writer.indexWriter.hasDeletions()) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    public long getNumDocs() {\n+        ensureOpen();\n+        long count = 0L;\n+        for (CacheIndexWriter writer : writers) {\n+            count += writer.indexWriter.getPendingNumDocs();\n+        }\n+        return count;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        if (closed.compareAndSet(false, true)) {\n+            IOUtils.close(writers);\n+        }\n+    }\n+\n+    /**\n+     * Creates a list of {@link CacheIndexWriter}, one for each data path of the specified {@link NodeEnvironment}.\n+     *\n+     * @param nodeEnvironment the data node environment\n+     * @return a list of {@link CacheIndexWriter}\n+     */\n+    private static List<CacheIndexWriter> createWriters(NodeEnvironment nodeEnvironment) {\n+        final List<CacheIndexWriter> writers = new ArrayList<>();\n+        boolean success = false;\n+        try {\n+            final NodeEnvironment.NodePath[] nodePaths = nodeEnvironment.nodePaths();\n+            for (NodeEnvironment.NodePath nodePath : nodePaths) {\n+                writers.add(createCacheIndexWriter(nodePath));\n+            }\n+            success = true;\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to create persistent cache writers\", e);\n+        } finally {\n+            if (success == false) {\n+                IOUtils.closeWhileHandlingException(writers);\n+            }\n+        }\n+        return unmodifiableList(writers);\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheIndexWriter} for the specified data path. The is a single instance per data path.\n+     *\n+     * @param nodePath the data path\n+     * @return a new {@link CacheIndexWriter} instance\n+     * @throws IOException if something went wrong\n+     */\n+    static CacheIndexWriter createCacheIndexWriter(NodeEnvironment.NodePath nodePath) throws IOException {\n+        final List<Closeable> closeables = new ArrayList<>();\n+        boolean success = false;\n+        try {\n+            Path directoryPath = createCacheIndexFolder(nodePath);\n+            final Directory directory = FSDirectory.open(directoryPath);\n+            closeables.add(directory);\n+\n+            final Map<String, Document> documents = new HashMap<>();\n+            try (IndexReader indexReader = DirectoryReader.open(directory)) {\n+                for (LeafReaderContext leafReaderContext : indexReader.leaves()) {\n+                    final LeafReader leafReader = leafReaderContext.reader();\n+                    final Bits liveDocs = leafReader.getLiveDocs();\n+                    for (int i = 0; i < leafReader.maxDoc(); i++) {\n+                        if (liveDocs == null || liveDocs.get(i)) {\n+                            final Document document = leafReader.document(i);\n+                            documents.put(getValue(document, CACHE_ID_FIELD), document);\n+                        }\n+                    }\n+                }\n+            } catch (IndexNotFoundException e) {\n+                logger.debug(\"persistent cache index does not exist yet\", e);\n+            }", "originalCommit": "a864370f4de9ee6aabf82fa40271b19b0476395d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4NTk0Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r535185946", "bodyText": "No valid reason, I agree that seems a bit backward. I pushed 5dcd53a", "author": "tlrx", "createdAt": "2020-12-03T12:25:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI0NDA5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTAwODk1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r535008952", "bodyText": "Are these different from the shard index name/id below?", "author": "henningandersen", "createdAt": "2020-12-03T09:34:52Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,616 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                final NodeEnvironment.NodePath nodePath = writer.nodePath();\n+                logger.debug(\"loading persistent cache on data path [{}]\", nodePath);\n+\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(writer.nodePath())) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = writer.nodePath().resolve(shardId);\n+                        final Path shardCachePath = getShardCachePath(new ShardPath(false, shardDataPath, shardDataPath, shardId));\n+\n+                        if (Files.isDirectory(shardCachePath)) {\n+                            logger.trace(\"found snapshot cache dir at [{}], loading cache files from disk and index\", shardCachePath);\n+                            Files.walkFileTree(shardCachePath, new CacheFileVisitor(cacheService, writer));\n+                        }\n+                    }\n+                }\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+            logger.info(\"persistent cache index loaded\");\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw new UncheckedIOException(\"Failed to load persistent cache\", e);\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    void commit() throws IOException {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index writer\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw e;\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    private void closeIfAnyIndexWriterHasTragedyOrIsClosed() {\n+        if (writers.stream().map(writer -> writer.indexWriter).anyMatch(iw -> iw.getTragicException() != null || iw.isOpen() == false)) {\n+            try {\n+                close();\n+            } catch (Exception e) {\n+                logger.warn(\"failed to close persistent cache index\", e);\n+            }\n+        }\n+    }\n+\n+    public boolean hasDeletions() {\n+        ensureOpen();\n+        for (CacheIndexWriter writer : writers) {\n+            if (writer.indexWriter.hasDeletions()) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    public long getNumDocs() {\n+        ensureOpen();\n+        long count = 0L;\n+        for (CacheIndexWriter writer : writers) {\n+            count += writer.indexWriter.getPendingNumDocs();\n+        }\n+        return count;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        if (closed.compareAndSet(false, true)) {\n+            IOUtils.close(writers);\n+        }\n+    }\n+\n+    /**\n+     * Creates a list of {@link CacheIndexWriter}, one for each data path of the specified {@link NodeEnvironment}.\n+     *\n+     * @param nodeEnvironment the data node environment\n+     * @return a list of {@link CacheIndexWriter}\n+     */\n+    private static List<CacheIndexWriter> createWriters(NodeEnvironment nodeEnvironment) {\n+        final List<CacheIndexWriter> writers = new ArrayList<>();\n+        boolean success = false;\n+        try {\n+            final NodeEnvironment.NodePath[] nodePaths = nodeEnvironment.nodePaths();\n+            for (NodeEnvironment.NodePath nodePath : nodePaths) {\n+                writers.add(createCacheIndexWriter(nodePath));\n+            }\n+            success = true;\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to create persistent cache writers\", e);\n+        } finally {\n+            if (success == false) {\n+                IOUtils.closeWhileHandlingException(writers);\n+            }\n+        }\n+        return unmodifiableList(writers);\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheIndexWriter} for the specified data path. The is a single instance per data path.\n+     *\n+     * @param nodePath the data path\n+     * @return a new {@link CacheIndexWriter} instance\n+     * @throws IOException if something went wrong\n+     */\n+    static CacheIndexWriter createCacheIndexWriter(NodeEnvironment.NodePath nodePath) throws IOException {\n+        final List<Closeable> closeables = new ArrayList<>();\n+        boolean success = false;\n+        try {\n+            Path directoryPath = createCacheIndexFolder(nodePath);\n+            final Directory directory = FSDirectory.open(directoryPath);\n+            closeables.add(directory);\n+\n+            final Map<String, Document> documents = new HashMap<>();\n+            try (IndexReader indexReader = DirectoryReader.open(directory)) {\n+                for (LeafReaderContext leafReaderContext : indexReader.leaves()) {\n+                    final LeafReader leafReader = leafReaderContext.reader();\n+                    final Bits liveDocs = leafReader.getLiveDocs();\n+                    for (int i = 0; i < leafReader.maxDoc(); i++) {\n+                        if (liveDocs == null || liveDocs.get(i)) {\n+                            final Document document = leafReader.document(i);\n+                            documents.put(getValue(document, CACHE_ID_FIELD), document);\n+                        }\n+                    }\n+                }\n+            } catch (IndexNotFoundException e) {\n+                logger.debug(\"persistent cache index does not exist yet\", e);\n+            }\n+\n+            final IndexWriterConfig config = new IndexWriterConfig(new KeywordAnalyzer());\n+            config.setIndexDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());\n+            config.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n+            config.setMergeScheduler(new SerialMergeScheduler());\n+            config.setRAMBufferSizeMB(1.0);\n+            config.setCommitOnClose(false);\n+\n+            final IndexWriter indexWriter = new IndexWriter(directory, config);\n+            closeables.add(indexWriter);\n+\n+            final CacheIndexWriter cacheIndexWriter = new CacheIndexWriter(nodePath, directory, indexWriter, documents);\n+            success = true;\n+            return cacheIndexWriter;\n+        } finally {\n+            if (success == false) {\n+                IOUtils.close(closeables);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Cleans any leftover searchable snapshot caches (files and Lucene indices) when a non-data node is starting up.\n+     * This is useful when the node is repurposed and is not a data node anymore.\n+     *\n+     * @param nodeEnvironment the {@link NodeEnvironment} to cleanup\n+     */\n+    public static void cleanUp(Settings settings, NodeEnvironment nodeEnvironment) {\n+        final boolean isDataNode = DiscoveryNode.isDataNode(settings);\n+        if (isDataNode) {\n+            assert false : \"should not be called on data nodes\";\n+            throw new IllegalStateException(\"Cannot clean searchable snapshot caches: node is a data node\");\n+        }\n+        try {\n+            for (NodeEnvironment.NodePath nodePath : nodeEnvironment.nodePaths()) {\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(nodePath)) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = nodePath.resolve(shardId);\n+                        final ShardPath shardPath = new ShardPath(false, shardDataPath, shardDataPath, shardId);\n+                        final Path cacheDir = getShardCachePath(shardPath);\n+                        if (Files.isDirectory(cacheDir)) {\n+                            logger.debug(\"deleting searchable snapshot shard cache directory [{}]\", cacheDir);\n+                            IOUtils.rm(cacheDir);\n+                        }\n+                    }\n+                }\n+                final Path cacheIndexDir = resolveCacheIndexFolder(nodePath);\n+                if (Files.isDirectory(cacheIndexDir)) {\n+                    logger.debug(\"deleting searchable snapshot lucene directory [{}]\", cacheIndexDir);\n+                    IOUtils.rm(cacheIndexDir);\n+                }\n+            }\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to clean up searchable snapshots cache\", e);\n+        }\n+    }\n+\n+    /**\n+     * A {@link CacheIndexWriter} contains a Lucene {@link Directory} with an {@link IndexWriter} that can be used to index documents in\n+     * the persistent cache index. The list of existing cache documents is loaded at startup and kept around until a first commit is done.\n+     * There is one {@link CacheIndexWriter} for each data path.\n+     */\n+    static class CacheIndexWriter implements Closeable {\n+\n+        private final AtomicReference<Map<String, Document>> documentsRef;\n+        private final NodeEnvironment.NodePath nodePath;\n+        private final IndexWriter indexWriter;\n+        private final Directory directory;\n+\n+        private CacheIndexWriter(\n+            NodeEnvironment.NodePath nodePath,\n+            Directory directory,\n+            IndexWriter indexWriter,\n+            Map<String, Document> documents\n+        ) {\n+            this.documentsRef = new AtomicReference<>(Objects.requireNonNull(documents));\n+            this.nodePath = nodePath;\n+            this.directory = directory;\n+            this.indexWriter = indexWriter;\n+        }\n+\n+        NodeEnvironment.NodePath nodePath() {\n+            return nodePath;\n+        }\n+\n+        Map<String, Document> getDocuments() {\n+            return documentsRef.get();\n+        }\n+\n+        @Nullable\n+        Document getDocument(String cacheFileId) {\n+            final Map<String, Document> documents = getDocuments();\n+            if (documents == null) {\n+                assert false : \"this method should only be used when loading persistent cache, before any prior commit\";\n+                throw new IllegalStateException(\"Persistent cache index was already committed\");\n+            }\n+            return documents.get(cacheFileId);\n+        }\n+\n+        void updateCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> cacheRanges) throws IOException {\n+            assert getDocuments() == null : \"this method should only be used after loading persistent cache\";\n+            final Term term = buildTerm(cacheFile);\n+            logger.debug(\"updating document with term [{}]\", term);\n+            indexWriter.updateDocument(term, buildDocument(nodePath, cacheFile, cacheRanges));\n+        }\n+\n+        void updateCacheFile(String cacheFileId, Document cacheFileDocument) throws IOException {\n+            assert getDocuments() != null : \"this method should only be used when loading persistent cache, before any prior commit\";\n+            final Term term = buildTerm(cacheFileId);\n+            logger.debug(\"updating document with term [{}]\", term);\n+            indexWriter.updateDocument(term, cacheFileDocument);\n+        }\n+\n+        void deleteCacheFile(CacheFile cacheFile) throws IOException {\n+            deleteCacheFile(buildId(cacheFile));\n+        }\n+\n+        void deleteCacheFile(String cacheFileId) throws IOException {\n+            final Term term = buildTerm(cacheFileId);\n+            logger.debug(\"deleting document with term [{}]\", term);\n+            indexWriter.deleteDocuments(term);\n+        }\n+\n+        void prepareCommit() throws IOException {\n+            logger.debug(\"preparing commit\");\n+            final Map<String, String> commitData = new HashMap<>(1);\n+            commitData.put(NODE_VERSION_COMMIT_KEY, Integer.toString(Version.CURRENT.id));\n+            indexWriter.setLiveCommitData(commitData.entrySet());\n+            indexWriter.prepareCommit();\n+        }\n+\n+        void commit() throws IOException {\n+            boolean success = false;\n+            try {\n+                logger.debug(\"committing\");\n+                indexWriter.commit();\n+                success = true;\n+            } finally {\n+                if (success) {\n+                    Map<String, Document> documents = documentsRef.getAndSet(null);\n+                    if (documents != null) {\n+                        logger.trace(\"clearing existing cache documents\");\n+                        documents.clear();\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() throws IOException {\n+            logger.debug(\"closing persistent cache index\");\n+            IOUtils.close(indexWriter, directory);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"[persistent cache index][\" + nodePath + ']';\n+        }\n+    }\n+\n+    /**\n+     * {@link CacheFileVisitor} is used to visit cache files on disk and find information about them using the Lucene documents loaded\n+     * at startup from the persistent cache index. If there are no corresponding document for a cache file, the cache file is deleted\n+     * from disk. If a corresponding document is found, the cache file is added to the current persistent cache index and inserted in\n+     * the searchable snapshots cache.\n+     */\n+    private static class CacheFileVisitor extends SimpleFileVisitor<Path> {\n+\n+        private final CacheService cacheService;\n+        private final CacheIndexWriter writer;\n+\n+        private CacheFileVisitor(CacheService cacheService, CacheIndexWriter writer) {\n+            this.cacheService = Objects.requireNonNull(cacheService);\n+            this.writer = Objects.requireNonNull(writer);\n+        }\n+\n+        @Override\n+        public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) {\n+            try {\n+                final String id = buildId(file);\n+                final Document cacheDocument = writer.getDocument(id);\n+                if (cacheDocument != null) {\n+                    logger.trace(\"indexing cache file with id [{}] in persistent cache index\", id);\n+                    writer.updateCacheFile(id, cacheDocument);\n+\n+                    final CacheKey cacheKey = buildCacheKey(cacheDocument);\n+                    logger.trace(\"adding cache file with [id={}, cache key={}]\", id, cacheKey);\n+                    final long fileLength = getFileLength(cacheDocument);\n+                    cacheService.put(cacheKey, fileLength, file.getParent(), id, buildCacheFileRanges(cacheDocument));\n+                } else {\n+                    logger.trace(\"deleting cache file [{}] (does not exist in persistent cache index)\", file);\n+                    Files.delete(file);\n+                }\n+            } catch (Exception e) {\n+                throw ExceptionsHelper.convertToRuntime(e);\n+            }\n+            return FileVisitResult.CONTINUE;\n+        }\n+    }\n+\n+    private static final String CACHE_ID_FIELD = \"cache_id\";\n+    private static final String CACHE_PATH_FIELD = \"cache_path\";\n+    private static final String CACHE_RANGES_FIELD = \"cache_ranges\";\n+    private static final String SNAPSHOT_ID_FIELD = \"snapshot_id\";\n+    private static final String SNAPSHOT_NAME_FIELD = \"snapshot_name\";\n+    private static final String INDEX_ID_FIELD = \"index_id\";\n+    private static final String INDEX_NAME_FIELD = \"index_name\";\n+    private static final String SHARD_INDEX_NAME_FIELD = \"shard_index_name\";\n+    private static final String SHARD_INDEX_ID_FIELD = \"shard_index_id\";\n+    private static final String SHARD_ID_FIELD = \"shard_id\";\n+    private static final String FILE_NAME_FIELD = \"file_name\";\n+    private static final String FILE_LENGTH_FIELD = \"file_length\";\n+\n+    private static String buildId(CacheFile cacheFile) {\n+        return buildId(cacheFile.getFile());\n+    }\n+\n+    private static String buildId(Path path) {\n+        return path.getFileName().toString();\n+    }\n+\n+    private static Term buildTerm(CacheFile cacheFile) {\n+        return buildTerm(buildId(cacheFile));\n+    }\n+\n+    private static Term buildTerm(String cacheFileUuid) {\n+        return new Term(CACHE_ID_FIELD, cacheFileUuid);\n+    }\n+\n+    private static Document buildDocument(NodeEnvironment.NodePath nodePath, CacheFile cacheFile, SortedSet<Tuple<Long, Long>> cacheRanges)\n+        throws IOException {\n+        final Document document = new Document();\n+        document.add(new StringField(CACHE_ID_FIELD, buildId(cacheFile), Field.Store.YES));\n+        document.add(new StringField(CACHE_PATH_FIELD, nodePath.indicesPath.relativize(cacheFile.getFile()).toString(), Field.Store.YES));\n+\n+        try (BytesStreamOutput output = new BytesStreamOutput()) {\n+            output.writeVInt(cacheRanges.size());\n+            for (Tuple<Long, Long> cacheRange : cacheRanges) {\n+                output.writeVLong(cacheRange.v1());\n+                output.writeVLong(cacheRange.v2());\n+            }\n+            output.flush();\n+            document.add(new StoredField(CACHE_RANGES_FIELD, output.bytes().toBytesRef()));\n+        }\n+\n+        final CacheKey cacheKey = cacheFile.getCacheKey();\n+        document.add(new StringField(FILE_NAME_FIELD, cacheKey.getFileName(), Field.Store.YES));\n+        document.add(new StringField(FILE_LENGTH_FIELD, Long.toString(cacheFile.getLength()), Field.Store.YES));\n+\n+        final SnapshotId snapshotId = cacheKey.getSnapshotId();\n+        document.add(new StringField(SNAPSHOT_NAME_FIELD, snapshotId.getName(), Field.Store.YES));\n+        document.add(new StringField(SNAPSHOT_ID_FIELD, snapshotId.getUUID(), Field.Store.YES));\n+\n+        final IndexId indexId = cacheKey.getIndexId();\n+        document.add(new StringField(INDEX_NAME_FIELD, indexId.getName(), Field.Store.YES));\n+        document.add(new StringField(INDEX_ID_FIELD, indexId.getId(), Field.Store.YES));", "originalCommit": "a864370f4de9ee6aabf82fa40271b19b0476395d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEwMjgzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r535102831", "bodyText": "Yes, those ones are referring to the index in the snapshot while shard's index name/id are referring to the index assigned to the data node.", "author": "tlrx", "createdAt": "2020-12-03T11:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTAwODk1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE3NDE3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542174176", "bodyText": "Maybe one point question here:\nShould we simplify CacheKey maybe before we move to persisting the format? like we're going to here? There is no need to store the indexId IMO. The following information is enough to uniquely identify a shard in a given snapshot:\n\nsnapshot_uuid\nindex name\nshard id (int)\n\nAll the other information like index uuid and and IndexId are just noise. Having redundant index_uuid in here also technically allows for having the same file in the cache twice for different mounts of an index? (not that it makes sense to have those but it complicates the logic for figuring out what is already in the cache for a given index in the allocation work)", "author": "original-brownbear", "createdAt": "2020-12-14T07:50:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTAwODk1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE4NTY4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542185682", "bodyText": "We can indeed simplify the CacheKey and removes snapshot's name and snapshot's index uuid. We need to keep the shard's IndexId because as of today cache files are located within a specified shard data path, so they belong to a specific shard within the cluster and need to follow this shard lifecycle.", "author": "tlrx", "createdAt": "2020-12-14T08:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTAwODk1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5NDIzMA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542194230", "bodyText": "because as of today cache files are located within a specified shard data path, so they belong to a specific shard within the cluster and need to follow this shard lifecycle\n\nMakes sense thanks :) shall we drop that here or is BwC trivial if we chose to simplify this later (it looks trivial actually and we can just ignore the extra fields in the documents in a follow-up ... but maybe I'm missing something)?", "author": "original-brownbear", "createdAt": "2020-12-14T08:27:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTAwODk1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5NTY1OA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542195658", "bodyText": "You're welcome :) I think that the change will be trivial as you said. Actually I think I can do it as a follow up but because this is spread in many classes doing it now will introduce a lot of noise (and potential merge conflicts).", "author": "tlrx", "createdAt": "2020-12-14T08:29:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTAwODk1Mg=="}], "type": "inlineReview"}, {"oid": "9bef60dea1477da2abcbd27b9ddb2c86a40d9d37", "url": "https://github.com/elastic/elasticsearch/commit/9bef60dea1477da2abcbd27b9ddb2c86a40d9d37", "message": "nodePath", "committedDate": "2020-12-03T11:02:14Z", "type": "commit"}, {"oid": "5dcd53aaede280dc6a2114ff5bf74ce0af3c3214", "url": "https://github.com/elastic/elasticsearch/commit/5dcd53aaede280dc6a2114ff5bf74ce0af3c3214", "message": "extract map", "committedDate": "2020-12-03T12:15:32Z", "type": "commit"}, {"oid": "3364d442874665db8e470b0d03556b434b2416f9", "url": "https://github.com/elastic/elasticsearch/commit/3364d442874665db8e470b0d03556b434b2416f9", "message": "Merge branch 'master' into add-persistent-cache", "committedDate": "2020-12-03T12:15:54Z", "type": "commit"}, {"oid": "381188f6fa67cff7c63647d6e3442b50ad56cc3e", "url": "https://github.com/elastic/elasticsearch/commit/381188f6fa67cff7c63647d6e3442b50ad56cc3e", "message": "Also match persistent cache index docs with cache files", "committedDate": "2020-12-04T16:02:19Z", "type": "commit"}, {"oid": "685baac627b812c6884c83c2d947eb75619dee40", "url": "https://github.com/elastic/elasticsearch/commit/685baac627b812c6884c83c2d947eb75619dee40", "message": "Merge branch 'master' into add-persistent-cache", "committedDate": "2020-12-04T16:04:08Z", "type": "commit"}, {"oid": "f6ecc3ab59afc84c91caab59da554efd3dfd1716", "url": "https://github.com/elastic/elasticsearch/commit/f6ecc3ab59afc84c91caab59da554efd3dfd1716", "message": "add comment + IndexRemovalReason.NO_LONGER_ASSIGNED", "committedDate": "2020-12-04T16:09:16Z", "type": "commit"}, {"oid": "c36996bdde0b0e4811beeb77b0fb95cc2fe1af83", "url": "https://github.com/elastic/elasticsearch/commit/c36996bdde0b0e4811beeb77b0fb95cc2fe1af83", "message": "Merge branch 'master' into add-persistent-cache", "committedDate": "2020-12-07T10:29:56Z", "type": "commit"}, {"oid": "5b2d65339106358cb219a4da73284638540fb119", "url": "https://github.com/elastic/elasticsearch/commit/5b2d65339106358cb219a4da73284638540fb119", "message": "Merge branch 'master' into add-persistent-cache", "committedDate": "2020-12-11T11:05:09Z", "type": "commit"}, {"oid": "8c12a960c4402f01d99695efc35dfb652d9f6b44", "url": "https://github.com/elastic/elasticsearch/commit/8c12a960c4402f01d99695efc35dfb652d9f6b44", "message": "remove after merge leftovers", "committedDate": "2020-12-11T11:33:08Z", "type": "commit"}, {"oid": "255853ddc231672e78eee7ec0b27e7aab365776a", "url": "https://github.com/elastic/elasticsearch/commit/255853ddc231672e78eee7ec0b27e7aab365776a", "message": "more spotless fixes", "committedDate": "2020-12-11T11:42:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE2NTYwMg==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542165602", "bodyText": "NIT: Maybe just  IOUtils.close(writers, documents::clear); to make it clear no matter what? (not that it matters much)", "author": "original-brownbear", "createdAt": "2020-12-14T07:31:40Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,623 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Collections.synchronizedMap;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final Map<String, Document> documents;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.documents = synchronizedMap(loadDocuments(nodeEnvironment));\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                final NodeEnvironment.NodePath nodePath = writer.nodePath();\n+                logger.debug(\"loading persistent cache on data path [{}]\", nodePath);\n+\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(nodePath)) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = writer.nodePath().resolve(shardId);\n+                        final Path shardCachePath = getShardCachePath(new ShardPath(false, shardDataPath, shardDataPath, shardId));\n+\n+                        if (Files.isDirectory(shardCachePath)) {\n+                            logger.trace(\"found snapshot cache dir at [{}], loading cache files from disk and index\", shardCachePath);\n+                            Files.walkFileTree(shardCachePath, new CacheFileVisitor(cacheService, writer, documents));\n+                        }\n+                    }\n+                }\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+            logger.info(\"persistent cache index loaded\");\n+            documents.clear();\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw new UncheckedIOException(\"Failed to load persistent cache\", e);\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    void commit() throws IOException {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index writer\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw e;\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    private void closeIfAnyIndexWriterHasTragedyOrIsClosed() {\n+        if (writers.stream().map(writer -> writer.indexWriter).anyMatch(iw -> iw.getTragicException() != null || iw.isOpen() == false)) {\n+            try {\n+                close();\n+            } catch (Exception e) {\n+                logger.warn(\"failed to close persistent cache index\", e);\n+            }\n+        }\n+    }\n+\n+    public boolean hasDeletions() {\n+        ensureOpen();\n+        for (CacheIndexWriter writer : writers) {\n+            if (writer.indexWriter.hasDeletions()) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    public long getNumDocs() {\n+        ensureOpen();\n+        long count = 0L;\n+        for (CacheIndexWriter writer : writers) {\n+            count += writer.indexWriter.getPendingNumDocs();\n+        }\n+        return count;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        if (closed.compareAndSet(false, true)) {\n+            IOUtils.close(writers);", "originalCommit": "255853ddc231672e78eee7ec0b27e7aab365776a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5ODY5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542198691", "bodyText": "I don't think I can bake documents::clear into IOUtils.close() but I can definitely clear the documents map in a finally block. I pushed ac6050f.", "author": "tlrx", "createdAt": "2020-12-14T08:34:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE2NTYwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjIwMTg1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542201853", "bodyText": "\ud83e\udd26 right thanks, missed the plural on writers :)", "author": "original-brownbear", "createdAt": "2020-12-14T08:40:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE2NTYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE2ODYxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542168615", "bodyText": "NIT: This visitor may be easier to read if it was just inlined and one doesn't have to jump around to the other class to figure out what's going on (also saves a few lines for the constructor)?\n                            logger.trace(\"found snapshot cache dir at [{}], loading cache files from disk and index\", shardCachePath);\n                            Files.walkFileTree(shardCachePath, new SimpleFileVisitor<>() {\n                                @Override\n                                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) {\n                                    try {\n                                        final String id = buildId(file);\n                                        final Document cacheDocument = documents.get(id);\n                                        if (cacheDocument != null) {\n                                            logger.trace(\"indexing cache file with id [{}] in persistent cache index\", id);\n                                            writer.updateCacheFile(id, cacheDocument);\n\n                                            final CacheKey cacheKey = buildCacheKey(cacheDocument);\n                                            logger.trace(\"adding cache file with [id={}, cache key={}]\", id, cacheKey);\n                                            final long fileLength = getFileLength(cacheDocument);\n                                            cacheService.put(cacheKey, fileLength, file.getParent(), id, buildCacheFileRanges(cacheDocument));\n                                        } else {\n                                            logger.trace(\"deleting cache file [{}] (does not exist in persistent cache index)\", file);\n                                            Files.delete(file);\n                                        }\n                                    } catch (Exception e) {\n                                        throw ExceptionsHelper.convertToRuntime(e);\n                                    }\n                                    return FileVisitResult.CONTINUE;\n                                }\n                            });\n                        }", "author": "original-brownbear", "createdAt": "2020-12-14T07:38:12Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,623 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Collections.synchronizedMap;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final Map<String, Document> documents;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.documents = synchronizedMap(loadDocuments(nodeEnvironment));\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                final NodeEnvironment.NodePath nodePath = writer.nodePath();\n+                logger.debug(\"loading persistent cache on data path [{}]\", nodePath);\n+\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(nodePath)) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = writer.nodePath().resolve(shardId);\n+                        final Path shardCachePath = getShardCachePath(new ShardPath(false, shardDataPath, shardDataPath, shardId));\n+\n+                        if (Files.isDirectory(shardCachePath)) {\n+                            logger.trace(\"found snapshot cache dir at [{}], loading cache files from disk and index\", shardCachePath);\n+                            Files.walkFileTree(shardCachePath, new CacheFileVisitor(cacheService, writer, documents));", "originalCommit": "255853ddc231672e78eee7ec0b27e7aab365776a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5ODc4NA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542198784", "bodyText": "Sure, I pushed ac6050f", "author": "tlrx", "createdAt": "2020-12-14T08:35:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE2ODYxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE3MTA2MA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542171060", "bodyText": "Do we actually need two loops for preparing and then committing? We do that for the CS persistence so we can throw IOError and stop everything if a commit fails but for the cache service it seems like we could just commit in a loop and ahve that deal with prepareCommit? That would also make exceptions less noisy because if you prepare a commit on one writer  and then fail committing another writer and thus call close close the first writer will throw on closing because it has a pending commit as well.", "author": "original-brownbear", "createdAt": "2020-12-14T07:43:17Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,623 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Collections.synchronizedMap;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final Map<String, Document> documents;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.documents = synchronizedMap(loadDocuments(nodeEnvironment));\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                final NodeEnvironment.NodePath nodePath = writer.nodePath();\n+                logger.debug(\"loading persistent cache on data path [{}]\", nodePath);\n+\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(nodePath)) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = writer.nodePath().resolve(shardId);\n+                        final Path shardCachePath = getShardCachePath(new ShardPath(false, shardDataPath, shardDataPath, shardId));\n+\n+                        if (Files.isDirectory(shardCachePath)) {\n+                            logger.trace(\"found snapshot cache dir at [{}], loading cache files from disk and index\", shardCachePath);\n+                            Files.walkFileTree(shardCachePath, new CacheFileVisitor(cacheService, writer, documents));\n+                        }\n+                    }\n+                }\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+            logger.info(\"persistent cache index loaded\");\n+            documents.clear();\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw new UncheckedIOException(\"Failed to load persistent cache\", e);\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    void commit() throws IOException {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();", "originalCommit": "255853ddc231672e78eee7ec0b27e7aab365776a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5OTQ2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542199463", "bodyText": "That's a good point, thanks for catching this. Commiting in a single loop makes it less error prone as you pointed. I pushed ac6050f", "author": "tlrx", "createdAt": "2020-12-14T08:36:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE3MTA2MA=="}], "type": "inlineReview"}, {"oid": "9e9ba7c01591d1414b5fd0c6796713d38728bb6c", "url": "https://github.com/elastic/elasticsearch/commit/9e9ba7c01591d1414b5fd0c6796713d38728bb6c", "message": "Merge branch 'master' into add-persistent-cache", "committedDate": "2020-12-14T08:14:57Z", "type": "commit"}, {"oid": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "url": "https://github.com/elastic/elasticsearch/commit/ac6050f206ff4e096736ef19a2812c33a365b9b7", "message": "feedback", "committedDate": "2020-12-14T08:34:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjIyMjQyOA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542222428", "bodyText": "Maybe we can call this repopulateCache instead? Or just \"start\", since this method must be called before most other methods.", "author": "henningandersen", "createdAt": "2020-12-14T09:12:09Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Collections.synchronizedMap;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final Map<String, Document> documents;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.documents = synchronizedMap(loadDocuments(nodeEnvironment));\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDUyNg==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384526", "bodyText": "I do like repopulateCache. I pushed 51da63c", "author": "tlrx", "createdAt": "2020-12-14T13:31:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjIyMjQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjIyNDYyMw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542224623", "bodyText": "I think we should add protection against calling this twice? Perhaps we need a started flag that we can also assert on in the methods accessing the writers?", "author": "henningandersen", "createdAt": "2020-12-14T09:15:29Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Collections.synchronizedMap;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final Map<String, Document> documents;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.documents = synchronizedMap(loadDocuments(nodeEnvironment));\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDQ5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384491", "bodyText": "I agree, I pushed 51755bd", "author": "tlrx", "createdAt": "2020-12-14T13:31:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjIyNDYyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjIyNjgxNg==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542226816", "bodyText": "nit: I think this could be simplified to:\nupdateCacheFile(buildId(cacheFile), buildDocument(nodePath, cacheFile, cacheRanges));", "author": "henningandersen", "createdAt": "2020-12-14T09:18:49Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Collections.synchronizedMap;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final Map<String, Document> documents;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.documents = synchronizedMap(loadDocuments(nodeEnvironment));\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                final NodeEnvironment.NodePath nodePath = writer.nodePath();\n+                logger.debug(\"loading persistent cache on data path [{}]\", nodePath);\n+\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(nodePath)) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = writer.nodePath().resolve(shardId);\n+                        final Path shardCachePath = getShardCachePath(new ShardPath(false, shardDataPath, shardDataPath, shardId));\n+\n+                        if (Files.isDirectory(shardCachePath)) {\n+                            logger.trace(\"found snapshot cache dir at [{}], loading cache files from disk and index\", shardCachePath);\n+                            Files.walkFileTree(shardCachePath, new SimpleFileVisitor<>() {\n+                                @Override\n+                                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) {\n+                                    try {\n+                                        final String id = buildId(file);\n+                                        final Document cacheDocument = documents.get(id);\n+                                        if (cacheDocument != null) {\n+                                            logger.trace(\"indexing cache file with id [{}] in persistent cache index\", id);\n+                                            writer.updateCacheFile(id, cacheDocument);\n+\n+                                            final CacheKey cacheKey = buildCacheKey(cacheDocument);\n+                                            final long fileLength = getFileLength(cacheDocument);\n+                                            final SortedSet<Tuple<Long, Long>> ranges = buildCacheFileRanges(cacheDocument);\n+\n+                                            logger.trace(\"adding cache file with [id={}, cache key={}, ranges={}]\", id, cacheKey, ranges);\n+                                            cacheService.put(cacheKey, fileLength, file.getParent(), id, ranges);\n+                                        } else {\n+                                            logger.trace(\"deleting cache file [{}] (does not exist in persistent cache index)\", file);\n+                                            Files.delete(file);\n+                                        }\n+                                    } catch (Exception e) {\n+                                        throw ExceptionsHelper.convertToRuntime(e);\n+                                    }\n+                                    return FileVisitResult.CONTINUE;\n+                                }\n+                            });\n+                        }\n+                    }\n+                }\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+            logger.info(\"persistent cache index loaded\");\n+            documents.clear();\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw new UncheckedIOException(\"Failed to load persistent cache\", e);\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    void commit() throws IOException {\n+        ensureOpen();\n+        try {\n+            for (CacheIndexWriter writer : writers) {\n+                writer.prepareCommit();\n+            }\n+            for (CacheIndexWriter writer : writers) {\n+                writer.commit();\n+            }\n+        } catch (IOException e) {\n+            try {\n+                close();\n+            } catch (Exception e2) {\n+                logger.warn(\"failed to close persistent cache index writer\", e2);\n+                e.addSuppressed(e2);\n+            }\n+            throw e;\n+        } finally {\n+            closeIfAnyIndexWriterHasTragedyOrIsClosed();\n+        }\n+    }\n+\n+    private void closeIfAnyIndexWriterHasTragedyOrIsClosed() {\n+        if (writers.stream().map(writer -> writer.indexWriter).anyMatch(iw -> iw.getTragicException() != null || iw.isOpen() == false)) {\n+            try {\n+                close();\n+            } catch (Exception e) {\n+                logger.warn(\"failed to close persistent cache index\", e);\n+            }\n+        }\n+    }\n+\n+    public boolean hasDeletions() {\n+        ensureOpen();\n+        for (CacheIndexWriter writer : writers) {\n+            if (writer.indexWriter.hasDeletions()) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    public long getNumDocs() {\n+        ensureOpen();\n+        long count = 0L;\n+        for (CacheIndexWriter writer : writers) {\n+            count += writer.indexWriter.getPendingNumDocs();\n+        }\n+        return count;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        if (closed.compareAndSet(false, true)) {\n+            try {\n+                IOUtils.close(writers);\n+            } finally {\n+                documents.clear();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates a list of {@link CacheIndexWriter}, one for each data path of the specified {@link NodeEnvironment}.\n+     *\n+     * @param nodeEnvironment the data node environment\n+     * @return a list of {@link CacheIndexWriter}\n+     */\n+    private static List<CacheIndexWriter> createWriters(NodeEnvironment nodeEnvironment) {\n+        final List<CacheIndexWriter> writers = new ArrayList<>();\n+        boolean success = false;\n+        try {\n+            final NodeEnvironment.NodePath[] nodePaths = nodeEnvironment.nodePaths();\n+            for (NodeEnvironment.NodePath nodePath : nodePaths) {\n+                writers.add(createCacheIndexWriter(nodePath));\n+            }\n+            success = true;\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to create persistent cache writers\", e);\n+        } finally {\n+            if (success == false) {\n+                IOUtils.closeWhileHandlingException(writers);\n+            }\n+        }\n+        return unmodifiableList(writers);\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheIndexWriter} for the specified data path. The is a single instance per data path.\n+     *\n+     * @param nodePath the data path\n+     * @return a new {@link CacheIndexWriter} instance\n+     * @throws IOException if something went wrong\n+     */\n+    static CacheIndexWriter createCacheIndexWriter(NodeEnvironment.NodePath nodePath) throws IOException {\n+        final List<Closeable> closeables = new ArrayList<>();\n+        boolean success = false;\n+        try {\n+            Path directoryPath = createCacheIndexFolder(nodePath);\n+            final Directory directory = FSDirectory.open(directoryPath);\n+            closeables.add(directory);\n+\n+            final IndexWriterConfig config = new IndexWriterConfig(new KeywordAnalyzer());\n+            config.setIndexDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());\n+            config.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n+            config.setMergeScheduler(new SerialMergeScheduler());\n+            config.setRAMBufferSizeMB(1.0);\n+            config.setCommitOnClose(false);\n+\n+            final IndexWriter indexWriter = new IndexWriter(directory, config);\n+            closeables.add(indexWriter);\n+\n+            final CacheIndexWriter cacheIndexWriter = new CacheIndexWriter(nodePath, directory, indexWriter);\n+            success = true;\n+            return cacheIndexWriter;\n+        } finally {\n+            if (success == false) {\n+                IOUtils.close(closeables);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Load existing documents from persistent cache indices located at the root of every node path.\n+     *\n+     * @param nodeEnvironment the data node environment\n+     * @return a map of {cache file uuid, Lucene document}\n+     */\n+    static Map<String, Document> loadDocuments(NodeEnvironment nodeEnvironment) {\n+        final Map<String, Document> documents = new HashMap<>();\n+        try {\n+            for (NodeEnvironment.NodePath nodePath : nodeEnvironment.nodePaths()) {\n+                final Path directoryPath = resolveCacheIndexFolder(nodePath);\n+                if (Files.exists(directoryPath)) {\n+                    documents.putAll(loadDocuments(directoryPath));\n+                }\n+            }\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to load existing documents from persistent cache index\", e);\n+        }\n+        return documents;\n+    }\n+\n+    /**\n+     * Load existing documents from a persistent cache Lucene directory.\n+     *\n+     * @param directoryPath the Lucene directory path\n+     * @return a map of {cache file uuid, Lucene document}\n+     */\n+    static Map<String, Document> loadDocuments(Path directoryPath) throws IOException {\n+        final Map<String, Document> documents = new HashMap<>();\n+        try (Directory directory = FSDirectory.open(directoryPath)) {\n+            try (IndexReader indexReader = DirectoryReader.open(directory)) {\n+                logger.trace(\"loading documents from persistent cache index [{}]\", directoryPath);\n+                for (LeafReaderContext leafReaderContext : indexReader.leaves()) {\n+                    final LeafReader leafReader = leafReaderContext.reader();\n+                    final Bits liveDocs = leafReader.getLiveDocs();\n+                    for (int i = 0; i < leafReader.maxDoc(); i++) {\n+                        if (liveDocs == null || liveDocs.get(i)) {\n+                            final Document document = leafReader.document(i);\n+                            logger.trace(\"loading document [{}]\", document);\n+                            documents.put(getValue(document, CACHE_ID_FIELD), document);\n+                        }\n+                    }\n+                }\n+            } catch (IndexNotFoundException e) {\n+                logger.debug(\"persistent cache index does not exist yet\", e);\n+            }\n+        }\n+        return documents;\n+    }\n+\n+    /**\n+     * Cleans any leftover searchable snapshot caches (files and Lucene indices) when a non-data node is starting up.\n+     * This is useful when the node is repurposed and is not a data node anymore.\n+     *\n+     * @param nodeEnvironment the {@link NodeEnvironment} to cleanup\n+     */\n+    public static void cleanUp(Settings settings, NodeEnvironment nodeEnvironment) {\n+        final boolean isDataNode = DiscoveryNode.isDataNode(settings);\n+        if (isDataNode) {\n+            assert false : \"should not be called on data nodes\";\n+            throw new IllegalStateException(\"Cannot clean searchable snapshot caches: node is a data node\");\n+        }\n+        try {\n+            for (NodeEnvironment.NodePath nodePath : nodeEnvironment.nodePaths()) {\n+                for (String indexUUID : nodeEnvironment.availableIndexFoldersForPath(nodePath)) {\n+                    for (ShardId shardId : nodeEnvironment.findAllShardIds(new Index(\"_unknown_\", indexUUID))) {\n+                        final Path shardDataPath = nodePath.resolve(shardId);\n+                        final ShardPath shardPath = new ShardPath(false, shardDataPath, shardDataPath, shardId);\n+                        final Path cacheDir = getShardCachePath(shardPath);\n+                        if (Files.isDirectory(cacheDir)) {\n+                            logger.debug(\"deleting searchable snapshot shard cache directory [{}]\", cacheDir);\n+                            IOUtils.rm(cacheDir);\n+                        }\n+                    }\n+                }\n+                final Path cacheIndexDir = resolveCacheIndexFolder(nodePath);\n+                if (Files.isDirectory(cacheIndexDir)) {\n+                    logger.debug(\"deleting searchable snapshot lucene directory [{}]\", cacheIndexDir);\n+                    IOUtils.rm(cacheIndexDir);\n+                }\n+            }\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to clean up searchable snapshots cache\", e);\n+        }\n+    }\n+\n+    /**\n+     * A {@link CacheIndexWriter} contains a Lucene {@link Directory} with an {@link IndexWriter} that can be used to index documents in\n+     * the persistent cache index. There is one {@link CacheIndexWriter} for each data path.\n+     */\n+    static class CacheIndexWriter implements Closeable {\n+\n+        private final NodeEnvironment.NodePath nodePath;\n+        private final IndexWriter indexWriter;\n+        private final Directory directory;\n+\n+        private CacheIndexWriter(NodeEnvironment.NodePath nodePath, Directory directory, IndexWriter indexWriter) {\n+            this.nodePath = nodePath;\n+            this.directory = directory;\n+            this.indexWriter = indexWriter;\n+        }\n+\n+        NodeEnvironment.NodePath nodePath() {\n+            return nodePath;\n+        }\n+\n+        void updateCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> cacheRanges) throws IOException {\n+            final Term term = buildTerm(cacheFile);\n+            logger.debug(\"updating document with term [{}]\", term);\n+            indexWriter.updateDocument(term, buildDocument(nodePath, cacheFile, cacheRanges));", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDQ2OA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384468", "bodyText": "Right, thanks! I pushed 14c680e", "author": "tlrx", "createdAt": "2020-12-14T13:31:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjIyNjgxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI2NjU3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542266577", "bodyText": "This looks unused.", "author": "henningandersen", "createdAt": "2020-12-14T10:16:18Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/AbstractSearchableSnapshotsTestCase.java", "diffHunk": "@@ -60,6 +62,7 @@\n     protected ThreadPool threadPool;\n     protected ClusterService clusterService;\n     protected NodeEnvironment nodeEnvironment;\n+    protected PersistentCache persistentCache;", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDQ1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384453", "bodyText": "Right, it is a leftover. I pushed 589cfda", "author": "tlrx", "createdAt": "2020-12-14T13:31:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI2NjU3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI3ODE4MA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542278180", "bodyText": "We should also add a test for this method.", "author": "henningandersen", "createdAt": "2020-12-14T10:33:39Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.analysis.core.KeywordAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StoredField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexNotFoundException;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SerialMergeScheduler;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.util.Bits;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Collections.synchronizedMap;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableSortedSet;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.CacheService.getShardCachePath;\n+\n+public class PersistentCache implements Closeable {\n+\n+    private static final Logger logger = LogManager.getLogger(PersistentCache.class);\n+\n+    private static final String NODE_VERSION_COMMIT_KEY = \"node_version\";\n+\n+    private final NodeEnvironment nodeEnvironment;\n+    private final Map<String, Document> documents;\n+    private final List<CacheIndexWriter> writers;\n+    private final AtomicBoolean closed;\n+\n+    public PersistentCache(NodeEnvironment nodeEnvironment) {\n+        this.documents = synchronizedMap(loadDocuments(nodeEnvironment));\n+        this.writers = createWriters(nodeEnvironment);\n+        this.nodeEnvironment = nodeEnvironment;\n+        this.closed = new AtomicBoolean();\n+    }\n+\n+    private void ensureOpen() {\n+        if (closed.get()) {\n+            throw new AlreadyClosedException(\"Persistent cache is already closed\");\n+        }\n+    }\n+\n+    /**\n+     * @return the {@link CacheIndexWriter} to use for the given {@link CacheFile}\n+     */\n+    private CacheIndexWriter getWriter(CacheFile cacheFile) {\n+        ensureOpen();\n+        if (writers.size() == 1) {\n+            return writers.get(0);\n+        } else {\n+            final Path path = cacheFile.getFile().toAbsolutePath();\n+            return writers.stream()\n+                .filter(writer -> path.startsWith(writer.nodePath().path))\n+                .findFirst()\n+                .orElseThrow(() -> new IllegalArgumentException(\"Failed to find a Lucene index for cache file path [\" + path + ']'));\n+        }\n+    }\n+\n+    public void addCacheFile(CacheFile cacheFile, SortedSet<Tuple<Long, Long>> ranges) throws IOException {\n+        getWriter(cacheFile).updateCacheFile(cacheFile, ranges);\n+    }\n+\n+    public void removeCacheFile(CacheFile cacheFile) throws IOException {\n+        getWriter(cacheFile).deleteCacheFile(cacheFile);\n+    }\n+\n+    /**\n+     * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest\n+     * synchronized information and puts the cache file into the searchable snapshots cache.\n+     *\n+     * This method iterates over all node data paths and all shard directories in order to found the \"snapshot_cache\" directories that\n+     * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in\n+     * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no\n+     * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted\n+     * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId}\n+     * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current\n+     * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache\n+     * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see\n+     * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}.\n+     *\n+     * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}.\n+     */\n+    void loadCacheFiles(CacheService cacheService) {", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDIxMg==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384212", "bodyText": "Added a basic test in 5ca99d2", "author": "tlrx", "createdAt": "2020-12-14T13:30:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI3ODE4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI4MDc3MA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542280770", "bodyText": "Would it be possible to add assert e instanceof IOException here? OK to warn in production, but would be good to catch if this fails in tests for other things than IO.", "author": "henningandersen", "createdAt": "2020-12-14T10:37:29Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -397,19 +402,29 @@ private void onCacheFileUpdate(CacheFile cacheFile) {\n     /**\n      * This method is invoked after a {@link CacheFile} is evicted from the cache.\n      * <p>\n-     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted.\n+     * It notifies the {@link CacheFile}'s eviction listeners that the instance is evicted and removes it from the persistent cache.\n      *\n      * @param cacheFile the evicted instance\n      */\n     private void onCacheFileRemoval(CacheFile cacheFile) {\n         IOUtils.closeWhileHandlingException(cacheFile::startEviction);\n+        try {\n+            persistentCache.removeCacheFile(cacheFile);\n+        } catch (Exception e) {\n+            logger.warn(\"failed to remove cache file from persistent cache\", e);", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDI1NA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384254", "bodyText": "Yes, that sounds a good thing to do so I pushed 5aa545a", "author": "tlrx", "createdAt": "2020-12-14T13:30:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI4MDc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI5MjgxMw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542292813", "bodyText": "I think this runs concurrently with prewarming? If so, is there a risk of initially seeing one set of files, but prewarming adding another file before the validation that follows below? Or do we promise to read bits of all files before starting shard?", "author": "henningandersen", "createdAt": "2020-12-14T10:56:10Z", "path": "x-pack/plugin/searchable-snapshots/src/internalClusterTest/java/org/elasticsearch/xpack/searchablesnapshots/cache/SearchableSnapshotsPersistentCacheIntegTests.java", "diffHunk": "@@ -143,7 +91,9 @@ public void testCacheDirectoriesRemovedOnStartup() throws Exception {\n             .getIndex();\n \n         final IndexService indexService = internalCluster().getInstance(IndicesService.class, dataNode).indexService(restoredIndex);\n-        final Path shardCachePath = CacheService.getShardCachePath(indexService.getShard(0).shardPath());\n+        final ShardPath shardPath = indexService.getShard(0).shardPath();\n+        final Path shardCachePath = CacheService.getShardCachePath(shardPath);\n+\n         assertTrue(Files.isDirectory(shardCachePath));\n         final Set<Path> cacheFiles = new HashSet<>();\n         try (DirectoryStream<Path> snapshotCacheStream = Files.newDirectoryStream(shardCachePath)) {", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjMyNzIzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542327235", "bodyText": "It runs concurrently with prewarming but I think it is OK. I expect Lucene to read at least the header checksum of every file when opening the Directory and that should constitute the set of cache files which should remain unchanged (because the cache size in this test is set to 1GB so we should not expect any cache eviction too).", "author": "tlrx", "createdAt": "2020-12-14T11:53:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI5MjgxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM1MTQ1OA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542351458", "bodyText": "\ud83d\udc4d", "author": "henningandersen", "createdAt": "2020-12-14T12:38:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI5MjgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI5OTc4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542299783", "bodyText": "nit: move to local variable?", "author": "henningandersen", "createdAt": "2020-12-14T11:07:09Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCacheTests.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.set.Sets;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.store.cache.CacheFile;\n+import org.elasticsearch.index.store.cache.CacheKey;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsTestCase;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.elasticsearch.cluster.node.DiscoveryNodeRole.BUILT_IN_ROLES;\n+import static org.elasticsearch.cluster.node.DiscoveryNodeRole.DATA_ROLE;\n+import static org.elasticsearch.index.store.cache.TestUtils.randomPopulateAndReads;\n+import static org.elasticsearch.node.NodeRoleSettings.NODE_ROLES_SETTING;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.PersistentCache.createCacheIndexWriter;\n+import static org.elasticsearch.xpack.searchablesnapshots.cache.PersistentCache.resolveCacheIndexFolder;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.notNullValue;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.hamcrest.Matchers.sameInstance;\n+\n+public class PersistentCacheTests extends AbstractSearchableSnapshotsTestCase {\n+\n+    public void testCacheIndexWriter() throws Exception {\n+        final NodeEnvironment.NodePath nodePath = randomFrom(nodeEnvironment.nodePaths());\n+\n+        int docId = 0;\n+        final Map<String, Integer> liveDocs = new HashMap<>();\n+        final Set<String> deletedDocs = new HashSet<>();\n+\n+        for (int iter = 0; iter < 20; iter++) {\n+\n+            final Path snapshotCacheIndexDir = resolveCacheIndexFolder(nodePath);\n+            assertThat(Files.exists(snapshotCacheIndexDir), equalTo(iter > 0));\n+\n+            // load existing documents from persistent cache index before each iteration\n+            final Map<String, Document> documents = PersistentCache.loadDocuments(nodeEnvironment);\n+            assertThat(documents.size(), equalTo(liveDocs.size()));\n+\n+            try (PersistentCache.CacheIndexWriter writer = createCacheIndexWriter(nodePath)) {\n+                assertThat(writer.nodePath(), sameInstance(nodePath));\n+\n+                // verify that existing documents are loaded\n+                for (Map.Entry<String, Integer> liveDoc : liveDocs.entrySet()) {\n+                    final Document document = documents.get(liveDoc.getKey());\n+                    assertThat(\"Document should be loaded\", document, notNullValue());\n+                    final String iteration = document.get(\"update_iteration\");\n+                    assertThat(iteration, equalTo(String.valueOf(liveDoc.getValue())));\n+                    writer.updateCacheFile(liveDoc.getKey(), document);\n+                }\n+\n+                // verify that deleted documents are not loaded\n+                for (String deletedDoc : deletedDocs) {\n+                    final Document document = documents.get(deletedDoc);\n+                    assertThat(\"Document should not be loaded\", document, nullValue());\n+                }\n+\n+                // random updates of existing documents\n+                final Map<String, Integer> updatedDocs = new HashMap<>();\n+                for (String cacheId : randomSubsetOf(liveDocs.keySet())) {\n+                    final Document document = new Document();\n+                    document.add(new StringField(\"cache_id\", cacheId, Field.Store.YES));\n+                    document.add(new StringField(\"update_iteration\", String.valueOf(iter), Field.Store.YES));\n+                    writer.updateCacheFile(cacheId, document);\n+\n+                    updatedDocs.put(cacheId, iter);\n+                }\n+\n+                // create new random documents\n+                final Map<String, Integer> newDocs = new HashMap<>();\n+                for (int i = 0; i < between(1, 10); i++) {\n+                    final String cacheId = String.valueOf(docId++);\n+                    final Document document = new Document();\n+                    document.add(new StringField(\"cache_id\", cacheId, Field.Store.YES));\n+                    document.add(new StringField(\"update_iteration\", String.valueOf(iter), Field.Store.YES));\n+                    writer.updateCacheFile(cacheId, document);\n+\n+                    newDocs.put(cacheId, iter);\n+                }\n+\n+                // deletes random documents\n+                final Map<String, Integer> removedDocs = new HashMap<>();\n+                for (String cacheId : randomSubsetOf(Sets.union(liveDocs.keySet(), newDocs.keySet()))) {\n+                    writer.deleteCacheFile(cacheId);\n+\n+                    removedDocs.put(cacheId, iter);\n+                }\n+\n+                boolean commit = false;\n+                if (frequently()) {\n+                    writer.prepareCommit();\n+                    if (frequently()) {\n+                        writer.commit();\n+                        commit = true;\n+                    }\n+                }\n+\n+                if (commit) {\n+                    liveDocs.putAll(updatedDocs);\n+                    liveDocs.putAll(newDocs);\n+                    for (String cacheId : removedDocs.keySet()) {\n+                        liveDocs.remove(cacheId);\n+                        deletedDocs.add(cacheId);\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static final byte[] buffer;", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDMwMw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384303", "bodyText": "Sure. I pushed 2940d20", "author": "tlrx", "createdAt": "2020-12-14T13:30:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI5OTc4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjMwMzU0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542303547", "bodyText": "In relation to the todo, could we instead avoid persisting those that fail into the PersistentCache? We should be able to check cacheDirs.contains(cacheDir) before fsync'ing the dir and only add to cacheDirs if fsync is successful?", "author": "henningandersen", "createdAt": "2020-12-14T11:13:30Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -459,21 +474,28 @@ protected void synchronizeCache() {\n                         final Path cacheDir = cacheFilePath.toAbsolutePath().getParent();\n                         if (cacheDirs.add(cacheDir)) {\n                             try {\n-                                IOUtils.fsync(cacheDir, true, false);\n+                                IOUtils.fsync(cacheDir, true, false); // TODO evict cache file if fsync failed", "originalCommit": "ac6050f206ff4e096736ef19a2812c33a365b9b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM4NDM1MA==", "url": "https://github.com/elastic/elasticsearch/pull/65725#discussion_r542384350", "bodyText": "Makes sense, I pushed eab0a0c.", "author": "tlrx", "createdAt": "2020-12-14T13:31:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjMwMzU0Nw=="}], "type": "inlineReview"}, {"oid": "51da63cad3d3f86343446fbbabb7627c51413027", "url": "https://github.com/elastic/elasticsearch/commit/51da63cad3d3f86343446fbbabb7627c51413027", "message": "repopulateCache", "committedDate": "2020-12-14T11:25:37Z", "type": "commit"}, {"oid": "51755bd9d64793e4a00ebac5246ec06438180064", "url": "https://github.com/elastic/elasticsearch/commit/51755bd9d64793e4a00ebac5246ec06438180064", "message": "started", "committedDate": "2020-12-14T11:35:50Z", "type": "commit"}, {"oid": "14c680efd0072f61d8f3e23d4efcdcf1432ee5c1", "url": "https://github.com/elastic/elasticsearch/commit/14c680efd0072f61d8f3e23d4efcdcf1432ee5c1", "message": "simpler updateCacheFile", "committedDate": "2020-12-14T11:37:40Z", "type": "commit"}, {"oid": "589cfdaf03ad33fde1dc0938d4a90462eefff2e0", "url": "https://github.com/elastic/elasticsearch/commit/589cfdaf03ad33fde1dc0938d4a90462eefff2e0", "message": "unused", "committedDate": "2020-12-14T11:39:29Z", "type": "commit"}, {"oid": "5aa545ad40686ab0666ea206e59137f88cedfa03", "url": "https://github.com/elastic/elasticsearch/commit/5aa545ad40686ab0666ea206e59137f88cedfa03", "message": "assert IOE", "committedDate": "2020-12-14T11:41:01Z", "type": "commit"}, {"oid": "2940d20433ddf0d9fcfbe7111d2f97112438872a", "url": "https://github.com/elastic/elasticsearch/commit/2940d20433ddf0d9fcfbe7111d2f97112438872a", "message": "local var buffer", "committedDate": "2020-12-14T11:42:48Z", "type": "commit"}, {"oid": "eab0a0caf48c8996e3f17438e5621a5bac2c9ccd", "url": "https://github.com/elastic/elasticsearch/commit/eab0a0caf48c8996e3f17438e5621a5bac2c9ccd", "message": "should persist", "committedDate": "2020-12-14T12:23:33Z", "type": "commit"}, {"oid": "9f041f6f0c9458fb6a8e753cc8176012fd09ba12", "url": "https://github.com/elastic/elasticsearch/commit/9f041f6f0c9458fb6a8e753cc8176012fd09ba12", "message": "Fix tests", "committedDate": "2020-12-14T12:50:52Z", "type": "commit"}, {"oid": "5ca99d266052ec7cb118334feac5452e866c53f4", "url": "https://github.com/elastic/elasticsearch/commit/5ca99d266052ec7cb118334feac5452e866c53f4", "message": "test", "committedDate": "2020-12-14T13:27:21Z", "type": "commit"}, {"oid": "3c9a8b1f2c668d3faffd29855fe6b126f553c354", "url": "https://github.com/elastic/elasticsearch/commit/3c9a8b1f2c668d3faffd29855fe6b126f553c354", "message": "Merge branch 'master' into add-persistent-cache", "committedDate": "2020-12-14T13:27:56Z", "type": "commit"}]}