{"pr_number": 62666, "pr_title": "Split up large HTTP responses in outbound pipeline", "pr_createdAt": "2020-09-18T20:49:39Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62666", "timeline": [{"oid": "5fe0acb52d83941f5391f88763217d1a325cb7d2", "url": "https://github.com/elastic/elasticsearch/commit/5fe0acb52d83941f5391f88763217d1a325cb7d2", "message": "Split up large HTTP responses in outbound pipeline\n\nCurrently Netty will batch compression an entire HTTP response\nregardless of its content size. It allocates a byte array at least of\nthe same size as the uncompressed content. This causes issues with our\nattempts to remove humungous G1GC allocations. This commit resolves the\nissue by split responses into 128KB chunks.\n\nThis has the side-effect of making large outbound HTTP responses that\nare compressed be send as chunked transfer-encoding.", "committedDate": "2020-09-18T20:45:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NTQ4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491185483", "bodyText": "Netty uses static direct byte buffers to chunked transfer encoded line breaks. So we have to handle them.", "author": "tbrooks8", "createdAt": "2020-09-18T20:52:58Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/transport/CopyBytesSocketChannel.java", "diffHunk": "@@ -162,9 +162,17 @@ private void adjustMaxBytesPerGatheringWrite(int attempted, int written, int old\n     private static void copyBytes(ByteBuffer[] source, int nioBufferCnt, ByteBuffer destination) {\n         for (int i = 0; i < nioBufferCnt && destination.hasRemaining(); i++) {\n             ByteBuffer buffer = source[i];\n-            assert buffer.hasArray() : \"Buffer must have heap array\";\n             int nBytesToCopy = Math.min(destination.remaining(), buffer.remaining());\n-            destination.put(buffer.array(), buffer.arrayOffset() + buffer.position(), nBytesToCopy);\n+            if (buffer.hasArray()) {\n+                destination.put(buffer.array(), buffer.arrayOffset() + buffer.position(), nBytesToCopy);\n+            } else {\n+                int initialLimit = buffer.limit();", "originalCommit": "5fe0acb52d83941f5391f88763217d1a325cb7d2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "url": "https://github.com/elastic/elasticsearch/commit/2b3f541885d67e37994fe2ca69685733bd23ad8f", "message": "Merge remote-tracking branch 'upstream/master' into remove_batch_http_compression", "committedDate": "2020-09-18T20:53:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTcxMTE4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491711183", "bodyText": "Should we only add this when handlingSettings.isCompression() == true?", "author": "original-brownbear", "createdAt": "2020-09-20T17:03:55Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpServerTransport.java", "diffHunk": "@@ -313,6 +315,7 @@ protected void initChannel(Channel ch) throws Exception {\n                 ch.pipeline().addLast(\"encoder_compress\", new HttpContentCompressor(handlingSettings.getCompressionLevel()));\n             }\n             ch.pipeline().addLast(\"request_creator\", requestCreator);\n+            ch.pipeline().addLast(\"response_creator\", responseCreator);", "originalCommit": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMzODE0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492338147", "bodyText": "I don't think so. It does not seem necessary to create a separate codepath for this pretty niche scenario.", "author": "tbrooks8", "createdAt": "2020-09-21T20:50:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTcxMTE4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM0OTgwMg==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492349802", "bodyText": "Don't we by default have compression off with tls enabled?", "author": "original-brownbear", "createdAt": "2020-09-21T21:14:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTcxMTE4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM4MjcxNA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492382714", "bodyText": "You're right. I had forgotten that we disable compression with TLS unless it is manually configured. I still think the single codepaths is the correct approach unless for some reason a nightly benchmark indicates we need a change.", "author": "tbrooks8", "createdAt": "2020-09-21T22:31:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTcxMTE4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUyMzkxNA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492523914", "bodyText": "I don't think this will show in benchmarks it's a small thing relative to the general noise in them for sure, but to me it seems strange to add general overhead for a workaround that's very specific to a concrete issue with the Netty compression implementation? (not too important to me, but still seems like it would also make it clearer why we're doing this).", "author": "original-brownbear", "createdAt": "2020-09-22T07:25:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTcxMTE4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg1Nzc0MA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492857740", "bodyText": "Because it is nice to have a consistent manner in which data flows down the pipeline instead of allowing the various features to create different combinations that must be tracked when investigating issues.\nAnd this PR does not go all the way, but splitting data up when sending is on the list of TODOs anyway. The fact that we batch compress and batch TLS encode on the HTTP outbound pipeline is not ideal. And a reason why the nio transport has outperformed the netty transport on scroll benchmarks. So I imagine there will be some follow-up work at some point to incrementally send data down the pipeline and flush, waiting for writability before sending more down.", "author": "tbrooks8", "createdAt": "2020-09-22T16:04:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTcxMTE4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg5NDg0MA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491894840", "bodyText": "Is this necessary/correct? It looks like other byte buffers position are fowarded during write.", "author": "henningandersen", "createdAt": "2020-09-21T09:13:19Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/transport/CopyBytesSocketChannel.java", "diffHunk": "@@ -162,9 +162,17 @@ private void adjustMaxBytesPerGatheringWrite(int attempted, int written, int old\n     private static void copyBytes(ByteBuffer[] source, int nioBufferCnt, ByteBuffer destination) {\n         for (int i = 0; i < nioBufferCnt && destination.hasRemaining(); i++) {\n             ByteBuffer buffer = source[i];\n-            assert buffer.hasArray() : \"Buffer must have heap array\";\n             int nBytesToCopy = Math.min(destination.remaining(), buffer.remaining());\n-            destination.put(buffer.array(), buffer.arrayOffset() + buffer.position(), nBytesToCopy);\n+            if (buffer.hasArray()) {\n+                destination.put(buffer.array(), buffer.arrayOffset() + buffer.position(), nBytesToCopy);\n+            } else {\n+                int initialLimit = buffer.limit();\n+                int initialPosition = buffer.position();\n+                buffer.limit(buffer.position() + nBytesToCopy);\n+                destination.put(buffer);\n+                buffer.position(initialPosition);", "originalCommit": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI3MjUzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492272535", "bodyText": "It is both necessary and correct. setWrittenBytes will adjust positions after the socket write call. Copying the bytes into the outbound buffer does not mean those bytes will be successfully flushed on this call.", "author": "tbrooks8", "createdAt": "2020-09-21T18:46:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg5NDg0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkxNDQzOA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491914438", "bodyText": "Is there an easy way to assert that we did not do a big unpooled allocation?\nIf not, we could consider adding an assert here that the encoding is chunked (if possible)?", "author": "henningandersen", "createdAt": "2020-09-21T09:46:03Z", "path": "modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpServerTransportTests.java", "diffHunk": "@@ -282,6 +282,52 @@ public void dispatchBadRequest(final RestChannel channel, final ThreadContext th\n         assertThat(causeReference.get(), instanceOf(TooLongFrameException.class));\n     }\n \n+    public void testLargeCompressedResponse() throws InterruptedException {\n+        final String responseString = randomAlphaOfLength(4 * 1024 * 1024);\n+        final String url = \"/thing\";\n+        final HttpServerTransport.Dispatcher dispatcher = new HttpServerTransport.Dispatcher() {\n+\n+            @Override\n+            public void dispatchRequest(final RestRequest request, final RestChannel channel, final ThreadContext threadContext) {\n+                if (url.equals(request.uri())) {\n+                    channel.sendResponse(new BytesRestResponse(OK, responseString));\n+                } else {\n+                    logger.error(\"--> Unexpected successful uri [{}]\", request.uri());\n+                    throw new AssertionError();\n+                }\n+            }\n+\n+            @Override\n+            public void dispatchBadRequest(final RestChannel channel, final ThreadContext threadContext, final Throwable cause) {\n+                logger.error(new ParameterizedMessage(\"--> Unexpected bad request [{}]\",\n+                    FakeRestRequest.requestToString(channel.request())), cause);\n+                throw new AssertionError();\n+            }\n+\n+        };\n+\n+        try (Netty4HttpServerTransport transport = new Netty4HttpServerTransport(\n+            Settings.EMPTY, networkService, bigArrays, threadPool, xContentRegistry(), dispatcher, clusterSettings,\n+            new SharedGroupFactory(Settings.EMPTY))) {\n+            transport.start();\n+            final TransportAddress remoteAddress = randomFrom(transport.boundAddress().boundAddresses());\n+\n+            try (Netty4HttpClient client = new Netty4HttpClient()) {\n+                DefaultFullHttpRequest request = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, url);\n+                request.headers().add(HttpHeaderNames.ACCEPT_ENCODING, randomFrom(\"deflate\", \"gzip\"));\n+                final FullHttpResponse response = client.send(remoteAddress.address(), request);\n+                try {\n+                    assertThat(response.status(), equalTo(HttpResponseStatus.OK));", "originalCommit": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM0MTYxNA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492341614", "bodyText": "There is no reliable way to determine if the response is chunked or not that does not involve textually decoding HTTP/1.1. That is completely handled by Netty. And through the pipeline Netty changes that header several times. (If it is content-length on the wire, Netty deletes that and changes it to chunked in the object decoder. It changes it back to content-length after the aggregation is complete.) Both transfer-encodings are valid so Netty abstracts it away.\nIn terms of preventing large allocations - I can do a bunch of casting if you want. I would need to cast the allocator to NoDirectBuffers (and make that inner class public) extract the delegate and iff it is pooled, then iterate through the arenas and see if there have been any large allocations. However, this allocator is static and used by both the testing and production code so it will be brittle to a concurrent failure where test code does a huge allocation. And a lot of the time this allocator will be unpooled based on the heap size of tests. But that seems to me to be the only way to verify.", "author": "tbrooks8", "createdAt": "2020-09-21T20:57:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkxNDQzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU1ODMyNw==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492558327", "bodyText": "I think the last part sounds reasonable to do. I am curious about:\n\nused by both the testing and production code so it will be brittle to a concurrent failure where test code does a huge allocation\n\nIf we check the number of huge allocations before and after doing the http request I am not sure how anything concurrent can happen too in this test?", "author": "henningandersen", "createdAt": "2020-09-22T08:27:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkxNDQzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkxNTIyNw==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491915227", "bodyText": "Do we need something similar for nio http or does that not have that behavior anyway?", "author": "henningandersen", "createdAt": "2020-09-21T09:46:58Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpServerTransport.java", "diffHunk": "@@ -313,6 +315,7 @@ protected void initChannel(Channel ch) throws Exception {\n                 ch.pipeline().addLast(\"encoder_compress\", new HttpContentCompressor(handlingSettings.getCompressionLevel()));\n             }\n             ch.pipeline().addLast(\"request_creator\", requestCreator);\n+            ch.pipeline().addLast(\"response_creator\", responseCreator);", "originalCommit": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI3MzkyOA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492273928", "bodyText": "We will probably want it at some point if the NIO transport becomes used in production. But it is not really necessary at this moment since it is mostly a test transport. I will probably follow-up at some point.", "author": "tbrooks8", "createdAt": "2020-09-21T18:48:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkxNTIyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkyODMwNA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491928304", "bodyText": "Will this not always result in chunked response? I would imagine we should check the size first and then pass the original message unmodified if below 128KB?", "author": "henningandersen", "createdAt": "2020-09-21T10:09:20Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpResponseCreator.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http.netty4;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.channel.ChannelHandler;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.handler.codec.MessageToMessageEncoder;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpResponse;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpResponse;\n+\n+import java.util.List;\n+\n+@ChannelHandler.Sharable\n+class Netty4HttpResponseCreator extends MessageToMessageEncoder<Netty4HttpResponse> {\n+\n+    public static final int ONE_TWENTY_EIGHT_KB = 128 * 1024;\n+\n+    @Override\n+    protected void encode(ChannelHandlerContext ctx, Netty4HttpResponse msg, List<Object> out) {\n+        HttpResponse response = new DefaultHttpResponse(msg.protocolVersion(), msg.status(), msg.headers());\n+        out.add(response);", "originalCommit": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMzMTgyOA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492331828", "bodyText": "I will make your change as it seems fine. But no, it will not always result in a chunked response. It is only the compressor that turns it into a chunked response.", "author": "tbrooks8", "createdAt": "2020-09-21T20:38:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkyODMwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkyOTQyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491929429", "bodyText": "I wonder if we should make this a system property? Mainly to be able to set it very high in case the additional chunked transfer causes performance issues somewhere. Not intending to document or anything, just a safe-guard.\nnit: should be private.", "author": "henningandersen", "createdAt": "2020-09-21T10:11:25Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpResponseCreator.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http.netty4;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.channel.ChannelHandler;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.handler.codec.MessageToMessageEncoder;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpResponse;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpResponse;\n+\n+import java.util.List;\n+\n+@ChannelHandler.Sharable\n+class Netty4HttpResponseCreator extends MessageToMessageEncoder<Netty4HttpResponse> {\n+\n+    public static final int ONE_TWENTY_EIGHT_KB = 128 * 1024;", "originalCommit": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4MjU2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r491982566", "bodyText": "I wonder if can do better here. If we just slice to 128k here, we'll still see massive amounts of 128k byte[] allocations for large responses. Couldn't we just copy to new ByteBuf allocated by Netty's allocator and pre-sized to 128k here and assume those will mostly be backed by a single byte[] (not sure if we can assume that I must admit but maybe there's some things we could do to make it so at least) and thus not incur the allocation in the compressor because ?\nOr asked the other way around, maybe we could just use a chunk size here that we know the Netty allocator would use single byte[] backed buffers for and use that?", "author": "original-brownbear", "createdAt": "2020-09-21T11:52:01Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpResponseCreator.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http.netty4;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.channel.ChannelHandler;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.handler.codec.MessageToMessageEncoder;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpResponse;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpResponse;\n+\n+import java.util.List;\n+\n+@ChannelHandler.Sharable\n+class Netty4HttpResponseCreator extends MessageToMessageEncoder<Netty4HttpResponse> {\n+\n+    public static final int ONE_TWENTY_EIGHT_KB = 128 * 1024;\n+\n+    @Override\n+    protected void encode(ChannelHandlerContext ctx, Netty4HttpResponse msg, List<Object> out) {\n+        HttpResponse response = new DefaultHttpResponse(msg.protocolVersion(), msg.status(), msg.headers());\n+        out.add(response);\n+        ByteBuf content = msg.content();\n+        while (content.readableBytes() > ONE_TWENTY_EIGHT_KB) {\n+            out.add(new DefaultHttpContent(content.readRetainedSlice(ONE_TWENTY_EIGHT_KB)));", "originalCommit": "2b3f541885d67e37994fe2ca69685733bd23ad8f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjE2MjcxNA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492162714", "bodyText": "I'm not sure what this comment means. But I am planning on making the sliced size variable based on the underlying chunk sizes in the next iteration.", "author": "tbrooks8", "createdAt": "2020-09-21T15:43:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4MjU2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjE3NjU3OA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492176578", "bodyText": "I'm not sure what this comment means.\n\nsorry about that :) but:\n\nBut I am planning on making the sliced size variable based on the underlying chunk sizes in the next iteration.\n\nThanks that's what I was looking for :)", "author": "original-brownbear", "createdAt": "2020-09-21T16:03:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4MjU2Ng=="}], "type": "inlineReview"}, {"oid": "3c62705cbc9f0c6696e8fa027dac10a71878218e", "url": "https://github.com/elastic/elasticsearch/commit/3c62705cbc9f0c6696e8fa027dac10a71878218e", "message": "Merge remote-tracking branch 'upstream/master' into remove_batch_http_compression", "committedDate": "2020-09-21T16:47:37Z", "type": "commit"}, {"oid": "b582f2fdc40b2ca1d54d003a3cb4bfef96574a66", "url": "https://github.com/elastic/elasticsearch/commit/b582f2fdc40b2ca1d54d003a3cb4bfef96574a66", "message": "Changes", "committedDate": "2020-09-21T18:59:48Z", "type": "commit"}, {"oid": "a18f42f2b78ef5a1707cf4803c150de3c8ab0a57", "url": "https://github.com/elastic/elasticsearch/commit/a18f42f2b78ef5a1707cf4803c150de3c8ab0a57", "message": "WIP", "committedDate": "2020-09-21T19:50:43Z", "type": "commit"}, {"oid": "c5241bb853567cbe4c50d0366b39468b3c805764", "url": "https://github.com/elastic/elasticsearch/commit/c5241bb853567cbe4c50d0366b39468b3c805764", "message": "Merge remote-tracking branch 'upstream/master' into remove_batch_http_compression", "committedDate": "2020-09-21T19:50:51Z", "type": "commit"}, {"oid": "769148b2156aaabc6272280fd3b60bfcbe6d487b", "url": "https://github.com/elastic/elasticsearch/commit/769148b2156aaabc6272280fd3b60bfcbe6d487b", "message": "Changes", "committedDate": "2020-09-21T20:44:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM0OTk1MA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492349950", "bodyText": "Maybe add some short Javadoc here to explain why this is needed?", "author": "original-brownbear", "createdAt": "2020-09-21T21:14:36Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpResponseCreator.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http.netty4;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.channel.ChannelHandler;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.handler.codec.MessageToMessageEncoder;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpResponse;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpResponse;\n+import org.elasticsearch.common.Booleans;\n+import org.elasticsearch.transport.NettyAllocator;\n+\n+import java.util.List;\n+\n+@ChannelHandler.Sharable\n+class Netty4HttpResponseCreator extends MessageToMessageEncoder<Netty4HttpResponse> {", "originalCommit": "769148b2156aaabc6272280fd3b60bfcbe6d487b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM4MjAxNA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492382014", "bodyText": "I added that. And to the nio version.", "author": "tbrooks8", "createdAt": "2020-09-21T22:29:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM0OTk1MA=="}], "type": "inlineReview"}, {"oid": "4dad3e675362026f7c850e8210c684108c6f9762", "url": "https://github.com/elastic/elasticsearch/commit/4dad3e675362026f7c850e8210c684108c6f9762", "message": "nio and javadoc", "committedDate": "2020-09-21T22:29:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUyNTgxOA==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r492525818", "bodyText": "Can't we just clearly link to Netty's JdkZlibEncoder here since that is the only actual reason we're splitting responses as of today? If we say \"or other CPU intensive operations\" posterity will have a hard time figuring out if it's safe to remove this or not? :)", "author": "original-brownbear", "createdAt": "2020-09-22T07:29:08Z", "path": "modules/transport-netty4/src/main/java/org/elasticsearch/http/netty4/Netty4HttpResponseCreator.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http.netty4;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.channel.ChannelHandler;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.handler.codec.MessageToMessageEncoder;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpResponse;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpResponse;\n+import org.elasticsearch.common.Booleans;\n+import org.elasticsearch.transport.NettyAllocator;\n+\n+import java.util.List;\n+\n+/**\n+ * Split up large responses to prevent batch compression or other CPU intensive operations down the pipeline.", "originalCommit": "4dad3e675362026f7c850e8210c684108c6f9762", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "05bb7e0edd566d006f3127b9f98c2c55e097baca", "url": "https://github.com/elastic/elasticsearch/commit/05bb7e0edd566d006f3127b9f98c2c55e097baca", "message": "Merge remote-tracking branch 'upstream/master' into remove_batch_http_compression", "committedDate": "2020-09-22T15:10:40Z", "type": "commit"}, {"oid": "ddeb152cd12edcfcdda2eae1b1de0f2545470872", "url": "https://github.com/elastic/elasticsearch/commit/ddeb152cd12edcfcdda2eae1b1de0f2545470872", "message": "Changes", "committedDate": "2020-09-22T15:24:16Z", "type": "commit"}, {"oid": "89cada52f65cb6118500b07e3d26e93a6ddedc2d", "url": "https://github.com/elastic/elasticsearch/commit/89cada52f65cb6118500b07e3d26e93a6ddedc2d", "message": "Fix test", "committedDate": "2020-09-22T15:49:24Z", "type": "commit"}, {"oid": "223f90160ca1a4615b842bbfee5620eaf22ae51f", "url": "https://github.com/elastic/elasticsearch/commit/223f90160ca1a4615b842bbfee5620eaf22ae51f", "message": "Add comment", "committedDate": "2020-09-22T15:54:26Z", "type": "commit"}, {"oid": "28f342d4a711a94ead8405c687a715ed31a85363", "url": "https://github.com/elastic/elasticsearch/commit/28f342d4a711a94ead8405c687a715ed31a85363", "message": "log raw", "committedDate": "2020-09-22T16:44:08Z", "type": "commit"}, {"oid": "3b497f7f8dc1a7b0aecd0c397630d07bd41f4082", "url": "https://github.com/elastic/elasticsearch/commit/3b497f7f8dc1a7b0aecd0c397630d07bd41f4082", "message": "Merge remote-tracking branch 'upstream/master' into remove_batch_http_compression", "committedDate": "2020-09-22T16:44:22Z", "type": "commit"}, {"oid": "2b5fcdb2b0dee6e3fe8e0526b979106388289e24", "url": "https://github.com/elastic/elasticsearch/commit/2b5fcdb2b0dee6e3fe8e0526b979106388289e24", "message": "Merge remote-tracking branch 'upstream/master' into remove_batch_http_compression", "committedDate": "2020-09-22T18:08:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUxMTU3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r494511572", "bodyText": "nit: rename to numOfHugeAllocations", "author": "henningandersen", "createdAt": "2020-09-24T18:05:53Z", "path": "modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpServerTransportTests.java", "diffHunk": "@@ -282,6 +286,68 @@ public void dispatchBadRequest(final RestChannel channel, final ThreadContext th\n         assertThat(causeReference.get(), instanceOf(TooLongFrameException.class));\n     }\n \n+    public void testLargeCompressedResponse() throws InterruptedException {\n+        final String responseString = randomAlphaOfLength(4 * 1024 * 1024);\n+        final String url = \"/thing\";\n+        final HttpServerTransport.Dispatcher dispatcher = new HttpServerTransport.Dispatcher() {\n+\n+            @Override\n+            public void dispatchRequest(final RestRequest request, final RestChannel channel, final ThreadContext threadContext) {\n+                if (url.equals(request.uri())) {\n+                    channel.sendResponse(new BytesRestResponse(OK, responseString));\n+                } else {\n+                    logger.error(\"--> Unexpected successful uri [{}]\", request.uri());\n+                    throw new AssertionError();\n+                }\n+            }\n+\n+            @Override\n+            public void dispatchBadRequest(final RestChannel channel, final ThreadContext threadContext, final Throwable cause) {\n+                logger.error(new ParameterizedMessage(\"--> Unexpected bad request [{}]\",\n+                    FakeRestRequest.requestToString(channel.request())), cause);\n+                throw new AssertionError();\n+            }\n+\n+        };\n+\n+        try (Netty4HttpServerTransport transport = new Netty4HttpServerTransport(\n+            Settings.EMPTY, networkService, bigArrays, threadPool, xContentRegistry(), dispatcher, clusterSettings,\n+            new SharedGroupFactory(Settings.EMPTY))) {\n+            transport.start();\n+            final TransportAddress remoteAddress = randomFrom(transport.boundAddress().boundAddresses());\n+\n+            try (Netty4HttpClient client = new Netty4HttpClient()) {\n+                DefaultFullHttpRequest request = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, url);\n+                request.headers().add(HttpHeaderNames.ACCEPT_ENCODING, randomFrom(\"deflate\", \"gzip\"));\n+                long numOfHugAllocations = getHugeAllocationCount();", "originalCommit": "2b5fcdb2b0dee6e3fe8e0526b979106388289e24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU1MjcxMw==", "url": "https://github.com/elastic/elasticsearch/pull/62666#discussion_r494552713", "bodyText": "I think this is always true in tests? If so, I would prefer an assert to help ensure we do get a huge allocation count out.", "author": "henningandersen", "createdAt": "2020-09-24T19:13:45Z", "path": "modules/transport-netty4/src/test/java/org/elasticsearch/http/netty4/Netty4HttpServerTransportTests.java", "diffHunk": "@@ -282,6 +286,68 @@ public void dispatchBadRequest(final RestChannel channel, final ThreadContext th\n         assertThat(causeReference.get(), instanceOf(TooLongFrameException.class));\n     }\n \n+    public void testLargeCompressedResponse() throws InterruptedException {\n+        final String responseString = randomAlphaOfLength(4 * 1024 * 1024);\n+        final String url = \"/thing\";\n+        final HttpServerTransport.Dispatcher dispatcher = new HttpServerTransport.Dispatcher() {\n+\n+            @Override\n+            public void dispatchRequest(final RestRequest request, final RestChannel channel, final ThreadContext threadContext) {\n+                if (url.equals(request.uri())) {\n+                    channel.sendResponse(new BytesRestResponse(OK, responseString));\n+                } else {\n+                    logger.error(\"--> Unexpected successful uri [{}]\", request.uri());\n+                    throw new AssertionError();\n+                }\n+            }\n+\n+            @Override\n+            public void dispatchBadRequest(final RestChannel channel, final ThreadContext threadContext, final Throwable cause) {\n+                logger.error(new ParameterizedMessage(\"--> Unexpected bad request [{}]\",\n+                    FakeRestRequest.requestToString(channel.request())), cause);\n+                throw new AssertionError();\n+            }\n+\n+        };\n+\n+        try (Netty4HttpServerTransport transport = new Netty4HttpServerTransport(\n+            Settings.EMPTY, networkService, bigArrays, threadPool, xContentRegistry(), dispatcher, clusterSettings,\n+            new SharedGroupFactory(Settings.EMPTY))) {\n+            transport.start();\n+            final TransportAddress remoteAddress = randomFrom(transport.boundAddress().boundAddresses());\n+\n+            try (Netty4HttpClient client = new Netty4HttpClient()) {\n+                DefaultFullHttpRequest request = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, url);\n+                request.headers().add(HttpHeaderNames.ACCEPT_ENCODING, randomFrom(\"deflate\", \"gzip\"));\n+                long numOfHugAllocations = getHugeAllocationCount();\n+                final FullHttpResponse response = client.send(remoteAddress.address(), request);\n+                try {\n+                    assertThat(getHugeAllocationCount(), equalTo(numOfHugAllocations));\n+                    assertThat(response.status(), equalTo(HttpResponseStatus.OK));\n+                    byte[] bytes = new byte[response.content().readableBytes()];\n+                    response.content().readBytes(bytes);\n+                    assertThat(new String(bytes, StandardCharsets.UTF_8), equalTo(responseString));\n+                } finally {\n+                    response.release();\n+                }\n+            }\n+        }\n+    }\n+\n+    private long getHugeAllocationCount() {\n+        long numOfHugAllocations = 0;\n+        ByteBufAllocator allocator = NettyAllocator.getAllocator();\n+        if (allocator instanceof NettyAllocator.NoDirectBuffers) {", "originalCommit": "2b5fcdb2b0dee6e3fe8e0526b979106388289e24", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "bd857887a0f5e1e214199a843883bb80160d166b", "url": "https://github.com/elastic/elasticsearch/commit/bd857887a0f5e1e214199a843883bb80160d166b", "message": "Review changes", "committedDate": "2020-09-24T19:28:43Z", "type": "commit"}, {"oid": "2a14c4cc2f518e0da127f03e27c5073c8213003c", "url": "https://github.com/elastic/elasticsearch/commit/2a14c4cc2f518e0da127f03e27c5073c8213003c", "message": "Merge remote-tracking branch 'upstream/master' into remove_batch_http_compression", "committedDate": "2020-09-24T19:29:04Z", "type": "commit"}]}