{"pr_number": 62284, "pr_title": "Make SLM Run Snapshot Deletes in Parallel", "pr_createdAt": "2020-09-14T01:26:16Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62284", "timeline": [{"oid": "66fefc3a863cdb44f17ee191c4d267542de9dd6e", "url": "https://github.com/elastic/elasticsearch/commit/66fefc3a863cdb44f17ee191c4d267542de9dd6e", "message": "Make SLM Run Snapshot Deletes in Parallel\n\nNow that we support parallel snapshot and delete operations, there\nis no reason for SLM to wait for any snapshots to finish before starting\na delete. Deletes themselves can also be executed in parallel and in arbitrary\norder.\nThis commit makes use of parallel snapshot operations by removing all waits on\nother operations to finish. I tried to keep the changecount limited here so the\nfollowing follow ups were not included in this PR:\n* Use snapshot multi-delete instead of looping over individual deletes\n  * This isn't super important now as the deletes are internally batched into\none or two deletes anyway but requires a number of changes with the way\nstats are tracked currently)\n* Deprecate and remove the time bound setting for the retention run.\n  * This setting doesn't really make sense when all deletes run in parallel.\nFor one, there is no actual point at which to check for stopping the delete operation\nany longer. Also, multiple deletes in parallel tend to (in most cases) run about as\nfast as a single delete internally making the limit irrelevant sine we would always try\nand run at least one delete.\n\nRelates #59655", "committedDate": "2020-09-14T01:16:47Z", "type": "commit"}, {"oid": "d5974ca318fae59c41f9706b0756637ac557b558", "url": "https://github.com/elastic/elasticsearch/commit/d5974ca318fae59c41f9706b0756637ac557b558", "message": "formatting", "committedDate": "2020-09-14T01:27:21Z", "type": "commit"}, {"oid": "fd36d0507c3533ce4e8faf809bedbceaf071aed6", "url": "https://github.com/elastic/elasticsearch/commit/fd36d0507c3533ce4e8faf809bedbceaf071aed6", "message": "formatting", "committedDate": "2020-09-14T01:27:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MDczNA==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487660734", "bodyText": "Technically repo cleanup still isn't a concurrent operation and neither is deleting a snapshot that is currently being restored.\nI don't think these two cases are worth keeping this complexity around though. Both cases are kind of fringe and also the current way of checking for the cluster state not having anything conflicting in-progress isn't bullet proof to begin with (lots of possible races between checking the CS and actually starting the delete). => I think this is the reasonable simplification.", "author": "original-brownbear", "createdAt": "2020-09-14T05:33:44Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -414,111 +343,22 @@ void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n     void deleteSnapshot(String slmPolicy, String repo, SnapshotId snapshot, SnapshotLifecycleStats slmStats,\n                         ActionListener<AcknowledgedResponse> listener) {\n         logger.info(\"[{}] snapshot retention deleting snapshot [{}]\", repo, snapshot);\n-        CountDownLatch latch = new CountDownLatch(1);\n-        client.admin().cluster().prepareDeleteSnapshot(repo, snapshot.getName())\n-            .execute(new LatchedActionListener<>(ActionListener.wrap(acknowledgedResponse -> {\n-                    if (acknowledgedResponse.isAcknowledged()) {\n-                        logger.debug(\"[{}] snapshot [{}] deleted successfully\", repo, snapshot);\n-                    } else {\n-                        logger.warn(\"[{}] snapshot [{}] delete issued but the request was not acknowledged\", repo, snapshot);\n-                    }\n+        client.admin().cluster().prepareDeleteSnapshot(repo, snapshot.getName()).execute(ActionListener.wrap(acknowledgedResponse -> {\n                     slmStats.snapshotDeleted(slmPolicy);\n                     listener.onResponse(acknowledgedResponse);\n                 },\n                 e -> {\n-                    logger.warn(new ParameterizedMessage(\"[{}] failed to delete snapshot [{}] for retention\",\n-                        repo, snapshot), e);\n-                    slmStats.snapshotDeleteFailure(slmPolicy);\n-                    listener.onFailure(e);\n-                }), latch));\n-        try {\n-            // Deletes cannot occur simultaneously, so wait for this\n-            // deletion to complete before attempting the next one\n-            latch.await();\n-        } catch (InterruptedException e) {\n-            logger.error(new ParameterizedMessage(\"[{}] deletion of snapshot [{}] interrupted\",\n-                repo, snapshot), e);\n-            listener.onFailure(e);\n-            slmStats.snapshotDeleteFailure(slmPolicy);\n-        }\n+                    try {\n+                        logger.warn(new ParameterizedMessage(\"[{}] failed to delete snapshot [{}] for retention\",\n+                                repo, snapshot), e);\n+                        slmStats.snapshotDeleteFailure(slmPolicy);\n+                    } finally {\n+                        listener.onFailure(e);\n+                    }\n+                }));\n     }\n \n     void updateStateWithStats(SnapshotLifecycleStats newStats) {\n         clusterService.submitStateUpdateTask(\"update_slm_stats\", new UpdateSnapshotLifecycleStatsTask(newStats));\n     }\n-\n-    public static boolean okayToDeleteSnapshots(ClusterState state) {\n-        // Cannot delete during a snapshot\n-        if (state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries().size() > 0) {\n-            logger.trace(\"deletion cannot proceed as there are snapshots in progress\");\n-            return false;\n-        }\n-\n-        // Cannot delete during an existing delete\n-        if (state.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY).hasDeletionsInProgress()) {\n-            logger.trace(\"deletion cannot proceed as there are snapshot deletions in progress\");\n-            return false;\n-        }\n-\n-        // Cannot delete while a repository is being cleaned\n-        if (state.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY).hasCleanupInProgress()) {", "originalCommit": "fd36d0507c3533ce4e8faf809bedbceaf071aed6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MDg1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487660853", "bodyText": "These tests seemed irrelevant with the removal of the time bound on delete runs.", "author": "original-brownbear", "createdAt": "2020-09-14T05:34:10Z", "path": "x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/slm/SnapshotRetentionTaskTests.java", "diffHunk": "@@ -239,154 +223,6 @@ private void retentionTaskTest(final boolean deletionSuccess) throws Exception {\n         }\n     }\n \n-    public void testSuccessfulTimeBoundedDeletion() throws Exception {\n-        timeBoundedDeletion(true);\n-    }\n-\n-    public void testFailureTimeBoundedDeletion() throws Exception {\n-        timeBoundedDeletion(false);\n-    }\n-\n-    private void timeBoundedDeletion(final boolean deletionSuccess) throws Exception {", "originalCommit": "fd36d0507c3533ce4e8faf809bedbceaf071aed6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MTQxMg==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487661412", "bodyText": "This flag seems unnecessary with concurrent snapshots. If (unlikely case now anyway) two delete runs collide and re-submit the same snapshot(s) for deletion the delete functionality will de-duplicate those deletes => we don't save anything by having this guard IMO and it's just more complexity (especially with the necessary changes in here making all the APIs async)", "author": "original-brownbear", "createdAt": "2020-09-14T05:36:24Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -65,21 +54,18 @@\n public class SnapshotRetentionTask implements SchedulerEngine.Listener {\n \n     private static final Logger logger = LogManager.getLogger(SnapshotRetentionTask.class);\n-    private static final AtomicBoolean running = new AtomicBoolean(false);", "originalCommit": "fd36d0507c3533ce4e8faf809bedbceaf071aed6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNzY5OA==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496227698", "bodyText": "One concern I have here is stats counting, if SLM retention is super aggressive and deletes the same snapshot 15 times (let's assume it's spending forever deleting the snapshot) then it tells the client that it's succeeded 15 times right? that means we'd increment the deleted stats incorrectly for the actual number of snapshots that have been deleted", "author": "dakrone", "createdAt": "2020-09-28T20:57:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MTQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY1NDkwNQ==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496654905", "bodyText": "Thanks for spotting, yes this would've caused broken counting I think.\nI fixed this in 83dbef8 by keeping track of the currently deleting SnapshotIds.\nThat also helps prevent a bunch of listeners pilling up needlessly in this scenario.\nAdmittedly there's still some non-zero chance of running into a race here if a delete just finished right after the loop over the snapshots to delete started but IMO it's ok to ignore that for now because it's unlikely, will only add to the failure counter and fixing it would way complicating the code. Also, it's maybe a nice pointer for people to reduce their retention frequency if they're seeing these kinds of issues?\n(that said, if you think the race here might be an issue I'm happy to see what I can do about a fix, the main thing preventing a short and sweet fix here is the lack of access to the cluster service in this class which would allow fixing this by nicely ordering things on the CS thread like we do for other snapshot things)", "author": "original-brownbear", "createdAt": "2020-09-29T11:52:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MTQxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MTg3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487661871", "bodyText": "All the other changes that make methods async by adding ActionListeners come from removing the blocking here. I tried to not make any logical changes outside of moving to async and to keep those changes purely mechanical.", "author": "original-brownbear", "createdAt": "2020-09-14T05:38:07Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);", "originalCommit": "fd36d0507c3533ce4e8faf809bedbceaf071aed6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY2MjA4NA==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r487662084", "bodyText": "This is irrelevant now, the loop over the deletes doesn't block so we will never break the time limit practically.", "author": "original-brownbear", "createdAt": "2020-09-14T05:38:54Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {", "originalCommit": "fd36d0507c3533ce4e8faf809bedbceaf071aed6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "30b61b5a3ffec9673e19e347ad7899c0864f5d80", "url": "https://github.com/elastic/elasticsearch/commit/30b61b5a3ffec9673e19e347ad7899c0864f5d80", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel", "committedDate": "2020-09-14T08:04:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyMzI4OA==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496223288", "bodyText": "This now needs to call the listener onResponse handler", "author": "dakrone", "createdAt": "2020-09-28T20:48:35Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;", "originalCommit": "30b61b5a3ffec9673e19e347ad7899c0864f5d80", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY1MDQwNQ==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496650405", "bodyText": "Right fixed :)", "author": "original-brownbear", "createdAt": "2020-09-29T11:43:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyMzI4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNDAyMg==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496224022", "bodyText": "This comment is out of date now", "author": "dakrone", "createdAt": "2020-09-28T20:50:06Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +264,72 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {\n-                    logger.info(\"maximum snapshot retention deletion time reached, time spent: [{}],\" +\n-                            \" maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]\",\n-                        totalDeletionTime, maximumTime, deleted, count, failed);\n-                    slmStats.deletionTime(totalDeletionTime);\n-                    slmStats.retentionTimedOut();\n-                    return;\n-                }\n-            }\n+            deleteSnapshots(slmStats, deleted, failed, repo, snapshots, allDeletesListener);\n+        }\n+    }\n+\n+    private void deleteSnapshots(SnapshotLifecycleStats slmStats, AtomicInteger deleted, AtomicInteger failed, String repo,\n+                                 List<SnapshotInfo> snapshots, ActionListener<Void> listener) {\n+\n+        final ActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.map(listener, v -> null), snapshots.size());\n+        for (SnapshotInfo info : snapshots) {\n+            final String policyId = getPolicyId(info);\n+            final long deleteStartTime = nowNanoSupplier.getAsLong();\n+            // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n+            //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n+            deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.runAfter(\n+                    ActionListener.wrap(acknowledgedResponse -> {\n+                        deleted.incrementAndGet();\n+                        assert acknowledgedResponse.isAcknowledged();\n+                        historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n+                                info.snapshotId().getName(), policyId, repo));\n+                        allDeletesListener.onResponse(null);\n+                    }, e -> {\n+                        failed.incrementAndGet();\n+                        try {\n+                            final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n+                                    info.snapshotId().getName(), policyId, repo, e);\n+                            historyStore.putAsync(result);\n+                        } catch (IOException ex) {\n+                            // This shouldn't happen unless there's an issue with serializing the original exception\n+                            logger.error(new ParameterizedMessage(\n+                                    \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n+                                    policyId), ex);\n+                        } finally {\n+                            allDeletesListener.onFailure(e);\n+                        }\n+                    }), () -> {\n+                        // Check whether we have exceeded the maximum time allowed to spend deleting\n+                        // snapshots, if we have, short-circuit the rest of the deletions", "originalCommit": "30b61b5a3ffec9673e19e347ad7899c0864f5d80", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY1MDUyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r496650521", "bodyText": "Removed", "author": "original-brownbear", "createdAt": "2020-09-29T11:44:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIyNDAyMg=="}], "type": "inlineReview"}, {"oid": "c1c7108c34697f7723366cac7469ab0f471aaef5", "url": "https://github.com/elastic/elasticsearch/commit/c1c7108c34697f7723366cac7469ab0f471aaef5", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel", "committedDate": "2020-09-29T11:07:07Z", "type": "commit"}, {"oid": "83dbef89b26f4242cee800a4fc1e13a9195e40da", "url": "https://github.com/elastic/elasticsearch/commit/83dbef89b26f4242cee800a4fc1e13a9195e40da", "message": "CR: comments", "committedDate": "2020-09-29T11:43:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5NDc3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r497094772", "bodyText": "Can you move this up to the top before all the methods? It's a little strange to have a private class var randomly in the middle of the class", "author": "dakrone", "createdAt": "2020-09-29T22:22:06Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +265,81 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n+            listener.onResponse(null);\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {\n-                    logger.info(\"maximum snapshot retention deletion time reached, time spent: [{}],\" +\n-                            \" maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]\",\n-                        totalDeletionTime, maximumTime, deleted, count, failed);\n-                    slmStats.deletionTime(totalDeletionTime);\n-                    slmStats.retentionTimedOut();\n-                    return;\n-                }\n+            deleteSnapshots(slmStats, deleted, failed, repo, snapshots, allDeletesListener);\n+        }\n+    }\n+\n+    /**\n+     * Set of all currently deleting {@link SnapshotId} used to prevent starting multiple deletes for the same snapshot.\n+     */\n+    private final Set<SnapshotId> runningDeletions = Collections.synchronizedSet(new HashSet<>());", "originalCommit": "83dbef89b26f4242cee800a4fc1e13a9195e40da", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY2NTE5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r497665197", "bodyText": "Sure, force of habit. The snapshot classes in core eventually got so complex that this style started being more readable :D but here it's unnecessary :)", "author": "original-brownbear", "createdAt": "2020-09-30T17:01:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5NDc3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5NzIxMw==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r497097213", "bodyText": "I think we leak memory (it would be very rare) if the next two lines (after the if) were to throw an exception.\nFor example, if getPolicyId(info) were to throw an exception (possible since it has an orElseThrow(...) clause) we'd never remove the id from the deletions, and we'd never be able to remove the snapshot because we'd always think we were currently removing it.\nNot sure what the best way to handle it is, but maybe we can just wrap the following lines to remove it otherwise? I still feel a little uneasy about deleteSnapshot potentially not calling the listener (for example, if its implementation were changed and it threw an exception rather than calling listener.onFailure(...))", "author": "dakrone", "createdAt": "2020-09-29T22:28:17Z", "path": "x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotRetentionTask.java", "diffHunk": "@@ -290,117 +265,81 @@ static String getPolicyId(SnapshotInfo snapshotInfo) {\n                 \" to have a policy in its metadata, but it did not\"));\n     }\n \n-    /**\n-     * Maybe delete the given snapshots. If a snapshot is currently running according to the cluster\n-     * state, this waits (using a {@link ClusterStateObserver} until a cluster state with no running\n-     * snapshots before executing the blocking\n-     * {@link #deleteSnapshots(Map, TimeValue, SnapshotLifecycleStats)} request. At most, we wait\n-     * for the maximum allowed deletion time before timing out waiting for a state with no\n-     * running snapshots.\n-     *\n-     * It's possible the task may still run into a SnapshotInProgressException, if a snapshot is\n-     * started between the state retrieved here and the actual deletion. Since is is expected to be\n-     * a rare case, no special handling is present.\n-     */\n-    private void maybeDeleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                                      TimeValue maximumTime,\n-                                      SnapshotLifecycleStats slmStats) {\n+    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n+                         SnapshotLifecycleStats slmStats,\n+                         ActionListener<Void> listener) {\n         int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n         if (count == 0) {\n+            listener.onResponse(null);\n             logger.debug(\"no snapshots are eligible for deletion\");\n             return;\n         }\n \n-        ClusterState state = clusterService.state();\n-        if (okayToDeleteSnapshots(state)) {\n-            logger.trace(\"there are no snapshots currently running, proceeding with snapshot deletion of [{}]\",\n-                formatSnapshots(snapshotsToDelete));\n-            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-        } else {\n-            logger.debug(\"a snapshot is currently running, rescheduling SLM retention for after snapshot has completed\");\n-            ClusterStateObserver observer = new ClusterStateObserver(clusterService, maximumTime, logger, threadPool.getThreadContext());\n-            CountDownLatch latch = new CountDownLatch(1);\n-            observer.waitForNextChange(\n-                new NoSnapshotRunningListener(observer,\n-                    newState -> threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> {\n-                        try {\n-                            logger.trace(\"received cluster state without running snapshots, proceeding with snapshot deletion of [{}]\",\n-                                formatSnapshots(snapshotsToDelete));\n-                            deleteSnapshots(snapshotsToDelete, maximumTime, slmStats);\n-                        } finally {\n-                            latch.countDown();\n-                        }\n-                    }),\n-                    e -> {\n-                        latch.countDown();\n-                        throw new ElasticsearchException(e);\n-                    }));\n-            try {\n-                logger.trace(\"waiting for snapshot deletion to complete\");\n-                // Wait until we find a cluster state not running a snapshot operation.\n-                // If we can't find one within a day, give up and throw an error.\n-                latch.await(1, TimeUnit.DAYS);\n-                logger.trace(\"deletion complete\");\n-            } catch (InterruptedException e) {\n-                throw new ElasticsearchException(e);\n-            }\n-        }\n-    }\n-\n-    void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete,\n-                         TimeValue maximumTime,\n-                         SnapshotLifecycleStats slmStats) {\n-        int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum();\n-\n         logger.info(\"starting snapshot retention deletion for [{}] snapshots\", count);\n         long startTime = nowNanoSupplier.getAsLong();\n-        final AtomicInteger deleted =  new AtomicInteger(0);\n+        final AtomicInteger deleted = new AtomicInteger(0);\n         final AtomicInteger failed = new AtomicInteger(0);\n+        final GroupedActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.runAfter(ActionListener.map(listener, v -> null),\n+                        () -> {\n+                            TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime);\n+                            logger.debug(\"total elapsed time for deletion of [{}] snapshots: {}\", deleted, totalElapsedTime);\n+                            slmStats.deletionTime(totalElapsedTime);\n+                        }), snapshotsToDelete.size());\n         for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) {\n             String repo = entry.getKey();\n             List<SnapshotInfo> snapshots = entry.getValue();\n-            for (SnapshotInfo info : snapshots) {\n-                final String policyId = getPolicyId(info);\n-                final long deleteStartTime = nowNanoSupplier.getAsLong();\n-                // TODO: Use snapshot multi-delete instead of this loop if all nodes in the cluster support it\n-                //       i.e are newer or equal to SnapshotsService#MULTI_DELETE_VERSION\n-                deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> {\n-                    deleted.incrementAndGet();\n-                    assert acknowledgedResponse.isAcknowledged();\n-                    historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo));\n-                }, e -> {\n-                    failed.incrementAndGet();\n-                    try {\n-                        final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(),\n-                            info.snapshotId().getName(), policyId, repo, e);\n-                        historyStore.putAsync(result);\n-                    } catch (IOException ex) {\n-                        // This shouldn't happen unless there's an issue with serializing the original exception\n-                        logger.error(new ParameterizedMessage(\n-                            \"failed to record snapshot deletion failure for snapshot lifecycle policy [{}]\",\n-                            policyId), ex);\n-                    }\n-                }));\n-                // Check whether we have exceeded the maximum time allowed to spend deleting\n-                // snapshots, if we have, short-circuit the rest of the deletions\n-                long finishTime = nowNanoSupplier.getAsLong();\n-                TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime);\n-                logger.debug(\"elapsed time for deletion of [{}] snapshot: {}\", info.snapshotId(), deletionTime);\n-                TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime);\n-                if (totalDeletionTime.compareTo(maximumTime) > 0) {\n-                    logger.info(\"maximum snapshot retention deletion time reached, time spent: [{}],\" +\n-                            \" maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]\",\n-                        totalDeletionTime, maximumTime, deleted, count, failed);\n-                    slmStats.deletionTime(totalDeletionTime);\n-                    slmStats.retentionTimedOut();\n-                    return;\n-                }\n+            deleteSnapshots(slmStats, deleted, failed, repo, snapshots, allDeletesListener);\n+        }\n+    }\n+\n+    /**\n+     * Set of all currently deleting {@link SnapshotId} used to prevent starting multiple deletes for the same snapshot.\n+     */\n+    private final Set<SnapshotId> runningDeletions = Collections.synchronizedSet(new HashSet<>());\n+\n+    private void deleteSnapshots(SnapshotLifecycleStats slmStats, AtomicInteger deleted, AtomicInteger failed, String repo,\n+                                 List<SnapshotInfo> snapshots, ActionListener<Void> listener) {\n+\n+        final ActionListener<Void> allDeletesListener =\n+                new GroupedActionListener<>(ActionListener.map(listener, v -> null), snapshots.size());\n+        for (SnapshotInfo info : snapshots) {\n+            if (runningDeletions.add(info.snapshotId()) == false) {", "originalCommit": "83dbef89b26f4242cee800a4fc1e13a9195e40da", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczODQwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/62284#discussion_r497738401", "bodyText": "++ to all of the above. Sorry again for the noise here :( => I think I covered all cases with 41797fa now, let me know what you think :)", "author": "original-brownbear", "createdAt": "2020-09-30T19:08:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5NzIxMw=="}], "type": "inlineReview"}, {"oid": "712a974bb27ab8b57ee1d4a013892861348a6208", "url": "https://github.com/elastic/elasticsearch/commit/712a974bb27ab8b57ee1d4a013892861348a6208", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel", "committedDate": "2020-09-30T17:00:18Z", "type": "commit"}, {"oid": "41797fa4102f7c370c204dc30d9d810281e7236c", "url": "https://github.com/elastic/elasticsearch/commit/41797fa4102f7c370c204dc30d9d810281e7236c", "message": "CR: don't leak listener", "committedDate": "2020-09-30T17:19:05Z", "type": "commit"}, {"oid": "6b7e7879156fe9664018ab527631b2da448c91cd", "url": "https://github.com/elastic/elasticsearch/commit/6b7e7879156fe9664018ab527631b2da448c91cd", "message": "Merge remote-tracking branch 'elastic/master' into slm-start-deleting-in-parallel", "committedDate": "2020-10-14T05:14:27Z", "type": "commit"}]}