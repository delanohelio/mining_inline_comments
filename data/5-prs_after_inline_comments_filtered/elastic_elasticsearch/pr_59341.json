{"pr_number": 59341, "pr_title": "Clean up a few of vwh's rough edges", "pr_createdAt": "2020-07-09T20:55:09Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/59341", "timeline": [{"oid": "abd15a7c0974395c211b49a3baecd4b6e67f145c", "url": "https://github.com/elastic/elasticsearch/commit/abd15a7c0974395c211b49a3baecd4b6e67f145c", "message": "Clean up a few of vwh's rough edges\n\nThis cleans up a few rough edged in the `variable_width_histogram`,\nmostly found by @wwang500:\n1. Setting its tuning parameters in an unexpected order could cause the\n   request to fail.\n2. We checked that the maximum number of buckets was both less than\n   50000 and MAX_BUCKETS. This drops the 50000.\n3. Fixes a divide by 0 that can occur of the `shard_size` is 1.\n4. Fixes a divide by 0 that can occur if the `shard_size * 3` overflows\n   a signed int.\n5. Requires `shard_size * 3 / 4` to be at least `buckets`. If it is less\n   than `buckets` we will very consistently return fewer buckets than\n   requested. For the most part we expect folks to leave it at the\n   default. If they change it, we expect it to be much bigger than\n   `buckets`.\n6. Allocate a smaller `mergeMap` in when initially bucketing requests\n   that don't use the entire `shard_size * 3 / 4`. Its just a waste.\n7. Default `shard_size` to `10 * buckets` rather than `100`. It *looks*\n   like that was our intention the whole time. And it feels like it'd\n   keep the algorithm humming along more smoothly.\n8. Default the `initial_buffer` to `min(10 * shard_size, 50000)` like\n   we've documented it rather than `5000`. Like the point above, this\n   feels like the right thing to do to keep the algorithm happy.", "committedDate": "2020-07-09T20:48:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYwNDA1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/59341#discussion_r452604052", "bodyText": "\ud83d\udc4d good idea!", "author": "jamesdorfman", "createdAt": "2020-07-10T03:33:10Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/VariableWidthHistogramAggregator.java", "diffHunk": "@@ -265,7 +264,7 @@ private void bucketBufferedDocs(final DoubleArray buffer, final int bufferSize,\n                 }\n             }\n \n-            mergeBuckets(mergeMap, numBuckets);\n+            mergeBuckets(mergeMap, bucketOrd + 1);", "originalCommit": "abd15a7c0974395c211b49a3baecd4b6e67f145c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "66ebadc8fbda911cf228f132a463c6fe03693fbe", "url": "https://github.com/elastic/elasticsearch/commit/66ebadc8fbda911cf228f132a463c6fe03693fbe", "message": "Merge branch 'master' into variable_width_histo_cleanup_request", "committedDate": "2020-07-13T17:42:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA2NDQwNg==", "url": "https://github.com/elastic/elasticsearch/pull/59341#discussion_r456064406", "bodyText": "This is pretty late in the process... is there a reason we can't check these when parsing the request?  I don't see anything that needs the shard context but I might be missing something?", "author": "polyfractal", "createdAt": "2020-07-16T20:40:06Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/VariableWidthHistogramAggregationBuilder.java", "diffHunk": "@@ -149,12 +158,32 @@ protected ValuesSourceAggregatorFactory innerBuild(QueryShardContext queryShardC\n                                                        ValuesSourceConfig config,\n                                                        AggregatorFactory parent,\n                                                        AggregatorFactories.Builder subFactoriesBuilder) throws IOException {\n-\n         Settings settings = queryShardContext.getIndexSettings().getNodeSettings();\n         int maxBuckets = MultiBucketConsumerService.MAX_BUCKET_SETTING.get(settings);\n         if (numBuckets > maxBuckets) {\n-            throw new IllegalArgumentException(NUM_BUCKETS_FIELD.getPreferredName()+\n-                \" must be less than \" + maxBuckets);\n+            throw new IllegalArgumentException(NUM_BUCKETS_FIELD.getPreferredName() + \" must be less than \" + maxBuckets);\n+        }\n+        int initialBuffer = getInitialBuffer();\n+        int shardSize = getShardSize();\n+        if (initialBuffer < numBuckets) {\n+            // If numBuckets buckets are being returned, then at least that many must be stored in memory\n+            throw new IllegalArgumentException(", "originalCommit": "66ebadc8fbda911cf228f132a463c6fe03693fbe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU0Mzg0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/59341#discussion_r456543842", "bodyText": "Yeah, it is. We don't really have a validate stage for these and I didn't really want to do it in the parsing because I figured it was possible for it to \"escape\" validation sometimes.", "author": "nik9000", "createdAt": "2020-07-17T16:21:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA2NDQwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA2NTQ1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/59341#discussion_r456065452", "bodyText": "Side note for us to think about in the future: I wonder if we should escalate this sort of check to BucketsAggregator or something?  E.g. it doesn't make sense for any of the aggs to request > max_buckets and we can determine that up front at parsing.  Potentially save some heartache.\nLess of an issue since we've increased the threshold but still.", "author": "polyfractal", "createdAt": "2020-07-16T20:41:55Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/VariableWidthHistogramAggregationBuilder.java", "diffHunk": "@@ -149,12 +158,32 @@ protected ValuesSourceAggregatorFactory innerBuild(QueryShardContext queryShardC\n                                                        ValuesSourceConfig config,\n                                                        AggregatorFactory parent,\n                                                        AggregatorFactories.Builder subFactoriesBuilder) throws IOException {\n-\n         Settings settings = queryShardContext.getIndexSettings().getNodeSettings();\n         int maxBuckets = MultiBucketConsumerService.MAX_BUCKET_SETTING.get(settings);\n         if (numBuckets > maxBuckets) {", "originalCommit": "66ebadc8fbda911cf228f132a463c6fe03693fbe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "39c4a380ee6de00bac3a12481d38ced3cad0e712", "url": "https://github.com/elastic/elasticsearch/commit/39c4a380ee6de00bac3a12481d38ced3cad0e712", "message": "Merge branch 'master' into variable_width_histo_cleanup_request", "committedDate": "2020-07-17T16:33:02Z", "type": "commit"}]}