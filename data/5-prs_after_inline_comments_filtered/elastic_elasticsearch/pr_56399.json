{"pr_number": 56399, "pr_title": "Add Normalize Pipeline Aggregation", "pr_createdAt": "2020-05-08T03:06:19Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/56399", "timeline": [{"oid": "7e8510de333fd45f4dd4c82be12a275a6b7f6274", "url": "https://github.com/elastic/elasticsearch/commit/7e8510de333fd45f4dd4c82be12a275a6b7f6274", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005.", "committedDate": "2020-05-08T03:07:13Z", "type": "forcePushed"}, {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88", "url": "https://github.com/elastic/elasticsearch/commit/5427899ada2304388686d28ce8f3c1cd435dee88", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005.", "committedDate": "2020-05-08T03:15:16Z", "type": "commit"}, {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88", "url": "https://github.com/elastic/elasticsearch/commit/5427899ada2304388686d28ce8f3c1cd435dee88", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005.", "committedDate": "2020-05-08T03:15:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODIxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422108219", "bodyText": "Check out InstantiatingObjectParser!", "author": "nik9000", "createdAt": "2020-05-08T12:12:09Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(", "originalCommit": "5427899ada2304388686d28ce8f3c1cd435dee88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4ODcxMw==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423388713", "bodyText": "so, I tried changing that parser to work here, but I think it deserves its own change. The InstantiatingObjectParser does not expose the Context in such a way that more constructor arguments can be passed in. I believe this can change, but I'd rather not do that here", "author": "talevy", "createdAt": "2020-05-12T00:07:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODIxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzcwNTI2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423705263", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-05-12T12:48:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODIxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODc0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422108749", "bodyText": "The normalizer should probably be in hashCode and equals, right?", "author": "nik9000", "createdAt": "2020-05-08T12:13:34Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<List<Double>, NormalizePipelineNormalizer>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    NormalizePipelineAggregationBuilder(String name, String format, String normalizer, String bucketsPath) {\n+        super(name, NAME, new String[] { bucketsPath });\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+        context.validateParentAggSequentiallyOrdered(NAME, name);\n+    }\n+\n+    @Override\n+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {\n+        if (format != null) {\n+            builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);\n+        }\n+        builder.field(NORMALIZER_FIELD.getPreferredName(), normalizer);\n+        return builder;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(super.hashCode(), format);\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj) {\n+        if (this == obj) return true;\n+        if (obj == null || getClass() != obj.getClass()) return false;\n+        if (super.equals(obj) == false) return false;\n+        NormalizePipelineAggregationBuilder other = (NormalizePipelineAggregationBuilder) obj;\n+        return Objects.equals(format, other.format);", "originalCommit": "5427899ada2304388686d28ce8f3c1cd435dee88", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwOTU3NA==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422109574", "bodyText": "bucket.getAggregations().copyResults() does this without so much boiler plate.", "author": "nik9000", "createdAt": "2020-05-08T12:15:34Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;\n+        List<Bucket> newBuckets = new ArrayList<>(buckets.size());\n+\n+        List<Double> values = buckets.stream().map(bucket -> resolveBucketValue(histo, bucket, bucketsPaths()[0], GapPolicy.SKIP))\n+            .collect(Collectors.toList());\n+\n+        NormalizePipelineNormalizer normalizer = normalizerSupplier.apply(values);\n+\n+        for (int i = 0; i < buckets.size(); i++) {\n+            InternalMultiBucketAggregation.InternalBucket bucket = buckets.get(i);\n+            Double thisBucketValue = values.get(i);\n+\n+            final double normalizedBucketValue;\n+\n+            // Only account for finite values\n+            if (thisBucketValue.isNaN()) {\n+                normalizedBucketValue = Double.NaN;\n+            } else {\n+                normalizedBucketValue = normalizer.normalize(thisBucketValue);\n+            }\n+\n+            List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false)", "originalCommit": "5427899ada2304388686d28ce8f3c1cd435dee88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4ODkzOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423388939", "bodyText": "unfortunately, that method does not work in this context. I think a more dedicated cleanup for this boilerplate can be tackled outside of this PR", "author": "talevy", "createdAt": "2020-05-12T00:08:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwOTU3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzcwNTQxMg==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423705412", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-05-12T12:49:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwOTU3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110209", "bodyText": "Could this take a DoubleStream instead?", "author": "nik9000", "createdAt": "2020-05-08T12:17:18Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);\n+\n+    abstract static class SinglePassSimpleStatisticsNormalizer extends NormalizePipelineNormalizer {\n+        protected final double max;\n+        protected final double min;\n+        protected final double sum;\n+        protected final double mean;\n+        protected final int count;\n+\n+        SinglePassSimpleStatisticsNormalizer(List<Double> values) {", "originalCommit": "5427899ada2304388686d28ce8f3c1cd435dee88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDQ5MA==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110490", "bodyText": "You'd only get a single pass but you wouldn't need to make Double objects which is kind of nice.", "author": "nik9000", "createdAt": "2020-05-08T12:18:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDYxNA==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110614", "bodyText": "Not that pipeline aggs are all that efficient here, but I feel compelled to save autoboxing when I can.", "author": "nik9000", "createdAt": "2020-05-08T12:18:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMTE4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422111183", "bodyText": "I wonder if we could have these normalizers all implement DoubleUnaryOperator instead of making an abstract class for them.", "author": "nik9000", "createdAt": "2020-05-08T12:19:50Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);", "originalCommit": "5427899ada2304388686d28ce8f3c1cd435dee88", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "15e5ca8ca63ab302f75f2945849a64d406e7b444", "url": "https://github.com/elastic/elasticsearch/commit/15e5ca8ca63ab302f75f2945849a64d406e7b444", "message": "Merge remote-tracking branch 'elastic/master' into normalize", "committedDate": "2020-05-08T18:28:57Z", "type": "commit"}, {"oid": "9782c2a7c583d075733799eedcae5e5c1faac9f4", "url": "https://github.com/elastic/elasticsearch/commit/9782c2a7c583d075733799eedcae5e5c1faac9f4", "message": "moar", "committedDate": "2020-05-09T02:39:15Z", "type": "commit"}, {"oid": "2dae977ba99d64e0617c43394fbc82005c0cd686", "url": "https://github.com/elastic/elasticsearch/commit/2dae977ba99d64e0617c43394fbc82005c0cd686", "message": "respond to rev", "committedDate": "2020-05-12T00:05:23Z", "type": "commit"}, {"oid": "8cd088be0113fd0222feb925ade74a0b989c0fb2", "url": "https://github.com/elastic/elasticsearch/commit/8cd088be0113fd0222feb925ade74a0b989c0fb2", "message": "Merge remote-tracking branch 'elastic/master' into normalize", "committedDate": "2020-05-12T01:54:38Z", "type": "commit"}, {"oid": "9718e01a400b15db41b59bb17d9ee924d5ee0ec1", "url": "https://github.com/elastic/elasticsearch/commit/9718e01a400b15db41b59bb17d9ee924d5ee0ec1", "message": "revert change", "committedDate": "2020-05-12T05:29:20Z", "type": "commit"}, {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167", "url": "https://github.com/elastic/elasticsearch/commit/9c64e3613621bc3ca9e5187ce605e39ff4009167", "message": "Merge remote-tracking branch 'elastic/master' into normalize", "committedDate": "2020-05-12T15:40:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxOTMzMA==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423919330", "bodyText": "Fine with normalizer, but wanted to also suggest method as a potential param name.  No strong opinion though :)", "author": "polyfractal", "createdAt": "2020-05-12T17:45:40Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");", "originalCommit": "9c64e3613621bc3ca9e5187ce605e39ff4009167", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3ODA0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423978049", "bodyText": "I was wishy washy on the naming here as well, and decided not to fret, but I too have leaned towards method earlier, so I am happy to do so here. especially given the overloading of the term across the stack.", "author": "talevy", "createdAt": "2020-05-12T19:24:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxOTMzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NjgzNg==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424776836", "bodyText": "I've updated the naming to be method", "author": "talevy", "createdAt": "2020-05-13T22:55:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxOTMzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyMjA2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423922066", "bodyText": "Should we also check context.validateHasParent() to make sure this isn't at the top level?", "author": "polyfractal", "createdAt": "2020-05-12T17:50:12Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<double[], DoubleUnaryOperator>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    public NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+    }", "originalCommit": "9c64e3613621bc3ca9e5187ce605e39ff4009167", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3ODIyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423978229", "bodyText": "ah, yes. I wasn't aware of this. thanks for bringing it up", "author": "talevy", "createdAt": "2020-05-12T19:25:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyMjA2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3Njc0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424776741", "bodyText": "added a check and a test for this!", "author": "talevy", "createdAt": "2020-05-13T22:55:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyMjA2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423925852", "bodyText": "Do we know if this works with a terms agg as the parent?  It feels like it should (e.g. it doesn't require any specific ordering of the buckets, unlike something like a moving avg which needs an ordering).\nIf we think it should work with terms we should tweak this to not use a HistogramFactory directly.  BucketScriptPipelineAggregator has an example of how to generically build buckets from any InternalMultiBucketAggregation (the internal agg can create buckets too, not just the factory).", "author": "polyfractal", "createdAt": "2020-05-12T17:55:48Z", "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<double[], DoubleUnaryOperator> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<double[], DoubleUnaryOperator> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;", "originalCommit": "9c64e3613621bc3ca9e5187ce605e39ff4009167", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3ODg2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423978861", "bodyText": "thanks! I was slightly loose in my interpretation of the HistogramFactory's comment\n\n/** Implemented by histogram aggregations and used by pipeline aggregations to insert buckets. */\n\nWill look at how BucketScript does things and add a test for terms agg!", "author": "talevy", "createdAt": "2020-05-12T19:26:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NTUwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424775509", "bodyText": "Yikes! I'm sorry I didn't notice this one!", "author": "nik9000", "createdAt": "2020-05-13T22:51:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NjY3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r424776676", "bodyText": "thanks, I've updated to include a test for terms and use a more generic way to make new buckets", "author": "talevy", "createdAt": "2020-05-13T22:54:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg=="}], "type": "inlineReview"}, {"oid": "023c34fab289abb5bce094db492a726308ca8a3b", "url": "https://github.com/elastic/elasticsearch/commit/023c34fab289abb5bce094db492a726308ca8a3b", "message": "respond to changes", "committedDate": "2020-05-13T22:43:38Z", "type": "commit"}, {"oid": "4e76dbd9fb6e094015d7599b39ad1e0c972b7a67", "url": "https://github.com/elastic/elasticsearch/commit/4e76dbd9fb6e094015d7599b39ad1e0c972b7a67", "message": "Merge remote-tracking branch 'elastic/master' into normalize", "committedDate": "2020-05-13T22:43:56Z", "type": "commit"}, {"oid": "8da4960ed6e3adb4279f391168fe9f25a219067f", "url": "https://github.com/elastic/elasticsearch/commit/8da4960ed6e3adb4279f391168fe9f25a219067f", "message": "update docs", "committedDate": "2020-05-13T23:59:39Z", "type": "commit"}, {"oid": "5d4737c044ab9ac6802ea2ecab6990fca072b050", "url": "https://github.com/elastic/elasticsearch/commit/5d4737c044ab9ac6802ea2ecab6990fca072b050", "message": "format", "committedDate": "2020-05-14T00:09:06Z", "type": "commit"}, {"oid": "aa9eebca37427677f615c36e58a456d0c60d46dc", "url": "https://github.com/elastic/elasticsearch/commit/aa9eebca37427677f615c36e58a456d0c60d46dc", "message": "touch up", "committedDate": "2020-05-14T16:17:26Z", "type": "commit"}, {"oid": "72f29e71fec4421793887ad9e7a967fac53ce301", "url": "https://github.com/elastic/elasticsearch/commit/72f29e71fec4421793887ad9e7a967fac53ce301", "message": "use format in example", "committedDate": "2020-05-14T17:08:21Z", "type": "commit"}, {"oid": "1e9f01532080b402d65ace333dbbfe23d8a7ec7c", "url": "https://github.com/elastic/elasticsearch/commit/1e9f01532080b402d65ace333dbbfe23d8a7ec7c", "message": "comma", "committedDate": "2020-05-14T18:55:58Z", "type": "commit"}, {"oid": "e6db0f9e9436f7560ad8a881b8461480ee3d0dd7", "url": "https://github.com/elastic/elasticsearch/commit/e6db0f9e9436f7560ad8a881b8461480ee3d0dd7", "message": "final fix", "committedDate": "2020-05-14T19:43:15Z", "type": "commit"}]}