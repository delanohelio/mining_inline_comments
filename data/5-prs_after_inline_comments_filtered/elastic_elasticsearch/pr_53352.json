{"pr_number": 53352, "pr_title": "Encrypted blob store reuse DEK", "pr_createdAt": "2020-03-10T17:58:58Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/53352", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU5MzgwOA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395593808", "bodyText": "Not important, but do we need the random base path here?", "author": "original-brownbear", "createdAt": "2020-03-20T12:07:09Z", "path": "plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobStoreRepositoryTests.java", "diffHunk": "@@ -50,11 +50,14 @@ protected String repositoryType() {\n \n     @Override\n     protected Settings repositorySettings() {\n-        return Settings.builder()\n+        Settings.Builder settingsBuilder = Settings.builder()\n             .put(super.repositorySettings())\n             .put(AzureRepository.Repository.CONTAINER_SETTING.getKey(), \"container\")\n-            .put(AzureStorageSettings.ACCOUNT_SETTING.getKey(), \"test\")\n-            .build();\n+            .put(AzureStorageSettings.ACCOUNT_SETTING.getKey(), \"test\");\n+        if (randomBoolean()) {\n+            settingsBuilder.put(AzureRepository.Repository.BASE_PATH_SETTING.getKey(), randomFrom(\"test\", \"test/1\"));", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NDgwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r401184809", "bodyText": "Yes, I believe we need to test the base_path in integ tests. The encrypted repo had to be designed to honor the base path setting of the delegated repository, while itself having a blank base path setting.", "author": "albertzaharovits", "createdAt": "2020-03-31T20:11:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU5MzgwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU5NTU5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395595599", "bodyText": "Why do we want to randomly turn off verification in the general case? It seems that takes away from our chances to catch some IO issues here and there? Maybe we should make this always verify and selective turn off verification where we really need it?", "author": "original-brownbear", "createdAt": "2020-03-20T12:10:54Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -81,13 +83,15 @@ protected Settings repositorySettings() {\n         return Settings.builder().put(\"compress\", randomBoolean()).build();\n     }\n \n-    protected final String createRepository(final String name) {\n-        return createRepository(name, repositorySettings());\n+    protected Settings repositorySettings(String repositoryName) {\n+        return Settings.EMPTY;\n     }\n \n-    protected final String createRepository(final String name, final Settings settings) {\n-        final boolean verify = randomBoolean();\n+    protected final String createRepository(final String name) {\n+        return createRepository(name, Settings.builder().put(repositorySettings()).put(repositorySettings(name)).build(), randomBoolean());", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NTM5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r401185399", "bodyText": "I honored the original behavior, but you're right it should be true unless specified otherwise. I've made this change (verification true instead of randomBoolean).", "author": "albertzaharovits", "createdAt": "2020-03-31T20:12:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU5NTU5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwMTg2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395601869", "bodyText": "NIT: Maybe call this additionalRepositorySettings? Or better yet + in the sense of simplicity, maybe just add the repositoryName parameter to the existing repositorySettings() method? That way we don't have to complicate the repo creation code.", "author": "original-brownbear", "createdAt": "2020-03-20T12:24:55Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -81,13 +83,15 @@ protected Settings repositorySettings() {\n         return Settings.builder().put(\"compress\", randomBoolean()).build();\n     }\n \n-    protected final String createRepository(final String name) {\n-        return createRepository(name, repositorySettings());\n+    protected Settings repositorySettings(String repositoryName) {", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NTc5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r401185799", "bodyText": "add the repositoryName parameter to the existing repositorySettings() method\n\nDone, thanks for the suggestion!", "author": "albertzaharovits", "createdAt": "2020-03-31T20:13:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwMTg2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwNDkxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395604911", "bodyText": "I think you can just assert on e.repository() here and delete the \"missing\" assertion, that part really is implied by the exception type :)\nOr you could just not assert on the message at all IMO.", "author": "original-brownbear", "createdAt": "2020-03-20T12:31:18Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -97,14 +101,27 @@ protected final String createRepository(final String name, final Settings settin\n         internalCluster().getDataOrMasterNodeInstances(RepositoriesService.class).forEach(repositories -> {\n             assertThat(repositories.repository(name), notNullValue());\n             assertThat(repositories.repository(name), instanceOf(BlobStoreRepository.class));\n-            assertThat(repositories.repository(name).isReadOnly(), is(false));\n+            assertThat(repositories.repository(name).isReadOnly(), is(settings.getAsBoolean(\"readonly\", false)));\n             BlobStore blobStore = ((BlobStoreRepository) repositories.repository(name)).getBlobStore();\n             assertThat(\"blob store has to be lazy initialized\", blobStore, verify ? is(notNullValue()) : is(nullValue()));\n         });\n \n         return name;\n     }\n \n+    protected final String deleteRepository(final String name) {\n+        logger.debug(\"-->  deleting repository [name: {}]\", name);\n+        assertAcked(client().admin().cluster().prepareDeleteRepository(name));\n+\n+        internalCluster().getDataOrMasterNodeInstances(RepositoriesService.class).forEach(repositories -> {\n+            RepositoryMissingException e = expectThrows(RepositoryMissingException.class, () -> repositories.repository(name));\n+            assertThat(e.getMessage(), containsString(\"missing\"));\n+            assertThat(e.getMessage(), containsString(name));", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NjA3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r401186076", "bodyText": "Or you could just not assert on the message at all IMO\n\nDone, thanks again!", "author": "albertzaharovits", "createdAt": "2020-03-31T20:14:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwNDkxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwNTMwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395605301", "bodyText": "You never use the return here and it seems pointless since it's just the method input?", "author": "original-brownbear", "createdAt": "2020-03-20T12:32:03Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -97,14 +101,27 @@ protected final String createRepository(final String name, final Settings settin\n         internalCluster().getDataOrMasterNodeInstances(RepositoriesService.class).forEach(repositories -> {\n             assertThat(repositories.repository(name), notNullValue());\n             assertThat(repositories.repository(name), instanceOf(BlobStoreRepository.class));\n-            assertThat(repositories.repository(name).isReadOnly(), is(false));\n+            assertThat(repositories.repository(name).isReadOnly(), is(settings.getAsBoolean(\"readonly\", false)));\n             BlobStore blobStore = ((BlobStoreRepository) repositories.repository(name)).getBlobStore();\n             assertThat(\"blob store has to be lazy initialized\", blobStore, verify ? is(notNullValue()) : is(nullValue()));\n         });\n \n         return name;\n     }\n \n+    protected final String deleteRepository(final String name) {\n+        logger.debug(\"-->  deleting repository [name: {}]\", name);\n+        assertAcked(client().admin().cluster().prepareDeleteRepository(name));\n+\n+        internalCluster().getDataOrMasterNodeInstances(RepositoriesService.class).forEach(repositories -> {\n+            RepositoryMissingException e = expectThrows(RepositoryMissingException.class, () -> repositories.repository(name));\n+            assertThat(e.getMessage(), containsString(\"missing\"));\n+            assertThat(e.getMessage(), containsString(name));\n+        });\n+\n+        return name;", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE4NjYwNA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r401186604", "bodyText": "I've removed the return value; I was trying to be consistent with the return value of the createRepository method.", "author": "albertzaharovits", "createdAt": "2020-03-31T20:14:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwNTMwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwODM5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395608395", "bodyText": "NIT: Maybe it's easier to follow this logic if you just add a method:\nassertBlobSize(BlobMeta meta, long contentLength)\n\nand then use it here and override in the encrypted tests? (I think it saves a little complication and it's what we do for a bunch of other repo related assertions)", "author": "original-brownbear", "createdAt": "2020-03-20T12:38:19Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -175,7 +192,7 @@ public void testList() throws IOException {\n                 BlobMetaData blobMetaData = blobs.get(generated.getKey());\n                 assertThat(generated.getKey(), blobMetaData, CoreMatchers.notNullValue());\n                 assertThat(blobMetaData.name(), CoreMatchers.equalTo(generated.getKey()));\n-                assertThat(blobMetaData.length(), CoreMatchers.equalTo(generated.getValue()));\n+                assertThat(blobMetaData.length(), CoreMatchers.equalTo(blobLengthFromContentLength(generated.getValue())));", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYwODkxMw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395608913", "bodyText": "Revert formatting changes here?", "author": "original-brownbear", "createdAt": "2020-03-20T12:39:24Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -262,15 +279,21 @@ protected static void writeBlob(BlobContainer container, String blobName, BytesA\n     }\n \n     protected BlobStore newBlobStore() {\n-        final String repository = createRepository(randomName());\n+        final String repository = createRepository(randomRepositoryName());\n+        return newBlobStore(repository);\n+    }\n+\n+    protected BlobStore newBlobStore(String repository) {\n         final BlobStoreRepository blobStoreRepository =\n-            (BlobStoreRepository) internalCluster().getMasterNodeInstance(RepositoriesService.class).repository(repository);\n+                (BlobStoreRepository) internalCluster().getMasterNodeInstance(RepositoriesService.class).repository(repository);", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYxMDc4MA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395610780", "bodyText": "NIT: maybe rename to something like assertEmptyRepo or so that has assert in the name?", "author": "original-brownbear", "createdAt": "2020-03-20T12:43:11Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESMockAPIBasedRepositoryIntegTestCase.java", "diffHunk": "@@ -107,14 +107,17 @@ public void tearDownHttpServer() {\n             for(Map.Entry<String, HttpHandler> handler : handlers.entrySet()) {\n                 httpServer.removeContext(handler.getKey());\n                 if (handler.getValue() instanceof BlobStoreHttpHandler) {\n-                    List<String> blobs = ((BlobStoreHttpHandler) handler.getValue()).blobs().keySet().stream()\n-                        .filter(blob -> blob.contains(\"index\") == false).collect(Collectors.toList());\n-                    assertThat(\"Only index blobs should remain in repository but found \" + blobs, blobs, hasSize(0));\n+                    blobsOnTearDown(((BlobStoreHttpHandler) handler.getValue()).blobs());\n                 }\n             }\n         }\n     }\n \n+    protected void blobsOnTearDown(Map<String, BytesReference> blobsMap) {", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYxMzU3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395613571", "bodyText": "NIT: just call randomName here?", "author": "original-brownbear", "createdAt": "2020-03-20T12:48:59Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -493,7 +521,15 @@ private static void assertSuccessfulRestore(RestoreSnapshotResponse response) {\n         assertThat(response.getRestoreInfo().successfulShards(), equalTo(response.getRestoreInfo().totalShards()));\n     }\n \n-    protected static String randomName() {\n+    protected String randomName() {\n         return randomAlphaOfLength(randomIntBetween(1, 10)).toLowerCase(Locale.ROOT);\n     }\n+\n+    protected String randomRepositoryName() {\n+        return randomAlphaOfLength(randomIntBetween(1, 10)).toLowerCase(Locale.ROOT);", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYzMjkyMw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r395632923", "bodyText": "Hmm I'm not a big fan of this kind of unwrapping. We're losing part of the stacktrace here and it may be painful to understand (test-)failures because of it?", "author": "original-brownbear", "createdAt": "2020-03-20T13:24:55Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,562 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.Base64;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public final class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n-    static final String GCM_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n+    static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH_IN_BYTES = 16; // {@code org.elasticsearch.common.UUIDS} length\n+    private static final int DEK_ID_LENGTH_IN_CHARS = 22; // Base64 encoding without padding\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    private static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    private static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<SecretKey, String>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH_IN_BYTES /* UUID byte length */\n+                + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(RepositoryMetaData metadata, NamedXContentRegistry namedXContentRegistry, ClusterService clusterService,\n+                                  BlobStoreRepository delegatedRepository, Supplier<XPackLicenseState> licenseStateSupplier,\n+                                  char[] repositoryPassword) throws GeneralSecurityException {\n+        super(metadata, namedXContentRegistry, clusterService, BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+        base blob path but the base path setting is honored for the delegated repository */);\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(AESKeyUtils.generatePasswordBasedKey(repositoryPassword,\n+                Base64.getUrlDecoder().decode(this.localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))));\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\");\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        return snapshotUserMetadata;\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(SnapshotId snapshotId, ShardGenerations shardGenerations, long startTime, String failure,\n+                                 int totalShards, List<SnapshotShardFailure> shardFailures, long repositoryStateId,\n+                                 boolean includeGlobalState, MetaData clusterMetaData, Map<String, Object> userMetadata,\n+                                 Version repositoryMetaVersion, ActionListener<SnapshotInfo> listener) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+            // remove the repository key id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+        } catch (RepositoryException KEKValidationException) {\n+            listener.onFailure(KEKValidationException);\n+            return;\n+        }\n+        super.finalizeSnapshot(snapshotId, shardGenerations, startTime, failure, totalShards, shardFailures, repositoryStateId,\n+                includeGlobalState, clusterMetaData, userMetadata, repositoryMetaVersion, listener);\n+    }\n+\n+    @Override\n+    public void snapshotShard(Store store, MapperService mapperService, SnapshotId snapshotId, IndexId indexId,\n+                              IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus, Version repositoryMetaVersion,\n+                              Map<String, Object> userMetadata, ActionListener<String> listener) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException KEKValidationException) {\n+            listener.onFailure(KEKValidationException);\n+            return;\n+        }\n+        super.snapshotShard(store, mapperService, snapshotId, indexId, snapshotIndexCommit, snapshotStatus, repositoryMetaVersion,\n+                userMetadata, listener);\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<SecretKey, String>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\");\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(delegatedRepository.blobStore(), delegatedRepository.basePath(), repositoryPassword, DEKGenerator,\n+                DEKCache);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private static Supplier<Tuple<SecretKey, String>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids are generated randomly\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator dataEncryptionKeyGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        dataEncryptionKeyGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> new Tuple<>(dataEncryptionKeyGenerator.generateKey(), UUIDs.randomBase64UUID(DEKIdSecureRandom));\n+    }\n+\n+    static class EncryptedBlobStore implements BlobStore {\n+        private final BlobStore delegatedBlobStore;\n+        private final BlobPath delegatedBasePath;\n+        private final Function<String, Tuple<String, SecretKey>> getKEKforDEK;\n+        private final CheckedSupplier<SingleUseDEK, IOException> singleUseDEKSupplier;\n+        private final CheckedFunction<String, SecretKey, IOException> getDEKById;\n+\n+        EncryptedBlobStore(BlobStore delegatedBlobStore,\n+                           BlobPath delegatedBasePath,\n+                           char[] repositoryPassword,\n+                           Supplier<Tuple<SecretKey, String>> DEKGenerator,\n+                           Cache<String, SecretKey> DEKCache) {\n+            this.delegatedBlobStore = delegatedBlobStore;\n+            this.delegatedBasePath = delegatedBasePath;\n+            this.getKEKforDEK = DEKId -> {\n+                try {\n+                    SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword,\n+                            Base64.getUrlDecoder().decode(DEKId.getBytes(StandardCharsets.UTF_8)));\n+                    String KEKId = AESKeyUtils.computeId(KEK);\n+                    return new Tuple<>(KEKId, KEK);\n+                } catch (GeneralSecurityException e) {\n+                    throw new ElasticsearchException(\"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+                }\n+            };\n+            this.singleUseDEKSupplier = SingleUseDEK.createSingleUseDEKSupplier(() -> {\n+                Tuple<SecretKey, String> newDEK = DEKGenerator.get();\n+                // store newly generated DEK before making it available\n+                storeDEK(newDEK.v2(), newDEK.v1());\n+                return newDEK;\n+            });\n+            this.getDEKById = DEKId -> {\n+                try {\n+                    return DEKCache.computeIfAbsent(DEKId, ignored -> loadDEK(DEKId));\n+                } catch (ExecutionException e) {\n+                    // some exception types are to be expected\n+                    if (e.getCause() instanceof IOException) {", "originalCommit": "097393b25d57aae0527eb87de4e0abefec891218", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTE5MDk5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r401190999", "bodyText": "I used it because the loadDEK method is careful to throw IOExceptions when storage has problems and RepositoryException when there are encryption problems (wrong password or tampered metadata). I've learned that IOExceptions in readBlob can move the repository in a corrupted state, and if we wrap the IOException for DEK load (which is called before reading the actual blob) that'd amount to a change of behavior in those cases.\nMoreover, I don't believe the ExecutionException wrapping adds useful information.", "author": "albertzaharovits", "createdAt": "2020-03-31T20:22:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYzMjkyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA5NzExOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402097119", "bodyText": "I would suggest we just move these utility methods to EncryptedRepository since they are only used there (and we can replicate them as utility methods without changing BlobPath code as far as I can see).\nThese methods are pretty specific to how the encrypted repo works and do things we don't do elsewhere with the paths.", "author": "original-brownbear", "createdAt": "2020-04-02T07:11:51Z", "path": "server/src/main/java/org/elasticsearch/common/blobstore/BlobPath.java", "diffHunk": "@@ -62,6 +63,12 @@ public BlobPath add(String path) {\n         return new BlobPath(Collections.unmodifiableList(paths));\n     }\n \n+    public BlobPath append(BlobPath otherPath) {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjM0NjMzNw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402346337", "bodyText": "Generally I would prefer we keep utilities like these as methods rather than functions in other places. This way we avoid duplicating code.\nBut I understand your dislike of creeping small utility functions, so I've removed all the utilities and instead relied on the iterator.", "author": "albertzaharovits", "createdAt": "2020-04-02T14:14:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA5NzExOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA5ODM4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402098382", "bodyText": "NIT: paths is never null, I think you can just do a normal paths.equals(other.paths)", "author": "original-brownbear", "createdAt": "2020-04-02T07:14:37Z", "path": "server/src/main/java/org/elasticsearch/common/blobstore/BlobPath.java", "diffHunk": "@@ -92,4 +112,17 @@ public String toString() {\n         }\n         return sb.toString();\n     }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        BlobPath other = (BlobPath) o;\n+        return Objects.equals(paths, other.paths);", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjI3OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402102278", "bodyText": "NIT: I think I hate this on second thought (sorry), it's really confusing to extend a concrete test class here.\nI think it would be a lot less confusing if we kept repositoryType abstract and by that kept FsBlobStoreRepositoryBaseIntegTestCase abstract. This way it's much clearer why we need this construct IMO :)", "author": "original-brownbear", "createdAt": "2020-04-02T07:22:44Z", "path": "server/src/test/java/org/elasticsearch/repositories/fs/FsBlobStoreRepositoryIntegTests.java", "diffHunk": "@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.repositories.fs;\n+\n+public final class FsBlobStoreRepositoryIntegTests extends FsBlobStoreRepositoryBaseIntegTestCase {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjQzODc5NA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402438794", "bodyText": "I have made the repositorySettings abstract, rather than the repositoryType.\nIt felt bizarre to have the type abstract since the type is in the class name. It's better to have the settings abstract because settings can differ a lot between classes in the same  hierarchy.", "author": "albertzaharovits", "createdAt": "2020-04-02T16:16:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjI3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjQzOTA5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402439099", "bodyText": "Let me know if this looks OK for you.", "author": "albertzaharovits", "createdAt": "2020-04-02T16:16:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjI3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc2NjE2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402766162", "bodyText": "It felt bizarre to have the type abstract since the type is in the class name.\n\nI would've renamed the class then maybe :) But let's not hold this PR up on that IMO. I think this change also makes things clear enough for now. Thanks!", "author": "original-brownbear", "createdAt": "2020-04-03T06:41:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjI3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjc0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402102741", "bodyText": "No need for these changes anymore right?", "author": "original-brownbear", "createdAt": "2020-04-02T07:23:40Z", "path": "test/framework/src/main/java/org/elasticsearch/common/settings/MockSecureSettings.java", "diffHunk": "@@ -36,10 +37,10 @@\n  */\n public class MockSecureSettings implements SecureSettings {\n \n-    private Map<String, String> secureStrings = new HashMap<>();\n-    private Map<String, byte[]> files = new HashMap<>();\n-    private Map<String, byte[]> sha256Digests = new HashMap<>();\n-    private Set<String> settingNames = new HashSet<>();\n+    private Map<String, String> secureStrings = Collections.synchronizedMap(new HashMap<>());", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU2ODQwMw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402568403", "bodyText": "Actually they are required.\nIn integ tests, I change values of secure settings, which are picked up by the threads of the cluster nodes. I need to assure that a change of secure settings is visible after I restart the cluster node.\nI understand your reticence of changing the infra code, so I've created SynchronizedMockSecureSettings#SynchronizedMockSecureSettings which synchronizes access to every method, and reverted this changes.\nLet me know if you prefer back these changes.", "author": "albertzaharovits", "createdAt": "2020-04-02T19:45:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc3MDYyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402770621", "bodyText": "I still think you don't need synchronisation here at all. In the one test that you're mutating the secure settings in, you are first setting the password, then starting the nodes? Even if on different threads, the order of operations is clear in memory and you shouldn't need synchronisation?\nI'm not so much worried about the change, I'm more worried that tests will start behaving unpredictably if we need this change. If we need it, it would imply that it's not completely clear whether or not we picked up one version or the other of the changed password setting?", "author": "original-brownbear", "createdAt": "2020-04-03T06:53:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA2MzM3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403063375", "bodyText": "We've discussed this on another channel and Armin's right.\nRestarting a node should obviously pick up all changes effectuated on the shared data by the test thread. I was under the impression that the node restart is like a \"soft reset\" where the node thread continues execution and simply reloads the configuration (No reason to assume something like that but I was convinced of it).", "author": "albertzaharovits", "createdAt": "2020-04-03T14:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMjc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwMzAwNA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402103004", "bodyText": "Revert random format change?", "author": "original-brownbear", "createdAt": "2020-04-02T07:24:13Z", "path": "test/framework/src/main/java/org/elasticsearch/repositories/blobstore/ESBlobStoreRepositoryIntegTestCase.java", "diffHunk": "@@ -262,15 +275,21 @@ protected static void writeBlob(BlobContainer container, String blobName, BytesA\n     }\n \n     protected BlobStore newBlobStore() {\n-        final String repository = createRepository(randomName());\n+        final String repository = createRepository(randomRepositoryName());\n+        return newBlobStore(repository);\n+    }\n+\n+    protected BlobStore newBlobStore(String repository) {\n         final BlobStoreRepository blobStoreRepository =\n             (BlobStoreRepository) internalCluster().getMasterNodeInstance(RepositoriesService.class).repository(repository);\n         return PlainActionFuture.get(\n-            f -> blobStoreRepository.threadPool().generic().execute(ActionRunnable.supply(f, blobStoreRepository::blobStore)));\n+                f -> blobStoreRepository.threadPool().generic().execute(ActionRunnable.supply(f, blobStoreRepository::blobStore)));", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwODU3OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402108578", "bodyText": "NIT: weird formatting isn't it? :) Keep the exceptions on a single line?", "author": "original-brownbear", "createdAt": "2020-04-02T07:35:11Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/AESKeyUtils.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.repositories.encrypted;\n+\n+import javax.crypto.Cipher;\n+import javax.crypto.IllegalBlockSizeException;\n+import javax.crypto.NoSuchPaddingException;\n+import javax.crypto.SecretKey;\n+import javax.crypto.SecretKeyFactory;\n+import javax.crypto.spec.PBEKeySpec;\n+import javax.crypto.spec.SecretKeySpec;\n+import java.nio.charset.StandardCharsets;\n+import java.security.InvalidKeyException;\n+import java.security.Key;\n+import java.security.NoSuchAlgorithmException;\n+import java.security.spec.InvalidKeySpecException;\n+import java.util.Base64;\n+\n+public final class AESKeyUtils {\n+    public static final int KEY_LENGTH_IN_BYTES = 32; // 256-bit AES key\n+    public static final int WRAPPED_KEY_LENGTH_IN_BYTES = KEY_LENGTH_IN_BYTES + 8; // https://www.ietf.org/rfc/rfc3394.txt section 2.2\n+    // parameter for the KDF function, it's a funny and unusual iter count larger than 60k\n+    private static final int KDF_ITER = 61616;\n+    // the KDF algorithm that generate the symmetric key given the password\n+    private static final String KDF_ALGO = \"PBKDF2WithHmacSHA512\";\n+    // The Id of any AES SecretKey is the AES-Wrap-ciphertext of this fixed 32 byte wide array.\n+    // Key wrapping encryption is deterministic (same plaintext generates the same ciphertext)\n+    // and the probability that two different keys map the same plaintext to the same ciphertext is very small\n+    // (2^-256, much lower than the UUID collision of 2^-128), assuming AES is indistinguishable from a pseudorandom permutation.\n+    private static final byte[] KEY_ID_PLAINTEXT = \"wrapping known text forms key id\".getBytes(StandardCharsets.UTF_8);\n+\n+    public static byte[] wrap(SecretKey wrappingKey, SecretKey keyToWrap) throws NoSuchPaddingException,\n+        NoSuchAlgorithmException,", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU3NjU5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402576595", "bodyText": "I know, it's weird, I blame spotless (I have to apply the formatting for this new project).\n@pugnascotia can we whine about spotless formatting to you?", "author": "albertzaharovits", "createdAt": "2020-04-02T20:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwODU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc2NjYxMA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402766610", "bodyText": ":D ok Let's ignore this for now then. I'd +1 the whining though :)", "author": "original-brownbear", "createdAt": "2020-04-03T06:42:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwODU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjkxMDEyNA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402910124", "bodyText": "It's a little trickier that you'd think to settle of a good behaviour for exception lists. But, I think I can improve this by changing the config to only wrap exception lists where necessary. It's a deviation from the general philosophy of the config, but I think it'll be an overall improvement, which is what matters.", "author": "pugnascotia", "createdAt": "2020-04-03T10:27:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwODU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjkxNTM3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402915376", "bodyText": "I raised #54710", "author": "pugnascotia", "createdAt": "2020-04-03T10:38:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEwODU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExMDM2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402110361", "bodyText": "NIT/question: do we even need these checks, won't c.init or c.unwrap fail anyway if the algorithm is off?", "author": "original-brownbear", "createdAt": "2020-04-02T07:38:44Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/AESKeyUtils.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.repositories.encrypted;\n+\n+import javax.crypto.Cipher;\n+import javax.crypto.IllegalBlockSizeException;\n+import javax.crypto.NoSuchPaddingException;\n+import javax.crypto.SecretKey;\n+import javax.crypto.SecretKeyFactory;\n+import javax.crypto.spec.PBEKeySpec;\n+import javax.crypto.spec.SecretKeySpec;\n+import java.nio.charset.StandardCharsets;\n+import java.security.InvalidKeyException;\n+import java.security.Key;\n+import java.security.NoSuchAlgorithmException;\n+import java.security.spec.InvalidKeySpecException;\n+import java.util.Base64;\n+\n+public final class AESKeyUtils {\n+    public static final int KEY_LENGTH_IN_BYTES = 32; // 256-bit AES key\n+    public static final int WRAPPED_KEY_LENGTH_IN_BYTES = KEY_LENGTH_IN_BYTES + 8; // https://www.ietf.org/rfc/rfc3394.txt section 2.2\n+    // parameter for the KDF function, it's a funny and unusual iter count larger than 60k\n+    private static final int KDF_ITER = 61616;\n+    // the KDF algorithm that generate the symmetric key given the password\n+    private static final String KDF_ALGO = \"PBKDF2WithHmacSHA512\";\n+    // The Id of any AES SecretKey is the AES-Wrap-ciphertext of this fixed 32 byte wide array.\n+    // Key wrapping encryption is deterministic (same plaintext generates the same ciphertext)\n+    // and the probability that two different keys map the same plaintext to the same ciphertext is very small\n+    // (2^-256, much lower than the UUID collision of 2^-128), assuming AES is indistinguishable from a pseudorandom permutation.\n+    private static final byte[] KEY_ID_PLAINTEXT = \"wrapping known text forms key id\".getBytes(StandardCharsets.UTF_8);\n+\n+    public static byte[] wrap(SecretKey wrappingKey, SecretKey keyToWrap) throws NoSuchPaddingException,\n+        NoSuchAlgorithmException,\n+        InvalidKeyException,\n+        IllegalBlockSizeException {\n+        if (false == \"AES\".equals(wrappingKey.getAlgorithm())) {\n+            throw new IllegalArgumentException(\"wrappingKey argument is not an AES Key\");\n+        }\n+        if (false == \"AES\".equals(keyToWrap.getAlgorithm())) {\n+            throw new IllegalArgumentException(\"toWrapKey argument is not an AES Key\");\n+        }\n+        Cipher c = Cipher.getInstance(\"AESWrap\");\n+        c.init(Cipher.WRAP_MODE, wrappingKey);\n+        return c.wrap(keyToWrap);\n+    }\n+\n+    public static SecretKey unwrap(SecretKey wrappingKey, byte[] keyToUnwrap) throws NoSuchPaddingException,\n+        NoSuchAlgorithmException,\n+        InvalidKeyException {\n+        if (false == \"AES\".equals(wrappingKey.getAlgorithm())) {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU3OTc3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402579779", "bodyText": "It would probably fail, these checks serve more as documentation than anything else.\nI have a slight preference to keep them in, they're not too distracting, I think.", "author": "albertzaharovits", "createdAt": "2020-04-02T20:06:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExMDM2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc2NzE2MA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402767160", "bodyText": "these checks serve more as documentation than anything else.\n\n+1 to having them as documentation, but maybe turn them into asserts then to make it clear that this is documentation? (just a suggestion, I'm fine with it either way)", "author": "original-brownbear", "createdAt": "2020-04-03T06:43:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExMDM2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjkxODk4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402918982", "bodyText": "I looked into it a bit more.\nIndeed the wrapping key must be AES (or Rijndael), otherwise the Sun security provider throws InvalidKeyException. The wrapped key algorithm does not really matter as long as the byte array of the encoded key conforms to the length requirements of the AESWrap algorithm.\nGiven this, I've transformed all the IllegalArgumentExceptions into asserts because asserts better reflect the nuance that it's more an expectation given the calling context rather than a strong argument validation.", "author": "albertzaharovits", "createdAt": "2020-04-03T10:45:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExMDM2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNDY1OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402114658", "bodyText": "NIT: I added a utility for this the other day, you can use\nfinal InputStream firstComponent = Streams.noCloseStream(first);\nfinal InputStream secondComponent = Streams.noCloseStream(second);\nto shorten this :)", "author": "original-brownbear", "createdAt": "2020-04-02T07:46:55Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/ChainingInputStream.java", "diffHunk": "@@ -72,6 +75,74 @@\n      */\n     private boolean closed;\n \n+    /**\n+     * Returns a new {@link ChainingInputStream} that concatenates the bytes to be read from the first\n+     * input stream with the bytes from the second input stream. The stream arguments must support\n+     * the {@code mark} and {@code reset} operations; otherwise use {@link SequenceInputStream}.\n+     *\n+     * @param first the input stream supplying the first bytes of the returned {@link ChainingInputStream}\n+     * @param second the input stream supplying the bytes after the {@code first} input stream has been exhausted\n+     */\n+    public static ChainingInputStream chain(InputStream first, InputStream second) {\n+        if (false == Objects.requireNonNull(first).markSupported()) {\n+            throw new IllegalArgumentException(\"The first component input stream does not support mark\");\n+        }\n+        if (false == Objects.requireNonNull(second).markSupported()) {\n+            throw new IllegalArgumentException(\"The second component input stream does not support mark\");\n+        }\n+        final InputStream firstComponent = new FilterInputStream(first) {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU4NzY5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402587699", "bodyText": "Great, thank you!", "author": "albertzaharovits", "createdAt": "2020-04-02T20:23:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNDY1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNTY5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402115691", "bodyText": "NIT: IOUtils.close(super, first, second); should do the same as this method?", "author": "original-brownbear", "createdAt": "2020-04-02T07:48:34Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/ChainingInputStream.java", "diffHunk": "@@ -72,6 +75,74 @@\n      */\n     private boolean closed;\n \n+    /**\n+     * Returns a new {@link ChainingInputStream} that concatenates the bytes to be read from the first\n+     * input stream with the bytes from the second input stream. The stream arguments must support\n+     * the {@code mark} and {@code reset} operations; otherwise use {@link SequenceInputStream}.\n+     *\n+     * @param first the input stream supplying the first bytes of the returned {@link ChainingInputStream}\n+     * @param second the input stream supplying the bytes after the {@code first} input stream has been exhausted\n+     */\n+    public static ChainingInputStream chain(InputStream first, InputStream second) {\n+        if (false == Objects.requireNonNull(first).markSupported()) {\n+            throw new IllegalArgumentException(\"The first component input stream does not support mark\");\n+        }\n+        if (false == Objects.requireNonNull(second).markSupported()) {\n+            throw new IllegalArgumentException(\"The second component input stream does not support mark\");\n+        }\n+        final InputStream firstComponent = new FilterInputStream(first) {\n+            @Override\n+            public void close() {\n+                // silence close\n+                // \"first\" can be reused, and the {@code ChainingInputStream} eagerly closes components after every use\n+                // \"first\" is closed when the returned {@code ChainingInputStream} is closed\n+            }\n+        };\n+        final InputStream secondComponent = new FilterInputStream(second) {\n+            @Override\n+            public void close() {\n+                // silence close\n+                // \"second\" can be reused, and the {@code ChainingInputStream} eagerly closes components after every use\n+                // \"second\" is closed when the returned {@code ChainingInputStream} is closed\n+            }\n+        };\n+        // be sure to remember the start of components because they might be reused\n+        firstComponent.mark(Integer.MAX_VALUE);\n+        secondComponent.mark(Integer.MAX_VALUE);\n+\n+        return new ChainingInputStream() {\n+\n+            @Override\n+            InputStream nextComponent(InputStream currentComponentIn) throws IOException {\n+                if (currentComponentIn == null) {\n+                    // when returning the next component, start from its beginning\n+                    firstComponent.reset();\n+                    return firstComponent;\n+                } else if (currentComponentIn == firstComponent) {\n+                    // when returning the next component, start from its beginning\n+                    secondComponent.reset();\n+                    return secondComponent;\n+                } else if (currentComponentIn == secondComponent) {\n+                    return null;\n+                } else {\n+                    throw new IllegalStateException(\"Unexpected component input stream\");\n+                }\n+            }\n+\n+            @Override\n+            public void close() throws IOException {\n+                Exception superException = null;", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU4ODgzNw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402588837", "bodyText": "Close, IOUtils.close(super::close, first, second); does it, thanks for the suggestion!", "author": "albertzaharovits", "createdAt": "2020-04-02T20:26:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNTY5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU4ODk1MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402588951", "bodyText": "No pun intended.", "author": "albertzaharovits", "createdAt": "2020-04-02T20:26:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNTY5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNjIxMg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402116212", "bodyText": "throw new IOException(\"Mark/reset not supported\"); in this method?", "author": "original-brownbear", "createdAt": "2020-04-02T07:49:24Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/DecryptionPacketsInputStream.java", "diffHunk": "@@ -115,14 +117,25 @@ public boolean markSupported() {\n     }\n \n     @Override\n-    public void mark(int readlimit) {\n-    }\n+    public void mark(int readlimit) {}", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNjU5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402116596", "bodyText": "NIT: IOUtils.close(super, first, second); should do the same as this method?", "author": "original-brownbear", "createdAt": "2020-04-02T07:50:00Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/DecryptionPacketsInputStream.java", "diffHunk": "@@ -115,14 +117,25 @@ public boolean markSupported() {\n     }\n \n     @Override\n-    public void mark(int readlimit) {\n-    }\n+    public void mark(int readlimit) {}\n \n     @Override\n     public void reset() throws IOException {\n         throw new IOException(\"Mark/reset not supported\");\n     }\n \n+    @Override\n+    public void close() throws IOException {\n+        Exception superException = null;", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExODQzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402118431", "bodyText": "Now that you're not reading the int I think you can shorten this to long packetIvCounter = ByteUtils.readLongLE(packetBuffer, 0);", "author": "original-brownbear", "createdAt": "2020-04-02T07:52:56Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/DecryptionPacketsInputStream.java", "diffHunk": "@@ -131,11 +144,7 @@ private int decrypt(PrefixInputStream packetInputStream) throws IOException {\n         }\n         // extract the nonce and the counter from the packet IV\n         ByteBuffer ivBuffer = ByteBuffer.wrap(packetBuffer, 0, GCM_IV_LENGTH_IN_BYTES).order(ByteOrder.LITTLE_ENDIAN);", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA3OTQxMg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403079412", "bodyText": "Nice suggestion, thanks!", "author": "albertzaharovits", "createdAt": "2020-04-03T15:17:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExODQzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExOTg0MA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402119840", "bodyText": "NIT: that's a strange comment :) we don't really comment on why stuff isn't final anywhere else I think.", "author": "original-brownbear", "createdAt": "2020-04-02T07:55:21Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyMDgxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402120819", "bodyText": "NIT: weird formatting? I'd at least join this line with the ) above", "author": "original-brownbear", "createdAt": "2020-04-02T07:57:02Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjkyNDY0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402924649", "bodyText": "@pugnascotia This is also spotless's dealing. Does it look right? I'm on the fence about it.", "author": "albertzaharovits", "createdAt": "2020-04-03T10:56:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyMDgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjkyOTU1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402929555", "bodyText": "Ah, good catch - there's a separate set of parameters for formatting constructors. I'll update #54710.", "author": "pugnascotia", "createdAt": "2020-04-03T11:05:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyMDgxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyMjIzMg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402122232", "bodyText": "NIT: This can just be a volatile String? We never really use any AtomicReference magic on this do we?", "author": "original-brownbear", "createdAt": "2020-04-02T07:59:27Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyMzMyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402123321", "bodyText": "logger.trace(\"Snapshot metadata for local repository password  [{}] and [{}]\", localRepositoryPasswordIdSalt, localRepositoryPasswordId); should be fine here as well, we're not computing anything in the params so no need to use the message supplier pattern IMO.", "author": "original-brownbear", "createdAt": "2020-04-02T08:01:28Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjk0NzU1MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402947551", "bodyText": "Okay, I've reverted all uses of the message supplier pattern, as none used parameters that required computation.\nI tended to always use a message supplier because I can more easily ignore it if I see it part of every log statement, and it's one less decision to take when coding, but I understand others' reluctance and probably a better strategy is to default on never using the pattern.", "author": "albertzaharovits", "createdAt": "2020-04-03T11:44:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyMzMyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyNDA3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402124079", "bodyText": "Why nest exceptions instead of the below? :)\n            throw new RepositoryException(\n                metadata.name(),\n                \"The encrypted repository must be read-only iff the delegate repository is read-only\"\n            );", "author": "original-brownbear", "createdAt": "2020-04-02T08:02:52Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjk1NDE5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402954193", "bodyText": "From my perspective the IllegalStateException type is very informative.\nIt documents assumptions when reading the code (like an assert), and if you look at a stacktrace and see one of those you know this is certainly a bug (like a 500 error not a 400, also when you look at CI test failures some exceptions are expected, while an IllegalStateException is clearly the source of the test failure).\nIn my view, the wrapper RepositoryException is the baddie, the callers should probably take the responsibility of wrapping with a more appropriate exception.\nI've reluctantly removed the wrapping as suggested, but I'll gladly change back if I've persuaded you.", "author": "albertzaharovits", "createdAt": "2020-04-03T11:59:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyNDA3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjk1Njc1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402956753", "bodyText": "Ah you're right + convinced me => TIL :) Never really thought about the status code here. Sorry for the noise, please keep it then!", "author": "original-brownbear", "createdAt": "2020-04-03T12:04:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyNDA3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzczNjM3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403736371", "bodyText": "I have pushed 55f32d6 which reinstates the IllegalStateException which haven't been removed by asserts as suggested in other discussions.", "author": "albertzaharovits", "createdAt": "2020-04-05T18:02:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyNDA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyNDg3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402124873", "bodyText": "Metadata is never null, no need for this check I think", "author": "original-brownbear", "createdAt": "2020-04-02T08:04:18Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzczMjg3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403732875", "bodyText": "Except that it can, and integ tests catch it. The empty CreateSnapshotRequest does not set the userMetadata variable. I'll keep this test.", "author": "albertzaharovits", "createdAt": "2020-04-05T17:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyNDg3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzczNzU4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403737581", "bodyText": "Urgh, you're right :) I see now. I'll fix that, but yea we gotta keep the check for now then.", "author": "original-brownbear", "createdAt": "2020-04-05T18:12:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyNDg3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEzMzg0MA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402133840", "bodyText": "This can just be an assertion I think", "author": "original-brownbear", "createdAt": "2020-04-02T08:20:40Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            () -> new ParameterizedMessage(\n+                \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+                localRepositoryPasswordIdSalt,\n+                localRepositoryPasswordId\n+            )\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        MetaData clusterMetaData,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        ActionListener<SnapshotInfo> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetaData,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            DEKGenerator,\n+            DEKCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<String, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator DEKGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        DEKGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> {\n+            final String DEKId = UUIDs.randomBase64UUID(DEKIdSecureRandom);\n+            final SecretKey DEK = DEKGenerator.generateKey();\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}]] generated new DEK [{}]\", metadata.name(), DEKId));\n+            return new Tuple<>(DEKId, DEK);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String DEKId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final byte[] KEKsalt = DEKId.getBytes(StandardCharsets.UTF_8);\n+            final SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, KEKsalt);\n+            final String KEKId = AESKeyUtils.computeId(KEK);\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), KEKId, DEKId));\n+            return new Tuple<>(KEKId, KEK);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * secret on the master node that started the snapshot operation is identical to the repository secret on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository secret id to verify\n+     * @throws RepositoryException if the repository secret id on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        if (snapshotUserMetadata == null) {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEzNDA1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402134053", "bodyText": "Same here, assertion should be fine, we have full control of this map end-to-end I think", "author": "original-brownbear", "createdAt": "2020-04-02T08:21:04Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            () -> new ParameterizedMessage(\n+                \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+                localRepositoryPasswordIdSalt,\n+                localRepositoryPasswordId\n+            )\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        MetaData clusterMetaData,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        ActionListener<SnapshotInfo> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetaData,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            DEKGenerator,\n+            DEKCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<String, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator DEKGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        DEKGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> {\n+            final String DEKId = UUIDs.randomBase64UUID(DEKIdSecureRandom);\n+            final SecretKey DEK = DEKGenerator.generateKey();\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}]] generated new DEK [{}]\", metadata.name(), DEKId));\n+            return new Tuple<>(DEKId, DEK);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String DEKId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final byte[] KEKsalt = DEKId.getBytes(StandardCharsets.UTF_8);\n+            final SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, KEKsalt);\n+            final String KEKId = AESKeyUtils.computeId(KEK);\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), KEKId, DEKId));\n+            return new Tuple<>(KEKId, KEK);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * secret on the master node that started the snapshot operation is identical to the repository secret on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository secret id to verify\n+     * @throws RepositoryException if the repository secret id on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        if (snapshotUserMetadata == null) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"Null snapshot metadata\")\n+            );\n+        }\n+        final Object masterRepositoryPasswordId = snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY);\n+        if (false == masterRepositoryPasswordId instanceof String) {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEzNTY0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402135642", "bodyText": "No need for the message supplier pattern here either I think", "author": "original-brownbear", "createdAt": "2020-04-02T08:23:47Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            () -> new ParameterizedMessage(\n+                \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+                localRepositoryPasswordIdSalt,\n+                localRepositoryPasswordId\n+            )\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        MetaData clusterMetaData,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        ActionListener<SnapshotInfo> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetaData,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            DEKGenerator,\n+            DEKCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<String, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator DEKGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        DEKGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> {\n+            final String DEKId = UUIDs.randomBase64UUID(DEKIdSecureRandom);\n+            final SecretKey DEK = DEKGenerator.generateKey();\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}]] generated new DEK [{}]\", metadata.name(), DEKId));\n+            return new Tuple<>(DEKId, DEK);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String DEKId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final byte[] KEKsalt = DEKId.getBytes(StandardCharsets.UTF_8);\n+            final SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, KEKsalt);\n+            final String KEKId = AESKeyUtils.computeId(KEK);\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), KEKId, DEKId));\n+            return new Tuple<>(KEKId, KEK);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * secret on the master node that started the snapshot operation is identical to the repository secret on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository secret id to verify\n+     * @throws RepositoryException if the repository secret id on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        if (snapshotUserMetadata == null) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"Null snapshot metadata\")\n+            );\n+        }\n+        final Object masterRepositoryPasswordId = snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY);\n+        if (false == masterRepositoryPasswordId instanceof String) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"Snapshot metadata does not contain the repository password id as a String\")\n+            );\n+        }\n+        if (false == validatedRepositoryPasswordId.get().equals(masterRepositoryPasswordId)) {\n+            final Object masterRepositoryPasswordIdSalt = snapshotUserMetadata.get(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+            if (false == masterRepositoryPasswordIdSalt instanceof String) {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"Snapshot metadata does not contain the repository password salt as a String\")\n+                );\n+            }\n+            final String computedRepositoryPasswordId;\n+            try {\n+                computedRepositoryPasswordId = AESKeyUtils.computeId(\n+                    AESKeyUtils.generatePasswordBasedKey(\n+                        repositoryPassword,\n+                        ((String) masterRepositoryPasswordIdSalt).getBytes(StandardCharsets.UTF_8)\n+                    )\n+                );\n+            } catch (Exception e) {\n+                throw new RepositoryException(metadata.name(), \"Unexpected fatal internal error\", e);\n+            }\n+            if (computedRepositoryPasswordId.equals(masterRepositoryPasswordId)) {\n+                this.validatedRepositoryPasswordId.set(computedRepositoryPasswordId);\n+            } else {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Repository secret id mismatch. The local node's repository secret, the keystore setting [\"\n+                        + EncryptedRepositoryPlugin.ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(\n+                            EncryptedRepositoryPlugin.PASSWORD_NAME_SETTING.get(metadata.settings())\n+                        ).getKey()\n+                        + \"], is different compared to the elected master node's which started the snapshot operation\"\n+                );\n+            }\n+        }\n+    }\n+\n+    // pkg-private for tests\n+    static final class EncryptedBlobStore implements BlobStore {\n+        private final BlobStore delegatedBlobStore;\n+        private final BlobPath delegatedBasePath;\n+        private final String repositoryName;\n+        private final Function<String, Tuple<String, SecretKey>> getKEKforDEK;\n+        private final Cache<String, SecretKey> DEKCache;\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+\n+        EncryptedBlobStore(\n+            BlobStore delegatedBlobStore,\n+            BlobPath delegatedBasePath,\n+            String repositoryName,\n+            Function<String, Tuple<String, SecretKey>> getKEKforDEK,\n+            Supplier<Tuple<String, SecretKey>> DEKGenerator,\n+            Cache<String, SecretKey> DEKCache\n+        ) {\n+            this.delegatedBlobStore = delegatedBlobStore;\n+            this.delegatedBasePath = delegatedBasePath;\n+            this.repositoryName = repositoryName;\n+            this.getKEKforDEK = getKEKforDEK;\n+            this.DEKCache = DEKCache;\n+            this.singleUseDEKSupplier = SingleUseKey.createSingleUseKeySupplier(() -> {\n+                Tuple<String, SecretKey> newDEK = DEKGenerator.get();\n+                // store the newly generated DEK before making it available\n+                storeDEK(newDEK.v1(), newDEK.v2());\n+                return newDEK;\n+            });\n+        }\n+\n+        // pkg-private for tests\n+        SecretKey getDEKById(String DEKId) throws IOException {\n+            try {\n+                return DEKCache.computeIfAbsent(DEKId, ignored -> loadDEK(DEKId));\n+            } catch (ExecutionException e) {\n+                // some exception types are to be expected\n+                if (e.getCause() instanceof IOException) {\n+                    throw (IOException) e.getCause();\n+                } else if (e.getCause() instanceof ElasticsearchException) {\n+                    throw (ElasticsearchException) e.getCause();\n+                } else {\n+                    throw new RepositoryException(repositoryName, \"Unexpected exception retrieving DEK [\" + DEKId + \"]\", e);\n+                }\n+            }\n+        }\n+\n+        private SecretKey loadDEK(String DEKId) throws IOException {\n+            final BlobPath DEKBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(DEKId);\n+            logger.debug(\n+                () -> new ParameterizedMessage(", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1MjQyNw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402152427", "bodyText": "This should be an assertion? Also maybe just make getKeyId return a BytesReference so we don't have to do the string to byte[] conversion here all the time. It's kind of confusing to handle something that is a bunch of bytes as a string only to convert it to byte[] before every use?", "author": "original-brownbear", "createdAt": "2020-04-02T08:50:54Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            () -> new ParameterizedMessage(\n+                \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+                localRepositoryPasswordIdSalt,\n+                localRepositoryPasswordId\n+            )\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        MetaData clusterMetaData,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        ActionListener<SnapshotInfo> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetaData,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            DEKGenerator,\n+            DEKCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<String, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator DEKGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        DEKGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> {\n+            final String DEKId = UUIDs.randomBase64UUID(DEKIdSecureRandom);\n+            final SecretKey DEK = DEKGenerator.generateKey();\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}]] generated new DEK [{}]\", metadata.name(), DEKId));\n+            return new Tuple<>(DEKId, DEK);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String DEKId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final byte[] KEKsalt = DEKId.getBytes(StandardCharsets.UTF_8);\n+            final SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, KEKsalt);\n+            final String KEKId = AESKeyUtils.computeId(KEK);\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), KEKId, DEKId));\n+            return new Tuple<>(KEKId, KEK);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * secret on the master node that started the snapshot operation is identical to the repository secret on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository secret id to verify\n+     * @throws RepositoryException if the repository secret id on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        if (snapshotUserMetadata == null) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"Null snapshot metadata\")\n+            );\n+        }\n+        final Object masterRepositoryPasswordId = snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY);\n+        if (false == masterRepositoryPasswordId instanceof String) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"Snapshot metadata does not contain the repository password id as a String\")\n+            );\n+        }\n+        if (false == validatedRepositoryPasswordId.get().equals(masterRepositoryPasswordId)) {\n+            final Object masterRepositoryPasswordIdSalt = snapshotUserMetadata.get(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+            if (false == masterRepositoryPasswordIdSalt instanceof String) {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"Snapshot metadata does not contain the repository password salt as a String\")\n+                );\n+            }\n+            final String computedRepositoryPasswordId;\n+            try {\n+                computedRepositoryPasswordId = AESKeyUtils.computeId(\n+                    AESKeyUtils.generatePasswordBasedKey(\n+                        repositoryPassword,\n+                        ((String) masterRepositoryPasswordIdSalt).getBytes(StandardCharsets.UTF_8)\n+                    )\n+                );\n+            } catch (Exception e) {\n+                throw new RepositoryException(metadata.name(), \"Unexpected fatal internal error\", e);\n+            }\n+            if (computedRepositoryPasswordId.equals(masterRepositoryPasswordId)) {\n+                this.validatedRepositoryPasswordId.set(computedRepositoryPasswordId);\n+            } else {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Repository secret id mismatch. The local node's repository secret, the keystore setting [\"\n+                        + EncryptedRepositoryPlugin.ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(\n+                            EncryptedRepositoryPlugin.PASSWORD_NAME_SETTING.get(metadata.settings())\n+                        ).getKey()\n+                        + \"], is different compared to the elected master node's which started the snapshot operation\"\n+                );\n+            }\n+        }\n+    }\n+\n+    // pkg-private for tests\n+    static final class EncryptedBlobStore implements BlobStore {\n+        private final BlobStore delegatedBlobStore;\n+        private final BlobPath delegatedBasePath;\n+        private final String repositoryName;\n+        private final Function<String, Tuple<String, SecretKey>> getKEKforDEK;\n+        private final Cache<String, SecretKey> DEKCache;\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+\n+        EncryptedBlobStore(\n+            BlobStore delegatedBlobStore,\n+            BlobPath delegatedBasePath,\n+            String repositoryName,\n+            Function<String, Tuple<String, SecretKey>> getKEKforDEK,\n+            Supplier<Tuple<String, SecretKey>> DEKGenerator,\n+            Cache<String, SecretKey> DEKCache\n+        ) {\n+            this.delegatedBlobStore = delegatedBlobStore;\n+            this.delegatedBasePath = delegatedBasePath;\n+            this.repositoryName = repositoryName;\n+            this.getKEKforDEK = getKEKforDEK;\n+            this.DEKCache = DEKCache;\n+            this.singleUseDEKSupplier = SingleUseKey.createSingleUseKeySupplier(() -> {\n+                Tuple<String, SecretKey> newDEK = DEKGenerator.get();\n+                // store the newly generated DEK before making it available\n+                storeDEK(newDEK.v1(), newDEK.v2());\n+                return newDEK;\n+            });\n+        }\n+\n+        // pkg-private for tests\n+        SecretKey getDEKById(String DEKId) throws IOException {\n+            try {\n+                return DEKCache.computeIfAbsent(DEKId, ignored -> loadDEK(DEKId));\n+            } catch (ExecutionException e) {\n+                // some exception types are to be expected\n+                if (e.getCause() instanceof IOException) {\n+                    throw (IOException) e.getCause();\n+                } else if (e.getCause() instanceof ElasticsearchException) {\n+                    throw (ElasticsearchException) e.getCause();\n+                } else {\n+                    throw new RepositoryException(repositoryName, \"Unexpected exception retrieving DEK [\" + DEKId + \"]\", e);\n+                }\n+            }\n+        }\n+\n+        private SecretKey loadDEK(String DEKId) throws IOException {\n+            final BlobPath DEKBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(DEKId);\n+            logger.debug(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] loading wrapped DEK [{}] from blob path {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath\n+                )\n+            );\n+            final BlobContainer DEKBlobContainer = delegatedBlobStore.blobContainer(DEKBlobPath);\n+            final Tuple<String, SecretKey> KEK = getKEKforDEK.apply(DEKId);\n+            logger.trace(\n+                () -> new ParameterizedMessage(\"Repository [{}] using KEK [{}] to unwrap DEK [{}]\", repositoryName, KEK.v1(), DEKId)\n+            );\n+            final byte[] encryptedDEKBytes = new byte[AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES];\n+            try (InputStream encryptedDEKInputStream = DEKBlobContainer.readBlob(KEK.v1())) {\n+                final int bytesRead = Streams.readFully(encryptedDEKInputStream, encryptedDEKBytes);\n+                if (bytesRead != AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES) {\n+                    throw new RepositoryException(\n+                        repositoryName,\n+                        \"Wrapped DEK [\" + DEKId + \"] has smaller length [\" + bytesRead + \"] \" + \"than expected\"\n+                    );\n+                }\n+                if (encryptedDEKInputStream.read() != -1) {\n+                    throw new RepositoryException(repositoryName, \"Wrapped DEK [\" + DEKId + \"] is larger than expected\");\n+                }\n+            } catch (NoSuchFileException e) {\n+                // do NOT throw IOException when the DEK does not exist, as this is a decryption problem, and IOExceptions\n+                // can move the repository in the corrupted state\n+                throw new ElasticsearchException(\n+                    \"Failure to read and decrypt DEK [\"\n+                        + DEKId\n+                        + \"] from \"\n+                        + DEKBlobContainer.path()\n+                        + \". Most likely the repository password is incorrect, where previous \"\n+                        + \"snapshots have used a different password.\",\n+                    e\n+                );\n+            }\n+            logger.trace(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] successfully read DEK [{}] from path {} {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath,\n+                    KEK.v1()\n+                )\n+            );\n+            try {\n+                final SecretKey DEK = AESKeyUtils.unwrap(KEK.v2(), encryptedDEKBytes);\n+                logger.debug(\n+                    () -> new ParameterizedMessage(\n+                        \"Repository [{}] successfully loaded DEK [{}] from path {} {}\",\n+                        repositoryName,\n+                        DEKId,\n+                        DEKBlobPath,\n+                        KEK.v1()\n+                    )\n+                );\n+                return DEK;\n+            } catch (GeneralSecurityException e) {\n+                throw new RepositoryException(\n+                    repositoryName,\n+                    \"Failure to AES unwrap the DEK [\"\n+                        + DEKId\n+                        + \"]. \"\n+                        + \"Most likely the encryption metadata in the repository has been corrupted\",\n+                    e\n+                );\n+            }\n+        }\n+\n+        // pkg-private for tests\n+        void storeDEK(String DEKId, SecretKey DEK) throws IOException {\n+            final BlobPath DEKBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(DEKId);\n+            logger.debug(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] storing wrapped DEK [{}] under blob path {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath\n+                )\n+            );\n+            final BlobContainer DEKBlobContainer = delegatedBlobStore.blobContainer(DEKBlobPath);\n+            final Tuple<String, SecretKey> KEK = getKEKforDEK.apply(DEKId);\n+            logger.trace(\n+                () -> new ParameterizedMessage(\"Repository [{}] using KEK [{}] to wrap DEK [{}]\", repositoryName, KEK.v1(), DEKId)\n+            );\n+            final byte[] encryptedDEKBytes;\n+            try {\n+                encryptedDEKBytes = AESKeyUtils.wrap(KEK.v2(), DEK);\n+                if (encryptedDEKBytes.length != AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES) {\n+                    throw new RepositoryException(\n+                        repositoryName,\n+                        \"Wrapped DEK [\" + DEKId + \"] has unexpected length [\" + encryptedDEKBytes.length + \"]\"\n+                    );\n+                }\n+            } catch (GeneralSecurityException e) {\n+                // throw unchecked ElasticsearchException; IOExceptions are interpreted differently and can move the repository in the\n+                // corrupted state\n+                throw new RepositoryException(repositoryName, \"Failure to AES wrap the DEK [\" + DEKId + \"]\", e);\n+            }\n+            logger.trace(() -> new ParameterizedMessage(\"Repository [{}] successfully wrapped DEK [{}]\", repositoryName, DEKId));\n+            try (InputStream encryptedDEKInputStream = new ByteArrayInputStream(encryptedDEKBytes)) {\n+                DEKBlobContainer.writeBlobAtomic(KEK.v1(), encryptedDEKInputStream, encryptedDEKBytes.length, true);\n+            }\n+            logger.debug(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] successfully stored DEK [{}] under path {} {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath,\n+                    KEK.v1()\n+                )\n+            );\n+        }\n+\n+        @Override\n+        public BlobContainer blobContainer(BlobPath path) {\n+            final BlobContainer delegatedBlobContainer = delegatedBlobStore.blobContainer(delegatedBasePath.append(path));\n+            return new EncryptedBlobContainer(path, repositoryName, delegatedBlobContainer, singleUseDEKSupplier, this::getDEKById);\n+        }\n+\n+        @Override\n+        public void close() {\n+            // do NOT close delegatedBlobStore; it will be closed when the inner delegatedRepository is closed\n+        }\n+\n+    }\n+\n+    private static final class EncryptedBlobContainer extends AbstractBlobContainer {\n+        private final String repositoryName;\n+        private final BlobContainer delegatedBlobContainer;\n+        // supplier for the DEK used for encryption (snapshot)\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+        // retrieves the DEK required for decryption (restore)\n+        private final CheckedFunction<String, SecretKey, IOException> getDEKById;\n+\n+        EncryptedBlobContainer(\n+            BlobPath path, // this path contains the {@code EncryptedRepository#basePath} which, importantly, is empty\n+            String repositoryName,\n+            BlobContainer delegatedBlobContainer,\n+            CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier,\n+            CheckedFunction<String, SecretKey, IOException> getDEKById\n+        ) {\n+            super(path);\n+            this.repositoryName = repositoryName;\n+            if (DEK_ROOT_CONTAINER.equals(path.getRootPath())) {\n+                throw new RepositoryException(\n+                    repositoryName,\n+                    \"Unexpected internal error\",\n+                    new IllegalArgumentException(\"Cannot descend into the DEK blob container \" + path)\n+                );\n+            }\n+            this.delegatedBlobContainer = delegatedBlobContainer;\n+            this.singleUseDEKSupplier = singleUseDEKSupplier;\n+            this.getDEKById = getDEKById;\n+        }\n+\n+        /**\n+         * Returns a new {@link InputStream} for the given {@code blobName} that can be used to read the contents of the blob.\n+         * The returned {@code InputStream} transparently handles the decryption of the blob contents, by first working out\n+         * the blob name of the associated DEK id, reading and decrypting the DEK (given the repository secret key, unless the DEK is\n+         * already cached because other blobs required it before), and lastly reading and decrypting the data blob,\n+         * in a streaming fashion, by employing the {@link DecryptionPacketsInputStream}.\n+         * The {@code DecryptionPacketsInputStream} does not return un-authenticated data.\n+         *\n+         * @param   blobName The name of the blob to get an {@link InputStream} for.\n+         */\n+        @Override\n+        public InputStream readBlob(String blobName) throws IOException {\n+            // This MIGHT require two concurrent readBlob connections if the DEK is not already in the cache and if the encrypted blob\n+            // is large enough so that the underlying network library keeps the connection open after reading the prepended DEK ID.\n+            // Arguably this is a problem only under lab conditions, when the storage service is saturated only by the first read\n+            // connection of the pair, so that the second read connection (for the DEK) can not be fulfilled.\n+            // In this case the second connection will time-out which will trigger the closing of the first one, therefore\n+            // allowing other pair connections to complete.\n+            // In this situation the restore process should slowly make headway, albeit under read-timeout exceptions\n+            final InputStream encryptedDataInputStream = delegatedBlobContainer.readBlob(blobName);\n+            try {\n+                // read the DEK Id (fixed length) which is prepended to the encrypted blob\n+                final byte[] DEKIdBytes = new byte[DEK_ID_LENGTH];\n+                final int bytesRead = Streams.readFully(encryptedDataInputStream, DEKIdBytes);\n+                if (bytesRead != DEK_ID_LENGTH) {\n+                    throw new RepositoryException(repositoryName, \"The encrypted blob [\" + blobName + \"] is too small [\" + bytesRead + \"]\");\n+                }\n+                final String DEKId = new String(DEKIdBytes, StandardCharsets.UTF_8);\n+                // might open a connection to read and decrypt the DEK, but most likely it will be served from cache\n+                final SecretKey DEK = getDEKById.apply(DEKId);\n+                // read and decrypt the rest of the blob\n+                return new DecryptionPacketsInputStream(encryptedDataInputStream, DEK, PACKET_LENGTH_IN_BYTES);\n+            } catch (Exception e) {\n+                try {\n+                    encryptedDataInputStream.close();\n+                } catch (IOException closeEx) {\n+                    e.addSuppressed(closeEx);\n+                }\n+                throw e;\n+            }\n+        }\n+\n+        /**\n+         * Reads the blob content from the input stream and writes it to the container in a new blob with the given name.\n+         * If {@code failIfAlreadyExists} is {@code true} and a blob with the same name already exists, the write operation will fail;\n+         * otherwise, if {@code failIfAlreadyExists} is {@code false} the blob is overwritten.\n+         * The contents are encrypted in a streaming fashion. The DEK (encryption key) is randomly generated and reused for encrypting\n+         * subsequent blobs such that the same IV is not reused together with the same key.\n+         * The DEK encryption key is separately stored in a different blob, which is encrypted with the repository key.\n+         *\n+         * @param   blobName\n+         *          The name of the blob to write the contents of the input stream to.\n+         * @param   inputStream\n+         *          The input stream from which to retrieve the bytes to write to the blob.\n+         * @param   blobSize\n+         *          The size of the blob to be written, in bytes. The actual number of bytes written to the storage service is larger\n+         *          because of encryption and authentication overhead. It is implementation dependent whether this value is used\n+         *          in writing the blob to the repository.\n+         * @param   failIfAlreadyExists\n+         *          whether to throw a FileAlreadyExistsException if the given blob already exists\n+         */\n+        @Override\n+        public void writeBlob(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) throws IOException {\n+            // reuse, but possibly generate and store a new DEK\n+            final SingleUseKey singleUseNonceAndDEK = singleUseDEKSupplier.get();\n+            final byte[] DEKIdBytes = singleUseNonceAndDEK.getKeyId().getBytes(StandardCharsets.UTF_8);\n+            if (DEKIdBytes.length != DEK_ID_LENGTH) {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzczMjYxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403732619", "bodyText": "This should be an assertion?\n\nIt should probably be an assertion, BUT, as an exception (pun!), I would slightly prefer this case be an Exception (a RepositoryException wrapping an IllegalStateException). I prefer it be an exception because of the readBlob and writeBlob symmetry; one reads a fixed length prefix while the other writes the prefix. I think it's healthy to keep this redundant check at this level, even if probably the current code would fail in other places in the obverse. An assertion, in my view, signifies that the logic is broken if the assertion is not satisfied, but in this case the logic in the method works OK, but the repository gets corrupted.\n\nAlso maybe just make getKeyId return a BytesReference so we don't have to do the string to byte[] conversion here all the time. It's kind of confusing to handle something that is a bunch of bytes as a string only to convert it to byte[] before every use?\n\nI have pushed d49bf40 that makes key ids BytesReferences instead of Strings. At first, I thought it was a good suggestion, BUT the Ids were used as strings in read/writeBlob methods, in constructing BlobPath and in logging. Looking at the code after the change it doesn't look like an improvement to me. Also because now logging does require some processing we have to introduce the message supplier pattern again... I tend to favor the original approach, but, your call.", "author": "albertzaharovits", "createdAt": "2020-04-05T17:30:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1MjQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzczNzE1OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403737158", "bodyText": "Ah sorry I think there was a misunderstanding here, I was only referring to getKeyId, that seems to always just be a BytesReference and never be used as String? The others should stay String if we use them as String.\nAlso makes sense + I'm fine with keeping the exception.", "author": "original-brownbear", "createdAt": "2020-04-05T18:08:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1MjQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc0NzA5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403747091", "bodyText": "Okay, understood, I've pushed 87e64b3 . Thanks.", "author": "albertzaharovits", "createdAt": "2020-04-05T19:36:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1MjQyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1Mjk4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402152983", "bodyText": "NIT: random empty line", "author": "original-brownbear", "createdAt": "2020-04-02T08:51:45Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,739 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetaData;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+// not-final for tests\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final char[] repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private final AtomicReference<String> validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetaData metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        char[] repositoryPassword\n+    )\n+        throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt.getBytes(StandardCharsets.UTF_8))\n+        );\n+        this.validatedRepositoryPasswordId = new AtomicReference<>(this.localRepositoryPasswordId);\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isEncryptedSnapshotAllowed()) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            () -> new ParameterizedMessage(\n+                \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+                localRepositoryPasswordIdSalt,\n+                localRepositoryPasswordId\n+            )\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        MetaData clusterMetaData,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        ActionListener<SnapshotInfo> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetaData,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<String, SecretKey>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            DEKGenerator,\n+            DEKCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<String, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator DEKGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        DEKGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> {\n+            final String DEKId = UUIDs.randomBase64UUID(DEKIdSecureRandom);\n+            final SecretKey DEK = DEKGenerator.generateKey();\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}]] generated new DEK [{}]\", metadata.name(), DEKId));\n+            return new Tuple<>(DEKId, DEK);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String DEKId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final byte[] KEKsalt = DEKId.getBytes(StandardCharsets.UTF_8);\n+            final SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, KEKsalt);\n+            final String KEKId = AESKeyUtils.computeId(KEK);\n+            logger.debug(() -> new ParameterizedMessage(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), KEKId, DEKId));\n+            return new Tuple<>(KEKId, KEK);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * secret on the master node that started the snapshot operation is identical to the repository secret on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository secret id to verify\n+     * @throws RepositoryException if the repository secret id on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        if (snapshotUserMetadata == null) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"Null snapshot metadata\")\n+            );\n+        }\n+        final Object masterRepositoryPasswordId = snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY);\n+        if (false == masterRepositoryPasswordId instanceof String) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"Snapshot metadata does not contain the repository password id as a String\")\n+            );\n+        }\n+        if (false == validatedRepositoryPasswordId.get().equals(masterRepositoryPasswordId)) {\n+            final Object masterRepositoryPasswordIdSalt = snapshotUserMetadata.get(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+            if (false == masterRepositoryPasswordIdSalt instanceof String) {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"Snapshot metadata does not contain the repository password salt as a String\")\n+                );\n+            }\n+            final String computedRepositoryPasswordId;\n+            try {\n+                computedRepositoryPasswordId = AESKeyUtils.computeId(\n+                    AESKeyUtils.generatePasswordBasedKey(\n+                        repositoryPassword,\n+                        ((String) masterRepositoryPasswordIdSalt).getBytes(StandardCharsets.UTF_8)\n+                    )\n+                );\n+            } catch (Exception e) {\n+                throw new RepositoryException(metadata.name(), \"Unexpected fatal internal error\", e);\n+            }\n+            if (computedRepositoryPasswordId.equals(masterRepositoryPasswordId)) {\n+                this.validatedRepositoryPasswordId.set(computedRepositoryPasswordId);\n+            } else {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Repository secret id mismatch. The local node's repository secret, the keystore setting [\"\n+                        + EncryptedRepositoryPlugin.ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(\n+                            EncryptedRepositoryPlugin.PASSWORD_NAME_SETTING.get(metadata.settings())\n+                        ).getKey()\n+                        + \"], is different compared to the elected master node's which started the snapshot operation\"\n+                );\n+            }\n+        }\n+    }\n+\n+    // pkg-private for tests\n+    static final class EncryptedBlobStore implements BlobStore {\n+        private final BlobStore delegatedBlobStore;\n+        private final BlobPath delegatedBasePath;\n+        private final String repositoryName;\n+        private final Function<String, Tuple<String, SecretKey>> getKEKforDEK;\n+        private final Cache<String, SecretKey> DEKCache;\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+\n+        EncryptedBlobStore(\n+            BlobStore delegatedBlobStore,\n+            BlobPath delegatedBasePath,\n+            String repositoryName,\n+            Function<String, Tuple<String, SecretKey>> getKEKforDEK,\n+            Supplier<Tuple<String, SecretKey>> DEKGenerator,\n+            Cache<String, SecretKey> DEKCache\n+        ) {\n+            this.delegatedBlobStore = delegatedBlobStore;\n+            this.delegatedBasePath = delegatedBasePath;\n+            this.repositoryName = repositoryName;\n+            this.getKEKforDEK = getKEKforDEK;\n+            this.DEKCache = DEKCache;\n+            this.singleUseDEKSupplier = SingleUseKey.createSingleUseKeySupplier(() -> {\n+                Tuple<String, SecretKey> newDEK = DEKGenerator.get();\n+                // store the newly generated DEK before making it available\n+                storeDEK(newDEK.v1(), newDEK.v2());\n+                return newDEK;\n+            });\n+        }\n+\n+        // pkg-private for tests\n+        SecretKey getDEKById(String DEKId) throws IOException {\n+            try {\n+                return DEKCache.computeIfAbsent(DEKId, ignored -> loadDEK(DEKId));\n+            } catch (ExecutionException e) {\n+                // some exception types are to be expected\n+                if (e.getCause() instanceof IOException) {\n+                    throw (IOException) e.getCause();\n+                } else if (e.getCause() instanceof ElasticsearchException) {\n+                    throw (ElasticsearchException) e.getCause();\n+                } else {\n+                    throw new RepositoryException(repositoryName, \"Unexpected exception retrieving DEK [\" + DEKId + \"]\", e);\n+                }\n+            }\n+        }\n+\n+        private SecretKey loadDEK(String DEKId) throws IOException {\n+            final BlobPath DEKBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(DEKId);\n+            logger.debug(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] loading wrapped DEK [{}] from blob path {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath\n+                )\n+            );\n+            final BlobContainer DEKBlobContainer = delegatedBlobStore.blobContainer(DEKBlobPath);\n+            final Tuple<String, SecretKey> KEK = getKEKforDEK.apply(DEKId);\n+            logger.trace(\n+                () -> new ParameterizedMessage(\"Repository [{}] using KEK [{}] to unwrap DEK [{}]\", repositoryName, KEK.v1(), DEKId)\n+            );\n+            final byte[] encryptedDEKBytes = new byte[AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES];\n+            try (InputStream encryptedDEKInputStream = DEKBlobContainer.readBlob(KEK.v1())) {\n+                final int bytesRead = Streams.readFully(encryptedDEKInputStream, encryptedDEKBytes);\n+                if (bytesRead != AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES) {\n+                    throw new RepositoryException(\n+                        repositoryName,\n+                        \"Wrapped DEK [\" + DEKId + \"] has smaller length [\" + bytesRead + \"] \" + \"than expected\"\n+                    );\n+                }\n+                if (encryptedDEKInputStream.read() != -1) {\n+                    throw new RepositoryException(repositoryName, \"Wrapped DEK [\" + DEKId + \"] is larger than expected\");\n+                }\n+            } catch (NoSuchFileException e) {\n+                // do NOT throw IOException when the DEK does not exist, as this is a decryption problem, and IOExceptions\n+                // can move the repository in the corrupted state\n+                throw new ElasticsearchException(\n+                    \"Failure to read and decrypt DEK [\"\n+                        + DEKId\n+                        + \"] from \"\n+                        + DEKBlobContainer.path()\n+                        + \". Most likely the repository password is incorrect, where previous \"\n+                        + \"snapshots have used a different password.\",\n+                    e\n+                );\n+            }\n+            logger.trace(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] successfully read DEK [{}] from path {} {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath,\n+                    KEK.v1()\n+                )\n+            );\n+            try {\n+                final SecretKey DEK = AESKeyUtils.unwrap(KEK.v2(), encryptedDEKBytes);\n+                logger.debug(\n+                    () -> new ParameterizedMessage(\n+                        \"Repository [{}] successfully loaded DEK [{}] from path {} {}\",\n+                        repositoryName,\n+                        DEKId,\n+                        DEKBlobPath,\n+                        KEK.v1()\n+                    )\n+                );\n+                return DEK;\n+            } catch (GeneralSecurityException e) {\n+                throw new RepositoryException(\n+                    repositoryName,\n+                    \"Failure to AES unwrap the DEK [\"\n+                        + DEKId\n+                        + \"]. \"\n+                        + \"Most likely the encryption metadata in the repository has been corrupted\",\n+                    e\n+                );\n+            }\n+        }\n+\n+        // pkg-private for tests\n+        void storeDEK(String DEKId, SecretKey DEK) throws IOException {\n+            final BlobPath DEKBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(DEKId);\n+            logger.debug(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] storing wrapped DEK [{}] under blob path {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath\n+                )\n+            );\n+            final BlobContainer DEKBlobContainer = delegatedBlobStore.blobContainer(DEKBlobPath);\n+            final Tuple<String, SecretKey> KEK = getKEKforDEK.apply(DEKId);\n+            logger.trace(\n+                () -> new ParameterizedMessage(\"Repository [{}] using KEK [{}] to wrap DEK [{}]\", repositoryName, KEK.v1(), DEKId)\n+            );\n+            final byte[] encryptedDEKBytes;\n+            try {\n+                encryptedDEKBytes = AESKeyUtils.wrap(KEK.v2(), DEK);\n+                if (encryptedDEKBytes.length != AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES) {\n+                    throw new RepositoryException(\n+                        repositoryName,\n+                        \"Wrapped DEK [\" + DEKId + \"] has unexpected length [\" + encryptedDEKBytes.length + \"]\"\n+                    );\n+                }\n+            } catch (GeneralSecurityException e) {\n+                // throw unchecked ElasticsearchException; IOExceptions are interpreted differently and can move the repository in the\n+                // corrupted state\n+                throw new RepositoryException(repositoryName, \"Failure to AES wrap the DEK [\" + DEKId + \"]\", e);\n+            }\n+            logger.trace(() -> new ParameterizedMessage(\"Repository [{}] successfully wrapped DEK [{}]\", repositoryName, DEKId));\n+            try (InputStream encryptedDEKInputStream = new ByteArrayInputStream(encryptedDEKBytes)) {\n+                DEKBlobContainer.writeBlobAtomic(KEK.v1(), encryptedDEKInputStream, encryptedDEKBytes.length, true);\n+            }\n+            logger.debug(\n+                () -> new ParameterizedMessage(\n+                    \"Repository [{}] successfully stored DEK [{}] under path {} {}\",\n+                    repositoryName,\n+                    DEKId,\n+                    DEKBlobPath,\n+                    KEK.v1()\n+                )\n+            );\n+        }\n+\n+        @Override\n+        public BlobContainer blobContainer(BlobPath path) {\n+            final BlobContainer delegatedBlobContainer = delegatedBlobStore.blobContainer(delegatedBasePath.append(path));\n+            return new EncryptedBlobContainer(path, repositoryName, delegatedBlobContainer, singleUseDEKSupplier, this::getDEKById);\n+        }\n+\n+        @Override\n+        public void close() {\n+            // do NOT close delegatedBlobStore; it will be closed when the inner delegatedRepository is closed\n+        }\n+\n+    }\n+\n+    private static final class EncryptedBlobContainer extends AbstractBlobContainer {\n+        private final String repositoryName;\n+        private final BlobContainer delegatedBlobContainer;\n+        // supplier for the DEK used for encryption (snapshot)\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+        // retrieves the DEK required for decryption (restore)\n+        private final CheckedFunction<String, SecretKey, IOException> getDEKById;\n+\n+        EncryptedBlobContainer(\n+            BlobPath path, // this path contains the {@code EncryptedRepository#basePath} which, importantly, is empty\n+            String repositoryName,\n+            BlobContainer delegatedBlobContainer,\n+            CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier,\n+            CheckedFunction<String, SecretKey, IOException> getDEKById\n+        ) {\n+            super(path);\n+            this.repositoryName = repositoryName;\n+            if (DEK_ROOT_CONTAINER.equals(path.getRootPath())) {\n+                throw new RepositoryException(\n+                    repositoryName,\n+                    \"Unexpected internal error\",\n+                    new IllegalArgumentException(\"Cannot descend into the DEK blob container \" + path)\n+                );\n+            }\n+            this.delegatedBlobContainer = delegatedBlobContainer;\n+            this.singleUseDEKSupplier = singleUseDEKSupplier;\n+            this.getDEKById = getDEKById;\n+        }\n+\n+        /**\n+         * Returns a new {@link InputStream} for the given {@code blobName} that can be used to read the contents of the blob.\n+         * The returned {@code InputStream} transparently handles the decryption of the blob contents, by first working out\n+         * the blob name of the associated DEK id, reading and decrypting the DEK (given the repository secret key, unless the DEK is\n+         * already cached because other blobs required it before), and lastly reading and decrypting the data blob,\n+         * in a streaming fashion, by employing the {@link DecryptionPacketsInputStream}.\n+         * The {@code DecryptionPacketsInputStream} does not return un-authenticated data.\n+         *\n+         * @param   blobName The name of the blob to get an {@link InputStream} for.\n+         */\n+        @Override\n+        public InputStream readBlob(String blobName) throws IOException {\n+            // This MIGHT require two concurrent readBlob connections if the DEK is not already in the cache and if the encrypted blob\n+            // is large enough so that the underlying network library keeps the connection open after reading the prepended DEK ID.\n+            // Arguably this is a problem only under lab conditions, when the storage service is saturated only by the first read\n+            // connection of the pair, so that the second read connection (for the DEK) can not be fulfilled.\n+            // In this case the second connection will time-out which will trigger the closing of the first one, therefore\n+            // allowing other pair connections to complete.\n+            // In this situation the restore process should slowly make headway, albeit under read-timeout exceptions\n+            final InputStream encryptedDataInputStream = delegatedBlobContainer.readBlob(blobName);\n+            try {\n+                // read the DEK Id (fixed length) which is prepended to the encrypted blob\n+                final byte[] DEKIdBytes = new byte[DEK_ID_LENGTH];\n+                final int bytesRead = Streams.readFully(encryptedDataInputStream, DEKIdBytes);\n+                if (bytesRead != DEK_ID_LENGTH) {\n+                    throw new RepositoryException(repositoryName, \"The encrypted blob [\" + blobName + \"] is too small [\" + bytesRead + \"]\");\n+                }\n+                final String DEKId = new String(DEKIdBytes, StandardCharsets.UTF_8);\n+                // might open a connection to read and decrypt the DEK, but most likely it will be served from cache\n+                final SecretKey DEK = getDEKById.apply(DEKId);\n+                // read and decrypt the rest of the blob\n+                return new DecryptionPacketsInputStream(encryptedDataInputStream, DEK, PACKET_LENGTH_IN_BYTES);\n+            } catch (Exception e) {\n+                try {\n+                    encryptedDataInputStream.close();\n+                } catch (IOException closeEx) {\n+                    e.addSuppressed(closeEx);\n+                }\n+                throw e;\n+            }\n+        }\n+\n+        /**\n+         * Reads the blob content from the input stream and writes it to the container in a new blob with the given name.\n+         * If {@code failIfAlreadyExists} is {@code true} and a blob with the same name already exists, the write operation will fail;\n+         * otherwise, if {@code failIfAlreadyExists} is {@code false} the blob is overwritten.\n+         * The contents are encrypted in a streaming fashion. The DEK (encryption key) is randomly generated and reused for encrypting\n+         * subsequent blobs such that the same IV is not reused together with the same key.\n+         * The DEK encryption key is separately stored in a different blob, which is encrypted with the repository key.\n+         *\n+         * @param   blobName\n+         *          The name of the blob to write the contents of the input stream to.\n+         * @param   inputStream\n+         *          The input stream from which to retrieve the bytes to write to the blob.\n+         * @param   blobSize\n+         *          The size of the blob to be written, in bytes. The actual number of bytes written to the storage service is larger\n+         *          because of encryption and authentication overhead. It is implementation dependent whether this value is used\n+         *          in writing the blob to the repository.\n+         * @param   failIfAlreadyExists\n+         *          whether to throw a FileAlreadyExistsException if the given blob already exists\n+         */\n+        @Override\n+        public void writeBlob(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) throws IOException {\n+            // reuse, but possibly generate and store a new DEK\n+            final SingleUseKey singleUseNonceAndDEK = singleUseDEKSupplier.get();\n+            final byte[] DEKIdBytes = singleUseNonceAndDEK.getKeyId().getBytes(StandardCharsets.UTF_8);\n+            if (DEKIdBytes.length != DEK_ID_LENGTH) {\n+                throw new RepositoryException(\n+                    repositoryName,\n+                    \"Unexpected internal error\",\n+                    new IllegalStateException(\"Unexpected DEK Id length [\" + DEKIdBytes.length + \"]\")\n+                );\n+            }\n+            final long encryptedBlobSize = getEncryptedBlobByteLength(blobSize);\n+            try (\n+                InputStream encryptedInputStream = ChainingInputStream.chain(\n+                    new ByteArrayInputStream(DEKIdBytes),\n+                    new EncryptionPacketsInputStream(\n+                        inputStream,\n+                        singleUseNonceAndDEK.getKey(),\n+                        singleUseNonceAndDEK.getNonce(),\n+                        PACKET_LENGTH_IN_BYTES\n+                    )\n+                )\n+            ) {\n+                delegatedBlobContainer.writeBlob(blobName, encryptedInputStream, encryptedBlobSize, failIfAlreadyExists);\n+            }\n+        }\n+\n+        @Override\n+        public void writeBlobAtomic(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists)\n+            throws IOException {\n+            // the encrypted repository does not offer an alternative implementation for atomic writes\n+            // fallback to regular write\n+            writeBlob(blobName, inputStream, blobSize, failIfAlreadyExists);\n+        }\n+\n+        @Override\n+        public DeleteResult delete() throws IOException {\n+            return delegatedBlobContainer.delete();\n+        }\n+\n+        @Override\n+        public void deleteBlobsIgnoringIfNotExists(List<String> blobNames) throws IOException {\n+            delegatedBlobContainer.deleteBlobsIgnoringIfNotExists(blobNames);\n+        }\n+\n+        @Override\n+        public Map<String, BlobMetaData> listBlobs() throws IOException {\n+            return delegatedBlobContainer.listBlobs();\n+        }\n+\n+        @Override\n+        public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {\n+            return delegatedBlobContainer.listBlobsByPrefix(blobNamePrefix);\n+        }\n+\n+        @Override\n+        public Map<String, BlobContainer> children() throws IOException {\n+            final Map<String, BlobContainer> childEncryptedBlobContainers = delegatedBlobContainer.children();\n+            final Map<String, BlobContainer> resultBuilder = new HashMap<>(childEncryptedBlobContainers.size());\n+            for (Map.Entry<String, BlobContainer> childBlobContainer : childEncryptedBlobContainers.entrySet()) {\n+                if (childBlobContainer.getKey().equals(DEK_ROOT_CONTAINER) && path().isEmpty()) {\n+                    // do not descend into the DEK blob container\n+                    continue;\n+                }\n+                // get an encrypted blob container for each child\n+                // Note that the encryption metadata blob container might be missing\n+                resultBuilder.put(\n+                    childBlobContainer.getKey(),\n+                    new EncryptedBlobContainer(\n+                        path().add(childBlobContainer.getKey()),\n+                        repositoryName,\n+                        childBlobContainer.getValue(),\n+                        singleUseDEKSupplier,\n+                        getDEKById\n+                    )\n+                );\n+            }\n+            return Map.copyOf(resultBuilder);\n+        }\n+", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1NDA5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402154093", "bodyText": "Why not HDFS? :)", "author": "original-brownbear", "createdAt": "2020-04-02T08:53:34Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepositoryPlugin.java", "diffHunk": "@@ -6,26 +6,153 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.metadata.RepositoryMetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.SecureSetting;\n+import org.elasticsearch.common.settings.SecureString;\n import org.elasticsearch.common.settings.Setting;\n-import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.plugins.ReloadablePlugin;\n import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.xpack.core.XPackPlugin;\n \n+import java.security.GeneralSecurityException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n \n-public final class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin, ReloadablePlugin {\n+public class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepositoryPlugin.class);\n+    static final String REPOSITORY_TYPE_NAME = \"encrypted\";\n+    static final List<String> SUPPORTED_ENCRYPTED_TYPE_NAMES = Arrays.asList(\"fs\", \"gcs\", \"azure\", \"s3\");", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU4MjQ0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402582443", "bodyText": "No particular reason, I have simply not tested it. I understand I should look into it and have a test for it as well?", "author": "albertzaharovits", "createdAt": "2020-04-02T20:11:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1NDA5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc2NTA0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402765042", "bodyText": "Let's not hold this work (and especially this PR) up on that :) I think in the final version we should maybe just support any BlobStoreRepository? I'd say let's just add a TODO?", "author": "original-brownbear", "createdAt": "2020-04-03T06:37:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1NDA5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjk1OTc4Nw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402959787", "bodyText": "Sounds good adding the TODO, thanks!", "author": "albertzaharovits", "createdAt": "2020-04-03T12:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1NDA5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1NzA5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402157097", "bodyText": "I think this should just be a BytesReference a explained above", "author": "original-brownbear", "createdAt": "2020-04-02T08:58:10Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/SingleUseKey.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.repositories.encrypted;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.collect.Tuple;\n+\n+import javax.crypto.SecretKey;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Container class for a {@code SecretKey} with a unique identifier, and a 4-byte wide {@code Integer} nonce, that can be used for a\n+ * single encryption operation. Use {@link #createSingleUseKeySupplier(CheckedSupplier)} to obtain a {@code Supplier} that returns\n+ * a new {@link SingleUseKey} instance on every invocation. The number of unique {@code SecretKey}s (and their associated identifiers)\n+ * generated is minimized and, at the same time, ensuring that a given {@code nonce} is not reused with the same key.\n+ */\n+final class SingleUseKey {\n+    private static final Logger logger = LogManager.getLogger(SingleUseKey.class);\n+    static final int MIN_NONCE = Integer.MIN_VALUE;\n+    static final int MAX_NONCE = Integer.MAX_VALUE;\n+    private static final int MAX_ATTEMPTS = 9;\n+    private static final SingleUseKey EXPIRED_KEY = new SingleUseKey(null, null, MAX_NONCE);\n+\n+    private final String keyId;\n+    private final SecretKey key;\n+    private final Integer nonce;\n+\n+    // for tests use only!\n+    SingleUseKey(String KeyId, SecretKey Key, Integer nonce) {\n+        this.keyId = KeyId;\n+        this.key = Key;\n+        this.nonce = nonce;\n+    }\n+\n+    public String getKeyId() {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1NzQyOA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r402157428", "bodyText": "Can this ever be null?", "author": "original-brownbear", "createdAt": "2020-04-02T08:58:41Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/SingleUseKey.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.repositories.encrypted;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.collect.Tuple;\n+\n+import javax.crypto.SecretKey;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Container class for a {@code SecretKey} with a unique identifier, and a 4-byte wide {@code Integer} nonce, that can be used for a\n+ * single encryption operation. Use {@link #createSingleUseKeySupplier(CheckedSupplier)} to obtain a {@code Supplier} that returns\n+ * a new {@link SingleUseKey} instance on every invocation. The number of unique {@code SecretKey}s (and their associated identifiers)\n+ * generated is minimized and, at the same time, ensuring that a given {@code nonce} is not reused with the same key.\n+ */\n+final class SingleUseKey {\n+    private static final Logger logger = LogManager.getLogger(SingleUseKey.class);\n+    static final int MIN_NONCE = Integer.MIN_VALUE;\n+    static final int MAX_NONCE = Integer.MAX_VALUE;\n+    private static final int MAX_ATTEMPTS = 9;\n+    private static final SingleUseKey EXPIRED_KEY = new SingleUseKey(null, null, MAX_NONCE);\n+\n+    private final String keyId;\n+    private final SecretKey key;\n+    private final Integer nonce;\n+\n+    // for tests use only!\n+    SingleUseKey(String KeyId, SecretKey Key, Integer nonce) {\n+        this.keyId = KeyId;\n+        this.key = Key;\n+        this.nonce = nonce;\n+    }\n+\n+    public String getKeyId() {\n+        return keyId;\n+    }\n+\n+    public SecretKey getKey() {\n+        return key;\n+    }\n+\n+    public Integer getNonce() {", "originalCommit": "36d152a68924a4149d4b407017b5fab50c5d7979", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzY5NjM5OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r403696398", "bodyText": "No, good catch.", "author": "albertzaharovits", "createdAt": "2020-04-05T12:39:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1NzQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5Nzc4NA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r409297784", "bodyText": "Can you clarify why we don't validate the per-packet nonce anymore?", "author": "tvernum", "createdAt": "2020-04-16T05:51:37Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/DecryptionPacketsInputStream.java", "diffHunk": "@@ -124,19 +123,19 @@ public void reset() throws IOException {\n         throw new IOException(\"Mark/reset not supported\");\n     }\n \n+    @Override\n+    public void close() throws IOException {\n+        IOUtils.close(super::close, source);\n+    }\n+\n     private int decrypt(PrefixInputStream packetInputStream) throws IOException {\n         // read only the IV prefix into the packet buffer\n         int ivLength = packetInputStream.readNBytes(packetBuffer, 0, GCM_IV_LENGTH_IN_BYTES);\n         if (ivLength != GCM_IV_LENGTH_IN_BYTES) {\n             throw new IOException(\"Packet heading IV error. Unexpected length [\" + ivLength + \"].\");\n         }\n-        // extract the nonce and the counter from the packet IV\n-        ByteBuffer ivBuffer = ByteBuffer.wrap(packetBuffer, 0, GCM_IV_LENGTH_IN_BYTES).order(ByteOrder.LITTLE_ENDIAN);\n-        int packetIvNonce = ivBuffer.getInt(0);\n-        long packetIvCounter = ivBuffer.getLong(Integer.BYTES);\n-        if (packetIvNonce != nonce) {\n-            throw new IOException(\"Packet nonce mismatch. Expecting [\" + nonce + \"], but got [\" + packetIvNonce + \"].\");\n-        }", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTYxNTMyMw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419615323", "bodyText": "Removing the explicit nonce check was necessary in order to support using the same DEK for multiple encrypted blobs.\nPreviously, every blob had its own associated metadata blob, where we stored the nonce, which was explicitly verified here, during decryption. But now there is no place where we can associate metadata for every blob (and this is by design, because associating something to every blob incurs at least one API call and it's difficult to update).\nThere is no security problem if we're not explicitly checking the nonce, because the nonce is part of every packet's IV and is hence validated when the packet's authn tag is validated, implicitly by the GCM algorithm.", "author": "albertzaharovits", "createdAt": "2020-05-04T17:49:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5Nzc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0MTI3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432241273", "bodyText": "That makes sense.\nIs there still a reason why we store it in the stream then?", "author": "tvernum", "createdAt": "2020-05-29T03:51:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5Nzc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5ODExNg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r409298116", "bodyText": "The reason for the offset (Integer.BYTES) is a bit less clear now that we don't read the nonce, can we add a comment or something to explain it?", "author": "tvernum", "createdAt": "2020-04-16T05:52:35Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/DecryptionPacketsInputStream.java", "diffHunk": "@@ -124,19 +123,19 @@ public void reset() throws IOException {\n         throw new IOException(\"Mark/reset not supported\");\n     }\n \n+    @Override\n+    public void close() throws IOException {\n+        IOUtils.close(super::close, source);\n+    }\n+\n     private int decrypt(PrefixInputStream packetInputStream) throws IOException {\n         // read only the IV prefix into the packet buffer\n         int ivLength = packetInputStream.readNBytes(packetBuffer, 0, GCM_IV_LENGTH_IN_BYTES);\n         if (ivLength != GCM_IV_LENGTH_IN_BYTES) {\n             throw new IOException(\"Packet heading IV error. Unexpected length [\" + ivLength + \"].\");\n         }\n-        // extract the nonce and the counter from the packet IV\n-        ByteBuffer ivBuffer = ByteBuffer.wrap(packetBuffer, 0, GCM_IV_LENGTH_IN_BYTES).order(ByteOrder.LITTLE_ENDIAN);\n-        int packetIvNonce = ivBuffer.getInt(0);\n-        long packetIvCounter = ivBuffer.getLong(Integer.BYTES);\n-        if (packetIvNonce != nonce) {\n-            throw new IOException(\"Packet nonce mismatch. Expecting [\" + nonce + \"], but got [\" + packetIvNonce + \"].\");\n-        }\n+        // extract the counter from the packet IV\n+        long packetIvCounter = ByteUtils.readLongLE(packetBuffer, Integer.BYTES);", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTYyNzAyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419627021", "bodyText": "Fair point, it now looks like this:\n        // extract the counter from the packet IV and validate it (that the packet is in order)\n        // skips the first 4 bytes in the packet IV, which contain the encryption nonce, which cannot be explicitly validated\n        // because the nonce is not passed in during decryption, but it is implicitly because it is part of the IV,\n        // when GCM validates the packet authn tag\n        long packetIvCounter = ByteUtils.readLongLE(packetBuffer, Integer.BYTES);\n        if (packetIvCounter != counter) {\n            throw new IOException(\"Packet counter mismatch. Expecting [\" + counter + \"], but got [\" + packetIvCounter + \"].\");\n        }", "author": "albertzaharovits", "createdAt": "2020-05-04T18:07:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5ODExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMwOTIzOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r409309239", "bodyText": "Won't this plugin be loaded on every node of the default distribution? That means ever non-platinum node prints out this warning at every startup. That doesn't sound right to me.", "author": "tvernum", "createdAt": "2020-04-16T06:23:26Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepositoryPlugin.java", "diffHunk": "@@ -6,25 +6,153 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.SecureSetting;\n+import org.elasticsearch.common.settings.SecureString;\n import org.elasticsearch.common.settings.Setting;\n-import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.plugins.ReloadablePlugin;\n import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.xpack.core.XPackPlugin;\n \n+import java.security.GeneralSecurityException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n \n-public final class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin, ReloadablePlugin {\n+public class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepositoryPlugin.class);\n+    static final String REPOSITORY_TYPE_NAME = \"encrypted\";\n+    // TODO add at least hdfs, and investigate supporting all `BlobStoreRepository` implementations\n+    static final List<String> SUPPORTED_ENCRYPTED_TYPE_NAMES = Arrays.asList(\"fs\", \"gcs\", \"azure\", \"s3\");\n+    static final Setting.AffixSetting<SecureString> ENCRYPTION_PASSWORD_SETTING = Setting.affixKeySetting(\n+        \"repository.encrypted.\",\n+        \"password\",\n+        key -> SecureSetting.secureString(key, null)\n+    );\n+    static final Setting<String> DELEGATE_TYPE_SETTING = Setting.simpleString(\"delegate_type\", \"\");\n+    static final Setting<String> PASSWORD_NAME_SETTING = Setting.simpleString(\"password_name\", \"\");\n \n-    public EncryptedRepositoryPlugin(final Settings settings) {}\n+    // \"protected\" because it is overloaded for tests\n+    protected XPackLicenseState getLicenseState() {\n+        return XPackPlugin.getSharedLicenseState();\n+    }\n+\n+    public EncryptedRepositoryPlugin() {\n+        if (false == getLicenseState().isEncryptedSnapshotAllowed()) {\n+            logger.warn(\n+                \"Snapshotting to an encrypted repository is not permitted for the current license.\"\n+                    + \"All the other operations over the encrypted repository, eg. restore, work without restrictions.\",\n+                LicenseUtils.newComplianceException(\"encrypted snapshots\")\n+            );", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzMjEwNw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419432107", "bodyText": "Yeah, you're right, this is silly. I suppose I wanted to warn users that encrypted repos are only partially supported, before actually creating them, but that's silly because you cannot tell if encrypted repos are gonna be used at all.", "author": "albertzaharovits", "createdAt": "2020-05-04T13:24:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMwOTIzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMxMDMyMg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r409310322", "bodyText": "Why a char[] rather than SecureString ?", "author": "tvernum", "createdAt": "2020-04-16T06:26:12Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepositoryPlugin.java", "diffHunk": "@@ -6,25 +6,153 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.SecureSetting;\n+import org.elasticsearch.common.settings.SecureString;\n import org.elasticsearch.common.settings.Setting;\n-import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.plugins.ReloadablePlugin;\n import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.xpack.core.XPackPlugin;\n \n+import java.security.GeneralSecurityException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n \n-public final class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin, ReloadablePlugin {\n+public class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepositoryPlugin.class);\n+    static final String REPOSITORY_TYPE_NAME = \"encrypted\";\n+    // TODO add at least hdfs, and investigate supporting all `BlobStoreRepository` implementations\n+    static final List<String> SUPPORTED_ENCRYPTED_TYPE_NAMES = Arrays.asList(\"fs\", \"gcs\", \"azure\", \"s3\");\n+    static final Setting.AffixSetting<SecureString> ENCRYPTION_PASSWORD_SETTING = Setting.affixKeySetting(\n+        \"repository.encrypted.\",\n+        \"password\",\n+        key -> SecureSetting.secureString(key, null)\n+    );\n+    static final Setting<String> DELEGATE_TYPE_SETTING = Setting.simpleString(\"delegate_type\", \"\");\n+    static final Setting<String> PASSWORD_NAME_SETTING = Setting.simpleString(\"password_name\", \"\");\n \n-    public EncryptedRepositoryPlugin(final Settings settings) {}\n+    // \"protected\" because it is overloaded for tests\n+    protected XPackLicenseState getLicenseState() {\n+        return XPackPlugin.getSharedLicenseState();\n+    }\n+\n+    public EncryptedRepositoryPlugin() {\n+        if (false == getLicenseState().isEncryptedSnapshotAllowed()) {\n+            logger.warn(\n+                \"Snapshotting to an encrypted repository is not permitted for the current license.\"\n+                    + \"All the other operations over the encrypted repository, eg. restore, work without restrictions.\",\n+                LicenseUtils.newComplianceException(\"encrypted snapshots\")\n+            );\n+        }\n+    }\n \n     @Override\n     public List<Setting<?>> getSettings() {\n-        return List.of();\n+        return List.of(ENCRYPTION_PASSWORD_SETTING);\n     }\n \n     @Override\n-    public void reload(Settings settings) {\n-        // Secure settings should be readable inside this method.\n+    public Map<String, Repository.Factory> getRepositories(Environment env, NamedXContentRegistry registry, ClusterService clusterService) {\n+        // load all the passwords from the keystore in memory because the keystore is not readable when the repository is created\n+        final Map<String, char[]> repositoryPasswordsMapBuilder = new HashMap<>();", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY1MjUzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419652531", "bodyText": "The repo passwords must be left in the memory at all times, and I don't like how simple it is to blank them out when using the SecureString. I have nonetheless pushed 835b2db to switch from char[] to SecureString.", "author": "albertzaharovits", "createdAt": "2020-05-04T18:50:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMxMDMyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMxMjk3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r409312976", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                throw new IllegalArgumentException(\"Unsupported delegate repository type [\" + DELEGATE_TYPE_SETTING.getKey() + \"]\");\n          \n          \n            \n                                throw new IllegalArgumentException(\"Unsupported delegate repository type [\" + delegateType \n          \n          \n            \n                                    + \"] for setting [\" + DELEGATE_TYPE_SETTING.getKey() + \"]\");", "author": "tvernum", "createdAt": "2020-04-16T06:32:52Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepositoryPlugin.java", "diffHunk": "@@ -6,25 +6,153 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.SecureSetting;\n+import org.elasticsearch.common.settings.SecureString;\n import org.elasticsearch.common.settings.Setting;\n-import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.plugins.ReloadablePlugin;\n import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.xpack.core.XPackPlugin;\n \n+import java.security.GeneralSecurityException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n \n-public final class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin, ReloadablePlugin {\n+public class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepositoryPlugin.class);\n+    static final String REPOSITORY_TYPE_NAME = \"encrypted\";\n+    // TODO add at least hdfs, and investigate supporting all `BlobStoreRepository` implementations\n+    static final List<String> SUPPORTED_ENCRYPTED_TYPE_NAMES = Arrays.asList(\"fs\", \"gcs\", \"azure\", \"s3\");\n+    static final Setting.AffixSetting<SecureString> ENCRYPTION_PASSWORD_SETTING = Setting.affixKeySetting(\n+        \"repository.encrypted.\",\n+        \"password\",\n+        key -> SecureSetting.secureString(key, null)\n+    );\n+    static final Setting<String> DELEGATE_TYPE_SETTING = Setting.simpleString(\"delegate_type\", \"\");\n+    static final Setting<String> PASSWORD_NAME_SETTING = Setting.simpleString(\"password_name\", \"\");\n \n-    public EncryptedRepositoryPlugin(final Settings settings) {}\n+    // \"protected\" because it is overloaded for tests\n+    protected XPackLicenseState getLicenseState() {\n+        return XPackPlugin.getSharedLicenseState();\n+    }\n+\n+    public EncryptedRepositoryPlugin() {\n+        if (false == getLicenseState().isEncryptedSnapshotAllowed()) {\n+            logger.warn(\n+                \"Snapshotting to an encrypted repository is not permitted for the current license.\"\n+                    + \"All the other operations over the encrypted repository, eg. restore, work without restrictions.\",\n+                LicenseUtils.newComplianceException(\"encrypted snapshots\")\n+            );\n+        }\n+    }\n \n     @Override\n     public List<Setting<?>> getSettings() {\n-        return List.of();\n+        return List.of(ENCRYPTION_PASSWORD_SETTING);\n     }\n \n     @Override\n-    public void reload(Settings settings) {\n-        // Secure settings should be readable inside this method.\n+    public Map<String, Repository.Factory> getRepositories(Environment env, NamedXContentRegistry registry, ClusterService clusterService) {\n+        // load all the passwords from the keystore in memory because the keystore is not readable when the repository is created\n+        final Map<String, char[]> repositoryPasswordsMapBuilder = new HashMap<>();\n+        for (String passwordName : ENCRYPTION_PASSWORD_SETTING.getNamespaces(env.settings())) {\n+            Setting<SecureString> passwordSetting = ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(passwordName);\n+            SecureString encryptionPassword = passwordSetting.get(env.settings());\n+            repositoryPasswordsMapBuilder.put(passwordName, encryptionPassword.getChars());\n+            logger.debug(\"Loaded repository password [{}] from the node keystore\", passwordName);\n+        }\n+        final Map<String, char[]> repositoryPasswordsMap = Map.copyOf(repositoryPasswordsMapBuilder);\n+\n+        return Collections.singletonMap(REPOSITORY_TYPE_NAME, new Repository.Factory() {\n+\n+            @Override\n+            public Repository create(RepositoryMetadata metadata) {\n+                throw new UnsupportedOperationException();\n+            }\n+\n+            @Override\n+            public Repository create(RepositoryMetadata metadata, Function<String, Repository.Factory> typeLookup) throws Exception {\n+                final String delegateType = DELEGATE_TYPE_SETTING.get(metadata.settings());\n+                if (Strings.hasLength(delegateType) == false) {\n+                    throw new IllegalArgumentException(\"Repository setting [\" + DELEGATE_TYPE_SETTING.getKey() + \"] must be set\");\n+                }\n+                if (REPOSITORY_TYPE_NAME.equals(delegateType)) {\n+                    throw new IllegalArgumentException(\n+                        \"Cannot encrypt an already encrypted repository. [\"\n+                            + DELEGATE_TYPE_SETTING.getKey()\n+                            + \"] must not be equal to [\"\n+                            + REPOSITORY_TYPE_NAME\n+                            + \"]\"\n+                    );\n+                }\n+                final Repository.Factory factory = typeLookup.apply(delegateType);\n+                if (null == factory || false == SUPPORTED_ENCRYPTED_TYPE_NAMES.contains(delegateType)) {\n+                    throw new IllegalArgumentException(\"Unsupported delegate repository type [\" + DELEGATE_TYPE_SETTING.getKey() + \"]\");", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQzNTU5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419435591", "bodyText": "Thanks!", "author": "albertzaharovits", "createdAt": "2020-05-04T13:28:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMxMjk3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMxNDExOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r409314119", "bodyText": "We can come back to it later, but I feel like this setting name implies that the variable part should be the name of the repo, but it's actually the name of the password on the repo.\nI would be more inclined to have repostory.encrypted.password.<name> but it's not a priority for this PR.", "author": "tvernum", "createdAt": "2020-04-16T06:35:39Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepositoryPlugin.java", "diffHunk": "@@ -6,25 +6,153 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.SecureSetting;\n+import org.elasticsearch.common.settings.SecureString;\n import org.elasticsearch.common.settings.Setting;\n-import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.plugins.ReloadablePlugin;\n import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.xpack.core.XPackPlugin;\n \n+import java.security.GeneralSecurityException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n \n-public final class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin, ReloadablePlugin {\n+public class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepositoryPlugin.class);\n+    static final String REPOSITORY_TYPE_NAME = \"encrypted\";\n+    // TODO add at least hdfs, and investigate supporting all `BlobStoreRepository` implementations\n+    static final List<String> SUPPORTED_ENCRYPTED_TYPE_NAMES = Arrays.asList(\"fs\", \"gcs\", \"azure\", \"s3\");\n+    static final Setting.AffixSetting<SecureString> ENCRYPTION_PASSWORD_SETTING = Setting.affixKeySetting(\n+        \"repository.encrypted.\",\n+        \"password\",\n+        key -> SecureSetting.secureString(key, null)", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY2MTY0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419661642", "bodyText": "Yeah, this makes total sense! Thanks.\nI have updated the issue description here #41910 (comment) to include a small backlog of things we need to get to before the feature branch is merged in master.", "author": "albertzaharovits", "createdAt": "2020-05-04T19:05:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTMxNDExOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzNzc2OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r416337768", "bodyText": "A few small suggestions:\n\nI think printing the license mode will assist understanding the message.\nIt needs whitespace between the joined sentences\nOur typical idiom is to use parameters rather than string concatenation.\n\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warn(\n          \n          \n            \n                                    \"Encrypted snapshots are not allowed for the currently installed license.\"\n          \n          \n            \n                                        + \"Snapshots to the [\"\n          \n          \n            \n                                        + metadata.name()\n          \n          \n            \n                                        + \"] encrypted repository are not permitted.\"\n          \n          \n            \n                                        + \"All the other operations, including restore, work without restrictions.\",\n          \n          \n            \n                                    LicenseUtils.newComplianceException(\"encrypted snapshots\")\n          \n          \n            \n                                logger.warn(new ParameterizedMessage(\n          \n          \n            \n                                    \"Encrypted snapshots are not allowed for the currently installed license [{}].\"\n          \n          \n            \n                                        + \" Snapshots to the [{}] encrypted repository are not permitted.\"\n          \n          \n            \n                                        + \" All the other operations, including restore, work without restrictions.\",\n          \n          \n            \n                                        getLicenseState().getOperationMode().description(), metadata.name()),\n          \n          \n            \n                                    LicenseUtils.newComplianceException(\"encrypted snapshots\")", "author": "tvernum", "createdAt": "2020-04-28T05:31:05Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepositoryPlugin.java", "diffHunk": "@@ -6,25 +6,153 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.SecureSetting;\n+import org.elasticsearch.common.settings.SecureString;\n import org.elasticsearch.common.settings.Setting;\n-import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.plugins.ReloadablePlugin;\n import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.xpack.core.XPackPlugin;\n \n+import java.security.GeneralSecurityException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n \n-public final class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin, ReloadablePlugin {\n+public class EncryptedRepositoryPlugin extends Plugin implements RepositoryPlugin {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepositoryPlugin.class);\n+    static final String REPOSITORY_TYPE_NAME = \"encrypted\";\n+    // TODO add at least hdfs, and investigate supporting all `BlobStoreRepository` implementations\n+    static final List<String> SUPPORTED_ENCRYPTED_TYPE_NAMES = Arrays.asList(\"fs\", \"gcs\", \"azure\", \"s3\");\n+    static final Setting.AffixSetting<SecureString> ENCRYPTION_PASSWORD_SETTING = Setting.affixKeySetting(\n+        \"repository.encrypted.\",\n+        \"password\",\n+        key -> SecureSetting.secureString(key, null)\n+    );\n+    static final Setting<String> DELEGATE_TYPE_SETTING = Setting.simpleString(\"delegate_type\", \"\");\n+    static final Setting<String> PASSWORD_NAME_SETTING = Setting.simpleString(\"password_name\", \"\");\n \n-    public EncryptedRepositoryPlugin(final Settings settings) {}\n+    // \"protected\" because it is overloaded for tests\n+    protected XPackLicenseState getLicenseState() {\n+        return XPackPlugin.getSharedLicenseState();\n+    }\n+\n+    public EncryptedRepositoryPlugin() {\n+        if (false == getLicenseState().isEncryptedSnapshotAllowed()) {\n+            logger.warn(\n+                \"Snapshotting to an encrypted repository is not permitted for the current license.\"\n+                    + \"All the other operations over the encrypted repository, eg. restore, work without restrictions.\",\n+                LicenseUtils.newComplianceException(\"encrypted snapshots\")\n+            );\n+        }\n+    }\n \n     @Override\n     public List<Setting<?>> getSettings() {\n-        return List.of();\n+        return List.of(ENCRYPTION_PASSWORD_SETTING);\n     }\n \n     @Override\n-    public void reload(Settings settings) {\n-        // Secure settings should be readable inside this method.\n+    public Map<String, Repository.Factory> getRepositories(Environment env, NamedXContentRegistry registry, ClusterService clusterService) {\n+        // load all the passwords from the keystore in memory because the keystore is not readable when the repository is created\n+        final Map<String, char[]> repositoryPasswordsMapBuilder = new HashMap<>();\n+        for (String passwordName : ENCRYPTION_PASSWORD_SETTING.getNamespaces(env.settings())) {\n+            Setting<SecureString> passwordSetting = ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(passwordName);\n+            SecureString encryptionPassword = passwordSetting.get(env.settings());\n+            repositoryPasswordsMapBuilder.put(passwordName, encryptionPassword.getChars());\n+            logger.debug(\"Loaded repository password [{}] from the node keystore\", passwordName);\n+        }\n+        final Map<String, char[]> repositoryPasswordsMap = Map.copyOf(repositoryPasswordsMapBuilder);\n+\n+        return Collections.singletonMap(REPOSITORY_TYPE_NAME, new Repository.Factory() {\n+\n+            @Override\n+            public Repository create(RepositoryMetadata metadata) {\n+                throw new UnsupportedOperationException();\n+            }\n+\n+            @Override\n+            public Repository create(RepositoryMetadata metadata, Function<String, Repository.Factory> typeLookup) throws Exception {\n+                final String delegateType = DELEGATE_TYPE_SETTING.get(metadata.settings());\n+                if (Strings.hasLength(delegateType) == false) {\n+                    throw new IllegalArgumentException(\"Repository setting [\" + DELEGATE_TYPE_SETTING.getKey() + \"] must be set\");\n+                }\n+                if (REPOSITORY_TYPE_NAME.equals(delegateType)) {\n+                    throw new IllegalArgumentException(\n+                        \"Cannot encrypt an already encrypted repository. [\"\n+                            + DELEGATE_TYPE_SETTING.getKey()\n+                            + \"] must not be equal to [\"\n+                            + REPOSITORY_TYPE_NAME\n+                            + \"]\"\n+                    );\n+                }\n+                final Repository.Factory factory = typeLookup.apply(delegateType);\n+                if (null == factory || false == SUPPORTED_ENCRYPTED_TYPE_NAMES.contains(delegateType)) {\n+                    throw new IllegalArgumentException(\"Unsupported delegate repository type [\" + DELEGATE_TYPE_SETTING.getKey() + \"]\");\n+                }\n+                final String repositoryPasswordName = PASSWORD_NAME_SETTING.get(metadata.settings());\n+                if (Strings.hasLength(repositoryPasswordName) == false) {\n+                    throw new IllegalArgumentException(\"Repository setting [\" + PASSWORD_NAME_SETTING.getKey() + \"] must be set\");\n+                }\n+                final char[] repositoryPassword = repositoryPasswordsMap.get(repositoryPasswordName);\n+                if (repositoryPassword == null) {\n+                    throw new IllegalArgumentException(\n+                        \"Secure setting [\"\n+                            + ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(repositoryPasswordName).getKey()\n+                            + \"] must be set\"\n+                    );\n+                }\n+                final Repository delegatedRepository = factory.create(\n+                    new RepositoryMetadata(metadata.name(), delegateType, metadata.settings())\n+                );\n+                if (false == (delegatedRepository instanceof BlobStoreRepository) || delegatedRepository instanceof EncryptedRepository) {\n+                    throw new IllegalArgumentException(\"Unsupported delegate repository type [\" + DELEGATE_TYPE_SETTING.getKey() + \"]\");\n+                }\n+                if (false == getLicenseState().isEncryptedSnapshotAllowed()) {\n+                    logger.warn(\n+                        \"Encrypted snapshots are not allowed for the currently installed license.\"\n+                            + \"Snapshots to the [\"\n+                            + metadata.name()\n+                            + \"] encrypted repository are not permitted.\"\n+                            + \"All the other operations, including restore, work without restrictions.\",\n+                        LicenseUtils.newComplianceException(\"encrypted snapshots\")", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0NzUwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419447509", "bodyText": "Thank you.", "author": "albertzaharovits", "createdAt": "2020-05-04T13:45:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzNzc2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzOTI4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r416339285", "bodyText": "If this is for tests only, why is it referenced in the class javadoc? I'm a little confused about how this class ought to be used.", "author": "tvernum", "createdAt": "2020-04-28T05:35:26Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/SingleUseKey.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.repositories.encrypted;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.collect.Tuple;\n+\n+import javax.crypto.SecretKey;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Container class for a {@code SecretKey} with a unique identifier, and a 4-byte wide {@code Integer} nonce, that can be used for a\n+ * single encryption operation. Use {@link #createSingleUseKeySupplier(CheckedSupplier)} to obtain a {@code Supplier} that returns\n+ * a new {@link SingleUseKey} instance on every invocation. The number of unique {@code SecretKey}s (and their associated identifiers)\n+ * generated is minimized and, at the same time, ensuring that a given {@code nonce} is not reused with the same key.\n+ */\n+final class SingleUseKey {\n+    private static final Logger logger = LogManager.getLogger(SingleUseKey.class);\n+    static final int MIN_NONCE = Integer.MIN_VALUE;\n+    static final int MAX_NONCE = Integer.MAX_VALUE;\n+    private static final int MAX_ATTEMPTS = 9;\n+    private static final SingleUseKey EXPIRED_KEY = new SingleUseKey(null, null, MAX_NONCE);\n+\n+    private final BytesReference keyId;\n+    private final SecretKey key;\n+    private final int nonce;\n+\n+    // for tests use only!\n+    SingleUseKey(BytesReference KeyId, SecretKey Key, int nonce) {\n+        this.keyId = KeyId;\n+        this.key = Key;\n+        this.nonce = nonce;\n+    }\n+\n+    public BytesReference getKeyId() {\n+        return keyId;\n+    }\n+\n+    public SecretKey getKey() {\n+        return key;\n+    }\n+\n+    public int getNonce() {\n+        return nonce;\n+    }\n+\n+    /**\n+     * Returns a {@code Supplier} of {@code SingleUseKey}s so that no two instances contain the same key and nonce pair.\n+     * A new key is generated only when the {@code nonce} space has been exhausted.\n+     */\n+    static <T extends Exception> CheckedSupplier<SingleUseKey, T> createSingleUseKeySupplier(\n+        CheckedSupplier<Tuple<BytesReference, SecretKey>, T> keyGenerator\n+    ) {\n+        final AtomicReference<SingleUseKey> keyCurrentlyInUse = new AtomicReference<>(EXPIRED_KEY);\n+        return createSingleUseKeySupplier(keyGenerator, keyCurrentlyInUse);\n+    }\n+\n+    // for tests use only", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ4NDQyMg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419484422", "bodyText": "This method is for test only.\nWhat is referred to in the javadoc is the method with only one parameter (but the same name).\nTo alleviate the confusion I have renamed the method that ought to only be used in tests from createSingleUseKeySupplier to internalSingleUseKeySupplier.", "author": "albertzaharovits", "createdAt": "2020-05-04T14:35:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzOTI4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzOTkxNA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r416339914", "bodyText": "This feels like overuse of a Tuple, at the very least can we have a javadoc explaining what the elements mean?", "author": "tvernum", "createdAt": "2020-04-28T05:37:08Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/SingleUseKey.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.repositories.encrypted;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.collect.Tuple;\n+\n+import javax.crypto.SecretKey;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Container class for a {@code SecretKey} with a unique identifier, and a 4-byte wide {@code Integer} nonce, that can be used for a\n+ * single encryption operation. Use {@link #createSingleUseKeySupplier(CheckedSupplier)} to obtain a {@code Supplier} that returns\n+ * a new {@link SingleUseKey} instance on every invocation. The number of unique {@code SecretKey}s (and their associated identifiers)\n+ * generated is minimized and, at the same time, ensuring that a given {@code nonce} is not reused with the same key.\n+ */\n+final class SingleUseKey {\n+    private static final Logger logger = LogManager.getLogger(SingleUseKey.class);\n+    static final int MIN_NONCE = Integer.MIN_VALUE;\n+    static final int MAX_NONCE = Integer.MAX_VALUE;\n+    private static final int MAX_ATTEMPTS = 9;\n+    private static final SingleUseKey EXPIRED_KEY = new SingleUseKey(null, null, MAX_NONCE);\n+\n+    private final BytesReference keyId;\n+    private final SecretKey key;\n+    private final int nonce;\n+\n+    // for tests use only!\n+    SingleUseKey(BytesReference KeyId, SecretKey Key, int nonce) {\n+        this.keyId = KeyId;\n+        this.key = Key;\n+        this.nonce = nonce;\n+    }\n+\n+    public BytesReference getKeyId() {\n+        return keyId;\n+    }\n+\n+    public SecretKey getKey() {\n+        return key;\n+    }\n+\n+    public int getNonce() {\n+        return nonce;\n+    }\n+\n+    /**\n+     * Returns a {@code Supplier} of {@code SingleUseKey}s so that no two instances contain the same key and nonce pair.\n+     * A new key is generated only when the {@code nonce} space has been exhausted.\n+     */\n+    static <T extends Exception> CheckedSupplier<SingleUseKey, T> createSingleUseKeySupplier(\n+        CheckedSupplier<Tuple<BytesReference, SecretKey>, T> keyGenerator\n+    ) {\n+        final AtomicReference<SingleUseKey> keyCurrentlyInUse = new AtomicReference<>(EXPIRED_KEY);\n+        return createSingleUseKeySupplier(keyGenerator, keyCurrentlyInUse);\n+    }\n+\n+    // for tests use only\n+    static <T extends Exception> CheckedSupplier<SingleUseKey, T> createSingleUseKeySupplier(\n+        CheckedSupplier<Tuple<BytesReference, SecretKey>, T> keyGenerator,", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ4OTAyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419489025", "bodyText": "I have pushed 3ff7857  which explains that the key generator generates a key and the key id.\nI pondered introducing another container class for the key and the key id only (no nonce), but backed down because the existence together with the container for key, key id and nonce (SingleUseKey) creates confusion about when to use which.", "author": "albertzaharovits", "createdAt": "2020-05-04T14:42:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzOTkxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjM0MTUzMg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r416341532", "bodyText": "This feels like a complicated way to produce a Supplier of a set of keys with a sequential nonce.\nDoes it really need the complexity of AtomicReference over a synchronized method?\nIt feels like the general concept could be written quite simply with a class that extends Supplier, has an internal counter and is synchronized on get (or uses an AtomicInteger for the counter).\nI presume there is a reason why you took this approach instead, but the code doesn't tell me why that is.", "author": "tvernum", "createdAt": "2020-04-28T05:41:46Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/SingleUseKey.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.repositories.encrypted;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.collect.Tuple;\n+\n+import javax.crypto.SecretKey;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Container class for a {@code SecretKey} with a unique identifier, and a 4-byte wide {@code Integer} nonce, that can be used for a\n+ * single encryption operation. Use {@link #createSingleUseKeySupplier(CheckedSupplier)} to obtain a {@code Supplier} that returns\n+ * a new {@link SingleUseKey} instance on every invocation. The number of unique {@code SecretKey}s (and their associated identifiers)\n+ * generated is minimized and, at the same time, ensuring that a given {@code nonce} is not reused with the same key.\n+ */\n+final class SingleUseKey {\n+    private static final Logger logger = LogManager.getLogger(SingleUseKey.class);\n+    static final int MIN_NONCE = Integer.MIN_VALUE;\n+    static final int MAX_NONCE = Integer.MAX_VALUE;\n+    private static final int MAX_ATTEMPTS = 9;\n+    private static final SingleUseKey EXPIRED_KEY = new SingleUseKey(null, null, MAX_NONCE);\n+\n+    private final BytesReference keyId;\n+    private final SecretKey key;\n+    private final int nonce;\n+\n+    // for tests use only!\n+    SingleUseKey(BytesReference KeyId, SecretKey Key, int nonce) {\n+        this.keyId = KeyId;\n+        this.key = Key;\n+        this.nonce = nonce;\n+    }\n+\n+    public BytesReference getKeyId() {\n+        return keyId;\n+    }\n+\n+    public SecretKey getKey() {\n+        return key;\n+    }\n+\n+    public int getNonce() {\n+        return nonce;\n+    }\n+\n+    /**\n+     * Returns a {@code Supplier} of {@code SingleUseKey}s so that no two instances contain the same key and nonce pair.\n+     * A new key is generated only when the {@code nonce} space has been exhausted.\n+     */\n+    static <T extends Exception> CheckedSupplier<SingleUseKey, T> createSingleUseKeySupplier(\n+        CheckedSupplier<Tuple<BytesReference, SecretKey>, T> keyGenerator\n+    ) {\n+        final AtomicReference<SingleUseKey> keyCurrentlyInUse = new AtomicReference<>(EXPIRED_KEY);\n+        return createSingleUseKeySupplier(keyGenerator, keyCurrentlyInUse);\n+    }\n+\n+    // for tests use only\n+    static <T extends Exception> CheckedSupplier<SingleUseKey, T> createSingleUseKeySupplier(\n+        CheckedSupplier<Tuple<BytesReference, SecretKey>, T> keyGenerator,\n+        AtomicReference<SingleUseKey> keyCurrentlyInUse\n+    ) {\n+        final Object lock = new Object();\n+        return () -> {", "originalCommit": "fc4e98a30826dde5ae1b6abf50bc238aa485b481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTYwMDc5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r419600796", "bodyText": "It feels like the general concept could be written quite simply with a class that extends Supplier, has an internal counter and is synchronized on get (or uses an AtomicInteger for the counter).\n\nThis is pretty much what the code here does; here it is without production ornaments:\n        return () -> {\n                final SingleUseKey nonceAndKey = keyCurrentlyInUse.getAndUpdate(\n                    prev -> prev.nonce < MAX_NONCE ? new SingleUseKey(prev.keyId, prev.key, prev.nonce + 1) : EXPIRED_KEY\n                );\n                if (nonceAndKey.nonce < MAX_NONCE) {\n                    return nonceAndKey;\n                } else {\n                    synchronized (lock) {\n                        if (keyCurrentlyInUse.get().nonce == MAX_NONCE) {\n                            final Tuple<BytesReference, SecretKey> newKey = keyGenerator.get();\n                            keyCurrentlyInUse.set(new SingleUseKey(newKey.v1(), newKey.v2(), MIN_NONCE));\n                        }\n                    }\n                }\n        };\n\nInstead of extending Supplier this uses the fact that Supplier is a FunctionalInterface. The internal counter is the nonce itself. Instead of synchronizing on get this uses an AtomicReference as an AtomicInteger. The remaining code is to generate a new key and reset the counter. And in this last code is where the preference for AtomicReference over of AtomicInteger shows off because it allows to atomically set the key and the nonce.\nI have added a few more comments. I hope this all makes more sense now.\nDo you think I should change something here?", "author": "albertzaharovits", "createdAt": "2020-05-04T17:25:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjM0MTUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0NTMzMg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432245332", "bodyText": "It makes sense. I think I was missing the fact that you need to generate a new base key when the nonce rolls over. but I do think this code is unnecessarily difficult to follow.\nThat's a combination of trying too hard to do it all in a lambda and trying to keep the locks really small. Is that necessary?\nCouldn't we have:\n        return new CheckedSupplier<SingleUseKey, T>() {\n                private Tuple<BytesReference, SecretKey> key = null;\n                private int nonce = 0;\n                public synchronized SingleUseKey get() throws T() {\n                   if( key == null || nonce == MAX_NONCE) {\n                       key = keyGenerator.get();\n                       nonce = MIN_NONCE;\n                   } else {\n                       nonce++;\n                   }\n                   return new SingleUseKey(key.v1(), key.v2(), nonce);\n                }\n        };", "author": "tvernum", "createdAt": "2020-05-29T04:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjM0MTUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0OTU3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432249573", "bodyText": "A few questions.\n\n\nGiven that KEY_ID_PLAINTEXT is not really a key, just a magic string of bytes, AESWrap seems like an odd choice. Is that purely because it is deterministic, or is there some other reason why we'd use AESWrap for generate a key-id?\n\n\nYour statement that\n\nthe ciphertext reveals no information on the key\n\nseems overly confident. This is a known-plaintext scenario, so it seems like there is at least the risk of revealing information about the key. What's the basis for your analysis that this is safe?\n\n\nWhy is this done with purely with encryption, and not hashing? At first glance (based purely on the need to generate an id for a secret key without revealing information about the key) that a hash (or HMAC) is more appropriate here.", "author": "tvernum", "createdAt": "2020-05-29T04:29:34Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/AESKeyUtils.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.repositories.encrypted;\n+\n+import org.elasticsearch.common.settings.SecureString;\n+\n+import javax.crypto.Cipher;\n+import javax.crypto.SecretKey;\n+import javax.crypto.SecretKeyFactory;\n+import javax.crypto.spec.PBEKeySpec;\n+import javax.crypto.spec.SecretKeySpec;\n+import java.nio.charset.StandardCharsets;\n+import java.security.GeneralSecurityException;\n+import java.security.Key;\n+import java.util.Base64;\n+\n+public final class AESKeyUtils {\n+    public static final int KEY_LENGTH_IN_BYTES = 32; // 256-bit AES key\n+    public static final int WRAPPED_KEY_LENGTH_IN_BYTES = KEY_LENGTH_IN_BYTES + 8; // https://www.ietf.org/rfc/rfc3394.txt section 2.2\n+    // parameter for the KDF function, it's a funny and unusual iter count larger than 60k\n+    private static final int KDF_ITER = 61616;\n+    // the KDF algorithm that generate the symmetric key given the password\n+    private static final String KDF_ALGO = \"PBKDF2WithHmacSHA512\";\n+    // The Id of any AES SecretKey is the AES-Wrap-ciphertext of this fixed 32 byte wide array.\n+    // Key wrapping encryption is deterministic (same plaintext generates the same ciphertext)\n+    // and the probability that two different keys map the same plaintext to the same ciphertext is very small\n+    // (2^-256, much lower than the UUID collision of 2^-128), assuming AES is indistinguishable from a pseudorandom permutation.\n+    private static final byte[] KEY_ID_PLAINTEXT = \"wrapping known text forms key id\".getBytes(StandardCharsets.UTF_8);\n+\n+    public static byte[] wrap(SecretKey wrappingKey, SecretKey keyToWrap) throws GeneralSecurityException {\n+        assert \"AES\".equals(wrappingKey.getAlgorithm());\n+        assert \"AES\".equals(keyToWrap.getAlgorithm());\n+        Cipher c = Cipher.getInstance(\"AESWrap\");\n+        c.init(Cipher.WRAP_MODE, wrappingKey);\n+        return c.wrap(keyToWrap);\n+    }\n+\n+    public static SecretKey unwrap(SecretKey wrappingKey, byte[] keyToUnwrap) throws GeneralSecurityException {\n+        assert \"AES\".equals(wrappingKey.getAlgorithm());\n+        assert keyToUnwrap.length == WRAPPED_KEY_LENGTH_IN_BYTES;\n+        Cipher c = Cipher.getInstance(\"AESWrap\");\n+        c.init(Cipher.UNWRAP_MODE, wrappingKey);\n+        Key unwrappedKey = c.unwrap(keyToUnwrap, \"AES\", Cipher.SECRET_KEY);\n+        return new SecretKeySpec(unwrappedKey.getEncoded(), \"AES\"); // make sure unwrapped key is \"AES\"\n+    }\n+\n+    /**\n+     * Computes the ID of the given AES {@code SecretKey}.\n+     * The ID can be published as it does not leak any information about the key.\n+     * Different {@code SecretKey}s have different IDs with a very high probability.\n+     * <p>\n+     * The ID is the ciphertext of a known plaintext, using the AES Wrap cipher algorithm.\n+     * AES Wrap algorithm is deterministic, i.e. encryption using the same key, of the same plaintext, generates the same ciphertext.\n+     * Moreover, the ciphertext reveals no information on the key, and the probability of collision of ciphertexts given different\n+     * keys is statistically negligible.\n+     */", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzAwNzY5OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r433007698", "bodyText": "I'm going hold off reviewing the tests until we reach a conclusion on this topic ^^\nIf we decide to change something here that will mean I need to do another pass anyway, and even if we decide to keep this bit as-is, I think I need to get my head around it before can review the tests well.", "author": "tvernum", "createdAt": "2020-06-01T01:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0OTU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU2NDYzMw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r435564633", "bodyText": "These are good questions, and I've spent a lot of thinking in this area. I believe I have good answers to them.\nEncrypted (wrapped) DEKs are named after the Id of the KEK that was used to wrap them (in order to use a DEK, you need to know the KEK to unwrap it; we're aiming for possibly multiple living KEKs in a given repository, so the KEK Id is a way to identify which of the KEKs (you have already obtained using PBKDF2 on the node) to use). The crucial bit here is that the KEK Id is public, as anyone that can list the repo has access to KEK Ids. In order to avoid introducing other repo settings, the only way to derive an Id for a KEK is the content of the KEK itself (i.e. we could introduce a salt as a repo setting, in the cluster state, but then when the repo is installed on a different cluster, the password, as well as the salt, must be configured).\nSo, the problem statement is that we have a secret AES Key (a string of bytes), generated using PBKDF2, for which we need to generate a public identifier, given no extra input.\nI would say there are 3 good (engineering-grade) reasons to use AESWrap:\n\ncrypto gurus advise against reusing the same key for different crypto operations; in this case the KEK is already used for AESWrap of DEKs.\nAESWrap is deterministic.\nthere are various trusted sources that state that knowing the plaintext and the ciphertext does not reveal any information about the AES key (regardless of mode). We actually rely on this fact in other places too, because some repo files have predictable contents. AESWrap inherits this property, so that even if the ciphertext (KEK Id) and the plaintext (hardcoded byte string) are known, no information is exposed on the KEK (wrapping key).", "author": "albertzaharovits", "createdAt": "2020-06-04T21:31:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0OTU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MzI0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r435643249", "bodyText": "Let me sit with this a bit longer.\nI'm persuaded that:\n\nIf we use the KEK (as a key) to generate the ID then we should favour AESWrap due to wanting to restrict it to a single operation.\nJust hashing (SHA) the KEK is a bad idea because that leaks something about the key (only really an oracle to tell whether a predicted key is the real key, but that's still something).\n\nIt still wonder whether there's a better option, but I'm not sure what it would be.\n\nWe could HMAC the KEK, but what would we use as the key in the HMAC?\nWe could SHA(AESWrap(KEK, ID_TEXT)), which reduces the plaintext risks, but I'm not sure if it's actually worth it, or whether it's just throwing in more operations \"for the feels\".\n\n@jkakavas I'd appreciate any thoughts you have.", "author": "tvernum", "createdAt": "2020-06-05T01:33:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0OTU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDkxMw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r437154913", "bodyText": "I'm persuaded that:\nIf we use the KEK (as a key) to generate the ID then we should favour AESWrap due to wanting to restrict it to a single operation.\n\nI'm not, I think. While it is true that there are generic warnings around this in bibliography, this stems from specific vulnerabilities in sets of algorithms from key reuse ( Same RSA private key for encryption and signing, AEC-CBC for encryption plus AES-CBC-MAC ) and is not a universal truth. Given that we control the algorithms used, I can see us coming down to a safe algorithm pair.\n\nJust hashing (SHA) the KEK is a bad idea because that leaks something about the key (only really an oracle to tell whether a predicted key is the real key, but that's still something).\n\nI don't see the difference here. Given that the magic bytes that constitute the to-be-wrapped-key are stored in the source code, what stops a potential attacker from using AESWrap as an oracle for valid wrapping keys ( KEKs )? Also, assuming a malicious user has access to IDs, they have access to the repository and to DEKs, so they could also use unwrapping of DEKs as an oracle. Granted, a hash-and-check oracle is much more cost-efficient than a try-to-decrypt one.\nAlternative to the already proposed solutions, we could also use something like HKDF with a null salt to get two keys from the KEK. One for AESWrap of DEKs and another for a HMAC to produce the id of the KEK.", "author": "jkakavas", "createdAt": "2020-06-09T05:58:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0OTU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc0NTYyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r437745625", "bodyText": "Thanks for thinking this through, folks!\nI'm going to pile on my thoughts to maintain this as proposed:\nThere's indeed the risk that the contents of the encrypted repository on the cloud service can be used to brute force the keys. But, given the nature of the encrypted files, certain files or at least the headers for these files are known. We don't have any protection against this, besides making it expensive to generate the KEK (PBKDF2 cost factor). There will always be the option to brute force 256-bit AES keys.\nI think we can all agree that a hash function will be more appropriate in this case. So take the properties of a cryptographic hash function:\n\nIt's hard to find two inputs with the same output, called a collision\nIt's hard to find an input matching a given output, called a pre-image\n\nConsidering the AES-Wrap encryption with the KEK key as the input (and the ciphertext of the known plaintext as the output) , the properties above are trivially met by AES-Wrap otherwise it would be vulnerable to fatal key recovery attacks.", "author": "albertzaharovits", "createdAt": "2020-06-09T21:57:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0OTU3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MTk5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432251997", "bodyText": "Assuming I understand this correctly, I think it would be helpful to add a why here:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n          \n          \n            \n                // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n          \n          \n            \n                // This ensures that all nodes that participate in the snapshot agree on the value of the master encryption key, so that\n          \n          \n            \n                // every shard that is included in the snapshot is encrypted with a consistent key.\n          \n      \n    \n    \n  \n\nOr maybe just a link to #validateLocalRepositorySecret...", "author": "tvernum", "createdAt": "2020-05-29T04:41:06Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY1NzM0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r437657341", "bodyText": "You understood correctly, and indeed I've explained more of the how instead of the why.\nI've changed the javadoc here to say more about the why, similar to #validateLocalRepositorySecret (I don't think the repetition is really detrimental).", "author": "albertzaharovits", "createdAt": "2020-06-09T19:10:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MTk5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MjEwMA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432252100", "bodyText": "The use of identifier and hash here confuse me. Are they the same thing?\nI think they are, and some of the confusion comes from the fact (covered above) that we generate an \"id\" from the key's content, which is intended to work a bit like a hash, but isn't actually a hash.", "author": "tvernum", "createdAt": "2020-05-29T04:41:33Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MTc3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r437691776", "bodyText": "This choice of words is indeed murky.\nI now reckon the \"unforgeable identifier\" terminology, instead of the more explicit \"salted password hash\" complicates things unnecessarily. Although not technically a \"hash\" (it's the ciphertext of a known plaintext, when encrypting using a key derived from the password), I've switched to using the \"salted password hash\" terminology consistently because it conveys the intention much clearer than the \"identifier\" terminology.\nLet me know if this is OK with you.", "author": "albertzaharovits", "createdAt": "2020-06-09T20:15:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MjEwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA0NTIyNw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r444045227", "bodyText": "As discussed in a call, we've agreed to run with the current implementation.\nHowever this also highlighted that down the track we may decide we need to make changes to the implementation of encrypted snapshots (for this part, or some other part) and we ought to have some sort of versioning scheme so that we can (if necessary) make incompatible changes to the encryption/snapshot format but mark it in such a way that ES releases can distinguish between the 2 (or more) different formats. @albertzaharovits will think about that (as a separate piece of work).", "author": "tvernum", "createdAt": "2020-06-23T08:17:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MjEwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1OTk5OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432259998", "bodyText": "Personal preference\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n          \n          \n            \n                private final Supplier<Tuple<BytesReference, SecretKey>> dekGenerator;", "author": "tvernum", "createdAt": "2020-05-29T05:17:20Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzczMzI3MA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r437733270", "bodyText": "Yeah, I think this is the code format convention we generally use.\nThere are many other variable names that start with caps because the first word is an acronym , and I've renamed them all.", "author": "albertzaharovits", "createdAt": "2020-06-09T21:29:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1OTk5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2MDA2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432260061", "bodyText": "My preference:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private final Cache<String, SecretKey> DEKCache;\n          \n          \n            \n                private final Cache<String, SecretKey> dekCache;", "author": "tvernum", "createdAt": "2020-05-29T05:17:35Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final SecureString repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private volatile String validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2MTAzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432261031", "bodyText": "I wonder whether there's any value, at some future point, in adding encryption stats.\nI'm not familiar enough with repo stats to know, but it feels like it's worth considering.", "author": "tvernum", "createdAt": "2020-05-29T05:21:31Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final SecureString repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private volatile String validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetadata metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        SecureString repositoryPassword\n+    ) throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt)\n+        );\n+        this.validatedRepositoryPasswordId = this.localRepositoryPasswordId;\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    @Override\n+    public RepositoryStats stats() {\n+        return this.delegatedRepository.stats();", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwOTI1OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r437709258", "bodyText": "I see that repository stats currently report a live count for cloud API requests. API calls influence the cost of usage for the repository. In this context, I think we could count the \"extra\" API calls incurred because of encryption separately. That would be interesting I think. I've added this to the backlog at #41910 (comment) , but personally this looks very low priority (i.e. I would merge the feature branch before we touch on this).", "author": "albertzaharovits", "createdAt": "2020-06-09T20:47:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2MTAzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2NTIwNA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432265204", "bodyText": "I have a personal preference to avoid having local vars that shadow fields.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n          \n          \n            \n                    final Supplier<Tuple<BytesReference, SecretKey>> blobDEKGenerator;", "author": "tvernum", "createdAt": "2020-05-29T05:37:29Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final SecureString repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private volatile String validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetadata metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        SecureString repositoryPassword\n+    ) throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt)\n+        );\n+        this.validatedRepositoryPasswordId = this.localRepositoryPasswordId;\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    @Override\n+    public RepositoryStats stats() {\n+        return this.delegatedRepository.stats();\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isAllowed(XPackLicenseState.Feature.ENCRYPTED_SNAPSHOT)) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+            localRepositoryPasswordIdSalt,\n+            localRepositoryPasswordId\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        Metadata clusterMetadata,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        Function<ClusterState, ClusterState> stateTransformer,\n+        ActionListener<Tuple<RepositoryData, SnapshotInfo>> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetadata,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            stateTransformer,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI2NzA2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432267062", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    \"Wrapped DEK [\" + DEKId + \"] has smaller length [\" + bytesRead + \"] \" + \"than expected\"\n          \n          \n            \n                                    \"Wrapped DEK [\" + DEKId + \"] has smaller length [\" + bytesRead + \"] than expected\"", "author": "tvernum", "createdAt": "2020-05-29T05:44:41Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final SecureString repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private volatile String validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetadata metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        SecureString repositoryPassword\n+    ) throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt)\n+        );\n+        this.validatedRepositoryPasswordId = this.localRepositoryPasswordId;\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    @Override\n+    public RepositoryStats stats() {\n+        return this.delegatedRepository.stats();\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isAllowed(XPackLicenseState.Feature.ENCRYPTED_SNAPSHOT)) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+            localRepositoryPasswordIdSalt,\n+            localRepositoryPasswordId\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        Metadata clusterMetadata,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        Function<ClusterState, ClusterState> stateTransformer,\n+        ActionListener<Tuple<RepositoryData, SnapshotInfo>> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetadata,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            stateTransformer,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            DEKGenerator,\n+            DEKCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<BytesReference, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator DEKGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        DEKGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> {\n+            final BytesReference DEKId = new BytesArray(UUIDs.randomBase64UUID(DEKIdSecureRandom));\n+            final SecretKey DEK = DEKGenerator.generateKey();\n+            logger.debug(\"Repository [{}] generated new DEK [{}]\", metadata.name(), DEKId);\n+            return new Tuple<>(DEKId, DEK);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String DEKId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, DEKId);\n+            final String KEKId = AESKeyUtils.computeId(KEK);\n+            logger.debug(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), KEKId, DEKId);\n+            return new Tuple<>(KEKId, KEK);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * secret on the master node that started the snapshot operation is identical to the repository secret on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository secret id to verify\n+     * @throws RepositoryException if the repository secret id on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        assert snapshotUserMetadata != null;\n+        assert snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY) instanceof String;\n+        final String masterRepositoryPasswordId = (String) snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY);\n+        if (false == masterRepositoryPasswordId.equals(validatedRepositoryPasswordId)) {\n+            assert snapshotUserMetadata.get(PASSWORD_ID_SALT_USER_METADATA_KEY) instanceof String;\n+            final String masterRepositoryPasswordIdSalt = (String) snapshotUserMetadata.get(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+            final String computedRepositoryPasswordId;\n+            try {\n+                computedRepositoryPasswordId = AESKeyUtils.computeId(\n+                    AESKeyUtils.generatePasswordBasedKey(repositoryPassword, masterRepositoryPasswordIdSalt)\n+                );\n+            } catch (Exception e) {\n+                throw new RepositoryException(metadata.name(), \"Unexpected fatal internal error\", e);\n+            }\n+            if (computedRepositoryPasswordId.equals(masterRepositoryPasswordId)) {\n+                this.validatedRepositoryPasswordId = computedRepositoryPasswordId;\n+            } else {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Repository secret id mismatch. The local node's repository secret, the keystore setting [\"\n+                        + EncryptedRepositoryPlugin.ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(\n+                            EncryptedRepositoryPlugin.PASSWORD_NAME_SETTING.get(metadata.settings())\n+                        ).getKey()\n+                        + \"], is different compared to the elected master node's which started the snapshot operation\"\n+                );\n+            }\n+        }\n+    }\n+\n+    // pkg-private for tests\n+    static final class EncryptedBlobStore implements BlobStore {\n+        private final BlobStore delegatedBlobStore;\n+        private final BlobPath delegatedBasePath;\n+        private final String repositoryName;\n+        private final Function<String, Tuple<String, SecretKey>> getKEKforDEK;\n+        private final Cache<String, SecretKey> DEKCache;\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+\n+        EncryptedBlobStore(\n+            BlobStore delegatedBlobStore,\n+            BlobPath delegatedBasePath,\n+            String repositoryName,\n+            Function<String, Tuple<String, SecretKey>> getKEKforDEK,\n+            Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator,\n+            Cache<String, SecretKey> DEKCache\n+        ) {\n+            this.delegatedBlobStore = delegatedBlobStore;\n+            this.delegatedBasePath = delegatedBasePath;\n+            this.repositoryName = repositoryName;\n+            this.getKEKforDEK = getKEKforDEK;\n+            this.DEKCache = DEKCache;\n+            this.singleUseDEKSupplier = SingleUseKey.createSingleUseKeySupplier(() -> {\n+                Tuple<BytesReference, SecretKey> newDEK = DEKGenerator.get();\n+                // store the newly generated DEK before making it available\n+                storeDEK(newDEK.v1().utf8ToString(), newDEK.v2());\n+                return newDEK;\n+            });\n+        }\n+\n+        // pkg-private for tests\n+        SecretKey getDEKById(String DEKId) throws IOException {\n+            try {\n+                return DEKCache.computeIfAbsent(DEKId, ignored -> loadDEK(DEKId));\n+            } catch (ExecutionException e) {\n+                // some exception types are to be expected\n+                if (e.getCause() instanceof IOException) {\n+                    throw (IOException) e.getCause();\n+                } else if (e.getCause() instanceof ElasticsearchException) {\n+                    throw (ElasticsearchException) e.getCause();\n+                } else {\n+                    throw new RepositoryException(repositoryName, \"Unexpected exception retrieving DEK [\" + DEKId + \"]\", e);\n+                }\n+            }\n+        }\n+\n+        private SecretKey loadDEK(String DEKId) throws IOException {\n+            final BlobPath DEKBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(DEKId);\n+            logger.debug(\"Repository [{}] loading wrapped DEK [{}] from blob path {}\", repositoryName, DEKId, DEKBlobPath);\n+            final BlobContainer DEKBlobContainer = delegatedBlobStore.blobContainer(DEKBlobPath);\n+            final Tuple<String, SecretKey> KEK = getKEKforDEK.apply(DEKId);\n+            logger.trace(\"Repository [{}] using KEK [{}] to unwrap DEK [{}]\", repositoryName, KEK.v1(), DEKId);\n+            final byte[] encryptedDEKBytes = new byte[AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES];\n+            try (InputStream encryptedDEKInputStream = DEKBlobContainer.readBlob(KEK.v1())) {\n+                final int bytesRead = Streams.readFully(encryptedDEKInputStream, encryptedDEKBytes);\n+                if (bytesRead != AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES) {\n+                    throw new RepositoryException(\n+                        repositoryName,\n+                        \"Wrapped DEK [\" + DEKId + \"] has smaller length [\" + bytesRead + \"] \" + \"than expected\"", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI3MDUxNw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r432270517", "bodyText": "Again, personal preference, but I much prefer to unpack a tuple as soon as its read.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        final Tuple<String, SecretKey> KEK = getKEKforDEK.apply(DEKId);\n          \n          \n            \n                        final Tuple<String, SecretKey> kekTup = getKEKforDEK.apply(DEKId);\n          \n          \n            \n                        final String kekId = kekTup.v1();\n          \n          \n            \n                        final SecretKey kek = kekTup.v2();", "author": "tvernum", "createdAt": "2020-05-29T05:56:46Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,682 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+    // each snapshot metadata contains an unforgeable identifier of the repository password of the master node that started the snapshot\n+    // this hash is then verified on each data node before the actual shard files snapshot, as well as on the\n+    // master node that finalizes the snapshot (could be a different master node, if a master failover occurred during the snapshot)\n+    static final String PASSWORD_ID_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordId\";\n+    static final String PASSWORD_ID_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordIdSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final SecureString repositoryPassword;\n+    private final String localRepositoryPasswordId;\n+    private final String localRepositoryPasswordIdSalt;\n+    private volatile String validatedRepositoryPasswordId;\n+    private final Cache<String, SecretKey> DEKCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetadata metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        SecureString repositoryPassword\n+    ) throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.DEKGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the password \"id\" and validated\n+        this.localRepositoryPasswordIdSalt = UUIDs.randomBase64UUID();\n+        this.localRepositoryPasswordId = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordIdSalt)\n+        );\n+        this.validatedRepositoryPasswordId = this.localRepositoryPasswordId;\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.DEKCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    @Override\n+    public RepositoryStats stats() {\n+        return this.delegatedRepository.stats();\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with encrypted snapshots, this throws an exception, which aborts the snapshot\n+     * operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isAllowed(XPackLicenseState.Feature.ENCRYPTED_SNAPSHOT)) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // set out the ID of the repository secret\n+        // this is then checked before every snapshot operation (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot})\n+        // to assure that all participating nodes in the snapshot operation are using the same repository secret\n+        snapshotUserMetadata.put(PASSWORD_ID_SALT_USER_METADATA_KEY, localRepositoryPasswordIdSalt);\n+        snapshotUserMetadata.put(PASSWORD_ID_USER_METADATA_KEY, localRepositoryPasswordId);\n+        logger.trace(\n+            \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+            localRepositoryPasswordIdSalt,\n+            localRepositoryPasswordId\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        Metadata clusterMetadata,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        Function<ClusterState, ClusterState> stateTransformer,\n+        ActionListener<Tuple<RepositoryData, SnapshotInfo>> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password id from the snapshot metadata so that the id is not displayed in the API response to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_ID_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetadata,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            stateTransformer,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            DEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            DEKGenerator = this.DEKGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            DEKGenerator,\n+            DEKCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<BytesReference, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom DEKSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom DEKIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator DEKGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        DEKGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, DEKSecureRandom);\n+        return () -> {\n+            final BytesReference DEKId = new BytesArray(UUIDs.randomBase64UUID(DEKIdSecureRandom));\n+            final SecretKey DEK = DEKGenerator.generateKey();\n+            logger.debug(\"Repository [{}] generated new DEK [{}]\", metadata.name(), DEKId);\n+            return new Tuple<>(DEKId, DEK);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String DEKId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final SecretKey KEK = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, DEKId);\n+            final String KEKId = AESKeyUtils.computeId(KEK);\n+            logger.debug(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), KEKId, DEKId);\n+            return new Tuple<>(KEKId, KEK);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + DEKId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * secret on the master node that started the snapshot operation is identical to the repository secret on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository secret id to verify\n+     * @throws RepositoryException if the repository secret id on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        assert snapshotUserMetadata != null;\n+        assert snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY) instanceof String;\n+        final String masterRepositoryPasswordId = (String) snapshotUserMetadata.get(PASSWORD_ID_USER_METADATA_KEY);\n+        if (false == masterRepositoryPasswordId.equals(validatedRepositoryPasswordId)) {\n+            assert snapshotUserMetadata.get(PASSWORD_ID_SALT_USER_METADATA_KEY) instanceof String;\n+            final String masterRepositoryPasswordIdSalt = (String) snapshotUserMetadata.get(PASSWORD_ID_SALT_USER_METADATA_KEY);\n+            final String computedRepositoryPasswordId;\n+            try {\n+                computedRepositoryPasswordId = AESKeyUtils.computeId(\n+                    AESKeyUtils.generatePasswordBasedKey(repositoryPassword, masterRepositoryPasswordIdSalt)\n+                );\n+            } catch (Exception e) {\n+                throw new RepositoryException(metadata.name(), \"Unexpected fatal internal error\", e);\n+            }\n+            if (computedRepositoryPasswordId.equals(masterRepositoryPasswordId)) {\n+                this.validatedRepositoryPasswordId = computedRepositoryPasswordId;\n+            } else {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Repository secret id mismatch. The local node's repository secret, the keystore setting [\"\n+                        + EncryptedRepositoryPlugin.ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(\n+                            EncryptedRepositoryPlugin.PASSWORD_NAME_SETTING.get(metadata.settings())\n+                        ).getKey()\n+                        + \"], is different compared to the elected master node's which started the snapshot operation\"\n+                );\n+            }\n+        }\n+    }\n+\n+    // pkg-private for tests\n+    static final class EncryptedBlobStore implements BlobStore {\n+        private final BlobStore delegatedBlobStore;\n+        private final BlobPath delegatedBasePath;\n+        private final String repositoryName;\n+        private final Function<String, Tuple<String, SecretKey>> getKEKforDEK;\n+        private final Cache<String, SecretKey> DEKCache;\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+\n+        EncryptedBlobStore(\n+            BlobStore delegatedBlobStore,\n+            BlobPath delegatedBasePath,\n+            String repositoryName,\n+            Function<String, Tuple<String, SecretKey>> getKEKforDEK,\n+            Supplier<Tuple<BytesReference, SecretKey>> DEKGenerator,\n+            Cache<String, SecretKey> DEKCache\n+        ) {\n+            this.delegatedBlobStore = delegatedBlobStore;\n+            this.delegatedBasePath = delegatedBasePath;\n+            this.repositoryName = repositoryName;\n+            this.getKEKforDEK = getKEKforDEK;\n+            this.DEKCache = DEKCache;\n+            this.singleUseDEKSupplier = SingleUseKey.createSingleUseKeySupplier(() -> {\n+                Tuple<BytesReference, SecretKey> newDEK = DEKGenerator.get();\n+                // store the newly generated DEK before making it available\n+                storeDEK(newDEK.v1().utf8ToString(), newDEK.v2());\n+                return newDEK;\n+            });\n+        }\n+\n+        // pkg-private for tests\n+        SecretKey getDEKById(String DEKId) throws IOException {\n+            try {\n+                return DEKCache.computeIfAbsent(DEKId, ignored -> loadDEK(DEKId));\n+            } catch (ExecutionException e) {\n+                // some exception types are to be expected\n+                if (e.getCause() instanceof IOException) {\n+                    throw (IOException) e.getCause();\n+                } else if (e.getCause() instanceof ElasticsearchException) {\n+                    throw (ElasticsearchException) e.getCause();\n+                } else {\n+                    throw new RepositoryException(repositoryName, \"Unexpected exception retrieving DEK [\" + DEKId + \"]\", e);\n+                }\n+            }\n+        }\n+\n+        private SecretKey loadDEK(String DEKId) throws IOException {\n+            final BlobPath DEKBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(DEKId);\n+            logger.debug(\"Repository [{}] loading wrapped DEK [{}] from blob path {}\", repositoryName, DEKId, DEKBlobPath);\n+            final BlobContainer DEKBlobContainer = delegatedBlobStore.blobContainer(DEKBlobPath);\n+            final Tuple<String, SecretKey> KEK = getKEKforDEK.apply(DEKId);", "originalCommit": "6eac6bfd83a87e37cf5f0f1e83dca50156a1bf31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzczMTk3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r437731976", "bodyText": "Sounds good to me too!", "author": "albertzaharovits", "createdAt": "2020-06-09T21:26:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI3MDUxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MzU2OA==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r444383568", "bodyText": "@tvernum regarding your inquiry about encrypted repo versioning:\nEncrypted blobs have only the DEK Id (plaintext UUID) prepended, so the repo currently doesn't support snapshots of multiple versions. If we plan that future encrypted repos be backwards compatible with the current version, in a way that the new version be able to work with a repo of the current version, then we might be in trouble.  In this case, any given snapshot can technically contain blobs encrypted in multiple versions, so we have to rely on the encrypted blob itself to specify which version of the repo encryption \"scheme\" to use (when decrypting, i.e. snapshot restore).\nI will be adding this item to the list from #41910 . I concur we mustn't release before we have some way to accommodate possible future versions of the repo.\nBut, blob level versioning is tricky. I remember I coded serialisation of version headers tacked on encrypted blobs. Probably the version header must be authenticated, but not encrypted. All that is messy and I really appreciate the simplicity of just prepending a plaintext UUID to the encrypted blob (the current approach).\nI think prepending a \"magic byte\" to the UUID (which prepends the encrypted blobs) should be enough. Future versions might have a different value for the magic byte (which they might need to authenticate but not encrypt) and they might not even need one because they would infer the DEK by some other, more efficient,  means (eg. a snapshot-level dictionary of blob name to DEK content).", "author": "albertzaharovits", "createdAt": "2020-06-23T17:18:06Z", "path": "x-pack/plugin/repository-encrypted/src/main/java/org/elasticsearch/repositories/encrypted/EncryptedRepository.java", "diffHunk": "@@ -6,11 +6,694 @@\n \n package org.elasticsearch.repositories.encrypted;\n \n-public class EncryptedRepository {\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.metadata.RepositoryMetadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.CheckedFunction;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobMetadata;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.DeleteResult;\n+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.SecureString;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.index.mapper.MapperService;\n+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.license.LicenseUtils;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.RepositoryStats;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotInfo;\n+import org.elasticsearch.snapshots.SnapshotShardFailure;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.SecretKey;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.NoSuchFileException;\n+import java.security.GeneralSecurityException;\n+import java.security.SecureRandom;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+public class EncryptedRepository extends BlobStoreRepository {\n+    static final Logger logger = LogManager.getLogger(EncryptedRepository.class);\n+    // the following constants are fixed by definition\n     static final int GCM_TAG_LENGTH_IN_BYTES = 16;\n     static final int GCM_IV_LENGTH_IN_BYTES = 12;\n-    static final int AES_BLOCK_SIZE_IN_BYTES = 128;\n+    static final int AES_BLOCK_LENGTH_IN_BYTES = 128;\n+    // the following constants require careful thought before changing because they will break backwards compatibility\n     static final String DATA_ENCRYPTION_SCHEME = \"AES/GCM/NoPadding\";\n     static final long PACKET_START_COUNTER = Long.MIN_VALUE;\n-    static final int MAX_PACKET_LENGTH_IN_BYTES = 1 << 30;\n+    static final int MAX_PACKET_LENGTH_IN_BYTES = 8 << 20; // 8MB\n+    // this should be smaller than {@code #MAX_PACKET_LENGTH_IN_BYTES} and it's what {@code EncryptionPacketsInputStream} uses\n+    // during encryption and what {@code DecryptionPacketsInputStream} expects during decryption (it is not configurable)\n+    static final int PACKET_LENGTH_IN_BYTES = 64 * (1 << 10); // 64KB\n+    // the path of the blob container holding all the DEKs\n+    // this is relative to the root base path holding the encrypted blobs (i.e. the repository root base path)\n+    static final String DEK_ROOT_CONTAINER = \".encryption-metadata\"; // package private for tests\n+    static final int DEK_ID_LENGTH = 22; // {@code org.elasticsearch.common.UUIDS} length\n+\n+    // the following constants can be changed freely\n+    private static final String RAND_ALGO = \"SHA1PRNG\";\n+\n+    // the snapshot metadata (residing in the cluster state for the lifetime of the snapshot)\n+    // contains the salted hash of the repository password as present on the master node (which starts the snapshot operation).\n+    // The hash is verified on each data node, before initiating the actual shard files snapshot, as well\n+    // as on the master node that finalizes the snapshot (which could be a different master node from the one that started\n+    // the operation if a master failover occurred during the snapshot).\n+    // This ensures that all participating nodes in the snapshot operation agree on the value of the key encryption key, so that\n+    // all the data included in a snapshot is encrypted using the same password.\n+    static final String PASSWORD_HASH_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordHash\";\n+    static final String PASSWORD_SALT_USER_METADATA_KEY = EncryptedRepository.class.getName() + \".repositoryPasswordSalt\";\n+    private static final int DEK_CACHE_WEIGHT = 2048;\n+\n+    // this is the repository instance to which all blob reads and writes are forwarded to (it stores both the encrypted blobs, as well\n+    // as the associated encrypted DEKs)\n+    private final BlobStoreRepository delegatedRepository;\n+    // every data blob is encrypted with its randomly generated AES key (DEK)\n+    private final Supplier<Tuple<BytesReference, SecretKey>> dekGenerator;\n+    // license is checked before every snapshot operations; protected non-final for tests\n+    protected Supplier<XPackLicenseState> licenseStateSupplier;\n+    private final SecureString repositoryPassword;\n+    private final String localRepositoryPasswordHash;\n+    private final String localRepositoryPasswordSalt;\n+    private volatile String validatedLocalRepositoryPasswordHash;\n+    private final Cache<String, SecretKey> dekCache;\n+\n+    /**\n+     * Returns the byte length (i.e. the storage size) of an encrypted blob, given the length of the blob's plaintext contents.\n+     *\n+     * @see EncryptionPacketsInputStream#getEncryptionLength(long, int)\n+     */\n+    public static long getEncryptedBlobByteLength(long plaintextBlobByteLength) {\n+        return (long) DEK_ID_LENGTH /* UUID byte length */\n+            + EncryptionPacketsInputStream.getEncryptionLength(plaintextBlobByteLength, PACKET_LENGTH_IN_BYTES);\n+    }\n+\n+    protected EncryptedRepository(\n+        RepositoryMetadata metadata,\n+        NamedXContentRegistry namedXContentRegistry,\n+        ClusterService clusterService,\n+        BlobStoreRepository delegatedRepository,\n+        Supplier<XPackLicenseState> licenseStateSupplier,\n+        SecureString repositoryPassword\n+    ) throws GeneralSecurityException {\n+        super(\n+            metadata,\n+            namedXContentRegistry,\n+            clusterService,\n+            BlobPath.cleanPath() /* the encrypted repository uses a hardcoded empty\n+                                 base blob path but the base path setting is honored for the delegated repository */\n+        );\n+        this.delegatedRepository = delegatedRepository;\n+        this.dekGenerator = createDEKGenerator();\n+        this.licenseStateSupplier = licenseStateSupplier;\n+        this.repositoryPassword = repositoryPassword;\n+        // the salt used to generate an irreversible \"hash\"; it is generated randomly but it's fixed for the lifetime of the\n+        // repository solely for efficiency reasons\n+        this.localRepositoryPasswordSalt = UUIDs.randomBase64UUID();\n+        // the \"hash\" of the repository password from the local node is not actually a hash but the ciphertext of a\n+        // known-plaintext using a key derived from the repository password using a random salt\n+        this.localRepositoryPasswordHash = AESKeyUtils.computeId(\n+            AESKeyUtils.generatePasswordBasedKey(repositoryPassword, localRepositoryPasswordSalt)\n+        );\n+        // a \"hash\" computed locally is also locally trusted (trivially)\n+        this.validatedLocalRepositoryPasswordHash = this.localRepositoryPasswordHash;\n+        // stores decrypted DEKs; DEKs are reused to encrypt/decrypt multiple independent blobs\n+        this.dekCache = CacheBuilder.<String, SecretKey>builder().setMaximumWeight(DEK_CACHE_WEIGHT).build();\n+        if (isReadOnly() != delegatedRepository.isReadOnly()) {\n+            throw new RepositoryException(\n+                metadata.name(),\n+                \"Unexpected fatal internal error\",\n+                new IllegalStateException(\"The encrypted repository must be read-only iff the delegate repository is read-only\")\n+            );\n+        }\n+    }\n+\n+    @Override\n+    public RepositoryStats stats() {\n+        return this.delegatedRepository.stats();\n+    }\n+\n+    /**\n+     * The repository hook method which populates the snapshot metadata with the salted password hash of the repository on the (master)\n+     * node that starts of the snapshot operation. All the other actions associated with the same snapshot operation will first verify\n+     * that the local repository password checks with the hash from the snapshot metadata.\n+     * <p>\n+     * In addition, if the installed license does not comply with the \"encrypted snapshots\" feature, this method throws an exception,\n+     * which aborts the snapshot operation.\n+     *\n+     * See {@link org.elasticsearch.repositories.Repository#adaptUserMetadata(Map)}.\n+     *\n+     * @param userMetadata the snapshot metadata as received from the calling user\n+     * @return the snapshot metadata containing the salted password hash of the node initializing the snapshot\n+     */\n+    @Override\n+    public Map<String, Object> adaptUserMetadata(Map<String, Object> userMetadata) {\n+        // because populating the snapshot metadata must be done before the actual snapshot is first initialized,\n+        // we take the opportunity to validate the license and abort if non-compliant\n+        if (false == licenseStateSupplier.get().isAllowed(XPackLicenseState.Feature.ENCRYPTED_SNAPSHOT)) {\n+            throw LicenseUtils.newComplianceException(\"encrypted snapshots\");\n+        }\n+        Map<String, Object> snapshotUserMetadata = new HashMap<>();\n+        if (userMetadata != null) {\n+            snapshotUserMetadata.putAll(userMetadata);\n+        }\n+        // fill in the hash of the repository password, which is then checked before every snapshot operation\n+        // (i.e. {@link #snapshotShard} and {@link #finalizeSnapshot}) to ensure that all participating nodes\n+        // in the snapshot operation use the same repository password\n+        snapshotUserMetadata.put(PASSWORD_SALT_USER_METADATA_KEY, localRepositoryPasswordSalt);\n+        snapshotUserMetadata.put(PASSWORD_HASH_USER_METADATA_KEY, localRepositoryPasswordHash);\n+        logger.trace(\n+            \"Snapshot metadata for local repository password  [{}] and [{}]\",\n+            localRepositoryPasswordSalt,\n+            localRepositoryPasswordHash\n+        );\n+        return Map.copyOf(snapshotUserMetadata);\n+    }\n+\n+    @Override\n+    public void finalizeSnapshot(\n+        SnapshotId snapshotId,\n+        ShardGenerations shardGenerations,\n+        long startTime,\n+        String failure,\n+        int totalShards,\n+        List<SnapshotShardFailure> shardFailures,\n+        long repositoryStateId,\n+        boolean includeGlobalState,\n+        Metadata clusterMetadata,\n+        Map<String, Object> userMetadata,\n+        Version repositoryMetaVersion,\n+        Function<ClusterState, ClusterState> stateTransformer,\n+        ActionListener<Tuple<RepositoryData, SnapshotInfo>> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        } finally {\n+            // remove the repository password hash (and salt) from the snapshot metadata so that it is not displayed in the API response\n+            // to the user\n+            userMetadata = new HashMap<>(userMetadata);\n+            userMetadata.remove(PASSWORD_HASH_USER_METADATA_KEY);\n+            userMetadata.remove(PASSWORD_SALT_USER_METADATA_KEY);\n+        }\n+        super.finalizeSnapshot(\n+            snapshotId,\n+            shardGenerations,\n+            startTime,\n+            failure,\n+            totalShards,\n+            shardFailures,\n+            repositoryStateId,\n+            includeGlobalState,\n+            clusterMetadata,\n+            userMetadata,\n+            repositoryMetaVersion,\n+            stateTransformer,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    public void snapshotShard(\n+        Store store,\n+        MapperService mapperService,\n+        SnapshotId snapshotId,\n+        IndexId indexId,\n+        IndexCommit snapshotIndexCommit,\n+        String shardStateIdentifier,\n+        IndexShardSnapshotStatus snapshotStatus,\n+        Version repositoryMetaVersion,\n+        Map<String, Object> userMetadata,\n+        ActionListener<String> listener\n+    ) {\n+        try {\n+            validateLocalRepositorySecret(userMetadata);\n+        } catch (RepositoryException passwordValidationException) {\n+            listener.onFailure(passwordValidationException);\n+            return;\n+        }\n+        super.snapshotShard(\n+            store,\n+            mapperService,\n+            snapshotId,\n+            indexId,\n+            snapshotIndexCommit,\n+            shardStateIdentifier,\n+            snapshotStatus,\n+            repositoryMetaVersion,\n+            userMetadata,\n+            listener\n+        );\n+    }\n+\n+    @Override\n+    protected BlobStore createBlobStore() {\n+        final Supplier<Tuple<BytesReference, SecretKey>> blobStoreDEKGenerator;\n+        if (isReadOnly()) {\n+            // make sure that a read-only repository can't encrypt anything\n+            blobStoreDEKGenerator = () -> {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Unexpected fatal internal error\",\n+                    new IllegalStateException(\"DEKs are required for encryption but this is a read-only repository\")\n+                );\n+            };\n+        } else {\n+            blobStoreDEKGenerator = this.dekGenerator;\n+        }\n+        return new EncryptedBlobStore(\n+            delegatedRepository.blobStore(),\n+            delegatedRepository.basePath(),\n+            metadata.name(),\n+            this::generateKEK,\n+            blobStoreDEKGenerator,\n+            dekCache\n+        );\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.delegatedRepository.start();\n+        super.doStart();\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        super.doStop();\n+        this.delegatedRepository.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        super.doClose();\n+        this.delegatedRepository.close();\n+    }\n+\n+    private Supplier<Tuple<BytesReference, SecretKey>> createDEKGenerator() throws GeneralSecurityException {\n+        // DEK and DEK Ids MUST be generated randomly (with independent random instances)\n+        final SecureRandom dekSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final SecureRandom dekIdSecureRandom = SecureRandom.getInstance(RAND_ALGO);\n+        final KeyGenerator dekGenerator = KeyGenerator.getInstance(DATA_ENCRYPTION_SCHEME.split(\"/\")[0]);\n+        dekGenerator.init(AESKeyUtils.KEY_LENGTH_IN_BYTES * Byte.SIZE, dekSecureRandom);\n+        return () -> {\n+            final BytesReference dekId = new BytesArray(UUIDs.randomBase64UUID(dekIdSecureRandom));\n+            final SecretKey dek = dekGenerator.generateKey();\n+            logger.debug(\"Repository [{}] generated new DEK [{}]\", metadata.name(), dekId);\n+            return new Tuple<>(dekId, dek);\n+        };\n+    }\n+\n+    // pkg-private for tests\n+    Tuple<String, SecretKey> generateKEK(String dekId) {\n+        try {\n+            // we rely on the DEK Id being generated randomly so it can be used as a salt\n+            final SecretKey kek = AESKeyUtils.generatePasswordBasedKey(repositoryPassword, dekId);\n+            final String kekId = AESKeyUtils.computeId(kek);\n+            logger.debug(\"Repository [{}] computed KEK [{}] for DEK [{}]\", metadata.name(), kekId, dekId);\n+            return new Tuple<>(kekId, kek);\n+        } catch (GeneralSecurityException e) {\n+            throw new RepositoryException(metadata.name(), \"Failure to generate KEK to wrap the DEK [\" + dekId + \"]\", e);\n+        }\n+    }\n+\n+    /**\n+     * Called before the shard snapshot and finalize operations, on the data and master nodes. This validates that the repository\n+     * password on the master node that started the snapshot operation is identical to the repository password on the local node.\n+     *\n+     * @param snapshotUserMetadata the snapshot metadata containing the repository password hash to assert\n+     * @throws RepositoryException if the repository password hash on the local node mismatches the master's\n+     */\n+    private void validateLocalRepositorySecret(Map<String, Object> snapshotUserMetadata) throws RepositoryException {\n+        assert snapshotUserMetadata != null;\n+        assert snapshotUserMetadata.get(PASSWORD_HASH_USER_METADATA_KEY) instanceof String;\n+        final String masterRepositoryPasswordId = (String) snapshotUserMetadata.get(PASSWORD_HASH_USER_METADATA_KEY);\n+        if (false == masterRepositoryPasswordId.equals(validatedLocalRepositoryPasswordHash)) {\n+            assert snapshotUserMetadata.get(PASSWORD_SALT_USER_METADATA_KEY) instanceof String;\n+            final String masterRepositoryPasswordIdSalt = (String) snapshotUserMetadata.get(PASSWORD_SALT_USER_METADATA_KEY);\n+            final String computedRepositoryPasswordId;\n+            try {\n+                computedRepositoryPasswordId = AESKeyUtils.computeId(\n+                    AESKeyUtils.generatePasswordBasedKey(repositoryPassword, masterRepositoryPasswordIdSalt)\n+                );\n+            } catch (Exception e) {\n+                throw new RepositoryException(metadata.name(), \"Unexpected fatal internal error\", e);\n+            }\n+            if (computedRepositoryPasswordId.equals(masterRepositoryPasswordId)) {\n+                this.validatedLocalRepositoryPasswordHash = computedRepositoryPasswordId;\n+            } else {\n+                throw new RepositoryException(\n+                    metadata.name(),\n+                    \"Repository password mismatch. The local node's repository password, from the keystore setting [\"\n+                        + EncryptedRepositoryPlugin.ENCRYPTION_PASSWORD_SETTING.getConcreteSettingForNamespace(\n+                            EncryptedRepositoryPlugin.PASSWORD_NAME_SETTING.get(metadata.settings())\n+                        ).getKey()\n+                        + \"], is different compared to the elected master node's which started the snapshot operation\"\n+                );\n+            }\n+        }\n+    }\n+\n+    // pkg-private for tests\n+    static final class EncryptedBlobStore implements BlobStore {\n+        private final BlobStore delegatedBlobStore;\n+        private final BlobPath delegatedBasePath;\n+        private final String repositoryName;\n+        private final Function<String, Tuple<String, SecretKey>> getKEKforDEK;\n+        private final Cache<String, SecretKey> dekCache;\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+\n+        EncryptedBlobStore(\n+            BlobStore delegatedBlobStore,\n+            BlobPath delegatedBasePath,\n+            String repositoryName,\n+            Function<String, Tuple<String, SecretKey>> getKEKforDEK,\n+            Supplier<Tuple<BytesReference, SecretKey>> dekGenerator,\n+            Cache<String, SecretKey> dekCache\n+        ) {\n+            this.delegatedBlobStore = delegatedBlobStore;\n+            this.delegatedBasePath = delegatedBasePath;\n+            this.repositoryName = repositoryName;\n+            this.getKEKforDEK = getKEKforDEK;\n+            this.dekCache = dekCache;\n+            this.singleUseDEKSupplier = SingleUseKey.createSingleUseKeySupplier(() -> {\n+                Tuple<BytesReference, SecretKey> newDEK = dekGenerator.get();\n+                // store the newly generated DEK before making it available\n+                storeDEK(newDEK.v1().utf8ToString(), newDEK.v2());\n+                return newDEK;\n+            });\n+        }\n+\n+        // pkg-private for tests\n+        SecretKey getDEKById(String dekId) throws IOException {\n+            try {\n+                return dekCache.computeIfAbsent(dekId, ignored -> loadDEK(dekId));\n+            } catch (ExecutionException e) {\n+                // some exception types are to be expected\n+                if (e.getCause() instanceof IOException) {\n+                    throw (IOException) e.getCause();\n+                } else if (e.getCause() instanceof ElasticsearchException) {\n+                    throw (ElasticsearchException) e.getCause();\n+                } else {\n+                    throw new RepositoryException(repositoryName, \"Unexpected exception retrieving DEK [\" + dekId + \"]\", e);\n+                }\n+            }\n+        }\n+\n+        private SecretKey loadDEK(String dekId) throws IOException {\n+            final BlobPath dekBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(dekId);\n+            logger.debug(\"Repository [{}] loading wrapped DEK [{}] from blob path {}\", repositoryName, dekId, dekBlobPath);\n+            final BlobContainer dekBlobContainer = delegatedBlobStore.blobContainer(dekBlobPath);\n+            final Tuple<String, SecretKey> kekTuple = getKEKforDEK.apply(dekId);\n+            final String kekId = kekTuple.v1();\n+            final SecretKey kek = kekTuple.v2();\n+            logger.trace(\"Repository [{}] using KEK [{}] to unwrap DEK [{}]\", repositoryName, kekId, dekId);\n+            final byte[] encryptedDEKBytes = new byte[AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES];\n+            try (InputStream encryptedDEKInputStream = dekBlobContainer.readBlob(kekId)) {\n+                final int bytesRead = Streams.readFully(encryptedDEKInputStream, encryptedDEKBytes);\n+                if (bytesRead != AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES) {\n+                    throw new RepositoryException(\n+                        repositoryName,\n+                        \"Wrapped DEK [\" + dekId + \"] has smaller length [\" + bytesRead + \"] than expected\"\n+                    );\n+                }\n+                if (encryptedDEKInputStream.read() != -1) {\n+                    throw new RepositoryException(repositoryName, \"Wrapped DEK [\" + dekId + \"] is larger than expected\");\n+                }\n+            } catch (NoSuchFileException e) {\n+                // do NOT throw IOException when the DEK does not exist, as this is a decryption problem, and IOExceptions\n+                // can move the repository in the corrupted state\n+                throw new ElasticsearchException(\n+                    \"Failure to read and decrypt DEK [\"\n+                        + dekId\n+                        + \"] from \"\n+                        + dekBlobContainer.path()\n+                        + \". Most likely the repository password is incorrect, where previous \"\n+                        + \"snapshots have used a different password.\",\n+                    e\n+                );\n+            }\n+            logger.trace(\"Repository [{}] successfully read DEK [{}] from path {} {}\", repositoryName, dekId, dekBlobPath, kekId);\n+            try {\n+                final SecretKey dek = AESKeyUtils.unwrap(kek, encryptedDEKBytes);\n+                logger.debug(\"Repository [{}] successfully loaded DEK [{}] from path {} {}\", repositoryName, dekId, dekBlobPath, kekId);\n+                return dek;\n+            } catch (GeneralSecurityException e) {\n+                throw new RepositoryException(\n+                    repositoryName,\n+                    \"Failure to AES unwrap the DEK [\"\n+                        + dekId\n+                        + \"]. \"\n+                        + \"Most likely the encryption metadata in the repository has been corrupted\",\n+                    e\n+                );\n+            }\n+        }\n+\n+        // pkg-private for tests\n+        void storeDEK(String dekId, SecretKey dek) throws IOException {\n+            final BlobPath dekBlobPath = delegatedBasePath.add(DEK_ROOT_CONTAINER).add(dekId);\n+            logger.debug(\"Repository [{}] storing wrapped DEK [{}] under blob path {}\", repositoryName, dekId, dekBlobPath);\n+            final BlobContainer dekBlobContainer = delegatedBlobStore.blobContainer(dekBlobPath);\n+            final Tuple<String, SecretKey> kek = getKEKforDEK.apply(dekId);\n+            logger.trace(\"Repository [{}] using KEK [{}] to wrap DEK [{}]\", repositoryName, kek.v1(), dekId);\n+            final byte[] encryptedDEKBytes;\n+            try {\n+                encryptedDEKBytes = AESKeyUtils.wrap(kek.v2(), dek);\n+                if (encryptedDEKBytes.length != AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES) {\n+                    throw new RepositoryException(\n+                        repositoryName,\n+                        \"Wrapped DEK [\" + dekId + \"] has unexpected length [\" + encryptedDEKBytes.length + \"]\"\n+                    );\n+                }\n+            } catch (GeneralSecurityException e) {\n+                // throw unchecked ElasticsearchException; IOExceptions are interpreted differently and can move the repository in the\n+                // corrupted state\n+                throw new RepositoryException(repositoryName, \"Failure to AES wrap the DEK [\" + dekId + \"]\", e);\n+            }\n+            logger.trace(\"Repository [{}] successfully wrapped DEK [{}]\", repositoryName, dekId);\n+            try (InputStream encryptedDEKInputStream = new ByteArrayInputStream(encryptedDEKBytes)) {\n+                dekBlobContainer.writeBlobAtomic(kek.v1(), encryptedDEKInputStream, encryptedDEKBytes.length, true);\n+            }\n+            logger.debug(\"Repository [{}] successfully stored DEK [{}] under path {} {}\", repositoryName, dekId, dekBlobPath, kek.v1());\n+        }\n+\n+        @Override\n+        public BlobContainer blobContainer(BlobPath path) {\n+            final Iterator<String> pathIterator = path.iterator();\n+            BlobPath delegatedBlobContainerPath = delegatedBasePath;\n+            while (pathIterator.hasNext()) {\n+                delegatedBlobContainerPath = delegatedBlobContainerPath.add(pathIterator.next());\n+            }\n+            final BlobContainer delegatedBlobContainer = delegatedBlobStore.blobContainer(delegatedBlobContainerPath);\n+            return new EncryptedBlobContainer(path, repositoryName, delegatedBlobContainer, singleUseDEKSupplier, this::getDEKById);\n+        }\n+\n+        @Override\n+        public void close() {\n+            // do NOT close delegatedBlobStore; it will be closed when the inner delegatedRepository is closed\n+        }\n+    }\n+\n+    private static final class EncryptedBlobContainer extends AbstractBlobContainer {\n+        private final String repositoryName;\n+        private final BlobContainer delegatedBlobContainer;\n+        // supplier for the DEK used for encryption (snapshot)\n+        private final CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier;\n+        // retrieves the DEK required for decryption (restore)\n+        private final CheckedFunction<String, SecretKey, IOException> getDEKById;\n+\n+        EncryptedBlobContainer(\n+            BlobPath path, // this path contains the {@code EncryptedRepository#basePath} which, importantly, is empty\n+            String repositoryName,\n+            BlobContainer delegatedBlobContainer,\n+            CheckedSupplier<SingleUseKey, IOException> singleUseDEKSupplier,\n+            CheckedFunction<String, SecretKey, IOException> getDEKById\n+        ) {\n+            super(path);\n+            this.repositoryName = repositoryName;\n+            final String rootPathElement = path.iterator().hasNext() ? path.iterator().next() : null;\n+            if (DEK_ROOT_CONTAINER.equals(rootPathElement)) {\n+                throw new RepositoryException(repositoryName, \"Cannot descend into the DEK blob container \" + path);\n+            }\n+            this.delegatedBlobContainer = delegatedBlobContainer;\n+            this.singleUseDEKSupplier = singleUseDEKSupplier;\n+            this.getDEKById = getDEKById;\n+        }\n+\n+        /**\n+         * Returns a new {@link InputStream} for the given {@code blobName} that can be used to read the contents of the blob.\n+         * The returned {@code InputStream} transparently handles the decryption of the blob contents, by first working out\n+         * the blob name of the associated DEK id, reading and decrypting the DEK (given the repository password, unless the DEK is\n+         * already cached because it had been used for other blobs before), and lastly reading and decrypting the data blob,\n+         * in a streaming fashion, by employing the {@link DecryptionPacketsInputStream}.\n+         * The {@code DecryptionPacketsInputStream} does not return un-authenticated data.\n+         *\n+         * @param   blobName The name of the blob to get an {@link InputStream} for.\n+         */\n+        @Override\n+        public InputStream readBlob(String blobName) throws IOException {\n+            // This MIGHT require two concurrent readBlob connections if the DEK is not already in the cache and if the encrypted blob\n+            // is large enough so that the underlying network library keeps the connection open after reading the prepended DEK ID.\n+            // Arguably this is a problem only under lab conditions, when the storage service is saturated only by the first read\n+            // connection of the pair, so that the second read connection (for the DEK) can not be fulfilled.\n+            // In this case the second connection will time-out which will trigger the closing of the first one, therefore\n+            // allowing other pair connections to complete.\n+            // In this situation the restore process should slowly make headway, albeit under read-timeout exceptions\n+            final InputStream encryptedDataInputStream = delegatedBlobContainer.readBlob(blobName);\n+            try {\n+                // read the DEK Id (fixed length) which is prepended to the encrypted blob\n+                final byte[] dekIdBytes = new byte[DEK_ID_LENGTH];\n+                final int bytesRead = Streams.readFully(encryptedDataInputStream, dekIdBytes);", "originalCommit": "dd3e8fba4c9e633ce015b4a87e2c011e7471aac0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEyOTMxMw==", "url": "https://github.com/elastic/elasticsearch/pull/53352#discussion_r448129313", "bodyText": "Should we not also verify that the id is constant? That is\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertThat(AESKeyUtils.computeId(key1), not(equalTo(AESKeyUtils.computeId(key2))));\n          \n          \n            \n                    assertThat(AESKeyUtils.computeId(key1), not(equalTo(AESKeyUtils.computeId(key2))));\n          \n          \n            \n                    assertThat(AESKeyUtils.computeId(key1), equalTo(AESKeyUtils.computeId(key1)));\n          \n          \n            \n                    assertThat(AESKeyUtils.computeId(key2), equalTo(AESKeyUtils.computeId(key2)));", "author": "tvernum", "createdAt": "2020-07-01T05:49:51Z", "path": "x-pack/plugin/repository-encrypted/src/test/java/org/elasticsearch/repositories/encrypted/AESKeyUtilsTests.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.repositories.encrypted;\n+\n+import org.elasticsearch.test.ESTestCase;\n+\n+import javax.crypto.SecretKey;\n+import javax.crypto.spec.SecretKeySpec;\n+\n+import java.security.InvalidKeyException;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.not;\n+\n+public class AESKeyUtilsTests extends ESTestCase {\n+\n+    public void testWrapUnwrap() throws Exception {\n+        byte[] keyToWrapBytes = randomByteArrayOfLength(AESKeyUtils.KEY_LENGTH_IN_BYTES);\n+        SecretKey keyToWrap = new SecretKeySpec(keyToWrapBytes, \"AES\");\n+        byte[] wrappingKeyBytes = randomByteArrayOfLength(AESKeyUtils.KEY_LENGTH_IN_BYTES);\n+        SecretKey wrappingKey = new SecretKeySpec(wrappingKeyBytes, \"AES\");\n+        byte[] wrappedKey = AESKeyUtils.wrap(wrappingKey, keyToWrap);\n+        assertThat(wrappedKey.length, equalTo(AESKeyUtils.WRAPPED_KEY_LENGTH_IN_BYTES));\n+        SecretKey unwrappedKey = AESKeyUtils.unwrap(wrappingKey, wrappedKey);\n+        assertThat(unwrappedKey, equalTo(keyToWrap));\n+    }\n+\n+    public void testComputeId() throws Exception {\n+        byte[] key1Bytes = randomByteArrayOfLength(AESKeyUtils.KEY_LENGTH_IN_BYTES);\n+        SecretKey key1 = new SecretKeySpec(key1Bytes, \"AES\");\n+        byte[] key2Bytes = randomByteArrayOfLength(AESKeyUtils.KEY_LENGTH_IN_BYTES);\n+        SecretKey key2 = new SecretKeySpec(key2Bytes, \"AES\");\n+        assertThat(AESKeyUtils.computeId(key1), not(equalTo(AESKeyUtils.computeId(key2))));", "originalCommit": "dd3e8fba4c9e633ce015b4a87e2c011e7471aac0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dd3e8fba4c9e633ce015b4a87e2c011e7471aac0", "url": "https://github.com/elastic/elasticsearch/commit/dd3e8fba4c9e633ce015b4a87e2c011e7471aac0", "message": "Merge branch 'repository-encrypted-client-side' into reuse-DEKs-universally", "committedDate": "2020-06-23T14:05:46Z", "type": "forcePushed"}, {"oid": "6a3dd25649418ec69f4ebb9962689cc485082207", "url": "https://github.com/elastic/elasticsearch/commit/6a3dd25649418ec69f4ebb9962689cc485082207", "message": "Build gradle for cloud repos exports test jar", "committedDate": "2020-11-28T14:37:49Z", "type": "commit"}, {"oid": "9037d00ff061bb980eca2561db06bbd7703dfec8", "url": "https://github.com/elastic/elasticsearch/commit/9037d00ff061bb980eca2561db06bbd7703dfec8", "message": "Repository integ tests", "committedDate": "2020-11-29T15:08:39Z", "type": "commit"}, {"oid": "cae71f2c1dd423f3bad033478953d7fc9874ba43", "url": "https://github.com/elastic/elasticsearch/commit/cae71f2c1dd423f3bad033478953d7fc9874ba43", "message": "Barely sufficient license check", "committedDate": "2020-11-29T15:39:01Z", "type": "commit"}, {"oid": "00308a952dcab13115c1a57d9dcb26e79009e014", "url": "https://github.com/elastic/elasticsearch/commit/00308a952dcab13115c1a57d9dcb26e79009e014", "message": "Repository encrypted apply patch", "committedDate": "2020-11-29T15:42:07Z", "type": "commit"}, {"oid": "467b36ba9aba9e7ce2b317e9193a3932942d5012", "url": "https://github.com/elastic/elasticsearch/commit/467b36ba9aba9e7ce2b317e9193a3932942d5012", "message": "build.gradle modernize and \"enable integ tests\"", "committedDate": "2020-11-29T15:55:07Z", "type": "commit"}, {"oid": "d7c3298c9ca35bfd039fd9cf333810c79e8d80d8", "url": "https://github.com/elastic/elasticsearch/commit/d7c3298c9ca35bfd039fd9cf333810c79e8d80d8", "message": "Compilation problems", "committedDate": "2020-11-29T18:59:29Z", "type": "commit"}, {"oid": "3ad727efece0ae42224ff40d82bd24c103d8a46d", "url": "https://github.com/elastic/elasticsearch/commit/3ad727efece0ae42224ff40d82bd24c103d8a46d", "message": "Spotless", "committedDate": "2020-11-29T19:01:40Z", "type": "commit"}, {"oid": "37fbd4c8acfcdc92c36001283007362618103e14", "url": "https://github.com/elastic/elasticsearch/commit/37fbd4c8acfcdc92c36001283007362618103e14", "message": "Repository azure and repository gcs work", "committedDate": "2020-11-30T07:41:19Z", "type": "commit"}, {"oid": "af03a92e40ed387122ddc143f095503c2a02b37a", "url": "https://github.com/elastic/elasticsearch/commit/af03a92e40ed387122ddc143f095503c2a02b37a", "message": "Before moving the internal cluster tests to the dedicated source set", "committedDate": "2020-11-30T19:53:09Z", "type": "commit"}, {"oid": "0023f770b88f4084b2a29cad4cb1061d366c9765", "url": "https://github.com/elastic/elasticsearch/commit/0023f770b88f4084b2a29cad4cb1061d366c9765", "message": "Hooray!", "committedDate": "2020-11-30T22:33:32Z", "type": "commit"}, {"oid": "40dab2dbd28d6b81493c9b8fd8f8f02c2bdcc483", "url": "https://github.com/elastic/elasticsearch/commit/40dab2dbd28d6b81493c9b8fd8f8f02c2bdcc483", "message": "More than one connection open at one time for ESMockAPIBasedRepo", "committedDate": "2020-11-30T22:53:52Z", "type": "commit"}, {"oid": "40dab2dbd28d6b81493c9b8fd8f8f02c2bdcc483", "url": "https://github.com/elastic/elasticsearch/commit/40dab2dbd28d6b81493c9b8fd8f8f02c2bdcc483", "message": "More than one connection open at one time for ESMockAPIBasedRepo", "committedDate": "2020-11-30T22:53:52Z", "type": "forcePushed"}, {"oid": "6db2ca20173ac73a16d4d639b34c0b6844e0655f", "url": "https://github.com/elastic/elasticsearch/commit/6db2ca20173ac73a16d4d639b34c0b6844e0655f", "message": "Merge branch 'repository-encrypted-client-side-reformated' into reuse-DEKs-universally-reformated", "committedDate": "2020-12-01T09:30:18Z", "type": "commit"}, {"oid": "e27502732ed18e42480cabf2e58aa44e3faff76d", "url": "https://github.com/elastic/elasticsearch/commit/e27502732ed18e42480cabf2e58aa44e3faff76d", "message": "Hdfs compilation check", "committedDate": "2020-12-01T09:45:55Z", "type": "commit"}, {"oid": "e70b55db68e274a25cbd87a82a9e1bf383d3e71e", "url": "https://github.com/elastic/elasticsearch/commit/e70b55db68e274a25cbd87a82a9e1bf383d3e71e", "message": "Checkstyle", "committedDate": "2020-12-01T10:19:29Z", "type": "commit"}, {"oid": "6e346a1a11596b7accbdf7afbe8a79c7377078b3", "url": "https://github.com/elastic/elasticsearch/commit/6e346a1a11596b7accbdf7afbe8a79c7377078b3", "message": "Nit", "committedDate": "2020-12-01T15:42:19Z", "type": "commit"}, {"oid": "50f97b1bf19852719740162b6954c8dd05300845", "url": "https://github.com/elastic/elasticsearch/commit/50f97b1bf19852719740162b6954c8dd05300845", "message": "Update x-pack/plugin/repository-encrypted/src/test/java/org/elasticsearch/repositories/encrypted/AESKeyUtilsTests.java\n\nCo-authored-by: Tim Vernum <tim@adjective.org>", "committedDate": "2020-12-01T15:53:29Z", "type": "commit"}, {"oid": "97ad4d5d2877db80b7f7d78ec1eb3a7f3a7041e0", "url": "https://github.com/elastic/elasticsearch/commit/97ad4d5d2877db80b7f7d78ec1eb3a7f3a7041e0", "message": "Hdfs integ tests do not preserve repo contents after delete", "committedDate": "2020-12-02T11:34:13Z", "type": "commit"}, {"oid": "c1585327cd885d2100e60a85f2cc9a810adf1476", "url": "https://github.com/elastic/elasticsearch/commit/c1585327cd885d2100e60a85f2cc9a810adf1476", "message": "Merge branch 'repository-encrypted-client-side-reformated' into reuse-DEKs-universally-reformated", "committedDate": "2020-12-02T13:14:01Z", "type": "commit"}]}