{"pr_number": 66769, "pr_title": "Fix azure repo stream exhaust check for multipart uploads", "pr_createdAt": "2020-12-23T07:28:33Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/66769", "timeline": [{"oid": "b5846a7d83670467d707957abec6b90a08dd1f92", "url": "https://github.com/elastic/elasticsearch/commit/b5846a7d83670467d707957abec6b90a08dd1f92", "message": "Fix", "committedDate": "2020-12-23T07:12:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE4NTg4OA==", "url": "https://github.com/elastic/elasticsearch/pull/66769#discussion_r550185888", "bodyText": "I wonder if we should just drop this check altogether instead? It seems kind of weird to check that we drained the stream fully when we also give the size we want to write (and the check relies on the fact that the stream reports available() as well which is not guaranteed).\nMaybe we should just drop this check and only check that currentTotalLength == blobSize at the end? I think I'd prefer that option and it simplifies things?", "author": "original-brownbear", "createdAt": "2020-12-30T12:56:11Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -438,8 +449,21 @@ private void executeMultipartUpload(String blobName, InputStream inputStream, lo\n             final List<String> blockIds = new ArrayList<>(nbParts);\n             for (int i = 0; i < nbParts; i++) {\n                 final long length = i < nbParts - 1 ? partSize : lastPartSize;\n-                final Flux<ByteBuffer> byteBufferFlux =\n-                    convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+                Flux<ByteBuffer> byteBufferFlux = convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+                if (i == nbParts - 1) {\n+                    byteBufferFlux.doOnComplete(() -> {\n+                        try {\n+                            if (inputStream.available() > 0) {", "originalCommit": "b5846a7d83670467d707957abec6b90a08dd1f92", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQxOTI2NA==", "url": "https://github.com/elastic/elasticsearch/pull/66769#discussion_r550419264", "bodyText": "I'll take your suggestion to remove the check, although I'm torn between the complexity it adds in the code vs the aid in troubleshooting, but maybe the existing check about \"currentTotalLength > blobSize length\" is sufficient.", "author": "albertzaharovits", "createdAt": "2020-12-31T07:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE4NTg4OA=="}], "type": "inlineReview"}, {"oid": "cd6048967e703cbd97de19123126fbfee297d333", "url": "https://github.com/elastic/elasticsearch/commit/cd6048967e703cbd97de19123126fbfee297d333", "message": "Merge branch 'master' into fix-azure-repo-flux-ensure-exhausted-stream", "committedDate": "2020-12-31T07:13:24Z", "type": "commit"}, {"oid": "491d348e5d3b8c15c782e2b42bfea6daff0feba9", "url": "https://github.com/elastic/elasticsearch/commit/491d348e5d3b8c15c782e2b42bfea6daff0feba9", "message": "Remove doOnComplete that stream is exhausted", "committedDate": "2020-12-31T07:36:57Z", "type": "commit"}, {"oid": "491d348e5d3b8c15c782e2b42bfea6daff0feba9", "url": "https://github.com/elastic/elasticsearch/commit/491d348e5d3b8c15c782e2b42bfea6daff0feba9", "message": "Remove doOnComplete that stream is exhausted", "committedDate": "2020-12-31T07:36:57Z", "type": "forcePushed"}, {"oid": "17a735e1be60e87fa1eb0000c32ec1471ee74296", "url": "https://github.com/elastic/elasticsearch/commit/17a735e1be60e87fa1eb0000c32ec1471ee74296", "message": "Merge branch 'master' into fix-azure-repo-flux-ensure-exhausted-stream", "committedDate": "2021-01-04T14:04:52Z", "type": "commit"}]}